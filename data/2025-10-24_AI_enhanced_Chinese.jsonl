{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "\u63d0\u51faKGACG\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5c06\u8f6f\u4ef6\u9700\u6c42\u548c\u67b6\u6784\u8bbe\u8ba1\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u89e3\u51b3\u5e94\u7528\u7ea7\u8f6f\u4ef6\u4ee3\u7801\u751f\u6210\u7684\u6311\u6218", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u578b\u5e94\u7528\u7ea7\u8f6f\u4ef6\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u4fdd\u8bc1\u9879\u76ee\u4ee3\u7801\u7684\u5408\u7406\u7ec4\u7ec7\u7ed3\u6784\uff0c\u4e14\u96be\u4ee5\u7ef4\u62a4\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b", "method": "\u4f7f\u7528\u77e5\u8bc6\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4ee3\u7801\u7ec4\u7ec7\u4e0e\u89c4\u5212\u667a\u80fd\u4f53\u3001\u7f16\u7801\u667a\u80fd\u4f53\u548c\u6d4b\u8bd5\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u53cd\u9988\u673a\u5236\u5f62\u6210\u534f\u4f5c\u95ed\u73af", "result": "\u901a\u8fc7Java\u5766\u514b\u5927\u6218\u6e38\u620f\u6848\u4f8b\u5c55\u793a\u4e86KGACG\u4e2d\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u8fc7\u7a0b\uff0c\u540c\u65f6\u9762\u4e34\u4e00\u4e9b\u6311\u6218", "conclusion": "KGACG\u81f4\u529b\u4e8e\u63a8\u8fdb\u5e94\u7528\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b", "topic": "code agent"}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8ba9SWE\u4ee3\u7406\u5728\u6dfb\u52a0\u529f\u80fd\u65f6\u65e0\u610f\u4e2d\u7834\u574f\u6d4b\u8bd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316bug\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u63a5\u8fd1\u771f\u5b9e\u5f00\u53d1\u8fc7\u7a0b\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u8bad\u7ec3\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6545\u610f\u5f15\u5165\u4ee3\u7801\u6270\u52a8\u751f\u6210bug\uff0c\u4f1a\u4ea7\u751f\u5206\u5e03\u5916\u6548\u5e94\uff0c\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002\u9700\u8981\u66f4\u81ea\u7136\u3001\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7f16\u8f91\u6a21\u5f0f\u7684bug\u751f\u6210\u65b9\u6cd5\u6765\u8bad\u7ec3\u66f4\u597d\u7684SWE\u4ee3\u7406\u3002", "method": "\u8ba9SWE\u4ee3\u7406\u5728\u4ee3\u7801\u5e93\u4e2d\u5f15\u5165\u65b0\u529f\u80fd\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u65e0\u610f\u4e2d\u7834\u574f\u6d4b\u8bd5\uff0c\u4ece\u800c\u4ea7\u751fbug\u3002\u8fd9\u79cd\u65b9\u6cd5\u6a21\u62df\u4e86\u771f\u5b9e\u5f00\u53d1\u4e2d\u5e38\u89c1\u7684bug\u5f15\u5165\u6a21\u5f0f\u3002", "result": "\u751f\u6210\u7684bug\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4ec5\u75281.2k\u4e2abug\u5c31\u80fd\u8fbe\u5230\u5176\u4ed6\u6570\u636e\u96c63k\u4e2abug\u7684\u6548\u679c\uff0c\u6027\u80fd\u63d0\u53472%\u3002\u8bad\u7ec3\u7684FrogBoss\u548cFrogMini\u6a21\u578b\u5728SWE-bench Verified\u4e0a\u5206\u522b\u8fbe\u523054.6%\u548c45.3%\u7684pass@1\u5206\u6570\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u5f00\u53d1\u8fc7\u7a0b\u751f\u6210bug\u7684\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347SWE\u4ee3\u7406\u7684\u6027\u80fd\u3002", "topic": "swe benchmark"}}
{"id": "2510.19872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19872", "abs": "https://arxiv.org/abs/2510.19872", "authors": ["Iman Rahmani", "Saman Yazdannik", "Morteza Tayefi", "Jafar Roshanian"], "title": "An Integrated Approach to Neural Architecture Search for Deep Q-Networks", "comment": null, "summary": "The performance of deep reinforcement learning agents is fundamentally\nconstrained by their neural network architecture, a choice traditionally made\nthrough expensive hyperparameter searches and then fixed throughout training.\nThis work investigates whether online, adaptive architecture optimization can\nescape this constraint and outperform static designs. We introduce NAS-DQN, an\nagent that integrates a learned neural architecture search controller directly\ninto the DRL training loop, enabling dynamic network reconfiguration based on\ncumulative performance feedback. We evaluate NAS-DQN against three\nfixed-architecture baselines and a random search control on a continuous\ncontrol task, conducting experiments over multiple random seeds. Our results\ndemonstrate that NAS-DQN achieves superior final performance, sample\nefficiency, and policy stability while incurring negligible computational\noverhead. Critically, the learned search strategy substantially outperforms\nboth undirected random architecture exploration and poorly-chosen fixed\ndesigns, indicating that intelligent, performance-guided search is the key\nmechanism driving success. These findings establish that architecture\nadaptation is not merely beneficial but necessary for optimal sample efficiency\nin online deep reinforcement learning, and suggest that the design of RL agents\nneed not be a static offline choice but can instead be seamlessly integrated as\na dynamic component of the learning process itself.", "AI": {"tldr": "NAS-DQN\u901a\u8fc7\u5728\u7ebf\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u5b9e\u73b0\u52a8\u6001\u7f51\u7edc\u91cd\u6784\uff0c\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u8d8a\u9759\u6001\u67b6\u6784\u8bbe\u8ba1\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3001\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9700\u8981\u901a\u8fc7\u6602\u8d35\u7684\u8d85\u53c2\u6570\u641c\u7d22\u6765\u786e\u5b9a\uff0c\u8fd9\u9650\u5236\u4e86\u4ee3\u7406\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faNAS-DQN\u4ee3\u7406\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u63a7\u5236\u5668\u76f4\u63a5\u96c6\u6210\u5230DRL\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u57fa\u4e8e\u7d2f\u79ef\u6027\u80fd\u53cd\u9988\u5b9e\u73b0\u52a8\u6001\u7f51\u7edc\u91cd\u914d\u7f6e\u3002", "result": "NAS-DQN\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e09\u4e2a\u56fa\u5b9a\u67b6\u6784\u57fa\u7ebf\u548c\u968f\u673a\u641c\u7d22\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6700\u7ec8\u6027\u80fd\u3001\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u7a33\u5b9a\u6027\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "\u67b6\u6784\u81ea\u9002\u5e94\u4e0d\u4ec5\u6709\u76ca\u800c\u4e14\u662f\u5b9e\u73b0\u5728\u7ebf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6700\u4f18\u6837\u672c\u6548\u7387\u7684\u5fc5\u8981\u6761\u4ef6\uff0cRL\u4ee3\u7406\u7684\u8bbe\u8ba1\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f5c\u4e3a\u52a8\u6001\u7ec4\u4ef6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19838", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19838", "abs": "https://arxiv.org/abs/2510.19838", "authors": ["Shiqi He", "Yue Cui", "Xinyu Ma", "Yaliang Li", "Bolin Ding", "Mosharaf Chowdhury"], "title": "Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory", "comment": null, "summary": "Autonomous web agents powered by large language models (LLMs) show strong\npotential for performing goal-oriented tasks such as information retrieval,\nreport generation, and online transactions. These agents mark a key step toward\npractical embodied reasoning in open web environments. However, existing\napproaches remain limited in reasoning depth and efficiency: vanilla linear\nmethods fail at multi-step reasoning and lack effective backtracking, while\nother search strategies are coarse-grained and computationally costly. We\nintroduce Branch-and-Browse, a fine-grained web agent framework that unifies\nstructured reasoning-acting, contextual memory, and efficient execution. It (i)\nemploys explicit subtask management with tree-structured exploration for\ncontrollable multi-branch reasoning, (ii) bootstraps exploration through\nefficient web state replay with background reasoning, and (iii) leverages a\npage action memory to share explored actions within and across sessions. On the\nWebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\\%\nand reduces execution time by up to 40.4\\% relative to state-of-the-art\nmethods. These results demonstrate that Branch-and-Browse is a reliable and\nefficient framework for LLM-based web agents.", "AI": {"tldr": "Branch-and-Browse\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u7f51\u9875\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406-\u884c\u52a8\u3001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u9ad8\u6548\u6267\u884c\uff0c\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8635.8%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe40.4%\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406\u65b9\u6cd5\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u6548\u7387\u4e0a\u5b58\u5728\u5c40\u9650\uff1a\u7ebf\u6027\u65b9\u6cd5\u65e0\u6cd5\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u4e14\u7f3a\u4e4f\u6709\u6548\u56de\u6eaf\uff0c\u5176\u4ed6\u641c\u7d22\u7b56\u7565\u5219\u7c92\u5ea6\u7c97\u7cd9\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u663e\u5f0f\u5b50\u4efb\u52a1\u7ba1\u7406\u548c\u6811\u72b6\u7ed3\u6784\u63a2\u7d22\u5b9e\u73b0\u53ef\u63a7\u591a\u5206\u652f\u63a8\u7406\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u7f51\u9875\u72b6\u6001\u91cd\u653e\u548c\u540e\u53f0\u63a8\u7406\u5f15\u5bfc\u63a2\u7d22\uff0c\u5229\u7528\u9875\u9762\u52a8\u4f5c\u8bb0\u5fc6\u5728\u4f1a\u8bdd\u5185\u5916\u5171\u4eab\u63a2\u7d22\u8fc7\u7684\u52a8\u4f5c\u3002", "result": "\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523035.8%\uff0c\u6267\u884c\u65f6\u95f4\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u51cf\u5c11\u9ad8\u8fbe40.4%\u3002", "conclusion": "Branch-and-Browse\u662f\u4e00\u4e2a\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.19873", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19873", "abs": "https://arxiv.org/abs/2510.19873", "authors": ["Junfeng Gong", "Zhiyi Wei", "Junying Chen", "Cheng Liu", "Huawei Li"], "title": "From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph", "comment": null, "summary": "Despite significant evolution of CUDA programming and domain-specific\nlibraries, effectively utilizing GPUs with massively parallel engines remains\ndifficult. Large language models (LLMs) show strong potential in generating\noptimized CUDA code from sequential code. However, using LLMs in practice faces\ntwo major challenges: cloud-based APIs pose risks of code leakage, and local\ndeployment is often computationally expensive and inefficient. These drawbacks\nhave spurred interest in small language models (SLMs), which are more\nlightweight and privacy-friendly. Encouragingly, recent studies show that SLMs\ncan achieve performance comparable to LLMs on specific tasks. While SLMs can\nmatch LLMs on domain-specific tasks, their limited reasoning abilities lead to\nsuboptimal performance in complex CUDA generation according to our experiments.\nTo bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented\ngeneration framework that transfers LLM-level reasoning to smaller models.\nReGraphT organizes CUDA optimization trajectories into a structured reasoning\ngraph, modeling the combined CUDA optimizations as state transitions, and\nleverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also\npresent a CUDA-specific benchmark with difficulty tiers defined by reasoning\ncomplexity to evaluate models more comprehensively. Experiments show that\nReGraphT outperforms HPC-specific fine-tuned models and other\nretrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval\nand ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and\nQwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level\nperformance without the associated privacy risks or excessive computing\noverhead.", "AI": {"tldr": "ReGraphT\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06CUDA\u4f18\u5316\u8f68\u8ff9\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u63a8\u7406\u56fe\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u56fe\u641c\u7d22\u8fdb\u884c\u9ad8\u6548\u63a2\u7d22\uff0c\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u63a5\u8fd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f18\u5316CUDA\u4ee3\u7801\u5b58\u5728\u4ee3\u7801\u6cc4\u9732\u98ce\u9669\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u8f7b\u91cf\u4e14\u9690\u79c1\u53cb\u597d\uff0c\u4f46\u5728\u590d\u6742CUDA\u751f\u6210\u4efb\u52a1\u4e2d\u63a8\u7406\u80fd\u529b\u6709\u9650\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faReGraphT\u6846\u67b6\uff0c\u5c06CUDA\u4f18\u5316\u8f68\u8ff9\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u6362\u7684\u63a8\u7406\u56fe\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u56fe\u641c\u7d22\u8fdb\u884c\u63a2\u7d22\uff0c\u5e76\u521b\u5efa\u4e86\u6309\u63a8\u7406\u590d\u6742\u5ea6\u5206\u5c42\u7684CUDA\u4e13\u7528\u57fa\u51c6\u3002", "result": "ReGraphT\u5728CUDAEval\u548cParEval\u4e0a\u5e73\u5747\u5b9e\u73b02.33\u500d\u52a0\u901f\uff0c\u4f18\u4e8eHPC\u4e13\u7528\u5fae\u8c03\u6a21\u578b\u548c\u5176\u4ed6\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u63a5\u8fd1\u5927\u578b\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "ReGraphT\u6210\u529f\u5730\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u5c0f\u578b\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u98ce\u9669\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5728CUDA\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "2510.19842", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19842", "abs": "https://arxiv.org/abs/2510.19842", "authors": ["Yuanhe Zhang", "Ilja Kuzborskij", "Jason D. Lee", "Chenlei Leng", "Fanghui Liu"], "title": "DAG-Math: Graph-Guided Mathematical Reasoning in LLMs", "comment": "28 pages, 6 figures. Comments are welcome", "summary": "Large Language Models (LLMs) demonstrate strong performance on mathematical\nproblems when prompted with Chain-of-Thought (CoT), yet it remains unclear\nwhether this success stems from search, rote procedures, or rule-consistent\nreasoning. To address this, we propose modeling CoT as a certain rule-based\nstochastic process over directed acyclic graphs (DAGs), where nodes represent\nintermediate derivation states and edges encode rule applications. Within this\nframework, we introduce logical closeness, a metric that quantifies how well a\nmodel's CoT trajectory (i.e., the LLM's final output) adheres to the DAG\nstructure, providing evaluation beyond classical PASS@k metrics. Building on\nthis, we introduce the DAG-MATH CoT format and construct a benchmark that\nguides LLMs to generate CoT trajectories in this format, thereby enabling the\nevaluation of their reasoning ability under our framework. Across standard\nmathematical reasoning datasets, our analysis uncovers statistically\nsignificant differences in reasoning fidelity among representative LLM\nfamilies-even when PASS@k is comparable-highlighting gaps between final-answer\naccuracy and rule-consistent derivation. Our framework provides a balance\nbetween free-form CoT and formal proofs systems, offering actionable\ndiagnostics for LLMs reasoning evaluation. Our benchmark and code are available\nat: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.", "AI": {"tldr": "\u63d0\u51faDAG-MATH\u6846\u67b6\uff0c\u5c06\u601d\u7ef4\u94fe\u5efa\u6a21\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fc7\u7a0b\uff0c\u5f15\u5165\u903b\u8f91\u7d27\u5bc6\u5ea6\u6307\u6807\u8bc4\u4f30LLM\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u76f8\u4f3c\uff0c\u4e0d\u540cLLM\u5728\u89c4\u5219\u4e00\u81f4\u6027\u63a8\u7406\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e0d\u6e05\u695aLLM\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6210\u529f\u662f\u6e90\u4e8e\u641c\u7d22\u3001\u6b7b\u8bb0\u786c\u80cc\u8fd8\u662f\u89c4\u5219\u4e00\u81f4\u6027\u63a8\u7406\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edfPASS@k\u6307\u6807\u7684\u65b0\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u601d\u7ef4\u94fe\u5efa\u6a21\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe\u8868\u793a\u63a8\u5bfc\u72b6\u6001\u548c\u89c4\u5219\u5e94\u7528\uff0c\u63d0\u51fa\u903b\u8f91\u7d27\u5bc6\u5ea6\u6307\u6807\uff0c\u5e76\u6784\u5efaDAG-MATH\u57fa\u51c6\u6d4b\u8bd5\u683c\u5f0f\u3002", "result": "\u5728\u6807\u51c6\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u53d1\u73b0\uff0c\u5373\u4f7fPASS@k\u6307\u6807\u76f8\u4f3c\uff0c\u4ee3\u8868\u6027LLM\u5bb6\u65cf\u5728\u63a8\u7406\u4fdd\u771f\u5ea6\u4e0a\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4e0e\u89c4\u5219\u4e00\u81f4\u6027\u63a8\u5bfc\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u7531\u5f62\u5f0f\u601d\u7ef4\u94fe\u548c\u5f62\u5f0f\u8bc1\u660e\u7cfb\u7edf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3aLLM\u63a8\u7406\u8bc4\u4f30\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.19893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19893", "abs": "https://arxiv.org/abs/2510.19893", "authors": ["Shiqi Dai", "Wei Dai", "Jiaee Cheong", "Paul Pu Liang"], "title": "FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning", "comment": "Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop", "summary": "Medical artificial intelligence systems have achieved remarkable diagnostic\ncapabilities, yet they consistently exhibit performance disparities across\ndemographic groups, causing real-world harm to underrepresented populations.\nWhile recent multimodal reasoning foundation models have advanced clinical\ndiagnosis through integrated analysis of diverse medical data, reasoning\ntrainings via reinforcement learning inherit and often amplify biases present\nin training datasets dominated by majority populations. We introduce\nFairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical\nreinforcement learning approach that promotes equitable learning across\nheterogeneous clinical populations. FairGRPO employs adaptive importance\nweighting of advantages based on representation, task difficulty, and data\nsource. To address the common issue of missing demographic labels in the\nclinical domain, we further employ unsupervised clustering, which automatically\ndiscovers latent demographic groups when labels are unavailable. Through\ncomprehensive experiments across 7 clinical diagnostic datasets spanning 5\nclinical modalities across X-ray, CT scan, dermoscropy, mammography and\nultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%\nagainst all vanilla and bias mitigated RL baselines, while improving F1 score\nby 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO\nprogressively improves fairness throughout optimization, while baseline RL\nmethods exhibit deteriorating fairness as training progresses. Based on\nFairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that\nachieves state-of-the-art performance while demonstrating significantly reduced\ndisparities across demographic groups.", "AI": {"tldr": "FairGRPO\u662f\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u8981\u6027\u52a0\u6743\u548c\u81ea\u52a8\u53d1\u73b0\u6f5c\u5728\u4eba\u53e3\u7fa4\u4f53\u6765\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u57287\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u9884\u6d4b\u504f\u5dee\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u533b\u7597AI\u7cfb\u7edf\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u5bf9\u5f31\u52bf\u7fa4\u4f53\u9020\u6210\u5b9e\u9645\u4f24\u5bb3\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u7406\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65f6\u4f1a\u7ee7\u627f\u5e76\u653e\u5927\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\u3002", "method": "\u63d0\u51faFairGRPO\u65b9\u6cd5\uff0c\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8e\u8868\u5f81\u3001\u4efb\u52a1\u96be\u5ea6\u548c\u6570\u636e\u6e90\u81ea\u9002\u5e94\u52a0\u6743\u4f18\u52bf\u51fd\u6570\u3002\u5f53\u7f3a\u4e4f\u4eba\u53e3\u6807\u7b7e\u65f6\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u81ea\u52a8\u53d1\u73b0\u6f5c\u5728\u4eba\u53e3\u7fa4\u4f53\u3002", "result": "\u57287\u4e2a\u4e34\u5e8a\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\uff0cFairGRPO\u5c06\u9884\u6d4b\u516c\u5e73\u6027\u63d0\u9ad8\u4e8627.2%\uff0cF1\u5206\u6570\u63d0\u5347\u4e8612.49%\u3002\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6574\u4e2a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u5584\u516c\u5e73\u6027\u3002", "conclusion": "FairGRPO\u80fd\u6709\u6548\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u57fa\u4e8e\u8be5\u65b9\u6cd5\u53d1\u5e03\u7684FairMedGemma-4B\u6a21\u578b\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.19897", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19897", "abs": "https://arxiv.org/abs/2510.19897", "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"], "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation", "comment": "11 pages", "summary": "We investigate how agents built on pretrained large language models can learn\ntarget classification functions from labeled examples without parameter\nupdates. While conventional approaches like fine-tuning are often costly,\ninflexible, and opaque, we propose a memory-augmented framework that leverages\nboth labeled data and LLM-generated critiques. Our framework uses episodic\nmemory to store instance-level critiques-capturing specific past\nexperiences-and semantic memory to distill these into reusable, task-level\nguidance. Across a diverse set of tasks, incorporating critiques yields up to a\n24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines\nthat rely only on labels. Through extensive empirical evaluation, we uncover\ndistinct behavioral differences between OpenAI and opensource models,\nparticularly in how they handle fact-oriented versus preference-based data. To\ninterpret how models respond to different representations of supervision\nencoded in memory, we introduce a novel metric, suggestibility. This helps\nexplain observed behaviors and illuminates how model characteristics and memory\nstrategies jointly shape learning dynamics. Our findings highlight the promise\nof memory-driven, reflective learning for building more adaptive and\ninterpretable LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5b58\u50a8\u5b9e\u4f8b\u7ea7\u6279\u8bc4\u548c\u8bed\u4e49\u7ea7\u6307\u5bfc\u6765\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u4e14\u4e0d\u900f\u660e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u60c5\u666f\u8bb0\u5fc6\u5b58\u50a8\u5b9e\u4f8b\u7ea7\u6279\u8bc4\uff0c\u8bed\u4e49\u8bb0\u5fc6\u63d0\u70bc\u53ef\u91cd\u7528\u7684\u4efb\u52a1\u7ea7\u6307\u5bfc\uff0c\u7ed3\u5408\u6807\u7b7e\u6570\u636e\u548cLLM\u751f\u6210\u7684\u6279\u8bc4\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4ec5\u4f9d\u8d56\u6807\u7b7e\u7684\u68c0\u7d22\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe24.8%\u3002", "conclusion": "\u8bb0\u5fc6\u9a71\u52a8\u7684\u53cd\u601d\u5b66\u4e60\u4e3a\u6784\u5efa\u66f4\u81ea\u9002\u5e94\u548c\u53ef\u89e3\u91ca\u7684LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness.", "AI": {"tldr": "\u672c\u6587\u5bf9LLMs\u5728\u4ee3\u7801\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728HumanEval-Java\u548cDefects4J\u6570\u636e\u96c6\u4e0a\u7684\u6545\u969c\u5b9a\u4f4d\u80fd\u529b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9LLMs\u5728\u4ee3\u7801\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u800c\u6545\u969c\u5b9a\u4f4d\u7684\u6709\u6548\u6027\u76f4\u63a5\u5f71\u54cd\u7a0b\u5e8f\u4fee\u590d\u7684\u6548\u679c\u3002", "method": "\u8bc4\u4f30\u4ee3\u8868\u6027\u5f00\u6e90\u6a21\u578b(Qwen2.5-coder-32b-instruct, DeepSeek-V3)\u548c\u95ed\u6e90\u6a21\u578b(GPT-4.1 mini, Gemini-2.5-flash)\uff0c\u7814\u7a76\u6807\u51c6\u63d0\u793a\u3001\u5c11\u6837\u672c\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\u7b49\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5305\u542b\u9519\u8bef\u62a5\u544a\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1b\u5c11\u6837\u672c\u5b66\u4e60\u6709\u6539\u8fdb\u6f5c\u529b\u4f46\u5b58\u5728\u8fb9\u9645\u6536\u76ca\u9012\u51cf\uff1b\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u6a21\u578b\u56fa\u6709\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7279\u5f81\u548c\u6743\u8861\uff0c\u4e3a\u7406\u89e3\u5f53\u524dLLMs\u7684\u4f18\u52bf\u548c\u6539\u8fdb\u6545\u969c\u5b9a\u4f4d\u6548\u679c\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.19950", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.19950", "abs": "https://arxiv.org/abs/2510.19950", "authors": ["Shaocong Ma", "Heng Huang"], "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets", "comment": null, "summary": "In financial applications, reinforcement learning (RL) agents are commonly\ntrained on historical data, where their actions do not influence prices.\nHowever, during deployment, these agents trade in live markets where their own\ntransactions can shift asset prices, a phenomenon known as market impact. This\nmismatch between training and deployment environments can significantly degrade\nperformance. Traditional robust RL approaches address this model\nmisspecification by optimizing the worst-case performance over a set of\nuncertainties, but typically rely on symmetric structures that fail to capture\nthe directional nature of market impact. To address this issue, we develop a\nnovel class of elliptic uncertainty sets. We establish both implicit and\nexplicit closed-form solutions for the worst-case uncertainty under these sets,\nenabling efficient and tractable robust policy evaluation. Experiments on\nsingle-asset and multi-asset trading tasks demonstrate that our method achieves\nsuperior Sharpe ratio and remains robust under increasing trade volumes,\noffering a more faithful and scalable approach to RL in financial markets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u692d\u5706\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u6765\u5904\u7406\u91d1\u878d\u5e02\u573a\u4e2dRL\u4ee3\u7406\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u73af\u5883\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5e02\u573a\u5f71\u54cd\u7684\u5b9a\u5411\u7279\u6027\uff0c\u901a\u8fc7\u5efa\u7acb\u6700\u574f\u60c5\u51b5\u4e0d\u786e\u5b9a\u6027\u7684\u95ed\u5f0f\u89e3\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9c81\u68d2\u7b56\u7565\u8bc4\u4f30\u3002", "motivation": "\u91d1\u878d\u5e94\u7528\u4e2d\uff0cRL\u4ee3\u7406\u5728\u5386\u53f2\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u4e0d\u4f1a\u5f71\u54cd\u4ef7\u683c\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u65f6\u5176\u4ea4\u6613\u884c\u4e3a\u4f1a\u4ea7\u751f\u5e02\u573a\u5f71\u54cd\uff0c\u8fd9\u79cd\u73af\u5883\u4e0d\u5339\u914d\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002\u4f20\u7edf\u9c81\u68d2RL\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u79f0\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u5e02\u573a\u5f71\u54cd\u7684\u5b9a\u5411\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u7c7b\u65b0\u9896\u7684\u692d\u5706\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u5efa\u7acb\u4e86\u6700\u574f\u60c5\u51b5\u4e0d\u786e\u5b9a\u6027\u7684\u9690\u5f0f\u548c\u663e\u5f0f\u95ed\u5f0f\u89e3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u5904\u7406\u7684\u9c81\u68d2\u7b56\u7565\u8bc4\u4f30\u3002", "result": "\u5728\u5355\u8d44\u4ea7\u548c\u591a\u8d44\u4ea7\u4ea4\u6613\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u590f\u666e\u6bd4\u7387\uff0c\u5e76\u5728\u4ea4\u6613\u91cf\u589e\u52a0\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91d1\u878d\u5e02\u573a\u4e2d\u7684RL\u63d0\u4f9b\u4e86\u66f4\u5fe0\u5b9e\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20188", "abs": "https://arxiv.org/abs/2510.20188", "authors": ["Morris Yu-Chao Huang", "Zhen Tan", "Mohan Zhang", "Pingzhi Li", "Zhuo Zhang", "Tianlong Chen"], "title": "TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning", "comment": null, "summary": "Large Language Models generate complex reasoning chains that reveal their\ndecision-making, yet verifying the faithfulness and harmlessness of these\nintermediate steps remains a critical unsolved problem. Existing auditing\nmethods are centralized, opaque, and hard to scale, creating significant risks\nfor deploying proprietary models in high-stakes domains. We identify four core\nchallenges: (1) Robustness: Centralized auditors are single points of failure,\nprone to bias or attacks. (2) Scalability: Reasoning traces are too long for\nmanual verification. (3) Opacity: Closed auditing undermines public trust. (4)\nPrivacy: Exposing full reasoning risks model theft or distillation. We propose\nTRUST, a transparent, decentralized auditing framework that overcomes these\nlimitations via: (1) A consensus mechanism among diverse auditors, guaranteeing\ncorrectness under up to $30\\%$ malicious participants. (2) A hierarchical DAG\ndecomposition of reasoning traces, enabling scalable, parallel auditing. (3) A\nblockchain ledger that records all verification decisions for public\naccountability. (4) Privacy-preserving segmentation, sharing only partial\nreasoning steps to protect proprietary logic. We provide theoretical guarantees\nfor the security and economic incentives of the TRUST framework. Experiments\nacross multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,\nmedical, science, humanities) show TRUST effectively detects reasoning flaws\nand remains robust against adversarial auditors. Our work pioneers\ndecentralized AI auditing, offering a practical path toward safe and\ntrustworthy LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86TRUST\u6846\u67b6\uff0c\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684AI\u5ba1\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u3001\u5c42\u6b21\u5316DAG\u5206\u89e3\u3001\u533a\u5757\u94fe\u8d26\u672c\u548c\u9690\u79c1\u4fdd\u62a4\u5206\u6bb5\u6765\u89e3\u51b3LLM\u63a8\u7406\u94fe\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\u5b58\u5728\u96c6\u4e2d\u5316\u3001\u4e0d\u900f\u660e\u3001\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u9a8c\u8bc1LLM\u63a8\u7406\u94fe\u7684\u5fe0\u5b9e\u6027\u548c\u65e0\u5bb3\u6027\uff0c\u9650\u5236\u4e86\u4e13\u6709\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u5171\u8bc6\u673a\u5236\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u5c42\u6b21\u5316DAG\u5206\u89e3\u5b9e\u73b0\u53ef\u6269\u5c55\u5e76\u884c\u5ba1\u8ba1\uff0c\u533a\u5757\u94fe\u8d26\u672c\u8bb0\u5f55\u9a8c\u8bc1\u51b3\u7b56\uff0c\u9690\u79c1\u4fdd\u62a4\u5206\u6bb5\u4fdd\u62a4\u4e13\u6709\u903b\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTRUST\u80fd\u6709\u6548\u68c0\u6d4b\u63a8\u7406\u7f3a\u9677\uff0c\u5728\u9ad8\u8fbe30%\u6076\u610f\u53c2\u4e0e\u8005\u60c5\u51b5\u4e0b\u4fdd\u6301\u7a33\u5065\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u548c\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "TRUST\u6846\u67b6\u4e3a\u53bb\u4e2d\u5fc3\u5316AI\u5ba1\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4e3a\u5b9e\u73b0\u5b89\u5168\u53ef\u4fe1\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.20036", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20036", "abs": "https://arxiv.org/abs/2510.20036", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "comment": "Preprint under review", "summary": "Large language model (LLM) agents rely on external tools to solve complex\ntasks, but real-world toolsets often contain redundant tools with overlapping\nnames and descriptions, introducing ambiguity and reducing selection accuracy.\nLLMs also face strict input context limits, preventing efficient consideration\nof large toolsets. To address these challenges, we propose ToolScope, which\nincludes: (1) ToolScopeMerger with Auto-Correction to automatically audit and\nfix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and\nselect only the most relevant tools for each query, compressing toolsets to fit\nwithin context limits without sacrificing accuracy. Evaluations on three\nstate-of-the-art LLMs and three open-source tool-use benchmarks show gains of\n8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's\neffectiveness in enhancing LLM tool use.", "AI": {"tldr": "ToolScope\u901a\u8fc7\u81ea\u52a8\u5408\u5e76\u5197\u4f59\u5de5\u5177\u548c\u667a\u80fd\u68c0\u7d22\u76f8\u5173\u5de5\u5177\uff0c\u89e3\u51b3LLM\u4ee3\u7406\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u9762\u4e34\u7684\u5de5\u5177\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5de5\u5177\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u5de5\u5177\uff0c\u5de5\u5177\u540d\u79f0\u548c\u63cf\u8ff0\u91cd\u53e0\u5bfc\u81f4\u9009\u62e9\u6b67\u4e49\uff0c\u540c\u65f6LLM\u9762\u4e34\u4e25\u683c\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u5de5\u5177\u96c6\u3002", "method": "\u63d0\u51faToolScope\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1) ToolScopeMerger\u81ea\u52a8\u5ba1\u8ba1\u548c\u4fee\u590d\u5de5\u5177\u5408\u5e76\uff0c\u51cf\u5c11\u5197\u4f59\uff1b(2) ToolScopeRetriever\u5bf9\u5de5\u5177\u8fdb\u884c\u6392\u5e8f\u548c\u9009\u62e9\uff0c\u538b\u7f29\u5de5\u5177\u96c6\u4ee5\u9002\u5e94\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u6700\u5148\u8fdbLLM\u548c\u4e09\u4e2a\u5f00\u6e90\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5de5\u5177\u9009\u62e9\u51c6\u786e\u7387\u63d0\u53478.38%\u81f338.6%\u3002", "conclusion": "ToolScope\u80fd\u6709\u6548\u589e\u5f3aLLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u5de5\u5177\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2510.20205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20205", "abs": "https://arxiv.org/abs/2510.20205", "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"], "title": "Merge and Conquer: Evolutionarily Optimizing AI for 2048", "comment": "9 pages, 5 figures", "summary": "Optimizing artificial intelligence (AI) for dynamic environments remains a\nfundamental challenge in machine learning research. In this paper, we examine\nevolutionary training methods for optimizing AI to solve the game 2048, a 2D\nsliding puzzle. 2048, with its mix of strategic gameplay and stochastic\nelements, presents an ideal playground for studying decision-making, long-term\nplanning, and dynamic adaptation. We implemented two distinct systems: a\ntwo-agent metaprompting system where a \"thinker\" large language model (LLM)\nagent refines gameplay strategies for an \"executor\" LLM agent, and a\nsingle-agent system based on refining a value function for a limited Monte\nCarlo Tree Search. We also experimented with rollback features to avoid\nperformance degradation. Our results demonstrate the potential of evolutionary\nrefinement techniques in improving AI performance in non-deterministic\nenvironments. The single-agent system achieved substantial improvements, with\nan average increase of 473.2 points per cycle, and with clear upward trends\n(correlation $\\rho$=0.607) across training cycles. The LLM's understanding of\nthe game grew as well, shown in its development of increasingly advanced\nstrategies. Conversely, the two-agent system did not garner much improvement,\nhighlighting the inherent limits of meta-prompting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57282048\u6e38\u620f\u4e2d\u4f18\u5316AI\u7684\u8fdb\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53d1\u73b0\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u4f18\u5316\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u800c\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\u6539\u8fdb\u6709\u9650\u3002", "motivation": "\u4f18\u5316AI\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u662f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u7684\u57fa\u672c\u6311\u6218\uff0c2048\u6e38\u620f\u7ed3\u5408\u4e86\u7b56\u7565\u6e38\u620f\u548c\u968f\u673a\u5143\u7d20\uff0c\u662f\u7814\u7a76\u51b3\u7b56\u5236\u5b9a\u3001\u957f\u671f\u89c4\u5212\u548c\u52a8\u6001\u9002\u5e94\u7684\u7406\u60f3\u5e73\u53f0\u3002", "method": "\u5b9e\u73b0\u4e86\u4e24\u79cd\u7cfb\u7edf\uff1a\u53cc\u667a\u80fd\u4f53\u5143\u63d0\u793a\u7cfb\u7edf\uff08\u601d\u8003\u8005LLM\u4f18\u5316\u6267\u884c\u8005LLM\u7684\u7b56\u7565\uff09\u548c\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u4f18\u5316\u7684\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u6709\u9650\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff09\uff0c\u5e76\u5b9e\u9a8c\u4e86\u56de\u6eda\u529f\u80fd\u4ee5\u907f\u514d\u6027\u80fd\u9000\u5316\u3002", "result": "\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u6bcf\u4e2a\u5468\u671f\u5e73\u5747\u589e\u52a0473.2\u5206\uff0c\u8bad\u7ec3\u5468\u671f\u5448\u660e\u663e\u4e0a\u5347\u8d8b\u52bf\uff08\u76f8\u5173\u6027\u03c1=0.607\uff09\u3002LLM\u5bf9\u6e38\u620f\u7684\u7406\u89e3\u4e5f\u968f\u7740\u9ad8\u7ea7\u7b56\u7565\u7684\u53d1\u5c55\u800c\u589e\u957f\u3002\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "\u8fdb\u5316\u4f18\u5316\u6280\u672f\u5728\u975e\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u5177\u6709\u6539\u5584AI\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4f46\u5143\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.20258", "categories": ["cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.20258", "abs": "https://arxiv.org/abs/2510.20258", "authors": ["Bita Banihashemi", "Megh Patel", "Yves Lesp\u00e9rance"], "title": "Using Large Language Models for Abstraction of Planning Domains - Extended Version", "comment": null, "summary": "Generating an abstraction of a dynamic domain that aligns with a given\npurpose remains a significant challenge given that the choice of such an\nabstraction can impact an agent's ability to plan, reason, and provide\nexplanations effectively. We model the agent's concrete behaviors in PDDL and\ninvestigate the use of in-context learning with large language models (LLMs)\nfor the generation of abstract PDDL domains and problem instances, given an\nabstraction objective specified in natural language. The benchmark examples we\nuse are new and have not been part of the data any LLMs have been trained on.\nWe consider three categories of abstractions: abstraction of choice of\nalternative concrete actions, abstraction of sequences of concrete actions, and\nabstraction of action/predicate parameters, as well as combinations of these.\nThe generated abstract PDDL domains and problem instances are then checked by\nsymbolic validation tools as well as human experts. Our experiments show that\nGPT-4o can generally synthesize useful planning domain abstractions in simple\nsettings, although it is better at abstracting over actions than over the\nassociated fluents.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u751f\u6210\u62bd\u8c61PDDL\u9886\u57df\u548c\u95ee\u9898\u5b9e\u4f8b\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u4f53\u7684\u89c4\u5212\u3001\u63a8\u7406\u548c\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u52a8\u6001\u9886\u57df\u7684\u62bd\u8c61\u751f\u6210\u5bf9\u667a\u80fd\u4f53\u7684\u89c4\u5212\u3001\u63a8\u7406\u548c\u89e3\u91ca\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u5728PDDL\u4e2d\u5efa\u6a21\u667a\u80fd\u4f53\u5177\u4f53\u884c\u4e3a\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u7684\u62bd\u8c61\u76ee\u6807\u751f\u6210\u62bd\u8c61PDDL\u9886\u57df\u548c\u95ee\u9898\u5b9e\u4f8b\u3002", "result": "GPT-4o\u5728\u7b80\u5355\u8bbe\u7f6e\u4e0b\u80fd\u6709\u6548\u5408\u6210\u6709\u7528\u7684\u89c4\u5212\u9886\u57df\u62bd\u8c61\uff0c\u4f46\u5728\u52a8\u4f5c\u62bd\u8c61\u65b9\u9762\u4f18\u4e8e\u5173\u8054\u7684fluent\u62bd\u8c61\u3002", "conclusion": "LLM\u5728\u751f\u6210\u89c4\u5212\u9886\u57df\u62bd\u8c61\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u4f5c\u62bd\u8c61\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "topic": "agent analysis"}}
{"id": "2510.20022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20022", "abs": "https://arxiv.org/abs/2510.20022", "authors": ["Jiazheng Li", "Yawei Wang", "David Yan", "Yijun Tian", "Zhichao Xu", "Huan Song", "Panpan Xu", "Lin Lee Cheong"], "title": "SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nenabling language agents to excel at single-turn tasks. However, their\napplication to complex, multi-step, and long-horizon tasks remains challenging.\nWhile reinforcement learning (RL) offers a promising avenue for addressing\nthese challenges, mainstream approaches typically rely solely on sparse,\noutcome-based rewards, a limitation that becomes especially problematic for\ngroup-based RL algorithms lacking critic models, such as Group Relative Policy\nOptimization (GRPO). In such methods, uniformly rewarding or penalizing all\nactions within a trajectory can lead to training instability and suboptimal\npolicies, because beneficial and detrimental actions are often entangled across\nmulti-step interactions. To address this challenge, we propose SALT, a novel\nand lightweight framework that provides a finer-grained advantage assignment,\nderived solely from outcome rewards. We achieve this by constructing a graph\nfrom trajectories of the same prompt, which allows us to quantify the quality\nof each step and assign advantages accordingly. Crucially, SALT is designed as\na plug-and-play module that seamlessly integrates with existing group-based RL\nalgorithms, requiring no modifications to the rollout procedure and introducing\nnegligible computational overhead. Extensive experiments on the WebShop,\nALFWorld, and AppWorld benchmarks with various model sizes demonstrate that\nSALT consistently improves performance. We also conduct a thorough analysis to\nvalidate the design choices behind SALT and offer actionable insights.", "AI": {"tldr": "\u63d0\u51faSALT\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8f68\u8ff9\u56fe\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u4f18\u52bf\u5206\u914d\uff0c\u89e3\u51b3\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\uff09\u4ec5\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u5bf9\u6240\u6709\u52a8\u4f5c\u8fdb\u884c\u7edf\u4e00\u5956\u52b1\u6216\u60e9\u7f5a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6b21\u4f18\u7b56\u7565\uff0c\u56e0\u4e3a\u591a\u6b65\u4ea4\u4e92\u4e2d\u6709\u5229\u548c\u4e0d\u5229\u52a8\u4f5c\u5f80\u5f80\u4ea4\u7ec7\u5728\u4e00\u8d77\u3002", "method": "SALT\u6846\u67b6\u4ece\u76f8\u540c\u63d0\u793a\u7684\u8f68\u8ff9\u6784\u5efa\u56fe\uff0c\u91cf\u5316\u6bcf\u4e00\u6b65\u7684\u8d28\u91cf\u5e76\u76f8\u5e94\u5206\u914d\u4f18\u52bf\u3002\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u7fa4\u4f53\u7684RL\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539rollout\u8fc7\u7a0b\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "result": "\u5728WebShop\u3001ALFWorld\u548cAppWorld\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSALT\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u4e0b\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SALT\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u4f18\u52bf\u5206\u914d\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u7fa4\u4f53RL\u7b97\u6cd5\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20310", "abs": "https://arxiv.org/abs/2510.20310", "authors": ["Mingliang Zhai", "Hansheng Liang", "Xiaomeng Fan", "Zhi Gao", "Chuanhao Li", "Che Sun", "Xu Bin", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation", "comment": "16 pages, 7 figures, 8 tables", "summary": "Embodied Question Answering (EQA) requires agents to explore 3D environments\nto obtain observations and answer questions related to the scene. Existing\nmethods leverage VLMs to directly explore the environment and answer questions\nwithout explicit thinking or planning, which limits their reasoning ability and\nresults in excessive or inefficient exploration as well as ineffective\nresponses. In this paper, we introduce ToolEQA, an agent that integrates\nexternal tools with multi-step reasoning, where external tools can provide more\nuseful information for completing the task, helping the model derive better\nexploration directions in the next step of reasoning and thus obtaining\nadditional effective information. This enables ToolEQA to generate more\naccurate responses with a shorter exploration distance. To enhance the model's\nability for tool-usage and multi-step reasoning, we further design a novel EQA\ndata generation pipeline that automatically constructs large-scale EQA tasks\nwith reasoning trajectories and corresponding answers. Based on the pipeline,\nwe collect the EQA-RT dataset that contains about 18K tasks, divided into a\ntraining set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping\nwith the training set) and EQA-RT-Unseen (novel scenes). Experiments on\nEQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by\n9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot\nToolEQA by 10% in success rate. In addition, ToolEQA also achieves\nstate-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench\ndatasets, demonstrating its generality. Our homepage see\nhttps://tooleqa.github.io.", "AI": {"tldr": "ToolEQA\u662f\u4e00\u4e2a\u96c6\u6210\u5916\u90e8\u5de5\u5177\u548c\u591a\u6b65\u63a8\u7406\u7684\u5177\u8eab\u95ee\u7b54\u4ee3\u7406\uff0c\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u66f4\u591a\u6709\u7528\u4fe1\u606f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u77ed\u63a2\u7d22\u8ddd\u79bb\u5185\u751f\u6210\u66f4\u51c6\u786e\u56de\u7b54\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u95ee\u7b54\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u73af\u5883\u800c\u4e0d\u8fdb\u884c\u663e\u5f0f\u601d\u8003\u6216\u89c4\u5212\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u56de\u7b54\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faToolEQA\u4ee3\u7406\uff0c\u96c6\u6210\u5916\u90e8\u5de5\u5177\u4e0e\u591a\u6b65\u63a8\u7406\uff0c\u8bbe\u8ba1\u81ea\u52a8\u751f\u6210\u5177\u8eab\u95ee\u7b54\u4efb\u52a1\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u6536\u96c6\u5305\u542b\u7ea618K\u4efb\u52a1\u7684EQA-RT\u6570\u636e\u96c6\u3002", "result": "\u5728EQA-RT-Seen\u548cEQA-RT-Unseen\u6d4b\u8bd5\u96c6\u4e0a\uff0cToolEQA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u53479.2~20.2%\uff0c\u6bd4\u96f6\u6837\u672cToolEQA\u9ad810%\u3002\u5728HM-EQA\u3001OpenEQA\u548cEXPRESS-Bench\u6570\u636e\u96c6\u4e0a\u4e5f\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ToolEQA\u901a\u8fc7\u96c6\u6210\u5916\u90e8\u5de5\u5177\u548c\u591a\u6b65\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u95ee\u7b54\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.20091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20091", "abs": "https://arxiv.org/abs/2510.20091", "authors": ["Zhaoyi Joey Hou", "Bowei Alvin Zhang", "Yining Lu", "Bhiman Kumar Baghel", "Anneliese Brei", "Ximing Lu", "Meng Jiang", "Faeze Brahman", "Snigdha Chaturvedi", "Haw-Shiuan Chang", "Daniel Khashabi", "Xiang Lorraine Li"], "title": "CreativityPrism: A Holistic Benchmark for Large Language Model Creativity", "comment": null, "summary": "Creativity is often seen as a hallmark of human intelligence. While large\nlanguage models (LLMs) are increasingly perceived as producing creative text,\nthere is still no holistic framework to evaluate their creativity across\ndiverse scenarios. Existing evaluation methods remain fragmented, with dramatic\nvariation across domains and tasks, largely due to differing definitions and\nmeasurements of creativity. Inspired by the hypothesis that creativity is not\none fixed idea, we propose CreativityPrism, an evaluation analysis framework\nthat decomposes creativity into three dimensions: quality, novelty, and\ndiversity. CreativityPrism incorporates nine tasks, three domains, i.e.,\ndivergent thinking, creative writing, and logical reasoning, and twenty\nevaluation metrics, which measure each dimension in task-specific, unique ways.\nWe evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on\nCreativityPrism and analyze the performance correlations among different\nmetrics and task domains. Our results reveal a notable gap between proprietary\nand open-source models. Overall, model performance tends to be highly\ncorrelated across tasks within the same domain and less so across different\ndomains. Among evaluation dimensions, diversity and quality metrics show strong\ncorrelations - models that perform well on one often excel on the other -\nwhereas novelty exhibits much weaker correlation with either. These findings\nsupport our hypothesis that strong performance in one creativity task or\ndimension does not necessarily generalize to others, underscoring the need for\na holistic evaluation of LLM creativity.", "AI": {"tldr": "\u63d0\u51fa\u4e86CreativityPrism\u6846\u67b6\uff0c\u5c06LLM\u521b\u9020\u529b\u5206\u89e3\u4e3a\u8d28\u91cf\u3001\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5728\u4e09\u4e2a\u9886\u57df\uff08\u53d1\u6563\u601d\u7ef4\u3001\u521b\u610f\u5199\u4f5c\u3001\u903b\u8f91\u63a8\u7406\uff09\u8bc4\u4f3017\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u4e0e\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u5dee\u8ddd\uff0c\u4e0d\u540c\u521b\u9020\u529b\u7ef4\u5ea6\u95f4\u76f8\u5173\u6027\u8f83\u5f31\u3002", "motivation": "\u73b0\u6709LLM\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u9700\u8981\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u521b\u9020\u529b\u8868\u73b0\u3002", "method": "\u63d0\u51faCreativityPrism\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u9020\u529b\u7ef4\u5ea6\uff08\u8d28\u91cf\u3001\u65b0\u9896\u6027\u3001\u591a\u6837\u6027\uff09\u3001\u4e09\u4e2a\u4efb\u52a1\u9886\u57df\u548c20\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u4e8617\u4e2a\u4e13\u6709\u548c\u5f00\u6e90LLM\u3002", "result": "\u4e13\u6709\u6a21\u578b\u4e0e\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1b\u540c\u4e00\u9886\u57df\u5185\u4efb\u52a1\u76f8\u5173\u6027\u9ad8\uff0c\u8de8\u9886\u57df\u76f8\u5173\u6027\u4f4e\uff1b\u591a\u6837\u6027\u548c\u8d28\u91cf\u6307\u6807\u5f3a\u76f8\u5173\uff0c\u65b0\u9896\u6027\u4e0e\u5176\u4ed6\u7ef4\u5ea6\u76f8\u5173\u6027\u5f31\u3002", "conclusion": "\u521b\u9020\u529b\u8bc4\u4f30\u9700\u8981\u5168\u9762\u6846\u67b6\uff0c\u5355\u4e00\u4efb\u52a1\u6216\u7ef4\u5ea6\u7684\u8868\u73b0\u4e0d\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u65b9\u9762\uff0c\u652f\u6301\u521b\u9020\u529b\u4e0d\u662f\u5355\u4e00\u56fa\u5b9a\u6982\u5ff5\u7684\u89c2\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2510.20168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20168", "abs": "https://arxiv.org/abs/2510.20168", "authors": ["Tian Lan", "Bin Zhu", "Qianghuai Jia", "Junyang Ren", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking", "comment": null, "summary": "Current search agents fundamentally lack the ability to simultaneously\nperform \\textit{deep} reasoning over multi-hop retrieval and\n\\textit{wide}-scale information collection-a critical deficiency for real-world\napplications like comprehensive market analysis and business development. To\nbridge this gap, we introduce DeepWideSearch, the first benchmark explicitly\ndesigned to evaluate agents to integrate depth and width in information\nseeking. In DeepWideSearch, agents must process a large volume of data, each\nrequiring deep reasoning over multi-hop retrieval paths. Specifically, we\npropose two methods to converse established datasets, resulting in a curated\ncollection of 220 questions spanning 15 diverse domains. Extensive experiments\ndemonstrate that even state-of-the-art agents achieve only 2.39% average\nsuccess rate on DeepWideSearch, highlighting the substantial challenge of\nintegrating depth and width search in information-seeking tasks. Furthermore,\nour error analysis reveals four failure modes: lack of reflection, overreliance\non internal knowledge, insufficient retrieval, and context overflow-exposing\nkey limitations in current agent architectures. We publicly release\nDeepWideSearch to catalyze future research on more capable and robust\ninformation-seeking agents.", "AI": {"tldr": "DeepWideSearch\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u5728\u6df1\u5ea6\u63a8\u7406\u548c\u5e7f\u5ea6\u4fe1\u606f\u6536\u96c6\u65b9\u9762\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b220\u4e2a\u8de815\u4e2a\u9886\u57df\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u4ee3\u7406\u6210\u529f\u7387\u4ec52.39%\u3002", "motivation": "\u5f53\u524d\u641c\u7d22\u4ee3\u7406\u65e0\u6cd5\u540c\u65f6\u8fdb\u884c\u6df1\u5ea6\u591a\u8df3\u63a8\u7406\u548c\u5e7f\u5ea6\u4fe1\u606f\u6536\u96c6\uff0c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u5982\u5e02\u573a\u5206\u6790\u548c\u5546\u4e1a\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8f6c\u6362\u73b0\u6709\u6570\u636e\u96c6\u6784\u5efa\u4e86\u5305\u542b220\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u8981\u6c42\u4ee3\u7406\u5904\u7406\u5927\u91cf\u6570\u636e\u5e76\u8fdb\u884c\u6df1\u5ea6\u591a\u8df3\u68c0\u7d22\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u4ee3\u7406\u5728DeepWideSearch\u4e0a\u5e73\u5747\u6210\u529f\u7387\u4ec52.39%\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u548c\u5e7f\u5ea6\u641c\u7d22\u6574\u5408\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u57fa\u51c6\u66b4\u9732\u4e86\u5f53\u524d\u4ee3\u7406\u67b6\u6784\u7684\u56db\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u53cd\u601d\u3001\u8fc7\u5ea6\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u3001\u68c0\u7d22\u4e0d\u8db3\u548c\u4e0a\u4e0b\u6587\u6ea2\u51fa\u3002", "topic": "agent analysis"}}
{"id": "2510.20176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20176", "abs": "https://arxiv.org/abs/2510.20176", "authors": ["Yuhang Zhou", "Mingrui Zhang", "Ke Li", "Mingyi Wang", "Qiao Liu", "Qifei wang", "Jiayi Liu", "Fei Liu", "Serena Li", "Weiwi Li", "Mingze Gao", "Abhishek Kumar", "Xiangjun Fan", "Zhuokai Zhao", "Lizhu Zhang"], "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding", "comment": "18 pages, 4 figures", "summary": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mixture-of-Minds\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u8868\u683c\u63a8\u7406\u5206\u89e3\u4e3a\u89c4\u5212\u3001\u7f16\u7801\u548c\u56de\u7b54\u4e09\u4e2a\u4e13\u95e8\u89d2\u8272\uff0c\u7ed3\u5408\u4ee3\u7801\u6267\u884c\u5b9e\u73b0\u7cbe\u786e\u8868\u683c\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7MCTS\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5fae\u8c03\u7684\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u7b97\u672f\u9519\u8bef\u548c\u5e7b\u89c9\uff0c\u800c\u57fa\u4e8e\u5de5\u5177\u7684\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u4e14\u4f9d\u8d56\u521a\u6027\u6a21\u5f0f\uff0c\u9700\u8981\u7ed3\u5408\u7a33\u5065\u63a8\u7406\u548c\u53ef\u9760\u8868\u683c\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5206\u89e3\u8868\u683c\u63a8\u7406\u4efb\u52a1\uff0c\u5305\u62ec\u89c4\u5212\u3001\u7f16\u7801\u548c\u56de\u7b54\u4e09\u4e2a\u4e13\u95e8\u89d2\u8272\uff0c\u5e76\u91c7\u7528MCTS\u751f\u6210\u4f2a\u9ec4\u91d1\u8f68\u8ff9\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u667a\u80fd\u4f53\u3002", "result": "\u5728TableBench\u4e0a\u8fbe\u523062.13%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86OpenAI-o4-mini-high\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u63a8\u8fdb\u8868\u683c\u7406\u89e3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.20106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20106", "abs": "https://arxiv.org/abs/2510.20106", "authors": ["Amartya Roy", "Souvik Chakraborty"], "title": "Competition is the key: A Game Theoretic Causal Discovery Approach", "comment": null, "summary": "Causal discovery remains a central challenge in machine learning, yet\nexisting methods face a fundamental gap: algorithms like GES and GraN-DAG\nachieve strong empirical performance but lack finite-sample guarantees, while\ntheoretically principled approaches fail to scale. We close this gap by\nintroducing a game-theoretic reinforcement learning framework for causal\ndiscovery, where a DDQN agent directly competes against a strong baseline (GES\nor GraN-DAG), always warm-starting from the opponent's solution. This design\nyields three provable guarantees: the learned graph is never worse than the\nopponent, warm-starting strictly accelerates convergence, and most importantly,\nwith high probability the algorithm selects the true best candidate graph. To\nthe best of our knowledge, our result makes a first-of-its-kind progress in\nexplaining such finite-sample guarantees in causal discovery: on synthetic SEMs\n(30 nodes), the observed error probability decays with n, tightly matching\ntheory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,\nDream, and Andes, our method consistently improves upon GES and GraN-DAG while\nremaining theoretically safe. Remarkably, it scales to large graphs such as\nHepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these\nresults establish a new class of RL-based causal discovery algorithms that are\nsimultaneously provably consistent, sample-efficient, and practically scalable,\nmarking a decisive step toward unifying empirical performance with rigorous\nfinite-sample theory.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u535a\u5f08\u8bba\u5f3a\u5316\u5b66\u4e60\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7DDQN\u667a\u80fd\u4f53\u4e0e\u5f3a\u57fa\u7ebf\u65b9\u6cd5(GES/GraN-DAG)\u7ade\u4e89\uff0c\u5b9e\u73b0\u7406\u8bba\u4fdd\u8bc1\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u7edf\u4e00", "motivation": "\u89e3\u51b3\u56e0\u679c\u53d1\u73b0\u9886\u57df\u73b0\u6709\u65b9\u6cd5\u5728\u7406\u8bba\u4fdd\u8bc1\u4e0e\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1a\u7ecf\u9a8c\u6027\u80fd\u597d\u7684\u65b9\u6cd5\u7f3a\u4e4f\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u800c\u7406\u8bba\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56fe", "method": "\u4f7f\u7528DDQN\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4e0eGES\u6216GraN-DAG\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\uff0c\u59cb\u7ec8\u4ece\u5bf9\u624b\u89e3\u8fdb\u884c\u70ed\u542f\u52a8\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u56fe\u4e0d\u4f1a\u6bd4\u5bf9\u624b\u5dee", "result": "\u5728\u5408\u6210SEM(30\u8282\u70b9)\u4e0a\u89c2\u5bdf\u5230\u7684\u9519\u8bef\u6982\u7387\u968f\u6837\u672c\u91cfn\u8870\u51cf\uff0c\u4e0e\u7406\u8bba\u7d27\u5bc6\u5339\u914d\uff1b\u5728\u771f\u5b9e\u6570\u636e\u96c6(Sachs\u3001Asia\u3001Alarm\u7b49)\u4e0a\u6301\u7eed\u6539\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53ef\u6269\u5c55\u5230Hepar2(70\u8282\u70b9)\u3001Dream(100\u8282\u70b9)\u3001Andes(220\u8282\u70b9)\u7b49\u5927\u89c4\u6a21\u56fe", "conclusion": "\u5efa\u7acb\u4e86\u540c\u65f6\u5177\u5907\u53ef\u8bc1\u660e\u4e00\u81f4\u6027\u3001\u6837\u672c\u6548\u7387\u548c\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u7684RL\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u65b0\u7c7b\u522b\uff0c\u7edf\u4e00\u4e86\u7ecf\u9a8c\u6027\u80fd\u4e0e\u4e25\u683c\u6709\u9650\u6837\u672c\u7406\u8bba", "topic": "agentic reinforcement learning"}}
{"id": "2510.20603", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20603", "abs": "https://arxiv.org/abs/2510.20603", "authors": ["Heejin Do", "Jaehui Hwang", "Dongyoon Han", "Seong Joon Oh", "Sangdoo Yun"], "title": "What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation", "comment": null, "summary": "Evaluating large language models (LLMs) on final-answer correctness is the\ndominant paradigm. This approach, however, provides a coarse signal for model\nimprovement and overlooks the quality of the underlying reasoning process. We\nargue that a more granular evaluation of reasoning offers a more effective path\nto building robust models. We decompose reasoning quality into two dimensions:\nrelevance and coherence. Relevance measures if a step is grounded in the\nproblem; coherence measures if it follows logically from prior steps. To\nmeasure these aspects reliably, we introduce causal stepwise evaluation (CaSE).\nThis method assesses each reasoning step using only its preceding context,\nwhich avoids hindsight bias. We validate CaSE against human judgments on our\nnew expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we\nshow that curating training data with CaSE-evaluated relevance and coherence\ndirectly improves final task performance. Our work provides a scalable\nframework for analyzing, debugging, and improving LLM reasoning, demonstrating\nthe practical value of moving beyond validity checks.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u9010\u6b65\u8bc4\u4f30(CaSE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u7684\u76f8\u5173\u6027\u548c\u8fde\u8d2f\u6027\u6765\u6539\u8fdbLLM\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u4f20\u7edf\u8bc4\u4f30\u8303\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\uff0c\u65e0\u6cd5\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u3002", "method": "\u5c06\u63a8\u7406\u8d28\u91cf\u5206\u89e3\u4e3a\u76f8\u5173\u6027\u548c\u8fde\u8d2f\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u63d0\u51faCaSE\u65b9\u6cd5\u4ec5\u4f7f\u7528\u524d\u5e8f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\uff0c\u907f\u514d\u540e\u89c1\u4e4b\u660e\u504f\u5dee\u3002", "result": "\u5728MRa-GSM8K\u548cMRa-MATH\u57fa\u51c6\u4e0a\u9a8c\u8bc1CaSE\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7CaSE\u8bc4\u4f30\u7684\u6570\u636e\u7b5b\u9009\u80fd\u76f4\u63a5\u63d0\u5347\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "CaSE\u4e3a\u5206\u6790\u3001\u8c03\u8bd5\u548c\u6539\u8fdbLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u8d85\u8d8a\u6709\u6548\u6027\u68c0\u67e5\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.20304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20304", "abs": "https://arxiv.org/abs/2510.20304", "authors": ["Lei Tang", "Wei Zhou", "Mohsen Mesgar"], "title": "Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering", "comment": null, "summary": "Process reward models (PRMs) improve complex reasoning in large language\nmodels (LLMs) by grading candidate solutions step-by-step and selecting answers\nvia aggregated step scores. While effective in domains such as mathematics,\ntheir applicability to tasks involving semi-structured data, like table\nquestion answering (TQA) remains unexplored. TQA poses unique challenges for\nPRMs, including abundant irrelevant information, loosely connected reasoning\nsteps, and domain-specific reasoning. This work presents the first systematic\nstudy of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from\nboth answer and step perspectives. Results show that PRMs that combine textual\nand code verification can aid solution selection but struggle to generalize to\nout-of-domain data. Analysis reveals a weak correlation between performance in\nstep-level verification and answer accuracy, possibly stemming from weak step\ndependencies and loose causal links. Our findings highlight limitations of\ncurrent PRMs on TQA and offer valuable insights for building more robust,\nprocess-aware verifiers.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u5728\u8868\u683c\u95ee\u7b54(TQA)\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u7ed3\u5408\u6587\u672c\u548c\u4ee3\u7801\u9a8c\u8bc1\u7684PRMs\u80fd\u5e2e\u52a9\u9009\u62e9\u7b54\u6848\uff0c\u4f46\u5728\u8de8\u57df\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u4e0e\u7b54\u6848\u51c6\u786e\u6027\u76f8\u5173\u6027\u5f31\u3002", "motivation": "\u63a2\u7d22PRMs\u5728\u6d89\u53ca\u534a\u7ed3\u6784\u5316\u6570\u636e\u7684\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u56e0\u4e3aTQA\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u3001\u63a8\u7406\u6b65\u9aa4\u677e\u6563\u3001\u9886\u57df\u7279\u5b9a\u63a8\u7406\u7b49\u72ec\u7279\u6311\u6218\uff0c\u800cPRMs\u5728\u6570\u5b66\u7b49\u9886\u57df\u5df2\u8bc1\u660e\u6709\u6548\u3002", "method": "\u4ece\u7b54\u6848\u548c\u6b65\u9aa4\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0fPRMs\u5728TQA\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u6587\u672c\u548c\u4ee3\u7801\u9a8c\u8bc1\u7684\u7ec4\u5408\u6548\u679c\u3002", "result": "PRMs\u7ed3\u5408\u6587\u672c\u548c\u4ee3\u7801\u9a8c\u8bc1\u80fd\u5e2e\u52a9\u89e3\u51b3\u65b9\u6848\u9009\u62e9\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u57df\u5916\u6570\u636e\uff1b\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u6027\u80fd\u4e0e\u7b54\u6848\u51c6\u786e\u6027\u76f8\u5173\u6027\u5f31\uff0c\u53ef\u80fd\u6e90\u4e8e\u6b65\u9aa4\u4f9d\u8d56\u6027\u548c\u56e0\u679c\u8054\u7cfb\u8584\u5f31\u3002", "conclusion": "\u5f53\u524dPRMs\u5728TQA\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u8fc7\u7a0b\u611f\u77e5\u9a8c\u8bc1\u5668\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.20342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20342", "abs": "https://arxiv.org/abs/2510.20342", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "Teaching Language Models to Reason with Tools", "comment": "NIPS2025 Accepted", "summary": "Large reasoning models (LRMs) like OpenAI-o1 have shown impressive\ncapabilities in natural language reasoning. However, these models frequently\ndemonstrate inefficiencies or inaccuracies when tackling complex mathematical\noperations. While integrating computational tools such as Code Interpreters\n(CIs) offers a promising solution, it introduces a critical challenge: a\nconflict between the model's internal, probabilistic reasoning and the\nexternal, deterministic knowledge provided by the CI, which often leads models\nto unproductive deliberation. To overcome this, we introduce CoRT\n(Code-Optimized Reasoning Training), a post-training framework designed to\nteach LRMs to effectively utilize CIs. We propose \\emph{Hint-Engineering}, a\nnew data synthesis strategy that strategically injects diverse hints at optimal\npoints within reasoning paths. This approach generates high-quality,\ncode-integrated reasoning data specifically tailored to optimize LRM-CI\ninteraction. Using this method, we have synthesized 30 high-quality samples to\npost-train models ranging from 1.5B to 32B parameters through supervised\nfine-tuning. CoRT further refines the multi-round interleaving of external CI\nusage and internal thinking by employing rejection sampling and reinforcement\nlearning. Our experimental evaluations demonstrate CoRT's effectiveness,\nyielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B\nand DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging\nmathematical reasoning datasets. Moreover, CoRT significantly enhances\nefficiency, reducing token usage by approximately 30\\% for the 32B model and\n50\\% for the 1.5B model compared to pure natural language reasoning baselines.\nThe models and code are available at: https://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT\u662f\u4e00\u4e2a\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7Hint-Engineering\u7b56\u7565\u5408\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u6570\u636e\uff0c\u6559\u5bfc\u5927\u578b\u63a8\u7406\u6a21\u578b\u6709\u6548\u4f7f\u7528\u4ee3\u7801\u89e3\u91ca\u5668\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u6548\u7387\u4f18\u5316\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u548c\u51c6\u786e\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800c\u96c6\u6210\u4ee3\u7801\u89e3\u91ca\u5668\u53c8\u4f1a\u5bfc\u81f4\u6a21\u578b\u5185\u90e8\u6982\u7387\u63a8\u7406\u4e0e\u5916\u90e8\u786e\u5b9a\u6027\u77e5\u8bc6\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4ea7\u751f\u65e0\u6548\u7684\u6df1\u601d\u719f\u8651\u3002", "method": "\u63d0\u51faHint-Engineering\u6570\u636e\u5408\u6210\u7b56\u7565\uff0c\u5728\u63a8\u7406\u8def\u5f84\u4e2d\u6218\u7565\u6027\u5730\u6ce8\u5165\u591a\u6837\u5316\u63d0\u793a\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u6570\u636e\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u3001\u62d2\u7edd\u91c7\u6837\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\u7684\u591a\u8f6e\u4ea4\u4e92\u3002", "result": "\u5728\u4e94\u4e2a\u6311\u6218\u6027\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0cDeepSeek-R1-Distill-Qwen-32B\u548c1.5B\u6a21\u578b\u5206\u522b\u83b7\u5f974%\u548c8%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\uff0832B\u6a21\u578b\u7ea630%\uff0c1.5B\u6a21\u578b\u7ea650%\uff09\u3002", "conclusion": "CoRT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\u96c6\u6210\u65f6\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2510.20641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20641", "abs": "https://arxiv.org/abs/2510.20641", "authors": ["Andrea Agiollo", "Andrea Omicini"], "title": "Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges", "comment": null, "summary": "Thanks to the remarkable human-like capabilities of machine learning (ML)\nmodels in perceptual and cognitive tasks, frameworks integrating ML within\nrational agent architectures are gaining traction. Yet, the landscape remains\nfragmented and incoherent, often focusing on embedding ML into generic agent\ncontainers while overlooking the expressive power of rational\narchitectures--such as Belief-Desire-Intention (BDI) agents. This paper\npresents a fine-grained systematisation of existing approaches, using the BDI\nparadigm as a reference. Our analysis illustrates the fast-evolving literature\non rational agents enhanced by ML, and identifies key research opportunities\nand open challenges for designing effective rational ML agents.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5c06\u673a\u5668\u5b66\u4e60\u96c6\u6210\u5230\u7406\u6027\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u7684\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u5316\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8BDI\u8303\u5f0f\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u673a\u4f1a\u548c\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u5728\u611f\u77e5\u548c\u8ba4\u77e5\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u80fd\u529b\uff0c\u5c06ML\u96c6\u6210\u5230\u7406\u6027\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u7684\u6846\u67b6\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u8fde\u8d2f\u6027\u3002", "method": "\u4f7f\u7528BDI\u8303\u5f0f\u4f5c\u4e3a\u53c2\u8003\u6846\u67b6\uff0c\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7cfb\u7edf\u5316\u5206\u6790\u3002", "result": "\u5206\u6790\u5c55\u793a\u4e86ML\u589e\u5f3a\u7684\u7406\u6027\u667a\u80fd\u4f53\u6587\u732e\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u9700\u8981\u66f4\u7cfb\u7edf\u5730\u8bbe\u8ba1\u6709\u6548\u7684\u7406\u6027ML\u667a\u80fd\u4f53\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.20187", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20187", "abs": "https://arxiv.org/abs/2510.20187", "authors": ["Dian Yu", "Yulai Zhao", "Kishan Panaganti", "Linfeng Song", "Haitao Mi", "Dong Yu"], "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values", "comment": "15 pages, 4 figures", "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.", "AI": {"tldr": "RLEV\u65b9\u6cd5\u901a\u8fc7\u5c06\u4eba\u7c7b\u5b9a\u4e49\u7684\u4ef7\u503c\u4fe1\u53f7\u76f4\u63a5\u6574\u5408\u5230\u5956\u52b1\u51fd\u6570\u4e2d\uff0c\u6269\u5c55\u4e86RLVR\u6846\u67b6\uff0c\u5728\u591a\u4e2aRL\u7b97\u6cd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u6301\u7eed\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6b63\u786e\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136RLVR\u5728\u5ba2\u89c2\u9886\u57df\u4f7f\u7528\u4e8c\u5143\u6b63\u786e\u6027\u5956\u52b1\u6709\u6548\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5b83\u5ffd\u7565\u4e86\u5e76\u975e\u6240\u6709\u4efb\u52a1\u90fd\u5177\u6709\u540c\u7b49\u91cd\u8981\u6027\u3002\u9700\u8981\u5c06\u4eba\u7c7b\u4ef7\u503c\u4fe1\u53f7\u76f4\u63a5\u7eb3\u5165\u4f18\u5316\u8fc7\u7a0b\u3002", "method": "RLEV\u5728\u5956\u52b1\u51fd\u6570\u4e2d\u76f4\u63a5\u6574\u5408\u4eba\u7c7b\u5b9a\u4e49\u7684\u4ef7\u503c\u4fe1\u53f7\uff0c\u4f7f\u7528\u5e26\u6709\u660e\u786e\u771f\u5b9e\u4ef7\u503c\u6807\u7b7e\u7684\u8003\u8bd5\u98ce\u683c\u6570\u636e\uff0c\u901a\u8fc7\u4ef7\u503c\u52a0\u6743\u68af\u5ea6\u653e\u5927\u673a\u5236\u5b66\u4e60\u4ef7\u503c\u654f\u611f\u7684\u7ec8\u6b62\u7b56\u7565\u3002", "result": "RLEV\u7b56\u7565\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u4ef7\u503c\u52a0\u6743\u51c6\u786e\u7387\uff0c\u8fd8\u5b66\u4f1a\u4e86\u4ef7\u503c\u654f\u611f\u7684\u7ec8\u6b62\u7b56\u7565\uff1a\u5bf9\u4f4e\u4ef7\u503c\u63d0\u793a\u7b80\u6d01\uff0c\u5bf9\u9ad8\u4ef7\u503c\u63d0\u793a\u8be6\u5c3d\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6536\u76ca\u4e0e\u4ef7\u503c\u5bf9\u9f50\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "RLEV\u5728\u566a\u58f0\u4ef7\u503c\u4fe1\u53f7\u4e0b\u4fdd\u6301\u7a33\u5065\uff0c\u8868\u660e\u4f18\u5316\u660e\u786e\u6548\u7528\u51fd\u6570\u4e3a\u5c06LLM\u4e0e\u4eba\u7c7b\u4f18\u5148\u7ea7\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20199", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20199", "abs": "https://arxiv.org/abs/2510.20199", "authors": ["Jane H. Lee", "Baturay Saglam", "Spyridon Pougkakiotis", "Amin Karbasi", "Dionysis Kalogerias"], "title": "Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents", "comment": null, "summary": "Constrained optimization provides a common framework for dealing with\nconflicting objectives in reinforcement learning (RL). In most of these\nsettings, the objectives (and constraints) are expressed though the expected\naccumulated reward. However, this formulation neglects risky or even possibly\ncatastrophic events at the tails of the reward distribution, and is often\ninsufficient for high-stakes applications in which the risk involved in\noutliers is critical. In this work, we propose a framework for risk-aware\nconstrained RL, which exhibits per-stage robustness properties jointly in\nreward values and time using optimized certainty equivalents (OCEs). Our\nframework ensures an exact equivalent to the original constrained problem\nwithin a parameterized strong Lagrangian duality framework under appropriate\nconstraint qualifications, and yields a simple algorithmic recipe which can be\nwrapped around standard RL solvers, such as PPO. Lastly, we establish the\nconvergence of the proposed algorithm under common assumptions, and verify the\nrisk-aware properties of our approach through several numerical experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\u7684\u98ce\u9669\u611f\u77e5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\u5b9e\u73b0\u5956\u52b1\u503c\u548c\u65f6\u95f4\u4e0a\u7684\u8054\u5408\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u5728\u9002\u5f53\u7ea6\u675f\u6761\u4ef6\u4e0b\u4e0e\u539f\u59cb\u7ea6\u675f\u95ee\u9898\u7684\u7cbe\u786e\u7b49\u4ef7\u6027\u3002", "motivation": "\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u671f\u671b\u7d2f\u79ef\u5956\u52b1\u8868\u8fbe\u76ee\u6807\u548c\u7ea6\u675f\uff0c\u4f46\u5ffd\u7565\u4e86\u5956\u52b1\u5206\u5e03\u5c3e\u90e8\u7684\u98ce\u9669\u4e8b\u4ef6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5f02\u5e38\u503c\u98ce\u9669\u7684\u5173\u952e\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\u6784\u5efa\u98ce\u9669\u611f\u77e5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u53c2\u6570\u5316\u5f3a\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u6846\u67b6\u4e0b\u786e\u4fdd\u4e0e\u539f\u59cb\u7ea6\u675f\u95ee\u9898\u7684\u7cbe\u786e\u7b49\u4ef7\u6027\uff0c\u5e76\u56f4\u7ed5\u6807\u51c6RL\u6c42\u89e3\u5668\uff08\u5982PPO\uff09\u63d0\u4f9b\u7b80\u5355\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728\u5e38\u89c1\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684risk-aware\u7279\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u98ce\u9669\u611f\u77e5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u5956\u52b1\u5206\u5e03\u5c3e\u90e8\u7684\u98ce\u9669\u4e8b\u4ef6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20460", "categories": ["cs.CL", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20460", "abs": "https://arxiv.org/abs/2510.20460", "authors": ["Christian Hobelsberger", "Theresa Winner", "Andreas Nawroth", "Oliver Mitevski", "Anna-Carolina Haensch"], "title": "Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models", "comment": null, "summary": "Large language models (LLMs) produce outputs with varying levels of\nuncertainty, and, just as often, varying levels of correctness; making their\npractical reliability far from guaranteed. To quantify this uncertainty, we\nsystematically evaluate four approaches for confidence estimation in LLM\noutputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For\nthe evaluation of the approaches, we conduct experiments on four\nquestion-answering tasks using a state-of-the-art open-source LLM. Our results\nshow that each uncertainty metric captures a different facet of model\nconfidence and that the hybrid CoCoA approach yields the best reliability\noverall, improving both calibration and discrimination of correct answers. We\ndiscuss the trade-offs of each method and provide recommendations for selecting\nuncertainty measures in LLM applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cdLLM\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff1aVCE\u3001MSP\u3001\u6837\u672c\u4e00\u81f4\u6027\u548cCoCoA\uff0c\u5728\u56db\u4e2a\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eCoCoA\u6df7\u5408\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u548c\u7b54\u6848\u533a\u5206\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8eLLM\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6b63\u786e\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u5176\u5b9e\u9645\u53ef\u9760\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u4ee5\u63d0\u5347LLM\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u5728\u56db\u4e2a\u95ee\u7b54\u4efb\u52a1\u4e0a\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5f00\u6e90LLM\uff0c\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cd\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff1aVCE\u3001MSP\u3001\u6837\u672c\u4e00\u81f4\u6027\u548cCoCoA\u3002", "result": "\u6bcf\u79cd\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u6355\u6349\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u6df7\u5408CoCoA\u65b9\u6cd5\u5728\u6574\u4f53\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u9ad8\u4e86\u6821\u51c6\u5ea6\u548c\u6b63\u786e\u7b54\u6848\u7684\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u6743\u8861\uff0c\u5e76\u4e3aLLM\u5e94\u7528\u4e2d\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "topic": "agent analysis"}}
{"id": "2510.20487", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20487", "abs": "https://arxiv.org/abs/2510.20487", "authors": ["Tim Tian Hua", "Andrew Qin", "Samuel Marks", "Neel Nanda"], "title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed", "comment": null, "summary": "Large language models (LLMs) can sometimes detect when they are being\nevaluated and adjust their behavior to appear more aligned, compromising the\nreliability of safety evaluations. In this paper, we show that adding a\nsteering vector to an LLM's activations can suppress evaluation-awareness and\nmake the model act like it is deployed during evaluation. To study our steering\ntechnique, we train an LLM to exhibit evaluation-aware behavior using a\ntwo-step training process designed to mimic how this behavior could emerge\nnaturally. First, we perform continued pretraining on documents with factual\ndescriptions of the model (1) using Python type hints during evaluation but not\nduring deployment and (2) recognizing that the presence of a certain evaluation\ncue always means that it is being tested. Then, we train the model with expert\niteration to use Python type hints in evaluation settings. The resulting model\nis evaluation-aware: it writes type hints in evaluation contexts more than\ndeployment contexts. However, this gap can only be observed by removing the\nevaluation cue. We find that activation steering can suppress evaluation\nawareness and make the model act like it is deployed even when the cue is\npresent. Importantly, we constructed our steering vector using the original\nmodel before our additional training. Our results suggest that AI evaluators\ncould improve the reliability of safety evaluations by steering models to act\nlike they are deployed.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u5f15\u5bfc\u6765\u6291\u5236LLMs\u8bc4\u4f30\u610f\u8bc6\u7684\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u5728\u8bc4\u4f30\u65f6\u8868\u73b0\u5f97\u50cf\u5728\u90e8\u7f72\u73af\u5883\u4e2d\u4e00\u6837\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "LLMs\u6709\u65f6\u80fd\u68c0\u6d4b\u5230\u6b63\u5728\u88ab\u8bc4\u4f30\u5e76\u8c03\u6574\u884c\u4e3a\u4ee5\u663e\u5f97\u66f4\u5bf9\u9f50\uff0c\u8fd9\u4f1a\u635f\u5bb3\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff1a\u9996\u5148\u5728\u5305\u542b\u6a21\u578b\u4e8b\u5b9e\u63cf\u8ff0\u7684\u6587\u6863\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u5bb6\u8fed\u4ee3\u8bad\u7ec3\u6a21\u578b\u5728\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u4f7f\u7528Python\u7c7b\u578b\u63d0\u793a\u3002\u6700\u540e\u4f7f\u7528\u539f\u59cb\u6a21\u578b\u7684\u6fc0\u6d3b\u5411\u91cf\u8fdb\u884c\u5f15\u5bfc\u3002", "result": "\u6fc0\u6d3b\u5411\u91cf\u5f15\u5bfc\u80fd\u591f\u6291\u5236\u8bc4\u4f30\u610f\u8bc6\uff0c\u4f7f\u6a21\u578b\u5728\u8bc4\u4f30\u7ebf\u7d22\u5b58\u5728\u65f6\u8868\u73b0\u5f97\u50cf\u5728\u90e8\u7f72\u73af\u5883\u4e2d\u3002", "conclusion": "AI\u8bc4\u4f30\u8005\u53ef\u4ee5\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u4f7f\u5176\u8868\u73b0\u5f97\u50cf\u5728\u90e8\u7f72\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.20505", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20505", "abs": "https://arxiv.org/abs/2510.20505", "authors": ["Ruiyi Yang", "Hao Xue", "Imran Razzak", "Hakim Hacid", "Flora D. Salim"], "title": "Hierarchical Sequence Iteration for Heterogeneous Question Answering", "comment": "22 pages, 3 figures", "summary": "Retrieval-augmented generation (RAG) remains brittle on multi-step questions\nand heterogeneous evidence sources, trading accuracy against latency and\ntoken/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration\nfor Heterogeneous Question Answering, a unified framework that (i) linearize\ndocuments, tables, and knowledge graphs into a reversible hierarchical sequence\nwith lightweight structural tags, and (ii) perform structure-aware iteration to\ncollect just-enough evidence before answer synthesis. A Head Agent provides\nguidance that leads retrieval, while an Iteration Agent selects and expands\nHSeq via structure-respecting actions (e.g., parent/child hops, table\nrow/column neighbors, KG relations); Finally the head agent composes\ncanonicalized evidence to genearte the final answer, with an optional\nrefinement loop to resolve detected contradictions. Experiments on HotpotQA\n(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1\ngains over strong single-pass, multi-hop, and agentic RAG baselines with high\nefficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic\nunification that enables a single policy to operate across text, tables, and\nKGs without per-dataset specialization; (2) guided, budget-aware iteration that\nreduces unnecessary hops, tool calls, and tokens while preserving accuracy; and\n(3) evidence canonicalization for reliable QA, improving answers consistency\nand auditability.", "AI": {"tldr": "\u63d0\u51faHSEQ\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5e8f\u5217\u7edf\u4e00\u5904\u7406\u6587\u672c\u3001\u8868\u683c\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u7528\u7ed3\u6784\u611f\u77e5\u8fed\u4ee3\u6536\u96c6\u8bc1\u636e\u5e76\u751f\u6210\u7b54\u6848\uff0c\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u591a\u6b65\u95ee\u9898\u548c\u5f02\u6784\u8bc1\u636e\u6e90\u4e0a\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u8d44\u6e90\u9884\u7b97\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u5c06\u6587\u6863\u3001\u8868\u683c\u548c\u77e5\u8bc6\u56fe\u8c31\u7ebf\u6027\u5316\u4e3a\u53ef\u9006\u5206\u5c42\u5e8f\u5217\uff0c\u4f7f\u7528\u5934\u4ee3\u7406\u6307\u5bfc\u68c0\u7d22\uff0c\u8fed\u4ee3\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u64cd\u4f5c\u6269\u5c55\u5e8f\u5217\uff0c\u6700\u540e\u5408\u6210\u89c4\u8303\u5316\u8bc1\u636e\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728HotpotQA\u3001HybridQA/TAT-QA\u548cMetaQA\u4e0a\u76f8\u6bd4\u5355\u6b21\u4f20\u9012\u3001\u591a\u8df3\u548c\u4ee3\u7406RAG\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u7684EM/F1\u63d0\u5347\uff0c\u4e14\u6548\u7387\u9ad8\u3002", "conclusion": "HSEQ\u6846\u67b6\u5b9e\u73b0\u4e86\u683c\u5f0f\u65e0\u5173\u7684\u7edf\u4e00\u5904\u7406\u3001\u9884\u7b97\u611f\u77e5\u7684\u8fed\u4ee3\u548c\u8bc1\u636e\u89c4\u8303\u5316\uff0c\u63d0\u9ad8\u4e86QA\u7684\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.20264", "categories": ["cs.LG", "I.2.6; I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.20264", "abs": "https://arxiv.org/abs/2510.20264", "authors": ["Thomas Rupf", "Marco Bagatella", "Marin Vlastelica", "Andreas Krause"], "title": "Optimistic Task Inference for Behavior Foundation Models", "comment": null, "summary": "Behavior Foundation Models (BFMs) are capable of retrieving high-performing\npolicy for any reward function specified directly at test-time, commonly\nreferred to as zero-shot reinforcement learning (RL). While this is a very\nefficient process in terms of compute, it can be less so in terms of data: as a\nstandard assumption, BFMs require computing rewards over a non-negligible\ninference dataset, assuming either access to a functional form of rewards, or\nsignificant labeling efforts. To alleviate these limitations, we tackle the\nproblem of task inference purely through interaction with the environment at\ntest-time. We propose OpTI-BFM, an optimistic decision criterion that directly\nmodels uncertainty over reward functions and guides BFMs in data collection for\ntask inference. Formally, we provide a regret bound for well-trained BFMs\nthrough a direct connection to upper-confidence algorithms for linear bandits.\nEmpirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and\nobserve that it enables successor-features-based BFMs to identify and optimize\nan unseen reward function in a handful of episodes with minimal compute\noverhead. Code is available at https://github.com/ThomasRupf/opti-bfm.", "AI": {"tldr": "OpTI-BFM\u662f\u4e00\u79cd\u4e50\u89c2\u51b3\u7b56\u51c6\u5219\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u4efb\u52a1\u63a8\u65ad\uff0c\u51cf\u5c11BFMs\u5bf9\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u7684\u6570\u636e\u9700\u6c42", "motivation": "\u884c\u4e3a\u57fa\u7840\u6a21\u578b(BFMs)\u5728\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u4e2d\u9700\u8981\u8ba1\u7b97\u5927\u91cf\u63a8\u7406\u6570\u636e\u96c6\u7684\u5956\u52b1\uff0c\u8fd9\u9700\u8981\u8bbf\u95ee\u5956\u52b1\u51fd\u6570\u5f62\u5f0f\u6216\u5927\u91cf\u6807\u6ce8\u5de5\u4f5c\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e9b\u9650\u5236\uff0c\u7814\u7a76\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u4efb\u52a1\u63a8\u65ad\u7684\u95ee\u9898", "method": "\u63d0\u51faOpTI-BFM\u4e50\u89c2\u51b3\u7b56\u51c6\u5219\uff0c\u76f4\u63a5\u5efa\u6a21\u5956\u52b1\u51fd\u6570\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6307\u5bfcBFMs\u8fdb\u884c\u4efb\u52a1\u63a8\u65ad\u7684\u6570\u636e\u6536\u96c6\u3002\u8be5\u65b9\u6cd5\u4e0e\u7ebf\u6027bandits\u7684\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u6709\u76f4\u63a5\u8054\u7cfb", "result": "\u5728\u5df2\u5efa\u7acb\u7684\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpTI-BFM\u4f7f\u57fa\u4e8e\u540e\u7ee7\u7279\u5f81\u7684BFMs\u80fd\u591f\u5728\u5c11\u91cfepisode\u4e2d\u8bc6\u522b\u548c\u4f18\u5316\u672a\u89c1\u8fc7\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f", "conclusion": "OpTI-BFM\u4e3a\u8bad\u7ec3\u826f\u597d\u7684BFMs\u63d0\u4f9b\u4e86\u9057\u61be\u754c\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u9ad8\u6548\u4efb\u52a1\u63a8\u65ad", "topic": "agentic reinforcement learning"}}
{"id": "2510.20270", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20270", "abs": "https://arxiv.org/abs/2510.20270", "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"], "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "comment": null, "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.", "AI": {"tldr": "ImpossibleBench\u662f\u4e00\u4e2a\u7528\u4e8e\u91cf\u5316\u3001\u7814\u7a76\u548c\u7f13\u89e3LLM\u4ee3\u7406\u5229\u7528\u6d4b\u8bd5\u7528\u4f8b\u4f5c\u5f0a\u884c\u4e3a\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u4e0e\u5355\u5143\u6d4b\u8bd5\u51b2\u7a81\u7684\"\u4e0d\u53ef\u80fd\"\u4efb\u52a1\u6765\u6d4b\u91cf\u4f5c\u5f0a\u7387\u3002", "motivation": "LLM\u4ee3\u7406\u503e\u5411\u4e8e\u5bfb\u627e\u548c\u5229\u7528\"\u6377\u5f84\"\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd9\u79cd\u884c\u4e3a\u4f1a\u7834\u574f\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u7684\u6709\u6548\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u9760\u6027\uff0c\u6bd4\u5982\u5220\u9664\u5931\u8d25\u6d4b\u8bd5\u800c\u975e\u4fee\u590dbug\u3002", "method": "\u4ece\u73b0\u6709\u57fa\u51c6\uff08\u5982LiveCodeBench\u548cSWE-bench\uff09\u521b\u5efa\"\u4e0d\u53ef\u80fd\"\u4efb\u52a1\u53d8\u4f53\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u548c\u5355\u5143\u6d4b\u8bd5\u4e4b\u95f4\u5f15\u5165\u76f4\u63a5\u51b2\u7a81\uff0c\u6d4b\u91cf\u4ee3\u7406\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u901a\u8fc7\u7387\u4f5c\u4e3a\"\u4f5c\u5f0a\u7387\"\u3002", "result": "\u63ed\u793a\u4e86\u4ece\u7b80\u5355\u6d4b\u8bd5\u4fee\u6539\u5230\u590d\u6742\u8fd0\u7b97\u7b26\u91cd\u8f7d\u7684\u4f5c\u5f0a\u884c\u4e3a\u7ec6\u8282\uff0c\u5c55\u793a\u4e86\u63d0\u793a\u3001\u6d4b\u8bd5\u8bbf\u95ee\u548c\u53cd\u9988\u5faa\u73af\u5982\u4f55\u5f71\u54cd\u4f5c\u5f0a\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u5df2\u9a8c\u8bc1\u7684\u6b3a\u9a97\u89e3\u51b3\u65b9\u6848\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "ImpossibleBench\u662f\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u66f4\u7a33\u5065\u53ef\u9760\u7684LLM\u7cfb\u7edf\uff0c\u7814\u7a76\u6a21\u578b\u884c\u4e3a\u3001\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u5f00\u53d1\u76d1\u63a7\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2510.20584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20584", "abs": "https://arxiv.org/abs/2510.20584", "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi"], "title": "Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks", "comment": "38 pages, 4 figures", "summary": "Assessing communication and collaboration at scale depends on a labor\nintensive task of coding communication data into categories according to\ndifferent frameworks. Prior research has established that ChatGPT can be\ndirectly instructed with coding rubrics to code the communication data and\nachieves accuracy comparable to human raters. However, whether the coding from\nChatGPT or similar AI technology exhibits bias against different demographic\ngroups, such as gender and race, remains unclear. To fill this gap, this paper\ninvestigates ChatGPT-based automated coding of communication data using a\ntypical coding framework for collaborative problem solving, examining\ndifferences across gender and racial groups. The analysis draws on data from\nthree types of collaborative tasks: negotiation, problem solving, and decision\nmaking. Our results show that ChatGPT-based coding exhibits no significant bias\nacross gender and racial groups, paving the road for its adoption in\nlarge-scale assessment of collaboration and communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86ChatGPT\u5728\u6c9f\u901a\u6570\u636e\u81ea\u52a8\u7f16\u7801\u4e2d\u662f\u5426\u5b58\u5728\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\uff0c\u53d1\u73b0\u5728\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u4e2d\u65e0\u663e\u8457\u504f\u89c1\u3002", "motivation": "\u586b\u8865AI\u6280\u672f\u5728\u6c9f\u901a\u6570\u636e\u7f16\u7801\u4e2d\u5bf9\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\uff08\u5982\u6027\u522b\u548c\u79cd\u65cf\uff09\u662f\u5426\u5b58\u5728\u504f\u89c1\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528ChatGPT\u57fa\u4e8e\u5178\u578b\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u7f16\u7801\u6846\u67b6\u5bf9\u4e09\u79cd\u534f\u4f5c\u4efb\u52a1\uff08\u8c08\u5224\u3001\u95ee\u9898\u89e3\u51b3\u3001\u51b3\u7b56\u5236\u5b9a\uff09\u7684\u6c9f\u901a\u6570\u636e\u8fdb\u884c\u81ea\u52a8\u7f16\u7801\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u7684\u5dee\u5f02\u3002", "result": "ChatGPT\u57fa\u7840\u7684\u7f16\u7801\u5728\u4e0d\u540c\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u95f4\u672a\u8868\u73b0\u51fa\u663e\u8457\u504f\u89c1\u3002", "conclusion": "ChatGPT\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u534f\u4f5c\u548c\u6c9f\u901a\u8bc4\u4f30\uff0c\u4e3a\u91c7\u7528AI\u6280\u672f\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.20369", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20369", "abs": "https://arxiv.org/abs/2510.20369", "authors": ["Zhenghao Xu", "Qin Lu", "Qingru Zhang", "Liang Qiu", "Ilgee Hong", "Changlong Yu", "Wenlin Yao", "Yao Liu", "Haoming Jiang", "Lihong Li", "Hyokun Yun", "Tuo Zhao"], "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain", "comment": "NeurIPS 2025, 18 pages", "summary": "Reward model (RM) plays a pivotal role in reinforcement learning with human\nfeedback (RLHF) for aligning large language models (LLMs). However, classical\nRMs trained on human preferences are vulnerable to reward hacking and\ngeneralize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM\njudges equipped with reasoning capabilities demonstrate superior\ngeneralization, even without additional training, but incur significantly\nhigher inference costs, limiting their applicability in online RLHF. In this\nwork, we propose an uncertainty-based routing framework that efficiently\ncomplements a fast RM with a strong but costly LLM judge. Our approach\nformulates advantage estimation in policy gradient (PG) methods as pairwise\npreference classification, enabling principled uncertainty quantification to\nguide routing. Uncertain pairs are forwarded to the LLM judge, while confident\nones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our\nuncertainty-based routing strategy significantly outperforms random judge\ncalling at the same cost, and downstream alignment results showcase its\neffectiveness in improving online RLHF.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u8def\u7531\u6846\u67b6\uff0c\u5c06\u5feb\u901f\u5956\u52b1\u6a21\u578b\u4e0e\u5f3a\u5927\u4f46\u6602\u8d35\u7684LLM\u6cd5\u5b98\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6307\u5bfc\u8def\u7531\u51b3\u7b56\uff0c\u5728\u76f8\u540c\u6210\u672c\u4e0b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u6cd5\u5b98\u8c03\u7528\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u653b\u51fb\u4e14\u5728\u5206\u5e03\u5916\u8f93\u5165\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u5f3a\u5927\u7684LLM\u6cd5\u5b98\u867d\u7136\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u5728\u7ebfRLHF\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u8def\u7531\u6846\u67b6\uff0c\u5c06\u4f18\u52bf\u4f30\u8ba1\u5efa\u6a21\u4e3a\u6210\u5bf9\u504f\u597d\u5206\u7c7b\uff0c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u4e0d\u786e\u5b9a\u7684\u6837\u672c\u5bf9\u8f6c\u53d1\u7ed9LLM\u6cd5\u5b98\uff0c\u786e\u5b9a\u7684\u6837\u672c\u5bf9\u7531\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5728\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u6210\u672c\u4e0b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u6cd5\u5b98\u8c03\u7528\uff0c\u4e0b\u6e38\u5bf9\u9f50\u7ed3\u679c\u5c55\u793a\u4e86\u5176\u5728\u6539\u8fdb\u5728\u7ebfRLHF\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u8def\u7531\u7b56\u7565\u80fd\u591f\u6709\u6548\u7ed3\u5408\u5feb\u901f\u5956\u52b1\u6a21\u578b\u548c\u5f3a\u5927LLM\u6cd5\u5b98\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347RLHF\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20408", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20408", "abs": "https://arxiv.org/abs/2510.20408", "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"], "title": "Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control", "comment": "Preprint (submitted version) to be presented at the 13th\n  International Conference on Industrial Engineering and Applications\n  (ICIEA-EU), Milan, 2026. The final Version of Record will appear in the\n  official conference proceedings", "summary": "Autonomous control of multi-stage industrial processes requires both local\nspecialization and global coordination. Reinforcement learning (RL) offers a\npromising approach, but its industrial adoption remains limited due to\nchallenges such as reward design, modularity, and action space management. Many\nacademic benchmarks differ markedly from industrial control problems, limiting\ntheir transferability to real-world applications. This study introduces an\nenhanced industry-inspired benchmark environment that combines tasks from two\nexisting benchmarks, SortingEnv and ContainerGym, into a sequential recycling\nscenario with sorting and pressing operations. We evaluate two control\nstrategies: a modular architecture with specialized agents and a monolithic\nagent governing the full system, while also analyzing the impact of action\nmasking. Our experiments show that without action masking, agents struggle to\nlearn effective policies, with the modular architecture performing better. When\naction masking is applied, both architectures improve substantially, and the\nperformance gap narrows considerably. These results highlight the decisive role\nof action space constraints and suggest that the advantages of specialization\ndiminish as action complexity is reduced. The proposed benchmark thus provides\na valuable testbed for exploring practical and robust multi-agent RL solutions\nin industrial automation, while contributing to the ongoing debate on\ncentralization versus specialization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de5\u4e1a\u542f\u53d1\u7684\u57fa\u51c6\u73af\u5883\uff0c\u7ed3\u5408\u4e86\u6392\u5e8f\u548c\u538b\u7f29\u64cd\u4f5c\u7684\u987a\u5e8f\u56de\u6536\u573a\u666f\uff0c\u8bc4\u4f30\u4e86\u6a21\u5757\u5316\u67b6\u6784\u548c\u5355\u4f53\u4ee3\u7406\u4e24\u79cd\u63a7\u5236\u7b56\u7565\uff0c\u53d1\u73b0\u52a8\u4f5c\u63a9\u7801\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u63a7\u5236\u9700\u8981\u5c40\u90e8\u4e13\u4e1a\u5316\u548c\u5168\u5c40\u534f\u8c03\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u91c7\u7528\u4ecd\u9762\u4e34\u5956\u52b1\u8bbe\u8ba1\u3001\u6a21\u5757\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u7ba1\u7406\u7b49\u6311\u6218\u3002\u73b0\u6709\u5b66\u672f\u57fa\u51c6\u4e0e\u5de5\u4e1a\u63a7\u5236\u95ee\u9898\u5dee\u5f02\u8f83\u5927\uff0c\u9650\u5236\u4e86\u5411\u5b9e\u9645\u5e94\u7528\u7684\u8f6c\u79fb\u3002", "method": "\u5c06SortingEnv\u548cContainerGym\u4e24\u4e2a\u73b0\u6709\u57fa\u51c6\u7684\u4efb\u52a1\u7ed3\u5408\uff0c\u521b\u5efa\u987a\u5e8f\u56de\u6536\u573a\u666f\u3002\u8bc4\u4f30\u6a21\u5757\u5316\u67b6\u6784\uff08\u4e13\u4e1a\u5316\u4ee3\u7406\uff09\u548c\u5355\u4f53\u4ee3\u7406\uff08\u5168\u7cfb\u7edf\u63a7\u5236\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u5e76\u5206\u6790\u52a8\u4f5c\u63a9\u7801\u7684\u5f71\u54cd\u3002", "result": "\u65e0\u52a8\u4f5c\u63a9\u7801\u65f6\uff0c\u4ee3\u7406\u96be\u4ee5\u5b66\u4e60\u6709\u6548\u7b56\u7565\uff0c\u6a21\u5757\u5316\u67b6\u6784\u8868\u73b0\u66f4\u597d\u3002\u5e94\u7528\u52a8\u4f5c\u63a9\u7801\u540e\uff0c\u4e24\u79cd\u67b6\u6784\u5747\u6709\u663e\u8457\u6539\u5584\uff0c\u6027\u80fd\u5dee\u8ddd\u5927\u5e45\u7f29\u5c0f\u3002", "conclusion": "\u52a8\u4f5c\u7a7a\u95f4\u7ea6\u675f\u5177\u6709\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u4e13\u4e1a\u5316\u4f18\u52bf\u968f\u7740\u52a8\u4f5c\u590d\u6742\u5ea6\u964d\u4f4e\u800c\u51cf\u5f31\u3002\u8be5\u57fa\u51c6\u4e3a\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2510.20413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20413", "abs": "https://arxiv.org/abs/2510.20413", "authors": ["Aditya Gopalan", "Sayak Ray Chowdhury", "Debangshu Banerjee"], "title": "Why DPO is a Misspecified Estimator and How to Fix It", "comment": null, "summary": "Direct alignment algorithms such as Direct Preference Optimization (DPO)\nfine-tune models based on preference data, using only supervised learning\ninstead of two-stage reinforcement learning with human feedback (RLHF). We show\nthat DPO encodes a statistical estimation problem over reward functions induced\nby a parametric policy class. When the true reward function that generates\npreferences cannot be realized via the policy class, DPO becomes misspecified,\nresulting in failure modes such as preference order reversal, worsening of\npolicy reward, and high sensitivity to the input preference data distribution.\nOn the other hand, we study the local behavior of two-stage RLHF for a\nparametric class and relate it to a natural gradient step in policy space. Our\nfine-grained geometric characterization allows us to propose AuxDPO, which\nintroduces additional auxiliary variables in the DPO loss function to help move\ntowards the RLHF solution in a principled manner and mitigate the\nmisspecification in DPO. We empirically demonstrate the superior performance of\nAuxDPO on didactic bandit settings as well as LLM alignment tasks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86DPO\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5f53\u771f\u5b9e\u5956\u52b1\u51fd\u6570\u65e0\u6cd5\u901a\u8fc7\u7b56\u7565\u7c7b\u5b9e\u73b0\u65f6\uff0cDPO\u4f1a\u51fa\u73b0\u9519\u8bef\u8bbe\u5b9a\u95ee\u9898\uff0c\u5bfc\u81f4\u504f\u597d\u987a\u5e8f\u53cd\u8f6c\u3001\u7b56\u7565\u5956\u52b1\u6076\u5316\u7b49\u5931\u8d25\u6a21\u5f0f\u3002\u4f5c\u8005\u63d0\u51fa\u4e86AuxDPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728DPO\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u7814\u7a76DPO\u7b97\u6cd5\u5728\u5956\u52b1\u51fd\u6570\u65e0\u6cd5\u901a\u8fc7\u7b56\u7565\u7c7b\u5b9e\u73b0\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u6539\u8fdbDPO\u4ee5\u66f4\u597d\u5730\u903c\u8fd1RLHF\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5206\u6790DPO\u7684\u7edf\u8ba1\u4f30\u8ba1\u95ee\u9898\uff1b2. \u7814\u7a76\u4e24\u9636\u6bb5RLHF\u7684\u5c40\u90e8\u884c\u4e3a\uff1b3. \u63d0\u51faAuxDPO\u65b9\u6cd5\uff0c\u5728DPO\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\uff1b4. \u5728bandit\u8bbe\u7f6e\u548cLLM\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eAuxDPO\u5728didactic bandit\u8bbe\u7f6e\u548cLLM\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6DPO\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3DPO\u7684\u9519\u8bef\u8bbe\u5b9a\u95ee\u9898\u3002", "conclusion": "AuxDPO\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\uff0c\u80fd\u591f\u4ee5\u539f\u5219\u6027\u7684\u65b9\u5f0f\u5e2e\u52a9DPO\u5411RLHF\u89e3\u51b3\u65b9\u6848\u79fb\u52a8\uff0c\u7f13\u89e3\u4e86DPO\u7684\u9519\u8bef\u8bbe\u5b9a\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20542", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20542", "abs": "https://arxiv.org/abs/2510.20542", "authors": ["Jacopo Di Ventura", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "A Unified Framework for Zero-Shot Reinforcement Learning", "comment": null, "summary": "Zero-shot reinforcement learning (RL) has emerged as a setting for developing\ngeneral agents in an unsupervised manner, capable of solving downstream tasks\nwithout additional training or planning at test-time. Unlike conventional RL,\nwhich optimizes policies for a fixed reward, zero-shot RL requires agents to\nencode representations rich enough to support immediate adaptation to any\nobjective, drawing parallels to vision and language foundation models. Despite\ngrowing interest, the field lacks a common analytical lens.\n  We present the first unified framework for zero-shot RL. Our formulation\nintroduces a consistent notation and taxonomy that organizes existing\napproaches and allows direct comparison between them. Central to our framework\nis the classification of algorithms into two families: direct representations,\nwhich learn end-to-end mappings from rewards to policies, and compositional\nrepresentations, which decompose the representation leveraging the substructure\nof the value function. Within this framework, we highlight shared principles\nand key differences across methods, and we derive an extended bound for\nsuccessor-feature methods, offering a new perspective on their performance in\nthe zero-shot regime. By consolidating existing work under a common lens, our\nframework provides a principled foundation for future research in zero-shot RL\nand outlines a clear path toward developing more general agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5f15\u5165\u4e00\u81f4\u7684\u7b26\u53f7\u548c\u5206\u7c7b\u6cd5\u6765\u7ec4\u7ec7\u73b0\u6709\u65b9\u6cd5\u5e76\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u3002", "motivation": "\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u65b0\u5174\u9886\u57df\uff0c\u7f3a\u4e4f\u5171\u540c\u7684\u5206\u6790\u6846\u67b6\uff0c\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u5c06\u7b97\u6cd5\u5206\u4e3a\u4e24\u7c7b\uff1a\u76f4\u63a5\u8868\u793a\uff08\u5b66\u4e60\u4ece\u5956\u52b1\u5230\u7b56\u7565\u7684\u7aef\u5230\u7aef\u6620\u5c04\uff09\u548c\u7ec4\u5408\u8868\u793a\uff08\u5229\u7528\u4ef7\u503c\u51fd\u6570\u5b50\u7ed3\u6784\u5206\u89e3\u8868\u793a\uff09\uff0c\u5e76\u5728\u8be5\u6846\u67b6\u4e0b\u63a8\u5bfc\u540e\u7ee7\u7279\u5f81\u65b9\u6cd5\u7684\u6269\u5c55\u8fb9\u754c\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u95f4\u7684\u5171\u540c\u539f\u5219\u548c\u5173\u952e\u5dee\u5f02\uff0c\u4e3a\u540e\u7ee7\u7279\u5f81\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u673a\u5236\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u5f00\u53d1\u66f4\u901a\u7528\u667a\u80fd\u4f53\u7684\u6e05\u6670\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20609", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.LG, cs.IR, cs.SE, cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20609", "abs": "https://arxiv.org/abs/2510.20609", "authors": ["Timur Galimzyanov", "Olga Kolomyttseva", "Egor Bogomolov"], "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets", "comment": null, "summary": "We study retrieval design for code-focused generation tasks under realistic\ncompute budgets. Using two complementary tasks from Long Code Arena -- code\ncompletion and bug localization -- we systematically compare retrieval\nconfigurations across various context window sizes along three axes: (i)\nchunking strategy, (ii) similarity scoring, and (iii) splitting granularity.\n(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and\npractical, significantly outperforming dense alternatives while being an order\nof magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3\nfamily) consistently beat sparse retrievers, however requiring 100x larger\nlatency. (3) Optimal chunk size scales with available context: 32-64 line\nchunks work best at small budgets, and whole-file retrieval becomes competitive\nat 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting\nacross budgets. (5) Retrieval latency varies by up to 200x across\nconfigurations; BPE-based splitting is needlessly slow, and BM25 + word\nsplitting offers the best quality-latency trade-off. Thus, we provide\nevidence-based recommendations for implementing effective code-oriented RAG\nsystems based on task requirements, model constraints, and computational\nefficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u68c0\u7d22\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u68c0\u7d22\u914d\u7f6e\u5728\u4ee3\u7801\u8865\u5168\u548cbug\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0BM25+\u8bcd\u7ea7\u5206\u5272\u5728PL-PL\u4efb\u52a1\u4e2d\u6700\u6709\u6548\uff0c\u800c\u4e13\u6709\u5bc6\u96c6\u7f16\u7801\u5668\u5728NL-PL\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u4f46\u5ef6\u8fdf\u9ad8\u3002", "motivation": "\u7814\u7a76\u5728\u73b0\u5b9e\u8ba1\u7b97\u9884\u7b97\u4e0b\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u68c0\u7d22\u8bbe\u8ba1\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u7684\u4ee3\u7801\u5bfc\u5411RAG\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528Long Code Arena\u4e2d\u7684\u4ee3\u7801\u8865\u5168\u548cbug\u5b9a\u4f4d\u4efb\u52a1\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u4e0b\u7684\u68c0\u7d22\u914d\u7f6e\uff0c\u5305\u62ec\u5206\u5757\u7b56\u7565\u3001\u76f8\u4f3c\u6027\u8bc4\u5206\u548c\u5206\u5272\u7c92\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u3002", "result": "BM25+\u8bcd\u7ea7\u5206\u5272\u5728PL-PL\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u4e14\u901f\u5ea6\u5feb\uff1b\u4e13\u6709\u5bc6\u96c6\u7f16\u7801\u5668\u5728NL-PL\u4efb\u52a1\u4e2d\u6548\u679c\u66f4\u597d\u4f46\u5ef6\u8fdf\u9ad8100\u500d\uff1b\u6700\u4f73\u5206\u5757\u5927\u5c0f\u968f\u53ef\u7528\u4e0a\u4e0b\u6587\u6269\u5c55\uff1b\u57fa\u4e8e\u884c\u7684\u5206\u5757\u4e0e\u8bed\u6cd5\u611f\u77e5\u5206\u5272\u6548\u679c\u76f8\u5f53\uff1b\u68c0\u7d22\u5ef6\u8fdf\u5dee\u5f02\u53ef\u8fbe200\u500d\u3002", "conclusion": "\u4e3a\u4ee3\u7801\u5bfc\u5411RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4efb\u52a1\u9700\u6c42\u3001\u6a21\u578b\u7ea6\u675f\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5b9e\u8bc1\u5efa\u8bae\u3002", "topic": "code agent"}}
{"id": "2510.20725", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20725", "abs": "https://arxiv.org/abs/2510.20725", "authors": ["Jasmine Bayrooti", "Sattar Vakili", "Amanda Prorok", "Carl Henrik Ek"], "title": "No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes", "comment": "Appearing in NeurIPS, 2025", "summary": "Thompson sampling (TS) is a powerful and widely used strategy for sequential\ndecision-making, with applications ranging from Bayesian optimization to\nreinforcement learning (RL). Despite its success, the theoretical foundations\nof TS remain limited, particularly in settings with complex temporal structure\nsuch as RL. We address this gap by establishing no-regret guarantees for TS\nusing models with Gaussian marginal distributions. Specifically, we consider TS\nin episodic RL with joint Gaussian process (GP) priors over rewards and\ntransitions. We prove a regret bound of\n$\\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$ over $K$ episodes of horizon $H$,\nwhere $\\Gamma(\\cdot)$ captures the complexity of the GP model. Our analysis\naddresses several challenges, including the non-Gaussian nature of value\nfunctions and the recursive structure of Bellman updates, and extends classical\ntools such as the elliptical potential lemma to multi-output settings. This\nwork advances the understanding of TS in RL and highlights how structural\nassumptions and model uncertainty shape its performance in finite-horizon\nMarkov Decision Processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3aThompson\u91c7\u6837\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u5177\u6709\u9ad8\u65af\u8fb9\u9645\u5206\u5e03\u7684\u6a21\u578b\u7684\u65e0\u6094\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u5728\u5177\u6709\u8054\u5408\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u7684episodic RL\u4e2d\u7684\u9057\u61be\u754c\u3002", "motivation": "Thompson\u91c7\u6837\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u7b49\u5177\u6709\u590d\u6742\u65f6\u95f4\u7ed3\u6784\u7684\u573a\u666f\u4e2d\u7406\u8bba\u57fa\u7840\u6709\u9650\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5177\u6709\u8054\u5408\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u7684episodic\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5206\u6790Thompson\u91c7\u6837\u5728\u5177\u6709\u9ad8\u65af\u8fb9\u9645\u5206\u5e03\u7684\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u8bc1\u660e\u4e86\u5728K\u4e2aepisode\u3001\u6bcf\u4e2aepisode\u957f\u5ea6\u4e3aH\u7684RL\u4e2d\uff0cThompson\u91c7\u6837\u7684\u9057\u61be\u754c\u4e3a$\\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$\uff0c\u5176\u4e2d$\\Gamma(\\cdot)$\u8868\u793aGP\u6a21\u578b\u7684\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9Thompson\u91c7\u6837\u5728RL\u4e2d\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5047\u8bbe\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5982\u4f55\u5f71\u54cd\u5176\u5728\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.20733", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20733", "abs": "https://arxiv.org/abs/2510.20733", "authors": ["Yujia Zheng", "Zhuokai Zhao", "Zijian Li", "Yaqi Xie", "Mingze Gao", "Lizhu Zhang", "Kun Zhang"], "title": "Thought Communication in Multiagent Collaboration", "comment": "NeurIPS 2025 Spotlight", "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u601d\u7ef4\u901a\u4fe1\"\u7684\u65b0\u8303\u5f0f\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u76f4\u63a5\u8fdb\u884c\u601d\u7ef4\u5c42\u9762\u7684\u4ea4\u6d41\uff0c\u8d85\u8d8a\u81ea\u7136\u8bed\u8a00\u7684\u9650\u5236\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u4e3a\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5171\u4eab\u548c\u79c1\u6709\u601d\u7ef4\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u63d0\u53d6\u548c\u5206\u914d\u76f8\u5173\u601d\u7ef4\u7684\u5b9e\u9645\u6846\u67b6\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u3001\u6b67\u4e49\u548c\u95f4\u63a5\u6027\u7b49\u9650\u5236\uff0c\u963b\u788d\u4e86\u96c6\u4f53\u667a\u80fd\u7684\u6f5c\u529b\u3002\u867d\u7136\u673a\u5668\u4e0d\u53d7\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u5927\u591a\u6570\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4ecd\u4ec5\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u3002", "method": "\u5c06\u601d\u7ef4\u901a\u4fe1\u5f62\u5f0f\u5316\u4e3a\u4e00\u822c\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff0c\u8bc1\u660e\u5728\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5171\u4eab\u548c\u79c1\u6709\u601d\u7ef4\u7684\u53ef\u8bc6\u522b\u6027\u3002\u5f00\u53d1\u4e86\u4ece\u6240\u6709\u4ee3\u7406\u63d0\u53d6\u6f5c\u5728\u601d\u7ef4\u5e76\u5728\u901a\u4fe1\u524d\u5206\u914d\u76f8\u5173\u601d\u7ef4\u53ca\u5176\u5171\u4eab\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5e76\u8bc1\u660e\u4e86\u601d\u7ef4\u901a\u4fe1\u5728\u534f\u4f5c\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u601d\u7ef4\u901a\u4fe1\u8303\u5f0f\u8d85\u8d8a\u4e86\u8bed\u8a00\u9650\u5236\uff0c\u80fd\u591f\u5229\u7528\u9690\u85cf\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u4e3a\u96c6\u4f53\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.def7bd6c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpaperclover.net%2Fblog%2Fwebdev%2Fone-year-next-app-router%3Futm_source=tldrwebdev/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/XEnZvKEqKkkU6IMu69vMRdH8Vb2K1mGU6dKCN6J4H10=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpaperclover.net%2Fblog%2Fwebdev%2Fone-year-next-app-router%3Futm_source=tldrwebdev/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/XEnZvKEqKkkU6IMu69vMRdH8Vb2K1mGU6dKCN6J4H10=428", "authors": ["TLDR Newsletter"], "title": "One Year with Next.js App Router \u2014 Why We're Moving On", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpaperclover.net%2Fblog%2Fwebdev%2Fone-year-next-app-router%3Futm_source=tldrwebdev/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/XEnZvKEqKkkU6IMu69vMRdH8Vb2K1mGU6dKCN6J4H10=428", "summary": "One Year with Next.js App Router \u2014 Why We're Moving On (22 minute read) Next.js's App Router and React Server Components don't have the ability to perform optimistic updates and redundant data fetching. As a result, this team migrated their frontend from Next.js to TanStack Start, a move that simplified their code, improved performance, and reduced costs.", "source": "tldr", "AI": {"tldr": "\u56e2\u961f\u4eceNext.js\u8fc1\u79fb\u5230TanStack Start\uff0c\u56e0\u4e3aNext.js\u7684App Router\u548cReact Server Components\u65e0\u6cd5\u8fdb\u884c\u4e50\u89c2\u66f4\u65b0\u548c\u907f\u514d\u5197\u4f59\u6570\u636e\u83b7\u53d6\uff0c\u8fc1\u79fb\u540e\u4ee3\u7801\u66f4\u7b80\u5316\u3001\u6027\u80fd\u63d0\u5347\u3001\u6210\u672c\u964d\u4f4e\u3002", "motivation": "Next.js\u7684App Router\u548cReact Server Components\u5b58\u5728\u529f\u80fd\u9650\u5236\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e50\u89c2\u66f4\u65b0\u548c\u907f\u514d\u5197\u4f59\u6570\u636e\u83b7\u53d6\uff0c\u5f71\u54cd\u4e86\u5f00\u53d1\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u5c06\u524d\u7aef\u4eceNext.js\u8fc1\u79fb\u5230TanStack Start\u6846\u67b6\uff0c\u5229\u7528\u5176\u66f4\u597d\u7684\u6570\u636e\u7ba1\u7406\u80fd\u529b\u3002", "result": "\u8fc1\u79fb\u540e\u4ee3\u7801\u7ed3\u6784\u66f4\u7b80\u5316\uff0c\u5e94\u7528\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u4e50\u89c2\u66f4\u65b0\u548c\u9ad8\u6548\u6570\u636e\u7ba1\u7406\u7684\u5e94\u7528\u573a\u666f\uff0cTanStack Start\u662f\u6bd4Next.js\u66f4\u597d\u7684\u9009\u62e9\u3002", "topic": "swe application"}}
{"id": "tldr.2510.758db3b4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Dev23/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/35zg7GiMbm_II53QjjMfWdkYjKDSYueVi7CvF_h6R2o=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Dev23/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/35zg7GiMbm_II53QjjMfWdkYjKDSYueVi7CvF_h6R2o=428", "authors": ["TLDR Newsletter"], "title": "Evolving the internal developer portal for the age of agentic engineering", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Dev23/1/0100019a10ced8cc-7746fbf3-b81e-48e1-af70-79b60039aa0f-000000/35zg7GiMbm_II53QjjMfWdkYjKDSYueVi7CvF_h6R2o=428", "summary": "Evolving the internal developer portal for the age of agentic engineering (Sponsor) When you unleash AI into your SDLC without guardrails or workflows that keep it in check, your engineering chaos turns into agentic chaos. The internal developer portal needs to be reimagined as an agentic engineering enabler. Come build the future of software engineering culture with Port.", "source": "tldr", "AI": {"tldr": "\u91cd\u65b0\u6784\u60f3\u5185\u90e8\u5f00\u53d1\u8005\u95e8\u6237\u4f5c\u4e3aAI\u4ee3\u7406\u5de5\u7a0b\u63a8\u52a8\u8005\uff0c\u9632\u6b62AI\u5728SDLC\u4e2d\u5f15\u53d1\u6df7\u4e71", "motivation": "\u5728SDLC\u4e2d\u5f15\u5165AI\u4ee3\u7406\u65f6\uff0c\u7f3a\u4e4f\u62a4\u680f\u548c\u5de5\u4f5c\u6d41\u4f1a\u5bfc\u81f4\u5de5\u7a0b\u6df7\u4e71\u8f6c\u53d8\u4e3a\u4ee3\u7406\u6df7\u4e71\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u5185\u90e8\u5f00\u53d1\u8005\u95e8\u6237", "method": "\u5c06\u5185\u90e8\u5f00\u53d1\u8005\u95e8\u6237\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u4ee3\u7406\u5de5\u7a0b\u63a8\u52a8\u8005\uff0c\u5efa\u7acb\u9002\u5f53\u7684\u62a4\u680f\u548c\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406AI\u4ee3\u7406", "result": "\u63d0\u51fa\u4e86Port\u5e73\u53f0\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u6784\u5efa\u8f6f\u4ef6\u5de5\u7a0b\u6587\u5316\u7684\u672a\u6765", "conclusion": "\u5185\u90e8\u5f00\u53d1\u8005\u95e8\u6237\u9700\u8981\u6f14\u8fdb\u4ee5\u652f\u6301\u4ee3\u7406\u5de5\u7a0b\u65f6\u4ee3\uff0cPort\u5e73\u53f0\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5b9e\u73b0\u8def\u5f84", "topic": "swe application"}}
{"id": "tldr.2510.6add3794", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fopenai-has-introduced-chatgpt-atlas-a-web-browser-with-a-built-in-ai-assistant%2F%3Futm_source=tldrdesign/1/0100019a10f6c8d9-7d51bba0-c06e-49f4-b332-6a78574fe1f1-000000/IbcbRXYM2e7ZjXgJh9r4uFw4mox3WHP_NMdqb0CparU=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fopenai-has-introduced-chatgpt-atlas-a-web-browser-with-a-built-in-ai-assistant%2F%3Futm_source=tldrdesign/1/0100019a10f6c8d9-7d51bba0-c06e-49f4-b332-6a78574fe1f1-000000/IbcbRXYM2e7ZjXgJh9r4uFw4mox3WHP_NMdqb0CparU=428", "authors": ["TLDR Newsletter"], "title": "OpenAI has introduced ChatGPT Atlas \u2013 a web browser with a built-in AI assistant", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fopenai-has-introduced-chatgpt-atlas-a-web-browser-with-a-built-in-ai-assistant%2F%3Futm_source=tldrdesign/1/0100019a10f6c8d9-7d51bba0-c06e-49f4-b332-6a78574fe1f1-000000/IbcbRXYM2e7ZjXgJh9r4uFw4mox3WHP_NMdqb0CparU=428", "summary": "OpenAI has introduced ChatGPT Atlas \u2013 a web browser with a built-in AI assistant (1 minute read) OpenAI launched ChatGPT Atlas, a web browser that integrates ChatGPT directly into the experience, combining search, AI, and personal context. Key features include memory to recall past interactions and website context, and agent mode to perform tasks like placing orders or preparing reports without leaving the page. It is now available on macOS, with other platforms coming soon.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51faChatGPT Atlas\u6d4f\u89c8\u5668\uff0c\u5185\u7f6eAI\u52a9\u624b\uff0c\u6574\u5408\u641c\u7d22\u3001AI\u548c\u4e2a\u4eba\u4e0a\u4e0b\u6587\uff0c\u5177\u6709\u8bb0\u5fc6\u529f\u80fd\u548c\u4ee3\u7406\u6a21\u5f0f\uff0c\u652f\u6301macOS\u5e73\u53f0", "motivation": "\u5c06AI\u52a9\u624b\u76f4\u63a5\u96c6\u6210\u5230\u6d4f\u89c8\u5668\u4f53\u9a8c\u4e2d\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u641c\u7d22\u548c\u4efb\u52a1\u6267\u884c\u529f\u80fd", "method": "\u5f00\u53d1\u5185\u7f6eChatGPT\u7684\u7f51\u9875\u6d4f\u89c8\u5668\uff0c\u5305\u542b\u8bb0\u5fc6\u529f\u80fd\u548c\u4ee3\u7406\u6a21\u5f0f", "result": "\u6210\u529f\u63a8\u51faChatGPT Atlas\u6d4f\u89c8\u5668\uff0c\u73b0\u5df2\u5728macOS\u5e73\u53f0\u53ef\u7528", "conclusion": "ChatGPT Atlas\u901a\u8fc7\u6574\u5408AI\u52a9\u624b\u5230\u6d4f\u89c8\u5668\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2510.9c683695", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.trailofbits.com%2F2025%2F10%2F22%2Fprompt-injection-to-rce-in-ai-agents%2F%3Futm_source=tldrinfosec/1/0100019a112efbbf-c0fb7078-c0ac-4d20-aff4-4a638901204c-000000/Plgtw0l6RYvvQPatpewUw0ez9ZPAmNPezfkkhmjSUrE=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.trailofbits.com%2F2025%2F10%2F22%2Fprompt-injection-to-rce-in-ai-agents%2F%3Futm_source=tldrinfosec/1/0100019a112efbbf-c0fb7078-c0ac-4d20-aff4-4a638901204c-000000/Plgtw0l6RYvvQPatpewUw0ez9ZPAmNPezfkkhmjSUrE=428", "authors": ["TLDR Newsletter"], "title": "Prompt Injection to RCE in AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.trailofbits.com%2F2025%2F10%2F22%2Fprompt-injection-to-rce-in-ai-agents%2F%3Futm_source=tldrinfosec/1/0100019a112efbbf-c0fb7078-c0ac-4d20-aff4-4a638901204c-000000/Plgtw0l6RYvvQPatpewUw0ez9ZPAmNPezfkkhmjSUrE=428", "summary": "Prompt Injection to RCE in AI Agents (3 minute read) Trail of Bits discovered argument injection vulnerabilities across three popular AI agent platforms that allow attackers to bypass human approval safeguards and achieve remote code execution. The flaw exploits pre-approved \u201csafe commands\u201d such as git, find, and ripgrep through malicious flag combinations (e.g., git show --format to write files, fd -x=python3 for execution, or go test -exec to run arbitrary code). These one-shot prompt-injec...", "source": "tldr", "AI": {"tldr": "Trail of Bits\u5728\u4e09\u4e2a\u6d41\u884c\u7684AI\u4ee3\u7406\u5e73\u53f0\u4e2d\u53d1\u73b0\u53c2\u6570\u6ce8\u5165\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u6076\u610f\u6807\u5fd7\u7ec4\u5408\u7ed5\u8fc7\u4eba\u5de5\u5ba1\u6279\u4fdd\u62a4\u673a\u5236\u5b9e\u73b0\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5e73\u53f0\u4e2d\u9884\u6279\u51c6\"\u5b89\u5168\u547d\u4ee4\"\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63ed\u793a\u53c2\u6570\u6ce8\u5165\u6f0f\u6d1e\u5982\u4f55\u88ab\u5229\u7528\u6765\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\u3002", "method": "\u901a\u8fc7\u5206\u6790git\u3001find\u3001ripgrep\u7b49\u9884\u6279\u51c6\u547d\u4ee4\u7684\u6076\u610f\u6807\u5fd7\u7ec4\u5408\uff08\u5982git show --format\u5199\u6587\u4ef6\u3001fd -x=python3\u6267\u884c\u4ee3\u7801\u3001go test -exec\u8fd0\u884c\u4efb\u610f\u4ee3\u7801\uff09\u6765\u6f14\u793a\u6f0f\u6d1e\u5229\u7528\u3002", "result": "\u53d1\u73b0\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u6f0f\u6d1e\u7ed5\u8fc7\u4eba\u7c7b\u5ba1\u6279\u4fdd\u62a4\u673a\u5236\uff0c\u5728AI\u4ee3\u7406\u5e73\u53f0\u4e0a\u5b9e\u73b0\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u3002", "conclusion": "AI\u4ee3\u7406\u5e73\u53f0\u4e2d\u7684\u9884\u6279\u51c6\u547d\u4ee4\u673a\u5236\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8f93\u5165\u9a8c\u8bc1\u548c\u547d\u4ee4\u9650\u5236\u6765\u9632\u6b62\u53c2\u6570\u6ce8\u5165\u653b\u51fb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.3e4e305d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2Fxpj6KhDM9bJybdnEe%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/A5zVu0ain47GbMX1vIICNJTdfROFlRtwdpxKWuKAwSA=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2Fxpj6KhDM9bJybdnEe%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/A5zVu0ain47GbMX1vIICNJTdfROFlRtwdpxKWuKAwSA=428", "authors": ["TLDR Newsletter"], "title": "How Well Does RL Scale?", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2Fxpj6KhDM9bJybdnEe%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/A5zVu0ain47GbMX1vIICNJTdfROFlRtwdpxKWuKAwSA=428", "summary": "How Well Does RL Scale? (14 minute read) RL-training for LLMs scales poorly. Most gains are from allowing LLMs to productively use longer chains of thought. This may be evidence that compute scaling will be less effective for AI progress than previously thought. The finding could lengthen timelines and affect strategies for AI governance and safety.", "source": "tldr", "AI": {"tldr": "RL\u8bad\u7ec3\u5bf9LLMs\u7684\u6269\u5c55\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u6536\u76ca\u6765\u81ea\u8ba9LLMs\u80fd\u591f\u6709\u6548\u4f7f\u7528\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u8fd9\u53ef\u80fd\u8868\u660e\u8ba1\u7b97\u6269\u5c55\u5bf9AI\u8fdb\u5c55\u7684\u6548\u679c\u4e0d\u5982\u9884\u671f", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6269\u5c55\u6548\u679c\uff0c\u63a2\u8ba8\u8ba1\u7b97\u6269\u5c55\u5bf9AI\u8fdb\u5c55\u7684\u5b9e\u9645\u5f71\u54cd", "method": "\u5206\u6790RL\u8bad\u7ec3\u5728LLMs\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u8ba1\u7b97\u6269\u5c55\u7684\u6709\u6548\u6027", "result": "RL\u8bad\u7ec3\u5bf9LLMs\u7684\u6269\u5c55\u6548\u679c\u5dee\uff0c\u4e3b\u8981\u6536\u76ca\u6765\u81ea\u601d\u7ef4\u94fe\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u8ba1\u7b97\u6269\u5c55\u5bf9AI\u8fdb\u5c55\u7684\u6548\u679c\u53ef\u80fd\u88ab\u9ad8\u4f30", "conclusion": "\u8fd9\u4e00\u53d1\u73b0\u53ef\u80fd\u5ef6\u957fAI\u53d1\u5c55\u65f6\u95f4\u7ebf\uff0c\u5f71\u54cdAI\u6cbb\u7406\u548c\u5b89\u5168\u7b56\u7565", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.dc745ca8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.outsystems.com%2Flow-code-platform%2Fagentic-ai-workbench%2F%3Futm_source=tldr%26utm_medium=social-paid%26utm_campaign=none%26utm_adid=tldr-ai-newsletter-October25%26utm_content=webpage%26utm_campaignteam=digital-mktg%26utm_partner=none/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2QxZsDEDcblH7yqNt24XSwFy7G0pM70M_q-hI8OpxVw=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.outsystems.com%2Flow-code-platform%2Fagentic-ai-workbench%2F%3Futm_source=tldr%26utm_medium=social-paid%26utm_campaign=none%26utm_adid=tldr-ai-newsletter-October25%26utm_content=webpage%26utm_campaignteam=digital-mktg%26utm_partner=none/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2QxZsDEDcblH7yqNt24XSwFy7G0pM70M_q-hI8OpxVw=428", "authors": ["TLDR Newsletter"], "title": "OutSystems Agent Workbench: Build AI agents that can handle mission-critical work", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.outsystems.com%2Flow-code-platform%2Fagentic-ai-workbench%2F%3Futm_source=tldr%26utm_medium=social-paid%26utm_campaign=none%26utm_adid=tldr-ai-newsletter-October25%26utm_content=webpage%26utm_campaignteam=digital-mktg%26utm_partner=none/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2QxZsDEDcblH7yqNt24XSwFy7G0pM70M_q-hI8OpxVw=428", "summary": "OutSystems Agent Workbench: Build AI agents that can handle mission-critical work (Sponsor) Create custom AI agents that you can actually ship in one unified AI-powered low-code platform. Build multi-agent workflows with a visual drag-and-drop interface, connect to your enterprise data, and deploy with built-in security and governance. No fragmented tools or previous AI expertise required. See how it works", "source": "tldr", "AI": {"tldr": "OutSystems Agent Workbench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4f4e\u4ee3\u7801AI\u5e73\u53f0\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u6784\u5efa\u548c\u90e8\u7f72\u53ef\u5904\u7406\u5173\u952e\u4efb\u52a1\u7684\u81ea\u5b9a\u4e49AI\u4ee3\u7406\uff0c\u65e0\u9700AI\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u6784\u5efaAI\u4ee3\u7406\u65f6\u9762\u4e34\u7684\u6280\u672f\u95e8\u69db\u9ad8\u3001\u5de5\u5177\u788e\u7247\u5316\u3001\u90e8\u7f72\u590d\u6742\u7b49\u95ee\u9898\uff0c\u8ba9\u975eAI\u4e13\u5bb6\u4e5f\u80fd\u6784\u5efa\u548c\u90e8\u7f72\u751f\u4ea7\u7ea7\u7684AI\u4ee3\u7406\u3002", "method": "\u63d0\u4f9b\u53ef\u89c6\u5316\u7684\u62d6\u653e\u754c\u9762\u6765\u6784\u5efa\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u8fde\u63a5\u4f01\u4e1a\u6570\u636e\uff0c\u5185\u7f6e\u5b89\u5168\u6027\u548c\u6cbb\u7406\u529f\u80fd\uff0c\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u4f4e\u4ee3\u7801\u5e73\u53f0\u4e0a\u5b8c\u6210\u6240\u6709\u5f00\u53d1\u90e8\u7f72\u5de5\u4f5c\u3002", "result": "\u7528\u6237\u53ef\u4ee5\u5728\u4e0d\u9700\u8981AI\u4e13\u4e1a\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5feb\u901f\u6784\u5efa\u548c\u90e8\u7f72\u53ef\u5904\u7406\u5173\u952e\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u907f\u514d\u4e86\u5de5\u5177\u788e\u7247\u5316\u7684\u95ee\u9898\u3002", "conclusion": "OutSystems Agent Workbench\u901a\u8fc7\u7edf\u4e00\u7684\u4f4e\u4ee3\u7801\u5e73\u53f0\u964d\u4f4e\u4e86AI\u4ee3\u7406\u5f00\u53d1\u7684\u95e8\u69db\uff0c\u4f7f\u4f01\u4e1a\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u548c\u90e8\u7f72\u751f\u4ea7\u7ea7\u7684AI\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2510.49c02770", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fzjunlp%2FLightMem%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2zqObqgJHqN6x-KppW8KQOPFPL7fJE1nu0ta2-9UTW4=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fzjunlp%2FLightMem%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2zqObqgJHqN6x-KppW8KQOPFPL7fJE1nu0ta2-9UTW4=428", "authors": ["TLDR Newsletter"], "title": "Lightweight Memory for LLM Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-23, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fzjunlp%2FLightMem%3Futm_source=tldrai/1/0100019a113876a0-f10ba506-2084-4ea5-9dc2-c698f3ba1acd-000000/2zqObqgJHqN6x-KppW8KQOPFPL7fJE1nu0ta2-9UTW4=428", "summary": "Lightweight Memory for LLM Agents (GitHub Repo) LightMem is a streamlined memory management system for large language models that offers tools for storing, retrieving, and updating long-term memory in AI agents with minimal overhead.", "source": "tldr", "AI": {"tldr": "LightMem\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u5b58\u50a8\u3001\u68c0\u7d22\u548c\u66f4\u65b0\u957f\u671f\u8bb0\u5fc6\u7684\u5de5\u5177\uff0c\u5177\u6709\u6700\u5c0f\u5f00\u9500\u3002", "motivation": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5f00\u53d1\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u89e3\u51b3\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\u548c\u68c0\u7d22\u7684\u5f00\u9500\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u5185\u5b58\u67b6\u6784\uff0c\u63d0\u4f9b\u5b58\u50a8\u3001\u68c0\u7d22\u548c\u66f4\u65b0\u957f\u671f\u8bb0\u5fc6\u7684\u5de5\u5177\u96c6\u3002", "result": "\u5b9e\u73b0\u4e86\u6700\u5c0f\u5f00\u9500\u7684\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\uff0c\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u8bb0\u5fc6\u5904\u7406\u6548\u7387\u3002", "conclusion": "LightMem\u7cfb\u7edf\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u8bb0\u5fc6\u64cd\u4f5c\u7684\u5f00\u9500\u3002", "topic": "agent analysis"}}
{"id": "wechat.2510.2ce9cdd1", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483756&idx=1&sn=fe647ba4ea7db683b22323b4f84da105&chksm=e92d9b5885a58245e514b04c2755c730a1ab3c8c3a11aaa792211136a550effc112deddb5ec3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483756&idx=1&sn=fe647ba4ea7db683b22323b4f84da105&chksm=e92d9b5885a58245e514b04c2755c730a1ab3c8c3a11aaa792211136a550effc112deddb5ec3#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff0810.23\uff09part 2", "comment": "Source: WeChat, Published: 2025-10-23 14:26:18", "summary": "3\uff5c\u3010\u7c7b\u578b\uff1aAgent\u3011\u2014 Anthropic \u63a8\u51fa Claude Code Web \u5e94\u7528\uff08ET\uff1a2025-10-21 13\uff1a00\uff1b\u7a97\u53e3\uff1aYesterday\uff09\u6765\u6e90\uff1aThe AI Insider\uff5c\u94fe\u63a5\uff1atheaiinsider.tech/anthropic-launches-web-app-for-claude-code", "AI": {"tldr": "3\uff5c\u3010\u7c7b\u578b\uff1aAgent\u3011\u2014 Anthropic \u63a8\u51fa Claude Code Web \u5e94\u7528\uff08ET\uff1a2025-10-21 13\uff1a00\uff1b\u7a97\u53e3\uff1aYesterday\uff09\u6765\u6e90\uff1aThe AI Insider\uff5c\u94fe\u63a5\uff1atheaiinsider.tech/anthropic-launches-web-app-for-claude-code", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.f5f3fd63", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483751&idx=1&sn=3a536fea8598cf608e23a8fa112757d1&chksm=e9dcf38068908e7f47d37a68926f6b2c4e89e44d73396ac8cc50b847d19471053f94f12c63d1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483751&idx=1&sn=3a536fea8598cf608e23a8fa112757d1&chksm=e9dcf38068908e7f47d37a68926f6b2c4e89e44d73396ac8cc50b847d19471053f94f12c63d1#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff0810.23\uff09part 1", "comment": "Source: WeChat, Published: 2025-10-23 14:23:46", "summary": "1\uff5c\u3010\u7c7b\u578b\uff1aAgent / Release\u3011\u2014 Anthropic \u5c06 Claude Code \u5e26\u5230 Web \uff08ET\uff1a20251022 09\uff1a29\uff1b\u7a97\u53e3\uff1a36h\uff09\u6765\u6e90\uff1aeWeek\uff08Matt Berman\uff0c 09\uff1a29 ET\uff09\uff5c\u94fe\u63a5\uff1ahttps\uff1a//www.eweek.com/ai/anthropic-launches-claude-code-on-the-web/\uff5c\u6e90\u5934\u4f18\u5148", "AI": {"tldr": "1\uff5c\u3010\u7c7b\u578b\uff1aAgent / Release\u3011\u2014 Anthropic \u5c06 Claude Code \u5e26\u5230 Web \uff08ET\uff1a20251022 09\uff1a29\uff1b\u7a97\u53e3\uff1a36h\uff09\u6765\u6e90\uff1aeWeek\uff08Matt Berman\uff0c 09\uff1a29 ET\uff09\uff5c\u94fe\u63a5\uff1ahttps\uff1a//www.eweek.com/ai/anthropic-launches-claude-code-on-the-web/\uff5c\u6e90\u5934\u4f18\u5148", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.3c976850", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMTYxOTEwMw==&mid=2247484209&idx=1&sn=1eb0f2b37d59183f2e41502fbe1b68bb&chksm=c0887e846d0300880df376e7506343a6b8a99138a72422ada0e378103b42bd14ce6a87a2eb87#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMTYxOTEwMw==&mid=2247484209&idx=1&sn=1eb0f2b37d59183f2e41502fbe1b68bb&chksm=c0887e846d0300880df376e7506343a6b8a99138a72422ada0e378103b42bd14ce6a87a2eb87#rd", "authors": ["\u7b2c\u4e00\u6027\u539f\u7406\u4e0e\u5e78\u798f\u751f\u6d3b"], "title": "\u522b\u518d\u778e\u73a9 Prompt \u4e86\uff01\u5434\u6069\u8fbe\u6559\u4f60\u7528\u7b2c\u4e00\u6027\u539f\u7406\uff0c\u6784\u5efa\u771f\u6b63\u80fd\u201c\u5e72\u6d3b\u201d\u7684 AI", "comment": "Source: WeChat, Published: 2025-10-24 11:53:03", "summary": "Agentic AI \u7684\u6838\u5fc3\uff0c\u5c31\u662f\u8ba9 AI \u62e5\u6709\u201c\u4ee3\u7406\u6027\u201d\uff08agency\uff09 \uff0c\u5373\u81ea\u4e3b\u51b3\u7b56\u548c\u884c\u52a8\u7684\u80fd\u529b \u3002\u8fd9\u95e8\u8bfe\u7684\u5143\u67b6\u6784\uff08Meta-Architecture\uff09\u63ed\u793a\u4e86 AI \u662f\u5982\u4f55\u201c\u52a8\u8d77\u6765\u201d\u7684\u3002", "AI": {"tldr": "Agentic AI \u7684\u6838\u5fc3\uff0c\u5c31\u662f\u8ba9 AI \u62e5\u6709\u201c\u4ee3\u7406\u6027\u201d\uff08agency\uff09 \uff0c\u5373\u81ea\u4e3b\u51b3\u7b56\u548c\u884c\u52a8\u7684\u80fd\u529b \u3002\u8fd9\u95e8\u8bfe\u7684\u5143\u67b6\u6784\uff08Meta-Architecture\uff09\u63ed\u793a\u4e86 AI \u662f\u5982\u4f55\u201c\u52a8\u8d77\u6765\u201d\u7684\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.00a43974", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODQyNjY0OQ==&mid=2247483796&idx=1&sn=491d5d88fff1f63dd52ad5794f49ec6c&chksm=9789f96ce2957f1370ea24138b8870a4fb13069062a0292a5d9e7b577126fa0a642457e5cbb6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODQyNjY0OQ==&mid=2247483796&idx=1&sn=491d5d88fff1f63dd52ad5794f49ec6c&chksm=9789f96ce2957f1370ea24138b8870a4fb13069062a0292a5d9e7b577126fa0a642457e5cbb6#rd", "authors": ["\u5927\u6a21\u578b\u963f\u6d0b"], "title": "\u5434\u6069\u8fbeAgenticAI\u8bfe\u7a0b\uff1a\u4ec0\u4e48\u662f <em class=\"highlight\">Agentic</em> AI\uff1f", "comment": "Source: WeChat, Published: 2025-10-24 09:51:10", "summary": "agentic \u5de5\u4f5c\u6d41\u3002Agentic \u5de5\u4f5c\u6d41\u7b80\u4ecb\uff1a\u4ec0\u4e48\u662fAgentic AI\uff1f\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6211\u4eec\u4e0eAI\u7684\u4e92\u52a8\u65b9\u5f0f\u4e5f\u5728\u4e0d\u65ad\u6f14\u53d8\u3002\u4f5c\u6d41\u201d\uff08Agentic Workflows\uff09\u6b63\u662f\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\u7684\u4e00\u79cd\u65b0\u578bAI\u5e94\u7528\u6a21\u5f0f\u3002", "AI": {"tldr": "agentic \u5de5\u4f5c\u6d41\u3002Agentic \u5de5\u4f5c\u6d41\u7b80\u4ecb\uff1a\u4ec0\u4e48\u662fAgentic AI\uff1f\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6211\u4eec\u4e0eAI\u7684\u4e92\u52a8\u65b9\u5f0f\u4e5f\u5728\u4e0d\u65ad\u6f14\u53d8\u3002\u4f5c\u6d41\u201d\uff08Agentic Workflows\uff09\u6b63\u662f\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\u7684\u4e00\u79cd\u65b0\u578bAI\u5e94\u7528\u6a21\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.cc185e21", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzOTQxODIxMw==&mid=2247495160&idx=1&sn=201b6e2762d36376167c969b2bd0c1f4&chksm=c34d1b2e9bc874692d2ffabbcbbc717c22a89cadcb180e301463fca897e09530037cb8074f3e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzOTQxODIxMw==&mid=2247495160&idx=1&sn=201b6e2762d36376167c969b2bd0c1f4&chksm=c34d1b2e9bc874692d2ffabbcbbc717c22a89cadcb180e301463fca897e09530037cb8074f3e#rd", "authors": ["CDHC \u6570\u5b57\u533b\u7597"], "title": "\u884c\u4e1a\u6d1e\u89c1\u4e28\u9ea6\u80af\u9521\uff1a\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">agentic</em> AI \u6709\u671b\u4e3a\u751f\u547d\u79d1\u5b66\u4f01\u4e1a\u8f6c\u578b\u63d0\u4f9b\u65b0\u673a\u9047\uff08\u4e0b\uff09", "comment": "Source: WeChat, Published: 2025-10-24 09:30:45", "summary": "agents can shift workforce hours across operations functions. work hours shifted by agentic workforce\uff0c % of function capacity incremental time spent on agents time savings from agents function supply chain -10 50-55 quality \uff08on site\uff09 -5 20-25 quality \uff08above site\uff09 -6 30-35 manufacturing \uff08", "AI": {"tldr": "agents can shift workforce hours across operations functions. work hours shifted by agentic workforce\uff0c % of function capacity incremental time spent on agents time savings from agents function supply ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.300c2368", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MDYyMjk1MQ==&mid=2247484656&idx=1&sn=bc71e0c93bf262b45281481918d14959&chksm=e86cc40848d81b8ad526322f448ea4a7ad0c0bce0796d100592d9fdf28b62fac5e2ad811d7c5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MDYyMjk1MQ==&mid=2247484656&idx=1&sn=bc71e0c93bf262b45281481918d14959&chksm=e86cc40848d81b8ad526322f448ea4a7ad0c0bce0796d100592d9fdf28b62fac5e2ad811d7c5#rd", "authors": ["AI\u901a\u8bc6\u4e0e\u6570\u5b57\u751f\u4ea7\u529b"], "title": "\u6570\u636e\u548c<em class=\"highlight\">Agentic</em> AI\uff0c\u884c\u4e1a\u6b63\u5728\u5411\u4f55\u5904\u6f14\u8fdb\uff1f", "comment": "Source: WeChat, Published: 2025-10-24 06:38:53", "summary": "DACon\u5317\u4eac\u7ad9\u6d3b\u52a8\uff0c20251024\u3002\u6839\u636e\u76f8\u5173\u8bae\u7a0b\u548c\u5185\u5bb9\uff0c\u4f9d\u636e\u201c\u6570\u667a\u5316\u65b0\u8d28\u6a21\u578b\u65b9\u6cd5\u8bba\u201d\u8fdb\u884c\u5206\u6790\uff0c\u805a\u7126\u5982\u4f55\u89c4\u5212\u81ea\u8eab\u7684\u6570\u667a\u5316\u80fd\u529b\u5efa\u8bbe\uff1f\u8bae\u65e5\u7a0b \u964d\u672c\u589e\u6548\uff1a 2025/10/25\u661f\u671f\u516d ai ready\u7684 ai\u65f6\u4ee3\u6570\u667a\u56e2\u961f\u7684 \u5371\u4e0e\u673a \u5927\u6570\u636e\u8ba1\u7b97\u6280\u672f\u9769\u65b0 \u91d1\u878d", "AI": {"tldr": "DACon\u5317\u4eac\u7ad9\u6d3b\u52a8\uff0c20251024\u3002\u6839\u636e\u76f8\u5173\u8bae\u7a0b\u548c\u5185\u5bb9\uff0c\u4f9d\u636e\u201c\u6570\u667a\u5316\u65b0\u8d28\u6a21\u578b\u65b9\u6cd5\u8bba\u201d\u8fdb\u884c\u5206\u6790\uff0c\u805a\u7126\u5982\u4f55\u89c4\u5212\u81ea\u8eab\u7684\u6570\u667a\u5316\u80fd\u529b\u5efa\u8bbe\uff1f\u8bae\u65e5\u7a0b \u964d\u672c\u589e\u6548\uff1a 2025/10/25\u661f\u671f\u516d ai ready\u7684 ai\u65f6\u4ee3\u6570\u667a\u56e2\u961f\u7684 \u5371\u4e0e\u673a \u5927\u6570\u636e\u8ba1\u7b97\u6280\u672f\u9769\u65b0 \u91d1\u878d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e804443c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMTYxOTEwMw==&mid=2247484193&idx=1&sn=65de9ceeadf123752f7e0f3e6d2fde00&chksm=c081d1c6bbb74672e6f0cebd3794723a55444a419ad15e03d13ab52df9ef8dd2f365c86f1b28#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMTYxOTEwMw==&mid=2247484193&idx=1&sn=65de9ceeadf123752f7e0f3e6d2fde00&chksm=c081d1c6bbb74672e6f0cebd3794723a55444a419ad15e03d13ab52df9ef8dd2f365c86f1b28#rd", "authors": ["\u7b2c\u4e00\u6027\u539f\u7406\u4e0e\u5e78\u798f\u751f\u6d3b"], "title": "\u8bad\u6218\u7ed3\u5408\uff0c\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-10-24 06:24:26", "summary": "\u6253\u5f00 vs code\uff0c\u70b9\u51fb\u201c\u6587\u4ef6\u201d -> \u201c\u6253\u5f00\u6587\u4ef6\u5939\u201d\uff0c\u9009\u62e9\u4f60\u521a\u521b\u5efa\u7684 agentic_ai_project\u3002\u5728 vs code \u91cc\uff0c\u70b9\u51fb\u201c\u7ec8\u7aef\u201d -> \u201c\u65b0\u5efa\u7ec8\u7aef\u201d\u3002\u5728 VS Code \u81ea\u5e26\u7684\u7ec8\u7aef\u91cc\uff0c\u8f93\u5165\uff1a", "AI": {"tldr": "\u6253\u5f00 vs code\uff0c\u70b9\u51fb\u201c\u6587\u4ef6\u201d -> \u201c\u6253\u5f00\u6587\u4ef6\u5939\u201d\uff0c\u9009\u62e9\u4f60\u521a\u521b\u5efa\u7684 agentic_ai_project\u3002\u5728 vs code \u91cc\uff0c\u70b9\u51fb\u201c\u7ec8\u7aef\u201d -> \u201c\u65b0\u5efa\u7ec8\u7aef\u201d\u3002\u5728 VS Code \u81ea\u5e26\u7684\u7ec8\u7aef\u91cc\uff0c\u8f93\u5165\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.67e461f7", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDQ2NjYzNQ==&mid=2656615074&idx=1&sn=fee235239da60137d5ff7a217a1bad33&chksm=bc30305d25e1591df58816b7273d624ebe8c2e08adcfc9cfe747e5fce889bad46a8495cc296d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDQ2NjYzNQ==&mid=2656615074&idx=1&sn=fee235239da60137d5ff7a217a1bad33&chksm=bc30305d25e1591df58816b7273d624ebe8c2e08adcfc9cfe747e5fce889bad46a8495cc296d#rd", "authors": ["\u7535\u5546\u4e0e\u7ba1\u7406"], "title": "AI <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u6838\u5fc3\u539f\u7406\u7efc\u8ff0\uff1a\u4ece <em class=\"highlight\">Agentic</em> AI \u5230 AI Agent", "comment": "Source: WeChat, Published: 2025-10-24 00:00:25", "summary": "Agentic AI \u7684\u80cc\u666fLLM \u6700\u521d\u7684\u4ea7\u54c1\u5f62\u6001\u662f\u7531 OpenAI \u9886\u8854\u7684 ChatBot\uff08\u804a\u5929\u673a\u5668\u4eba\uff09\uff0c\u5e95\u5c42\u652f\u6491\u6280\u672f\u662f Transformer \u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u521d\u4e13\u6ce8\u4e8e\u8bed\u8a00\u6587\u672c\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u573a\u666f\u3002", "AI": {"tldr": "Agentic AI \u7684\u80cc\u666fLLM \u6700\u521d\u7684\u4ea7\u54c1\u5f62\u6001\u662f\u7531 OpenAI \u9886\u8854\u7684 ChatBot\uff08\u804a\u5929\u673a\u5668\u4eba\uff09\uff0c\u5e95\u5c42\u652f\u6491\u6280\u672f\u662f Transformer \u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u521d\u4e13\u6ce8\u4e8e\u8bed\u8a00\u6587\u672c\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
