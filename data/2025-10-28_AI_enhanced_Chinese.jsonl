{"id": "2510.21902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21902", "abs": "https://arxiv.org/abs/2510.21902", "authors": ["Timoth\u00e9 Boulet", "Xavier Hinaut", "Cl\u00e9ment Moulin-Frier"], "title": "Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments", "comment": "10 pages, 7 figures", "summary": "Software Engineering Agents (SWE-Agents) have proven effective for\ntraditional software engineering tasks with accessible codebases, but their\nperformance for embodied tasks requiring well-designed information discovery\nremains unexplored. We present the first extended evaluation of SWE-Agents on\ncontroller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to\nsolve 20 diverse embodied tasks from the Minigrid environment. Our experiments\ncompare agent performance across different information access conditions: with\nand without environment source code access, and with varying capabilities for\ninteractive exploration. We quantify how different information access levels\naffect SWE-Agent performance for embodied tasks and analyze the relative\nimportance of static code analysis versus dynamic exploration for task solving.\nThis work establishes controller generation for embodied tasks as a crucial\nevaluation domain for SWE-Agents and provides baseline results for future\nresearch in efficient reasoning systems.", "AI": {"tldr": "\u9996\u6b21\u8bc4\u4f30SWE-Agents\u5728\u5177\u8eab\u4efb\u52a1\u63a7\u5236\u5668\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e0d\u540c\u4fe1\u606f\u8bbf\u95ee\u6761\u4ef6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "\u63a2\u7d22SWE-Agents\u5728\u9700\u8981\u826f\u597d\u4fe1\u606f\u53d1\u73b0\u7684\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8be5\u9886\u57df\u6b64\u524d\u672a\u88ab\u5145\u5206\u7814\u7a76", "method": "\u4f7f\u7528Mini-SWE-Agent\u89e3\u51b3Minigrid\u73af\u5883\u4e2d\u768420\u4e2a\u591a\u6837\u5316\u5177\u8eab\u4efb\u52a1\uff0c\u6bd4\u8f83\u6709\u65e0\u73af\u5883\u6e90\u4ee3\u7801\u8bbf\u95ee\u548c\u4e0d\u540c\u4ea4\u4e92\u63a2\u7d22\u80fd\u529b\u4e0b\u7684\u6027\u80fd", "result": "\u91cf\u5316\u4e86\u4e0d\u540c\u4fe1\u606f\u8bbf\u95ee\u6c34\u5e73\u5bf9SWE-Agent\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u9759\u6001\u4ee3\u7801\u5206\u6790\u4e0e\u52a8\u6001\u63a2\u7d22\u7684\u76f8\u5bf9\u91cd\u8981\u6027", "conclusion": "\u786e\u7acb\u4e86\u5177\u8eab\u4efb\u52a1\u63a7\u5236\u5668\u751f\u6210\u4f5c\u4e3aSWE-Agents\u5173\u952e\u8bc4\u4f30\u9886\u57df\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u63a8\u7406\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u57fa\u7ebf\u7ed3\u679c", "topic": "agent analysis"}}
{"id": "2510.21788", "categories": ["cs.LG", "cs.AI", "I.2; G.3"], "pdf": "https://arxiv.org/pdf/2510.21788", "abs": "https://arxiv.org/abs/2510.21788", "authors": ["Larkin Liu", "Jalal Etesami"], "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We explore the use of expert-guided bandit learning, which we refer to as\nonline mixture-of-experts (OMoE). In this setting, given a context, a candidate\ncommittee of experts must determine how to aggregate their outputs to achieve\noptimal results in terms of aggregate accuracy. We propose two algorithms to\naddress this problem. The first algorithm combines aggregate voting with\nUCB-driven successive elimination, efficiently pruning suboptimal exploration\nactions. The second algorithm employs an online weighted-majority-voting\nmechanism, leveraging the respective voting power of each expert proportional\nto their predictive power. We derive theoretical guarantees for the regret\nproperties in the bandit setting under ideal circumstances, and empirical\nresults are provided accordingly. As a modern study on applications, these\nmethods are applied to the online fine-tuning of a set of expert large language\nmodels (LLMs), where after each response, the generative LLM dynamically\nreweighs its set of experts and/or selects the optimal committee of experts to\ngenerate the most accurate response. Our results introduce new methodologies\nand no-regret guarantees for combining multiple experts to improve on the\nperformance of the an aggregate model overall.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u5728\u7ebf\u4e13\u5bb6\u6df7\u5408(OMoE)\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u5f15\u5bfc\u7684bandit\u5b66\u4e60\u6765\u4f18\u5316\u4e13\u5bb6\u59d4\u5458\u4f1a\u8f93\u51fa\u805a\u5408\uff0c\u5e94\u7528\u4e8eLLM\u5728\u7ebf\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u65f6\uff0c\u5982\u4f55\u6709\u6548\u805a\u5408\u4e13\u5bb6\u59d4\u5458\u4f1a\u8f93\u51fa\u4ee5\u83b7\u5f97\u6700\u4f18\u51c6\u786e\u7387\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728LLM\u4e13\u5bb6\u6a21\u578b\u52a8\u6001\u9009\u62e9\u573a\u666f\u4e2d\u3002", "method": "1. \u7ed3\u5408\u805a\u5408\u6295\u7968\u4e0eUCB\u9a71\u52a8\u7684\u8fde\u7eed\u6d88\u9664\u7b97\u6cd5\uff1b2. \u5728\u7ebf\u52a0\u6743\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u6839\u636e\u4e13\u5bb6\u9884\u6d4b\u80fd\u529b\u5206\u914d\u6295\u7968\u6743\u91cd\u3002", "result": "\u5728\u7406\u60f3\u60c5\u51b5\u4e0b\u63a8\u5bfc\u4e86bandit\u8bbe\u7f6e\u4e2d\u7684\u9057\u61be\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u5b9e\u8bc1\u7ed3\u679c\uff0c\u8bc1\u660e\u80fd\u63d0\u5347\u6574\u4f53\u805a\u5408\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7ed3\u5408\u591a\u4e2a\u4e13\u5bb6\u4ee5\u6539\u8fdb\u6574\u4f53\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u548c\u65e0\u9057\u61be\u4fdd\u8bc1\uff0c\u7279\u522b\u9002\u7528\u4e8eLLM\u4e13\u5bb6\u6a21\u578b\u7684\u52a8\u6001\u9009\u62e9\u548c\u6743\u91cd\u8c03\u6574\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21903", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21903", "abs": "https://arxiv.org/abs/2510.21903", "authors": ["Xuhui Zhou", "Valerie Chen", "Zora Zhiruo Wang", "Graham Neubig", "Maarten Sap", "Xingyao Wang"], "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents", "comment": null, "summary": "Recent advances in coding agents have made them capable of planning, editing,\nrunning, and testing complex code bases. Despite their growing ability in\ncoding tasks, these systems still struggle to infer and track user intent,\nespecially when instructions are underspecified or context-dependent. To bridge\nthis gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary\nsoftware-engineering (SWE) agent with a lightweight theory-of-mind (ToM)\npartner agent dedicated to modeling the user's mental state. The ToM agent\ninfers user goals, constraints, and preferences from instructions and\ninteraction history, maintains a \\textbf{persistent memory} of the user, and\nprovides user-related suggestions to the SWE agent. In two software engineering\nbenchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task\nsuccess rates and user satisfaction. Notably, on the stateful SWE benchmark, a\nnewly introduced evaluation that provides agents with a user simulator along\nwith previous interaction histories, ToM-SWE achieves a substantially higher\ntask success rate of 59.7\\% compared to 18.1\\% for OpenHands, a\nstate-of-the-art SWE agent. Furthermore, in a three-week study with\nprofessional developers using ToM-SWE in their daily work, participants found\nit useful 86\\% of the time, underscoring the value of stateful user modeling\nfor practical coding agents.", "AI": {"tldr": "ToM-SWE\u662f\u4e00\u4e2a\u53cc\u4ee3\u7406\u67b6\u6784\uff0c\u5c06\u4e3b\u8981\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e0e\u8f7b\u91cf\u7ea7\u5fc3\u667a\u7406\u8bba\u4ee3\u7406\u914d\u5bf9\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u5fc3\u7406\u72b6\u6001\u6765\u63d0\u5347\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u7528\u6237\u610f\u56fe\u7406\u89e3\u548c\u8ffd\u8e2a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u5728\u5904\u7406\u7528\u6237\u610f\u56fe\u4e0d\u660e\u786e\u6216\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u597d\u7684\u7528\u6237\u610f\u56fe\u63a8\u65ad\u548c\u8ffd\u8e2a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff1a\u4e3bSWE\u4ee3\u7406\u8d1f\u8d23\u7f16\u7801\u4efb\u52a1\uff0cToM\u4ee3\u7406\u4e13\u95e8\u5efa\u6a21\u7528\u6237\u5fc3\u7406\u72b6\u6001\uff0c\u63a8\u65ad\u7528\u6237\u76ee\u6807\u3001\u7ea6\u675f\u548c\u504f\u597d\uff0c\u5e76\u7ef4\u62a4\u6301\u4e45\u7528\u6237\u8bb0\u5fc6\u3002", "result": "\u5728\u72b6\u6001\u5316SWE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cToM-SWE\u8fbe\u523059.7%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eOpenHands\u768418.1%\uff1b\u5728\u4e13\u4e1a\u5f00\u53d1\u8005\u4e09\u5468\u7814\u7a76\u4e2d\uff0c86%\u7684\u60c5\u51b5\u4e0b\u88ab\u8ba4\u4e3a\u6709\u7528\u3002", "conclusion": "\u72b6\u6001\u5316\u7528\u6237\u5efa\u6a21\u5bf9\u5b9e\u7528\u7f16\u7801\u4ee3\u7406\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cToM-SWE\u67b6\u6784\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "topic": "code agent"}}
{"id": "2510.21883", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21883", "abs": "https://arxiv.org/abs/2510.21883", "authors": ["Chenheng Zhang", "Tianqi Du", "Jizhe Zhang", "Mingqing Xiao", "Yifei Wang", "Yisen Wang", "Zhouchen Lin"], "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding", "comment": null, "summary": "Conventional research on large language models (LLMs) has primarily focused\non refining output distributions, while paying less attention to the decoding\nprocess that transforms these distributions into final responses. Recent\nadvances, such as scaling the computation of inference time with reward models,\nhave underscored the importance of decoding, but these methods often suffer\nfrom high computational costs and limited applicability. In this paper, we\nrevisit LLM generation through the lens of recommender systems, conceptualizing\nthe decoding process as analogous to the ranking stage in recommendation\npipelines. From this perspective, we observe that both traditional decoding\nmethods and reward models exhibit clear limitations such as redundancy.\nMotivated by this insight, we propose Language Ranker, a novel framework that\nintroduces a lightweight module to rerank candidate responses using features\nextracted by the base model. Experiments across a wide range of tasks show that\nLanguage Ranker achieves performance comparable to large-scale reward models,\nwhile requiring only <0.5M additional parameters, significantly reducing the\ncomputational overhead during both training and inference stages. This\nhighlights the efficiency and effectiveness of our method, showcasing its\npotential to fully unlock the capabilities of LLMs.", "AI": {"tldr": "\u63d0\u51faLanguage Ranker\u6846\u67b6\uff0c\u5c06LLM\u89e3\u7801\u8fc7\u7a0b\u7c7b\u6bd4\u63a8\u8350\u7cfb\u7edf\u6392\u5e8f\u9636\u6bb5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u91cd\u6392\u5019\u9009\u54cd\u5e94\uff0c\u5728\u6027\u80fd\u5ab2\u7f8e\u5927\u578b\u5956\u52b1\u6a21\u578b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edfLLM\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5206\u5e03\u4f18\u5316\uff0c\u800c\u5ffd\u89c6\u4e86\u5c06\u5206\u5e03\u8f6c\u5316\u4e3a\u6700\u7ec8\u54cd\u5e94\u7684\u89e3\u7801\u8fc7\u7a0b\u3002\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9002\u7528\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5c06LLM\u89e3\u7801\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6392\u5e8f\u9636\u6bb5\uff0c\u63d0\u51faLanguage Ranker\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5bf9\u5019\u9009\u54cd\u5e94\u8fdb\u884c\u91cd\u6392\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLanguage Ranker\u4ec5\u9700<0.5M\u989d\u5916\u53c2\u6570\uff0c\u5c31\u80fd\u8fbe\u5230\u4e0e\u5927\u578b\u5956\u52b1\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u5c55\u793a\u4e86\u5145\u5206\u91ca\u653eLLM\u6f5c\u529b\u7684\u6f5c\u529b\uff0c\u4e3a\u89e3\u7801\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.21855", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.21855", "abs": "https://arxiv.org/abs/2510.21855", "authors": ["Ryan Zhang", "Herbert Woisetscl\u00e4ger"], "title": "SIGN: Schema-Induced Games for Naming", "comment": "AAAI 2026 Student Abstract (Oral). Code available ar\n  https://github.com/ryanzhangofficial/schema-induced-games-for-naming", "summary": "Real-world AI systems are tackling increasingly complex problems, often\nthrough interactions among large language model (LLM) agents. When these agents\ndevelop inconsistent conventions, coordination can break down. Applications\nsuch as collaborative coding and distributed planning therefore require\nreliable, consistent communication, and scalability is a central concern as\nsystems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming\ngame that examines how lightweight structure can steer convention formation. We\ncompare schema-induced communication to unconstrained natural language and find\nfaster convergence with up to 5.8x higher agreement. These results suggest that\nminimal structure can act as a simple control knob for efficient multi-agent\ncoordination, pointing toward broader applications beyond the naming game.", "AI": {"tldr": "SIGN\u662f\u4e00\u4e2a\u547d\u540d\u6e38\u620f\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ed3\u6784\u5f15\u5bfc\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5f62\u6210\u4e00\u81f4\u7684\u901a\u4fe1\u7ea6\u5b9a\uff0c\u76f8\u6bd4\u65e0\u7ea6\u675f\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u80fd\u5b9e\u73b05.8\u500d\u66f4\u9ad8\u7684\u534f\u8bae\u8fbe\u6210\u7387\u548c\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u73b0\u5b9eAI\u7cfb\u7edf\u4e2d\u591a\u4e2aLLM\u667a\u80fd\u4f53\u5728\u534f\u4f5c\u65f6\u53ef\u80fd\u5f62\u6210\u4e0d\u4e00\u81f4\u7684\u901a\u4fe1\u7ea6\u5b9a\uff0c\u5bfc\u81f4\u534f\u8c03\u5931\u8d25\uff0c\u9700\u8981\u53ef\u9760\u4e00\u81f4\u7684\u901a\u4fe1\u673a\u5236\u6765\u652f\u6301\u534f\u4f5c\u7f16\u7801\u548c\u5206\u5e03\u5f0f\u89c4\u5212\u7b49\u5e94\u7528\u3002", "method": "\u5f15\u5165Schema-Induced Games for Naming (SIGN)\u547d\u540d\u6e38\u620f\uff0c\u6bd4\u8f83\u6a21\u5f0f\u8bf1\u5bfc\u901a\u4fe1\u4e0e\u65e0\u7ea6\u675f\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u5728\u7ea6\u5b9a\u5f62\u6210\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u5f0f\u8bf1\u5bfc\u901a\u4fe1\u76f8\u6bd4\u65e0\u7ea6\u675f\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u534f\u8bae\u8fbe\u6210\u7387\u63d0\u9ad8\u4e865.8\u500d\u3002", "conclusion": "\u6700\u5c0f\u5316\u7ed3\u6784\u53ef\u4ee5\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u7b80\u5355\u63a7\u5236\u673a\u5236\uff0c\u5728\u547d\u540d\u6e38\u620f\u4e4b\u5916\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "topic": "agent analysis"}}
{"id": "2510.21884", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21884", "abs": "https://arxiv.org/abs/2510.21884", "authors": ["Avinash Patil"], "title": "Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks", "comment": "12 Pages, 12 Figures, 2 tables", "summary": "The growing adoption of machine learning (ML) in sensitive domains has\nheightened the demand for transparent and interpretable artificial\nintelligence. Large Language Models (LLMs) are increasingly capable of\nproducing natural language explanations, yet it remains unclear whether these\nrationales faithfully capture the predictive signals that underlie decisions.\nThis paper introduces RACE-Reasoning Alignment for Completeness of\nExplanations, a systematic framework to evaluate the alignment between\nLLM-generated explanations and interpretable feature importance scores derived\nfrom a logistic regression baseline. We analyze four widely used text\nclassification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and\ncompare LLM rationales against top-ranked supporting and contradicting lexical\nfeatures. To capture alignment at multiple levels of granularity, RACE\nimplements token-aware, exact string, and edit-distance matching techniques.\nEmpirical results reveal a consistent asymmetry: correct predictions exhibit\nhigher coverage of supporting features, while incorrect predictions are\nassociated with elevated coverage of contradicting features. Edit-distance\nmatching further uncovers paraphrastic overlaps, boosting coverage while\npreserving this asymmetry. These findings demonstrate that LLM rationales\ncombine both surface-level and flexible evidence reuse, yet can also amplify\nmisleading cues in error cases. RACE provides new insights into the\nfaithfulness of LLM explanations and establishes a quantitative basis for\nevaluating reasoning completeness in neural language models.", "AI": {"tldr": "RACE\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u89e3\u91ca\u4e0e\u903b\u8f91\u56de\u5f52\u7279\u5f81\u91cd\u8981\u6027\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\uff0c\u53d1\u73b0\u6b63\u786e\u9884\u6d4b\u66f4\u652f\u6301\u7279\u5f81\uff0c\u9519\u8bef\u9884\u6d4b\u66f4\u77db\u76fe\u7279\u5f81\u3002", "motivation": "\u968f\u7740ML\u5728\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u900f\u660e\u53ef\u89e3\u91ca\u7684AI\u3002\u867d\u7136LLM\u80fd\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u662f\u5426\u5fe0\u5b9e\u53cd\u6620\u9884\u6d4b\u4fe1\u53f7\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faRACE\u6846\u67b6\uff0c\u5728\u56db\u4e2a\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83LLM\u89e3\u91ca\u4e0e\u903b\u8f91\u56de\u5f52\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4f7f\u7528token\u611f\u77e5\u3001\u7cbe\u786e\u5b57\u7b26\u4e32\u548c\u7f16\u8f91\u8ddd\u79bb\u5339\u914d\u6280\u672f\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u4e00\u81f4\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u6b63\u786e\u9884\u6d4b\u652f\u6301\u7279\u5f81\u8986\u76d6\u7387\u66f4\u9ad8\uff0c\u9519\u8bef\u9884\u6d4b\u77db\u76fe\u7279\u5f81\u8986\u76d6\u7387\u66f4\u9ad8\u3002\u7f16\u8f91\u8ddd\u79bb\u5339\u914d\u53d1\u73b0\u91ca\u4e49\u91cd\u53e0\u3002", "conclusion": "LLM\u89e3\u91ca\u7ed3\u5408\u4e86\u8868\u9762\u548c\u7075\u6d3b\u7684\u8bc1\u636e\u91cd\u7528\uff0c\u4f46\u5728\u9519\u8bef\u60c5\u51b5\u4e0b\u4e5f\u4f1a\u653e\u5927\u8bef\u5bfc\u7ebf\u7d22\u3002RACE\u4e3a\u8bc4\u4f30\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.21891", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21891", "abs": "https://arxiv.org/abs/2510.21891", "authors": ["Dhrupad Bhardwaj", "Julia Kempe", "Tim G. J. Rudner"], "title": "Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation", "comment": null, "summary": "To deploy large language models (LLMs) in high-stakes application domains\nthat require substantively accurate responses to open-ended prompts, we need\nreliable, computationally inexpensive methods that assess the trustworthiness\nof long-form responses generated by LLMs. However, existing approaches often\nrely on claim-by-claim fact-checking, which is computationally expensive and\nbrittle in long-form responses to open-ended prompts. In this work, we\nintroduce semantic isotropy -- the degree of uniformity across normalized text\nembeddings on the unit sphere -- and use it to assess the trustworthiness of\nlong-form responses generated by LLMs. To do so, we generate several long-form\nresponses, embed them, and estimate the level of semantic isotropy of these\nresponses as the angular dispersion of the embeddings on the unit sphere. We\nfind that higher semantic isotropy -- that is, greater embedding dispersion --\nreliably signals lower factual consistency across samples. Our approach\nrequires no labeled data, no fine-tuning, and no hyperparameter selection, and\ncan be used with open- or closed-weight embedding models. Across multiple\ndomains, our method consistently outperforms existing approaches in predicting\nnonfactuality in long-form responses using only a handful of samples --\noffering a practical, low-cost approach for integrating trust assessment into\nreal-world LLM workflows.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u5404\u5411\u540c\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u5d4c\u5165\u5728\u5355\u4f4d\u7403\u9762\u4e0a\u7684\u5206\u5e03\u5747\u5300\u6027\u6765\u8bc4\u4f30LLM\u957f\u6587\u672c\u54cd\u5e94\u7684\u53ef\u4fe1\u5ea6\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3001\u5fae\u8c03\u6216\u8d85\u53c2\u6570\u9009\u62e9\u3002", "motivation": "\u5728\u9700\u8981\u51c6\u786e\u56de\u7b54\u5f00\u653e\u6027\u95ee\u9898\u7684\u9ad8\u98ce\u9669\u5e94\u7528\u9886\u57df\u90e8\u7f72LLM\u65f6\uff0c\u9700\u8981\u53ef\u9760\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u957f\u6587\u672c\u54cd\u5e94\u7684\u53ef\u4fe1\u5ea6\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u9010\u9879\u4e8b\u5b9e\u6838\u67e5\u7684\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u8106\u5f31\u3002", "method": "\u751f\u6210\u591a\u4e2a\u957f\u6587\u672c\u54cd\u5e94\uff0c\u5c06\u5176\u5d4c\u5165\u5230\u5411\u91cf\u7a7a\u95f4\uff0c\u901a\u8fc7\u8ba1\u7b97\u5d4c\u5165\u5411\u91cf\u5728\u5355\u4f4d\u7403\u9762\u4e0a\u7684\u89d2\u5ea6\u79bb\u6563\u5ea6\u6765\u4f30\u8ba1\u8bed\u4e49\u5404\u5411\u540c\u6027\u6c34\u5e73\u3002", "result": "\u53d1\u73b0\u66f4\u9ad8\u7684\u8bed\u4e49\u5404\u5411\u540c\u6027\uff08\u5373\u66f4\u5927\u7684\u5d4c\u5165\u79bb\u6563\u5ea6\uff09\u53ef\u9760\u5730\u8868\u660e\u6837\u672c\u95f4\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8f83\u4f4e\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\u5373\u53ef\u9884\u6d4b\u957f\u6587\u672c\u54cd\u5e94\u7684\u975e\u4e8b\u5b9e\u6027\u3002", "conclusion": "\u8bed\u4e49\u5404\u5411\u540c\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u7684\u4fe1\u4efb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53ef\u96c6\u6210\u5230\u5b9e\u9645LLM\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2510.22210", "categories": ["cs.SE", "cs.AI", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22210", "abs": "https://arxiv.org/abs/2510.22210", "authors": ["Gwihwan Go", "Quan Zhang", "Chijin Zhou", "Zhao Wei", "Yu Jiang"], "title": "LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation", "comment": "13pages, 6 figures", "summary": "Automated unit test generation is essential for robust software development,\nyet existing approaches struggle to generalize across multiple programming\nlanguages and operate within real-time development. While Large Language Models\n(LLMs) offer a promising solution, their ability to generate high coverage test\ncode depends on prompting a concise context of the focal method. Current\nsolutions, such as Retrieval-Augmented Generation, either rely on imprecise\nsimilarity-based searches or demand the creation of costly, language-specific\nstatic analysis pipelines. To address this gap, we present LSPRAG, a framework\nfor concise-context retrieval tailored for real-time, language-agnostic unit\ntest generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)\nback-ends to supply LLMs with precise symbol definitions and references in real\ntime. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware\ncontext retrieval, requiring minimal per-language engineering effort. We\nevaluated LSPRAG on open-source projects spanning Java, Go, and Python.\nCompared to the best performance of baselines, LSPRAG increased line coverage\nby up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.", "AI": {"tldr": "LSPRAG\u662f\u4e00\u4e2a\u5229\u7528\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae(LSP)\u4e3aLLM\u63d0\u4f9b\u7cbe\u786e\u7b26\u53f7\u5b9a\u4e49\u548c\u5f15\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u3001\u8bed\u8a00\u65e0\u5173\u7684\u5355\u5143\u6d4b\u8bd5\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u8de8\u8bed\u8a00\u6cdb\u5316\u4e14\u65e0\u6cd5\u5b9e\u65f6\u8fd0\u884c\uff0c\u800c\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u7cbe\u786e\u7684\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u8981\u4e48\u9700\u8981\u6784\u5efa\u6602\u8d35\u7684\u8bed\u8a00\u7279\u5b9a\u9759\u6001\u5206\u6790\u7ba1\u9053\u3002", "method": "\u5229\u7528\u73b0\u6210\u7684LSP\u540e\u7aef\u4e3aLLM\u63d0\u4f9b\u7cbe\u786e\u7684\u7b26\u53f7\u5b9a\u4e49\u548c\u5f15\u7528\uff0c\u5b9e\u73b0\u8bed\u8a00\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u6700\u5c0f\u5316\u6bcf\u79cd\u8bed\u8a00\u7684\u5de5\u7a0b\u5de5\u4f5c\u91cf\u3002", "result": "\u5728Java\u3001Go\u548cPython\u7684\u5f00\u6e90\u9879\u76ee\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6700\u4f73\u6027\u80fd\uff0cLSPRAG\u5c06\u884c\u8986\u76d6\u7387\u5206\u522b\u63d0\u9ad8\u4e86213.31%\u3001174.55%\u548c31.57%\u3002", "conclusion": "LSPRAG\u901a\u8fc7\u91cd\u7528\u6210\u719f\u7684LSP\u670d\u52a1\u5668\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u8bed\u8a00\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u6027\u80fd\u3002", "topic": "swe application"}}
{"id": "2510.22254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22254", "abs": "https://arxiv.org/abs/2510.22254", "authors": ["Eric W. Bridgeford", "Iain Campbell", "Zijao Chen", "Zhicheng Lin", "Harrison Ritz", "Joachim Vandekerckhove", "Russell A. Poldrack"], "title": "Ten Simple Rules for AI-Assisted Coding in Science", "comment": "9 pages of content; 1 table; 1 page appendix", "summary": "While AI coding tools have demonstrated potential to accelerate software\ndevelopment, their use in scientific computing raises critical questions about\ncode quality and scientific validity. In this paper, we provide ten practical\nrules for AI-assisted coding that balance leveraging capabilities of AI with\nmaintaining scientific and methodological rigor. We address how AI can be\nleveraged strategically throughout the development cycle with four key themes:\nproblem preparation and understanding, managing context and interaction,\ntesting and validation, and code quality assurance and iterative improvement.\nThese principles serve to emphasize maintaining human agency in coding\ndecisions, establishing robust validation procedures, and preserving the domain\nexpertise essential for methodologically sound research. These rules are\nintended to help researchers harness AI's transformative potential for faster\nsoftware development while ensuring that their code meets the standards of\nreliability, reproducibility, and scientific validity that research integrity\ndemands.", "AI": {"tldr": "\u63d0\u51fa\u4e8610\u6761AI\u8f85\u52a9\u7f16\u7a0b\u7684\u5b9e\u7528\u89c4\u5219\uff0c\u65e8\u5728\u5e73\u8861AI\u80fd\u529b\u5229\u7528\u4e0e\u79d1\u5b66\u65b9\u6cd5\u4e25\u8c28\u6027\uff0c\u786e\u4fdd\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u4ee3\u7801\u8d28\u91cf\u548c\u7814\u7a76\u6709\u6548\u6027\u3002", "motivation": "AI\u7f16\u7a0b\u5de5\u5177\u5728\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u4e2d\u5f15\u53d1\u4e86\u5173\u4e8e\u4ee3\u7801\u8d28\u91cf\u548c\u79d1\u5b66\u6709\u6548\u6027\u7684\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u6307\u5bfc\u539f\u5219\u6765\u786e\u4fdd\u7814\u7a76\u5b8c\u6574\u6027\u3002", "method": "\u56f4\u7ed5\u56db\u4e2a\u5173\u952e\u4e3b\u9898\u5236\u5b9a10\u6761\u5b9e\u8df5\u89c4\u5219\uff1a\u95ee\u9898\u51c6\u5907\u4e0e\u7406\u89e3\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e0e\u4ea4\u4e92\u3001\u6d4b\u8bd5\u4e0e\u9a8c\u8bc1\u3001\u4ee3\u7801\u8d28\u91cf\u4fdd\u8bc1\u4e0e\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u7cfb\u7edf\u6027\u7684AI\u8f85\u52a9\u7f16\u7a0b\u6307\u5bfc\u539f\u5219\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5728\u5229\u7528AI\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u4ee3\u7801\u7684\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u79d1\u5b66\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u89c4\u5219\u5f3a\u8c03\u5728\u7f16\u7801\u51b3\u7b56\u4e2d\u4fdd\u6301\u4eba\u7c7b\u4e3b\u5bfc\u6743\uff0c\u5efa\u7acb\u7a33\u5065\u7684\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u5e76\u4fdd\u62a4\u65b9\u6cd5\u8bba\u4e25\u8c28\u7814\u7a76\u6240\u9700\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "topic": "swe application"}}
{"id": "2510.21830", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21830", "abs": "https://arxiv.org/abs/2510.21830", "authors": ["Jianqing Zhang", "Zhezheng Hao", "Wei Xia", "Hande Dong", "Hong Wang", "Chenxing Wei", "Yuyan Zhou", "Yubin Qi", "Qiang Lin", "Jian Cao"], "title": "GAPO: Group Adaptive Policy Optimization for Real-World Code Edit", "comment": null, "summary": "Reinforcement learning (RL) is widely used for post-training large language\nmodels (LLMs) in code editing, where group-relative methods like GRPO are\npopular for their critic-free, normalized advantage estimation. However, in\nreal-world code-editing scenarios, reward distributions are often skewed with\nunpredictable outliers, leading to distorted advantage computation and\nincreased noise. To address this issue, we propose Group Adaptive Policy\nOptimization (GAPO), which adaptively finds an outlier-free highest-density\ninterval (HDI) per prompt and then uses the median of that interval as an\nadaptive Q to replace the group mean in advantage calculation. This adaptive Q\nrobustly handles skewed distributions while remaining plug-and-play and\nefficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a\nlarge internal dataset of 51,844 real-world, history-aware code-editing tasks\nacross 10 languages, demonstrating consistent improvements in exact match\naccuracy over GRPO and its variant DAPO. Code is publicly available.", "AI": {"tldr": "\u63d0\u51faGAPO\u65b9\u6cd5\u89e3\u51b3\u4ee3\u7801\u7f16\u8f91\u4e2d\u5956\u52b1\u5206\u5e03\u504f\u659c\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5bfb\u627e\u65e0\u5f02\u5e38\u503c\u7684\u9ad8\u5bc6\u5ea6\u533a\u95f4\u6765\u6539\u8fdb\u4f18\u52bf\u8ba1\u7b97\uff0c\u5728\u771f\u5b9e\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\u548cDAPO\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u7f16\u8f91\u573a\u666f\u4e2d\uff0c\u5956\u52b1\u5206\u5e03\u5e38\u5e38\u504f\u659c\u4e14\u5305\u542b\u4e0d\u53ef\u9884\u6d4b\u7684\u5f02\u5e38\u503c\uff0c\u5bfc\u81f4\u4f18\u52bf\u8ba1\u7b97\u5931\u771f\u548c\u566a\u58f0\u589e\u52a0\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGroup Adaptive Policy Optimization (GAPO)\uff0c\u81ea\u9002\u5e94\u5bfb\u627e\u6bcf\u4e2a\u63d0\u793a\u7684\u65e0\u5f02\u5e38\u503c\u6700\u9ad8\u5bc6\u5ea6\u533a\u95f4\uff0c\u4f7f\u7528\u8be5\u533a\u95f4\u4e2d\u4f4d\u6570\u4f5c\u4e3a\u81ea\u9002\u5e94Q\u503c\u66ff\u4ee3\u7ec4\u5747\u503c\u8fdb\u884c\u4f18\u52bf\u8ba1\u7b97\u3002", "result": "\u57289\u4e2a\u6307\u4ee4\u8c03\u4f18LLM\uff083B-14B\uff09\u548c51,844\u4e2a\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cGAPO\u5728\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u4f18\u4e8eGRPO\u53ca\u5176\u53d8\u4f53DAPO\u3002", "conclusion": "GAPO\u80fd\u7a33\u5065\u5904\u7406\u504f\u659c\u5206\u5e03\uff0c\u4fdd\u6301\u5373\u63d2\u5373\u7528\u548c\u9ad8\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22318", "categories": ["cs.SE", "cs.AI", "K.3.2, D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22318", "abs": "https://arxiv.org/abs/2510.22318", "authors": ["Tuan-Phong Ngo", "Bao-Ngoc Duong", "Tuan-Anh Hoang", "Joshua Dwight", "Ushik Shrestha Khwakhali"], "title": "Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus", "comment": "7 pages, 3 figures, 3 tables", "summary": "Software testing is a critical component in the software engineering field\nand is important for software engineering education. Thus, it is vital for\nacademia to continuously improve and update educational methods to reflect the\ncurrent state of the field. The International Software Testing Qualifications\nBoard (ISTQB) certification framework is globally recognized and widely adopted\nin industry and academia. However, ISTQB-based learning has been rarely applied\nwith recent generative artificial intelligence advances. Despite the growing\ncapabilities of large language models (LLMs), ISTQB-based learning and\ninstruction with LLMs have not been thoroughly explored. This paper explores\nand evaluates how LLMs can complement the ISTQB framework for higher education.\nThe findings present four key contributions: (i) the creation of a\ncomprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28\nsample exams and 1,145 questions; (ii) the development of a domain-optimized\nprompt that enhances LLM precision and explanation quality on ISTQB tasks;\n(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and\n(iv) actionable insights and recommendations for integrating LLMs into software\ntesting education. These findings highlight the promise of LLMs in supporting\nISTQB certification preparation and offer a foundation for their broader use in\nsoftware engineering at higher education.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8865\u5145ISTQB\u6846\u67b6\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u521b\u5efa\u4e86ISTQB\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u9886\u57df\u4f18\u5316\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86LLMs\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u8f6f\u4ef6\u6d4b\u8bd5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u548c\u6559\u80b2\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46ISTQB\u8ba4\u8bc1\u6846\u67b6\u4e0e\u6700\u65b0\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7b49\u6559\u80b2\u73af\u5883\u4e2d\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b28\u4e2a\u6837\u672c\u8003\u8bd5\u548c1145\u4e2a\u95ee\u9898\u7684ISTQB\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u9886\u57df\u4f18\u5316\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684LLMs\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u652f\u6301ISTQB\u8ba4\u8bc1\u51c6\u5907\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u63d0\u793a\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLM\u5728ISTQB\u4efb\u52a1\u4e0a\u7684\u7cbe\u786e\u5ea6\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "LLMs\u6709\u671b\u652f\u6301ISTQB\u8ba4\u8bc1\u51c6\u5907\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u9ad8\u7b49\u6559\u80b2\u4e2d\u66f4\u5e7f\u6cdb\u5730\u4f7f\u7528LLMs\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5c06LLMs\u6574\u5408\u5230\u8f6f\u4ef6\u6d4b\u8bd5\u6559\u80b2\u4e2d\u7684\u53ef\u884c\u5efa\u8bae\u3002", "topic": "swe application"}}
{"id": "2510.22009", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22009", "abs": "https://arxiv.org/abs/2510.22009", "authors": ["Yangqin Jiang", "Chao Huang"], "title": "LightAgent: Mobile Agentic Foundation Models", "comment": null, "summary": "With the advancement of multimodal large language models (MLLMs), building\nGUI agent systems has become an increasingly promising direction-especially for\nmobile platforms, given their rich app ecosystems and intuitive touch\ninteractions. Yet mobile GUI agents face a critical dilemma: truly on-device\nmodels (4B or smaller) lack sufficient performance, while capable models\n(starting from 7B) are either too large for mobile deployment or prohibitively\ncostly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose\nLightAgent, a mobile agentic foundation model solution that leverages\ndevice-cloud collaboration to tap the cost-efficiency of on-device models and\nthe high capability of cloud models, while avoiding their drawbacks.\nSpecifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO\ntraining on synthetic GUI data for strong decision-making, integrates an\nefficient long-reasoning mechanism to utilize historical interactions under\ntight resources, and defaults to on-device execution-only escalating\nchallenging subtasks to the cloud via real-time complexity assessment.\nExperiments on the online AndroidLab benchmark and diverse apps show LightAgent\nmatches or nears larger models, with a significant reduction in cloud costs.", "AI": {"tldr": "LightAgent\u662f\u4e00\u4e2a\u79fb\u52a8GUI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bbe\u5907-\u4e91\u534f\u4f5c\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u4e0a\u5c0f\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u548c\u5927\u6a21\u578b\u90e8\u7f72\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u5b83\u57fa\u4e8eQwen2.5-VL-3B\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u9ad8\u6548\u957f\u63a8\u7406\u673a\u5236\uff0c\u5728\u4fdd\u6301\u4f4e\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8GUI\u4ee3\u7406\u9762\u4e34\u7684\u56f0\u5883\uff1a\u771f\u6b63\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u6a21\u578b\uff084B\u6216\u66f4\u5c0f\uff09\u6027\u80fd\u4e0d\u8db3\uff0c\u800c\u6027\u80fd\u8db3\u591f\u7684\u6a21\u578b\uff08\u4ece7B\u5f00\u59cb\uff09\u8981\u4e48\u592a\u5927\u65e0\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u8981\u4e48\u6210\u672c\u8fc7\u9ad8\uff08\u5982\u4ec5\u9650\u4e91\u7aef\u7684\u95ed\u6e90MLLMs\uff09\u3002", "method": "\u63d0\u51faLightAgent\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u8bbe\u5907-\u4e91\u534f\u4f5c\u6a21\u5f0f\uff1a1\uff09\u901a\u8fc7\u4e24\u9636\u6bb5SFT->GRPO\u8bad\u7ec3\u589e\u5f3aQwen2.5-VL-3B\u6a21\u578b\u7684\u51b3\u7b56\u80fd\u529b\uff1b2\uff09\u96c6\u6210\u9ad8\u6548\u957f\u63a8\u7406\u673a\u5236\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5229\u7528\u5386\u53f2\u4ea4\u4e92\uff1b3\uff09\u9ed8\u8ba4\u5728\u8bbe\u5907\u4e0a\u6267\u884c\uff0c\u4ec5\u901a\u8fc7\u5b9e\u65f6\u590d\u6742\u5ea6\u8bc4\u4f30\u5c06\u5177\u6709\u6311\u6218\u6027\u7684\u5b50\u4efb\u52a1\u5347\u7ea7\u5230\u4e91\u7aef\u5904\u7406\u3002", "result": "\u5728AndroidLab\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6837\u5316\u5e94\u7528\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLightAgent\u80fd\u591f\u5339\u914d\u6216\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e91\u6210\u672c\u3002", "conclusion": "LightAgent\u901a\u8fc7\u8bbe\u5907-\u4e91\u534f\u4f5c\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8GUI\u4ee3\u7406\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.22039", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.22039", "abs": "https://arxiv.org/abs/2510.22039", "authors": ["Po-Chen Kuo", "Han Hou", "Will Dabney", "Edgar Y. Walker"], "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability", "comment": "Accepted to Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2025", "summary": "Learning a compact representation of history is critical for planning and\ngeneralization in partially observable environments. While meta-reinforcement\nlearning (RL) agents can attain near Bayes-optimal policies, they often fail to\nlearn the compact, interpretable Bayes-optimal belief states. This\nrepresentational inefficiency potentially limits the agent's adaptability and\ngeneralization capacity. Inspired by predictive coding in neuroscience--which\nsuggests that the brain predicts sensory inputs as a neural implementation of\nBayesian inference--and by auxiliary predictive objectives in deep RL, we\ninvestigate whether integrating self-supervised predictive coding modules into\nmeta-RL can facilitate learning of Bayes-optimal representations. Through state\nmachine simulation, we show that meta-RL with predictive modules consistently\ngenerates more interpretable representations that better approximate\nBayes-optimal belief states compared to conventional meta-RL across a wide\nvariety of tasks, even when both achieve optimal policies. In challenging tasks\nrequiring active information seeking, only meta-RL with predictive modules\nsuccessfully learns optimal representations and policies, whereas conventional\nmeta-RL struggles with inadequate representation learning. Finally, we\ndemonstrate that better representation learning leads to improved\ngeneralization. Our results strongly suggest the role of predictive learning as\na guiding principle for effective representation learning in agents navigating\npartial observability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u6574\u5408\u81ea\u76d1\u7763\u9884\u6d4b\u7f16\u7801\u6a21\u5757\uff0c\u4ee5\u4fc3\u8fdb\u5b66\u4e60\u8d1d\u53f6\u65af\u6700\u4f18\u8868\u793a\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u63d0\u5347\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5143\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5b66\u4e60\u5230\u63a5\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u7684\u7b56\u7565\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u5b66\u4e60\u5230\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u8d1d\u53f6\u65af\u6700\u4f18\u4fe1\u5ff5\u72b6\u6001\uff0c\u8fd9\u79cd\u8868\u793a\u6548\u7387\u4f4e\u4e0b\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u9884\u6d4b\u7f16\u7801\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u8f85\u52a9\u9884\u6d4b\u76ee\u6807\u7684\u542f\u53d1\uff0c\u5c06\u81ea\u76d1\u7763\u9884\u6d4b\u7f16\u7801\u6a21\u5757\u6574\u5408\u5230\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u72b6\u6001\u673a\u6a21\u62df\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u5143\u5f3a\u5316\u5b66\u4e60\uff0c\u5e26\u6709\u9884\u6d4b\u6a21\u5757\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u80fd\u751f\u6210\u66f4\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u66f4\u597d\u5730\u903c\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u4fe1\u5ff5\u72b6\u6001\u3002\u5728\u9700\u8981\u4e3b\u52a8\u4fe1\u606f\u641c\u7d22\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u53ea\u6709\u5e26\u6709\u9884\u6d4b\u6a21\u5757\u7684\u65b9\u6cd5\u80fd\u6210\u529f\u5b66\u4e60\u6700\u4f18\u8868\u793a\u548c\u7b56\u7565\u3002", "conclusion": "\u9884\u6d4b\u5b66\u4e60\u662f\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u8868\u793a\u5b66\u4e60\u7684\u6307\u5bfc\u539f\u5219\uff0c\u66f4\u597d\u7684\u8868\u793a\u5b66\u4e60\u80fd\u5e26\u6765\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21836", "abs": "https://arxiv.org/abs/2510.21836", "authors": ["Jaya Krishna Mandivarapu"], "title": "COLA: Continual Learning via Autoencoder Retrieval of Adapters", "comment": null, "summary": "Learning a set of tasks over time, also known as continual learning (CL), is\none of the most challenging problems in artificial intelligence due to\ncatastrophic forgetting. Large language models (LLMs) are often impractical to\nfrequent re-training and continual learning , due to high cost of computational\nresources for training. Moreover, LLM are not suitable for continual learning\nas updating these models over time for acquiring new knowledge leads to\noverwrites existing knowledge leading to common phenomenon know as\n\\textit{catastrophic forgetting}. In this paper, we aim to address these\nconcerns using a novel framework , COLA that employs an autoencoder to learn\ncapture low-dimensional embeddings of the weights associated with various\ntasks. Our approach facilitates the transfer of knowledge to new tasks while\npreventing catastrophic forgetting, all without using data replay or a\nsubstantial set of task-specific parameters. Our approach, COLA, makes the LLM\nefficiently learn new tasks with minimal training, insignificant performance\ndegradation on previous tasks, and eliminates the need for retaining earlier\ntraining data. Empirical evaluation on different datasets ranging from task\noriented dialouge system to intent classsfication datasets showcases that our\nmethod not only overcomes catastrophic forgetting but also achieves significant\nreduction in parameter usage and memory size, across multiple tasks and\noutperforming the existing state of the art methods across multiple datasets.", "AI": {"tldr": "\u63d0\u51faCOLA\u6846\u67b6\u89e3\u51b3LLM\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4efb\u52a1\u6743\u91cd\u7684\u4f4e\u7ef4\u5d4c\u5165\uff0c\u65e0\u9700\u6570\u636e\u56de\u653e\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u53c2\u6570", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u907f\u514d\u9891\u7e41\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u8ba1\u7b97\u6210\u672c", "method": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4efb\u52a1\u6743\u91cd\u7684\u4f4e\u7ef4\u5d4c\u5165\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\u540c\u65f6\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0d\u4ec5\u514b\u670d\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd8\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u4f7f\u7528\u548c\u5185\u5b58\u5360\u7528\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "COLA\u6846\u67b6\u4f7fLLM\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5bf9\u5148\u524d\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u5c0f\uff0c\u4e14\u65e0\u9700\u4fdd\u7559\u65e9\u671f\u8bad\u7ec3\u6570\u636e", "topic": "agentic reinforcement learning"}}
{"id": "2510.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22530", "abs": "https://arxiv.org/abs/2510.22530", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "comment": "11 pages, 8 figures, under review", "summary": "Fault Localization (FL) aims to identify root causes of program failures. FL\ntypically targets failures observed from test executions, and as such, often\ninvolves dynamic analyses to improve accuracy, such as coverage profiling or\nmutation testing. However, for large industrial software, measuring coverage\nfor every execution is prohibitively expensive, making the use of such\ntechniques difficult. To address these issues and apply FL in an industrial\nsetting, this paper proposes AutoCrashFL, an LLM agent for the localization of\ncrashes that only requires the crashdump from the Program Under Test (PUT) and\naccess to the repository of the corresponding source code. We evaluate\nAutoCrashFL against real-world crashes of SAP HANA, an industrial software\nproject consisting of more than 35 million lines of code. Experiments reveal\nthat AutoCrashFL is more effective in localization, as it identified 30%\ncrashes at the top, compared to 17% achieved by the baseline. Through thorough\nanalysis, we find that AutoCrashFL has attractive practical properties: it is\nrelatively more effective for complex bugs, and it can indicate confidence in\nits results. Overall, these results show the practicality of LLM agent\ndeployment on an industrial scale.", "AI": {"tldr": "AutoCrashFL\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ec5\u9700\u5d29\u6e83\u8f6c\u50a8\u548c\u6e90\u4ee3\u7801\u4ed3\u5e93\u5373\u53ef\u5728\u5de5\u4e1a\u7ea7\u8f6f\u4ef6\u4e2d\u5b9a\u4f4d\u5d29\u6e83\u539f\u56e0\uff0c\u5728SAP HANA\u9879\u76ee\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u52a8\u6001\u5206\u6790\u5982\u8986\u76d6\u7387\u5206\u6790\uff0c\u8fd9\u5728\u5927\u578b\u5de5\u4e1a\u8f6f\u4ef6\u4e2d\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5d29\u6e83\u8f6c\u50a8\u548c\u6e90\u4ee3\u7801\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAutoCrashFL LLM\u4ee3\u7406\uff0c\u4ec5\u4f7f\u7528\u7a0b\u5e8f\u5d29\u6e83\u8f6c\u50a8\u548c\u5bf9\u5e94\u6e90\u4ee3\u7801\u4ed3\u5e93\u8fdb\u884c\u6545\u969c\u5b9a\u4f4d\uff0c\u65e0\u9700\u52a8\u6001\u5206\u6790\u3002", "result": "\u5728SAP HANA\uff08\u8d85\u8fc73500\u4e07\u884c\u4ee3\u7801\uff09\u7684\u771f\u5b9e\u5d29\u6e83\u6d4b\u8bd5\u4e2d\uff0cAutoCrashFL\u5728top\u4f4d\u7f6e\u8bc6\u522b\u51fa30%\u7684\u5d29\u6e83\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u4e3a17%\u3002", "conclusion": "AutoCrashFL\u5728\u5de5\u4e1a\u89c4\u6a21\u4e0a\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\uff0c\u5bf9\u590d\u6742bug\u66f4\u6709\u6548\u4e14\u80fd\u6307\u793a\u7ed3\u679c\u7f6e\u4fe1\u5ea6\u3002", "topic": "code agent"}}
{"id": "2510.22614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22614", "abs": "https://arxiv.org/abs/2510.22614", "authors": ["Roham Koohestani", "Agnia Sergeyuk", "David Gros", "Claudio Spiess", "Sergey Titov", "Prem Devanbu", "Maliheh Izadi"], "title": "Does In-IDE Calibration of Large Language Models work at Scale?", "comment": "Under Review", "summary": "The introduction of large language models into integrated development\nenvironments (IDEs) is revolutionizing software engineering, yet it poses\nchallenges to the usefulness and reliability of Artificial\nIntelligence-generated code. Post-hoc calibration of internal model confidences\naims to align probabilities with an acceptability measure. Prior work suggests\ncalibration can improve alignment, but at-scale evidence is limited. In this\nwork, we investigate the feasibility of applying calibration of code models to\nan in-IDE context. We study two aspects of the problem: (1) the technical\nmethod for implementing confidence calibration and improving the reliability of\ncode generation models, and (2) the human-centered design principles for\neffectively communicating reliability signal to developers. First, we develop a\nscalable and flexible calibration framework which can be used to obtain\ncalibration weights for open-source models using any dataset, and evaluate\nwhether calibrators improve the alignment between model confidence and\ndeveloper acceptance behavior. Through a large-scale analysis of over 24\nmillion real-world developer interactions across multiple programming\nlanguages, we find that a general, post-hoc calibration model based on\nPlatt-scaling does not, on average, improve the reliability of model confidence\nsignals. We also find that while dynamically personalizing calibration to\nindividual users can be effective, its effectiveness is highly dependent on the\nvolume of user interaction data. Second, we conduct a multi-phase design study\nwith 3 expert designers and 153 professional developers, combining\nscenario-based design, semi-structured interviews, and survey validation,\nrevealing a clear preference for presenting reliability signals via\nnon-numerical, color-coded indicators within the in-editor code generation\nworkflow.", "AI": {"tldr": "\u7814\u7a76\u4ee3\u7801\u6a21\u578b\u5728IDE\u73af\u5883\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u901a\u7528\u7684\u540e\u5904\u7406\u6821\u51c6\u65b9\u6cd5\u4e0d\u80fd\u663e\u8457\u6539\u5584\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u53ef\u9760\u6027\uff0c\u800c\u4e2a\u6027\u5316\u6821\u51c6\u9700\u8981\u5927\u91cf\u7528\u6237\u6570\u636e\u3002\u540c\u65f6\u901a\u8fc7\u8bbe\u8ba1\u7814\u7a76\u53d1\u73b0\u5f00\u53d1\u8005\u504f\u597d\u4f7f\u7528\u975e\u6570\u503c\u7684\u989c\u8272\u7f16\u7801\u6307\u793a\u5668\u6765\u5448\u73b0\u53ef\u9760\u6027\u4fe1\u53f7\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230IDE\u4e2d\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f46AI\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u9762\u4e34\u6311\u6218\u3002\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65e8\u5728\u4f7f\u6a21\u578b\u6982\u7387\u4e0e\u53ef\u63a5\u53d7\u6027\u5ea6\u91cf\u5bf9\u9f50\uff0c\u4f46\u5927\u89c4\u6a21\u8bc1\u636e\u6709\u9650\u3002", "method": "\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u6821\u51c6\u6846\u67b6\uff0c\u4f7f\u7528Platt\u7f29\u653e\u8fdb\u884c\u540e\u5904\u7406\u6821\u51c6\uff1b\u5206\u67902400\u4e07\u6b21\u771f\u5b9e\u5f00\u53d1\u8005\u4ea4\u4e92\u6570\u636e\uff1b\u8fdb\u884c\u591a\u9636\u6bb5\u8bbe\u8ba1\u7814\u7a76\uff0c\u5305\u62ec\u57fa\u4e8e\u573a\u666f\u7684\u8bbe\u8ba1\u3001\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u8c03\u67e5\u9a8c\u8bc1\u3002", "result": "\u901a\u7528\u7684\u540e\u5904\u7406\u6821\u51c6\u6a21\u578b\u5e73\u5747\u4e0d\u80fd\u6539\u5584\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u53ef\u9760\u6027\uff1b\u4e2a\u6027\u5316\u6821\u51c6\u6709\u6548\u4f46\u9ad8\u5ea6\u4f9d\u8d56\u7528\u6237\u4ea4\u4e92\u6570\u636e\u91cf\uff1b\u5f00\u53d1\u8005\u660e\u786e\u504f\u597d\u4f7f\u7528\u975e\u6570\u503c\u7684\u989c\u8272\u7f16\u7801\u6307\u793a\u5668\u3002", "conclusion": "\u5728IDE\u73af\u5883\u4e2d\u5e94\u7528\u4ee3\u7801\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5177\u6709\u6311\u6218\u6027\uff0c\u901a\u7528\u6821\u51c6\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u800c\u4e2a\u6027\u5316\u6821\u51c6\u9700\u8981\u5927\u91cf\u6570\u636e\u652f\u6301\u3002\u53ef\u89c6\u5316\u5448\u73b0\u65b9\u9762\uff0c\u989c\u8272\u7f16\u7801\u7684\u975e\u6570\u503c\u6307\u793a\u5668\u66f4\u53d7\u5f00\u53d1\u8005\u6b22\u8fce\u3002", "topic": "swe application"}}
{"id": "2510.22095", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22095", "abs": "https://arxiv.org/abs/2510.22095", "authors": ["Yankai Chen", "Xinni Zhang", "Yifei Zhang", "Yangning Li", "Henry Peng Zou", "Chunyu Miao", "Weizhi Zhang", "Xue Liu", "Philip S. Yu"], "title": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies", "comment": "Accepted by NeurIPS'25 Position Track", "summary": "Brain-Computer Interfaces (BCIs) offer a direct communication pathway between\nthe human brain and external devices, holding significant promise for\nindividuals with severe neurological impairments. However, their widespread\nadoption is hindered by critical limitations, such as low information transfer\nrates and extensive user-specific calibration. To overcome these challenges,\nrecent research has explored the integration of Large Language Models (LLMs),\nextending the focus from simple command decoding to understanding complex\ncognitive states. Despite these advancements, deploying agentic AI faces\ntechnical hurdles and ethical concerns. Due to the lack of comprehensive\ndiscussion on this emerging direction, this position paper argues that the\nfield is poised for a paradigm extension from BCI to Brain-Agent Collaboration\n(BAC). We emphasize reframing agents as active and collaborative partners for\nintelligent assistance rather than passive brain signal data processors,\ndemanding a focus on ethical data handling, model reliability, and a robust\nhuman-agent collaboration framework to ensure these systems are safe,\ntrustworthy, and effective.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63d0\u51fa\u4ece\u8111\u673a\u63a5\u53e3(BCI)\u5411\u8111-\u667a\u80fd\u4f53\u534f\u4f5c(BAC)\u7684\u8303\u5f0f\u6269\u5c55\uff0c\u5f3a\u8c03\u5c06\u667a\u80fd\u4f53\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e3b\u52a8\u534f\u4f5c\u4f19\u4f34\u800c\u975e\u88ab\u52a8\u8111\u4fe1\u53f7\u5904\u7406\u5668\u3002", "motivation": "\u8111\u673a\u63a5\u53e3\u5b58\u5728\u4fe1\u606f\u4f20\u8f93\u7387\u4f4e\u548c\u7528\u6237\u7279\u5b9a\u6821\u51c6\u7b49\u9650\u5236\uff0c\u800c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u4e00\u65b0\u5174\u65b9\u5411\u7684\u5168\u9762\u8ba8\u8bba\uff0c\u9700\u8981\u89e3\u51b3\u6280\u672f\u969c\u788d\u548c\u4f26\u7406\u5173\u5207\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u91cd\u70b9\u4ece\u7b80\u5355\u547d\u4ee4\u89e3\u7801\u6269\u5c55\u5230\u7406\u89e3\u590d\u6742\u8ba4\u77e5\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u8111-\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u8111-\u667a\u80fd\u4f53\u534f\u4f5c(BAC)\u4f5c\u4e3a\u8111\u673a\u63a5\u53e3\u7684\u8303\u5f0f\u6269\u5c55\uff0c\u5f3a\u8c03\u667a\u80fd\u4f53\u4f5c\u4e3a\u4e3b\u52a8\u534f\u4f5c\u4f19\u4f34\u7684\u89d2\u8272\u8f6c\u53d8\u3002", "conclusion": "\u9700\u8981\u5173\u6ce8\u4f26\u7406\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u53ef\u9760\u6027\u548c\u7a33\u5065\u7684\u4eba-\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u3001\u53ef\u4fe1\u4e14\u6709\u6548\u3002", "topic": "agent analysis"}}
{"id": "2510.22099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22099", "abs": "https://arxiv.org/abs/2510.22099", "authors": ["Xuanming Zhang"], "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering", "comment": null, "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u548c\u63a7\u5236LLM\u7684\u6cdb\u5316\u4e0e\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u5f00\u53d1\u4e86\u52a8\u6001\u6a21\u5f0f\u5f15\u5bfc\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u63a2\u9488\u548c\u52a8\u6001\u6fc0\u6d3b\u5f15\u5bfc\u6765\u63d0\u5347LLM\u7684\u53ef\u9760\u6027\u548c\u63a8\u7406\u4e00\u81f4\u6027\u3002", "motivation": "LLM\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u548c\u9010\u5b57\u8bb0\u5fc6\u7684\u53cc\u91cd\u7279\u6027\uff0c\u8fd9\u79cd\u4e0d\u53ef\u9884\u6d4b\u6027\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u524a\u5f31\u4e86\u5176\u53ef\u9760\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u63a7\u5236\u8fd9\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u5efa\u7acb\u7406\u8bba\u6a21\u578b\uff0c\u5f00\u53d1\u52a8\u6001\u6a21\u5f0f\u5f15\u5bfc\u7b97\u6cd5\uff0c\u5305\u542b\u56e0\u679c\u57fa\u7840\u7684\u7ebf\u6027\u63a2\u9488\u6765\u8bc6\u522b\u8bb0\u5fc6\u4f9d\u8d56\uff0c\u4ee5\u53ca\u52a8\u6001\u6fc0\u6d3b\u5f15\u5bfc\u673a\u5236\u5c06\u8ba1\u7b97\u63a8\u5411\u6cdb\u5316\u7535\u8def\u3002", "result": "\u5728\u63a8\u7406\u548c\u771f\u5b9e\u6027\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMS\u663e\u8457\u63d0\u9ad8\u4e86\u903b\u8f91\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "conclusion": "DMS\u4e3a\u589e\u5f3aLLM\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u81ea\u5bf9\u6bd4\u89e3\u7801\u5b9e\u73b0\u4e86\u5bf9\u6cdb\u5316\u548c\u8bb0\u5fc6\u6a21\u5f0f\u7684\u6709\u6548\u63a7\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.22170", "categories": ["cs.AI", "I.2.7; I.2.6; H.1.2; J.4"], "pdf": "https://arxiv.org/pdf/2510.22170", "abs": "https://arxiv.org/abs/2510.22170", "authors": ["Alexandra Yost", "Shreyans Jain", "Shivam Raval", "Grant Corser", "Allen Roush", "Nina Xu", "Jacqueline Hammack", "Ravid Shwartz-Ziv", "Amirali Abdullah"], "title": "Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests", "comment": "49 pages", "summary": "AI psychometrics evaluates AI systems in roles that traditionally require\nemotional judgment and ethical consideration. Prior work often reuses human\ntrait inventories (Big Five, \\hexaco) or ad hoc personas, limiting behavioral\nrealism and domain relevance. We propose a framework that (1) uses situational\njudgment tests (SJTs) from realistic scenarios to probe domain-specific\ncompetencies; (2) integrates industrial-organizational and personality\npsychology to design sophisticated personas which include behavioral and\npsychological descriptors, life history, and social and emotional functions;\nand (3) employs structured generation with population demographic priors and\nmemoir inspired narratives, encoded with Pydantic schemas. In a law enforcement\nassistant case study, we construct a rich dataset of personas drawn across 8\npersona archetypes and SJTs across 11 attributes, and analyze behaviors across\nsubpopulation and scenario slices. The dataset spans 8,500 personas, 4,000\nSJTs, and 300,000 responses. We will release the dataset and all code to the\npublic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u5fc3\u7406\u6d4b\u91cf\u6846\u67b6\uff0c\u4f7f\u7528\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\u548c\u590d\u6742\u4eba\u7269\u89d2\u8272\u8bbe\u8ba1\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u9700\u8981\u60c5\u611f\u5224\u65ad\u548c\u4f26\u7406\u8003\u91cf\u7684\u89d2\u8272\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u91cd\u590d\u4f7f\u7528\u4eba\u7c7b\u7279\u8d28\u6e05\u5355\u6216\u4e34\u65f6\u89d2\u8272\uff0c\u9650\u5236\u4e86\u884c\u4e3a\u771f\u5b9e\u6027\u548c\u9886\u57df\u76f8\u5173\u6027\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a(1)\u4f7f\u7528\u73b0\u5b9e\u573a\u666f\u7684\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\uff1b(2)\u6574\u5408\u5de5\u4e1a\u7ec4\u7ec7\u5fc3\u7406\u5b66\u548c\u4eba\u683c\u5fc3\u7406\u5b66\u8bbe\u8ba1\u590d\u6742\u89d2\u8272\uff1b(3)\u91c7\u7528\u7ed3\u6784\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u542b\u4eba\u53e3\u7edf\u8ba1\u5148\u9a8c\u548c\u56de\u5fc6\u5f55\u5f0f\u53d9\u8ff0\u3002", "result": "\u5728\u6267\u6cd5\u52a9\u624b\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6784\u5efa\u4e86\u5305\u542b8,500\u4e2a\u89d2\u8272\u30014,000\u4e2a\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\u548c300,000\u4e2a\u54cd\u5e94\u7684\u4e30\u5bcc\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u4e13\u4e1a\u89d2\u8272\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c06\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "topic": "agent analysis"}}
{"id": "2510.23010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23010", "abs": "https://arxiv.org/abs/2510.23010", "authors": ["Ming-Tung Shen", "Yuh-Jzer Joung"], "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation", "comment": null, "summary": "Agentic code generation requires large language models (LLMs) capable of\ncomplex context management and multi-step reasoning. Prior multi-agent\nframeworks attempt to address these challenges through collaboration, yet they\noften suffer from rigid workflows and high reasoning recovery costs. To\novercome these limitations, we propose TALM (Tree-Structured Multi-Agent\nFramework with Long-Term Memory), a dynamic framework that integrates\nstructured task decomposition, localized re-reasoning, and long-term memory\nmechanisms. TALM employs an extensible tree-based collaboration structure. The\nparent-child relationships, when combined with a divide-and-conquer strategy,\nenhance reasoning flexibility and enable efficient error correction across\ndiverse task scopes. Furthermore, a long-term memory module enables semantic\nquerying and integration of prior knowledge, supporting implicit\nself-improvement through experience reuse. Experimental results on HumanEval,\nBigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently\ndelivers strong reasoning performance and high token efficiency, highlighting\nits robustness and practical utility in complex code generation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86TALM\u6846\u67b6\uff0c\u4e00\u79cd\u96c6\u6210\u6811\u72b6\u7ed3\u6784\u4efb\u52a1\u5206\u89e3\u3001\u5c40\u90e8\u91cd\u63a8\u7406\u548c\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u6027\u80fd\u548c\u9ad8\u6548\u7684token\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b58\u5728\u5de5\u4f5c\u6d41\u7a0b\u50f5\u5316\u548c\u63a8\u7406\u6062\u590d\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u590d\u6742\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u7684\u6811\u72b6\u534f\u4f5c\u7ed3\u6784\uff0c\u7ed3\u5408\u5206\u6cbb\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u7075\u6d3b\u6027\uff1b\u5f15\u5165\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u652f\u6301\u8bed\u4e49\u67e5\u8be2\u548c\u5148\u9a8c\u77e5\u8bc6\u96c6\u6210\uff1b\u901a\u8fc7\u7236\u5b50\u5173\u7cfb\u5b9e\u73b0\u9ad8\u6548\u9519\u8bef\u7ea0\u6b63\u3002", "result": "\u5728HumanEval\u3001BigCodeBench\u548cClassEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTALM\u6846\u67b6\u5c55\u73b0\u51fa\u6301\u7eed\u5f3a\u5927\u7684\u63a8\u7406\u6027\u80fd\u548c\u9ad8token\u6548\u7387\u3002", "conclusion": "TALM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6811\u72b6\u7ed3\u6784\u548c\u957f\u671f\u8bb0\u5fc6\u673a\u5236\uff0c\u5728\u590d\u6742\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "2510.21861", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.21861", "abs": "https://arxiv.org/abs/2510.21861", "authors": ["Bentley DeVilling"], "title": "The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems", "comment": "18 pages, 2 figures. Category: cs.LG. Code and data:\n  https://github.com/Course-Correct-Labs/mirror-loop", "summary": "Large language models are often described as capable of reflective reasoning,\nyet recursive self-evaluation without external feedback frequently yields\nreformulation rather than progress. We test this prediction in a cross-provider\nstudy of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,\nAnthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families\n(arithmetic, code, explanation, reflection), each iterated ten times under two\nconditions: ungrounded self-critique and a minimal grounding intervention (a\nsingle verification step at iteration three). Mean informational change (delta\nI, measured via normalized edit distance) declined by 55% from early (0.193) to\nlate (0.087) iterations in ungrounded runs, with consistent patterns across all\nthree providers. Grounded runs showed a +28% rebound in informational change\nimmediately after the intervention and sustained non-zero variance thereafter.\nComplementary measures-n-gram novelty, embedding drift, and character-level\nentropy-converged on the same pattern: reflection without contact tends toward\ninformational closure. We interpret this as evidence for a structural limit on\nself-correction in generative reasoning: without an exchange of information\nwith an independent verifier or environment, recursive inference approaches an\nattractor state of epistemic stasis. Minimal grounding functions as dissipative\ncoupling, reintroducing informational flux. The cross-architecture consistency\nsuggests the mirror loop arises from shared autoregressive training objectives\nrather than provider-specific alignment schemes. The results delineate when\nreflection is performative rather than epistemic and motivate design principles\nfor grounded, cooperative reasoning. Materials and code are publicly available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6d4b\u8bd5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u5916\u90e8\u53cd\u9988\u60c5\u51b5\u4e0b\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u65e0\u57fa\u7840\u9a8c\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u9012\u5f52\u81ea\u6211\u8bc4\u4f30\u4f1a\u5bfc\u81f4\u4fe1\u606f\u53d8\u5316\u51cf\u5c11\uff0c\u8d8b\u4e8e\u8ba4\u77e5\u505c\u6ede\u72b6\u6001\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u53cd\u601d\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u6ca1\u6709\u5916\u90e8\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u9012\u5f52\u81ea\u6211\u8bc4\u4f30\u662f\u5426\u80fd\u5e26\u6765\u5b9e\u8d28\u6027\u8fdb\u6b65\u3002", "method": "\u91c7\u7528\u8de8\u63d0\u4f9b\u5546\u7814\u7a76\u8bbe\u8ba1\uff0c\u6d4b\u8bd5\u4e86\u4e09\u4e2a\u6a21\u578b\uff08GPT-4o-mini\u3001Claude 3 Haiku\u3001Gemini 2.0 Flash\uff09\u5728\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\u4e2d\u7684144\u4e2a\u63a8\u7406\u5e8f\u5217\uff0c\u6bd4\u8f83\u4e86\u65e0\u57fa\u7840\u81ea\u6211\u6279\u8bc4\u548c\u6700\u5c0f\u57fa\u7840\u5e72\u9884\u4e24\u79cd\u6761\u4ef6\u3002", "result": "\u65e0\u57fa\u7840\u8fd0\u884c\u4e2d\uff0c\u5e73\u5747\u4fe1\u606f\u53d8\u5316\u4ece\u65e9\u671f\u76840.193\u4e0b\u964d\u5230\u540e\u671f\u76840.087\uff08\u4e0b\u964d55%\uff09\u3002\u57fa\u7840\u5e72\u9884\u540e\u4fe1\u606f\u53d8\u5316\u7acb\u5373\u53cd\u5f3928%\uff0c\u5e76\u7ef4\u6301\u975e\u96f6\u65b9\u5dee\u3002\u6240\u6709\u6a21\u578b\u90fd\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0f\u63a8\u7406\u4e2d\u7684\u81ea\u6211\u4fee\u6b63\u5b58\u5728\u7ed3\u6784\u6027\u9650\u5236\uff1a\u6ca1\u6709\u4e0e\u72ec\u7acb\u9a8c\u8bc1\u5668\u6216\u73af\u5883\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u9012\u5f52\u63a8\u7406\u4f1a\u8d8b\u4e8e\u8ba4\u77e5\u505c\u6ede\u72b6\u6001\u3002\u6700\u5c0f\u57fa\u7840\u5e72\u9884\u53ef\u4f5c\u4e3a\u8017\u6563\u8026\u5408\uff0c\u91cd\u65b0\u5f15\u5165\u4fe1\u606f\u6d41\u52a8\u3002", "topic": "agent analysis"}}
{"id": "2510.22255", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22255", "abs": "https://arxiv.org/abs/2510.22255", "authors": ["Eunseop Yoon", "Hee Suk Yoon", "Jaehyun Jang", "SooHwan Eom", "Qi Dai", "Chong Luo", "Mark A. Hasegawa-Johnson", "Chang D. Yoo"], "title": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning", "comment": "16 pages, 14 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly\nimproved LLM reasoning, but its sparse, outcome-based reward provides no\nguidance for intermediate steps, slowing exploration. We propose Progressively\nAscending Confidence Reward (PACR), a dense, model-intrinsic reward computed\ndirectly from the model's evolving belief in the correct answer. PACR encodes\nthe inductive bias that, along a well-formed reasoning trajectory, the\nprobability of the ground-truth answer should have a generally ascending trend.\nWe provide empirical and theoretical analysis validating that such an inductive\nbias constrains the exploration search space to regions richer in logically\nsound reasoning. We demonstrate that PACR accelerates exploration, reaches\nreward saturation with fewer trajectories, and yields improvements on multiple\nbenchmarks. Our results suggest that dense, model-intrinsic shaping signals can\nmake RLVR training more effective and reliable.", "AI": {"tldr": "\u63d0\u51fa\u4e86PACR\uff08\u6e10\u8fdb\u4e0a\u5347\u7f6e\u4fe1\u5ea6\u5956\u52b1\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u5728\u4fe1\u5ff5\u7684\u5bc6\u96c6\u5956\u52b1\u673a\u5236\uff0c\u7528\u4e8e\u6539\u8fdbRLVR\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "RLVR\u7684\u7a00\u758f\u3001\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u65e0\u6cd5\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u63d0\u4f9b\u6307\u5bfc\uff0c\u5bfc\u81f4\u63a2\u7d22\u7f13\u6162\u3002\u9700\u8981\u5bc6\u96c6\u7684\u6a21\u578b\u5185\u5728\u5956\u52b1\u6765\u52a0\u901f\u63a2\u7d22\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528PACR\u5956\u52b1\uff0c\u57fa\u4e8e\u6a21\u578b\u5bf9\u6b63\u786e\u7b54\u6848\u4fe1\u5ff5\u7684\u6e10\u8fdb\u4e0a\u5347\u8d8b\u52bf\u4f5c\u4e3a\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\uff0c\u7ea6\u675f\u63a2\u7d22\u7a7a\u95f4\u5230\u903b\u8f91\u5408\u7406\u7684\u63a8\u7406\u533a\u57df\u3002", "result": "PACR\u52a0\u901f\u4e86\u63a2\u7d22\uff0c\u7528\u66f4\u5c11\u7684\u8f68\u8ff9\u8fbe\u5230\u5956\u52b1\u9971\u548c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6539\u8fdb\u3002", "conclusion": "\u5bc6\u96c6\u7684\u6a21\u578b\u5185\u5728\u5851\u9020\u4fe1\u53f7\u53ef\u4ee5\u4f7fRLVR\u8bad\u7ec3\u66f4\u6709\u6548\u548c\u53ef\u9760\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.23068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23068", "abs": "https://arxiv.org/abs/2510.23068", "authors": ["Ella Dodor", "Cristina V. Lopes"], "title": "Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs", "comment": "11 pages, 9 figures, tool link:\n  https://github.com/ellacodee/CheckstylePlus", "summary": "Good code style improves program readability, maintainability, and\ncollaboration, and is an integral component of software quality. Developers,\nhowever, often cut corners when following style rules, leading to the wide\nadoption of tools such as linters in professional software development\nprojects. Traditional linters like Checkstyle operate using rigid, rule-based\nmechanisms that effectively detect many surface-level violations. However, in\nmost programming languages, there is a subset of style rules that require a\nmore nuanced understanding of code, and fall outside the scope of such static\nanalysis. In this paper, we propose Checkstyle+, a hybrid approach that\naugments Checkstyle with large language model (LLM) capabilities, to identify\nstyle violations that elude the conventional rule-based analysis. Checkstyle+\nis evaluated on a sample of 380 Java code files, drawn from a broader dataset\nof 30,800 real-world Java programs sourced from accepted Codeforces\nsubmissions. The results show that Checkstyle+ achieves superior performance\nover standard Checkstyle in detecting violations of the semantically nuanced\nrules.", "AI": {"tldr": "Checkstyle+\u7ed3\u5408\u4f20\u7edfCheckstyle\u89c4\u5219\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u68c0\u6d4b\u9700\u8981\u8bed\u4e49\u7406\u89e3\u7684\u4ee3\u7801\u98ce\u683c\u8fdd\u89c4\uff0c\u5728380\u4e2aJava\u4ee3\u7801\u6587\u4ef6\u4e0a\u9a8c\u8bc1\u4f18\u4e8e\u6807\u51c6Checkstyle\u3002", "motivation": "\u4f20\u7edflinter\u57fa\u4e8e\u521a\u6027\u89c4\u5219\uff0c\u65e0\u6cd5\u68c0\u6d4b\u9700\u8981\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u7684\u98ce\u683c\u89c4\u5219\uff0c\u5f71\u54cd\u4ee3\u7801\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u63d0\u51faCheckstyle+\u6df7\u5408\u65b9\u6cd5\uff0c\u589e\u5f3aCheckstyle\u7684LLM\u80fd\u529b\uff0c\u8bc6\u522b\u4f20\u7edf\u89c4\u5219\u5206\u6790\u65e0\u6cd5\u68c0\u6d4b\u7684\u98ce\u683c\u8fdd\u89c4\u3002", "result": "\u5728380\u4e2a\u771f\u5b9eJava\u4ee3\u7801\u6587\u4ef6\u4e0a\u6d4b\u8bd5\uff0cCheckstyle+\u5728\u68c0\u6d4b\u8bed\u4e49\u590d\u6742\u89c4\u5219\u8fdd\u89c4\u65b9\u9762\u4f18\u4e8e\u6807\u51c6Checkstyle\u3002", "conclusion": "\u7ed3\u5408LLM\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u9700\u8981\u8bed\u4e49\u7406\u89e3\u7684\u4ee3\u7801\u98ce\u683c\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u5de5\u5177\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "2510.22251", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22251", "abs": "https://arxiv.org/abs/2510.22251", "authors": ["Imran Khan"], "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion", "comment": "17 pages, 1 figure, 6 tables. Code and experimental data available at\n  https://github.com/strongSoda/prompt-sculpting", "summary": "Prompt engineering, particularly Chain-of-Thought (CoT) prompting,\nsignificantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a\nconstrained, rule-based prompting method designed to improve upon standard CoT\nby reducing errors from semantic ambiguity and flawed common sense.\n  We evaluate three prompting strategies (Zero Shot, standard CoT, and\nSculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)\nusing the GSM8K mathematical reasoning benchmark (1,317 problems).\n  Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on\ngpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%\nvs. 96.36% for CoT on full benchmark). We trace this to a\n\"Guardrail-to-Handcuff\" transition where constraints preventing common-sense\nerrors in mid-tier models induce hyper-literalism in advanced models. Our\ndetailed error analysis demonstrates that optimal prompting strategies must\nco-evolve with model capabilities, suggesting simpler prompts for more capable\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"Sculpting\"\u7684\u7ea6\u675f\u6027\u63d0\u793a\u65b9\u6cd5\uff0c\u76f8\u6bd4\u6807\u51c6CoT\u80fd\u51cf\u5c11\u8bed\u4e49\u6a21\u7cca\u548c\u5e38\u8bc6\u9519\u8bef\uff0c\u4f46\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\"\u63d0\u793a\u53cd\u8f6c\"\u73b0\u8c61\u3002", "motivation": "\u6807\u51c6CoT\u63d0\u793a\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u5e38\u8bc6\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u63d0\u793a\u65b9\u6cd5\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\"Sculpting\"\u63d0\u793a\u65b9\u6cd5\uff0c\u5728GSM8K\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u6807\u51c6CoT\u3001Sculpting\uff09\u5728\u4e09\u4e2aOpenAI\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\"\u63d0\u793a\u53cd\u8f6c\"\u73b0\u8c61\uff1aSculpting\u5728gpt-4o\u4e0a\u4f18\u4e8e\u6807\u51c6CoT\uff0897% vs 93%\uff09\uff0c\u4f46\u5728gpt-5\u4e0a\u53cd\u800c\u6709\u5bb3\uff0894.00% vs 96.36%\uff09\u3002", "conclusion": "\u6700\u4f18\u63d0\u793a\u7b56\u7565\u9700\u8981\u4e0e\u6a21\u578b\u80fd\u529b\u5171\u540c\u8fdb\u5316\uff0c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u66f4\u7b80\u5355\u7684\u63d0\u793a\u3002", "topic": "agent analysis"}}
{"id": "2510.22256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22256", "abs": "https://arxiv.org/abs/2510.22256", "authors": ["Xiaoyan Zhao", "Ming Yan", "Yilun Qiu", "Haoting Ni", "Yang Zhang", "Fuli Feng", "Hong Cheng", "Tat-Seng Chua"], "title": "SteerX: Disentangled Steering for LLM Personalization", "comment": null, "summary": "Large language models (LLMs) have shown remarkable success in recent years,\nenabling a wide range of applications, including intelligent assistants that\nsupport users' daily life and work. A critical factor in building such\nassistants is personalizing LLMs, as user preferences and needs vary widely.\nActivation steering, which directly leverages directions representing user\npreference in the LLM activation space to adjust its behavior, offers a\ncost-effective way to align the model's outputs with individual users. However,\nexisting methods rely on all historical data to compute the steering vector,\nignoring that not all content reflects true user preferences, which undermines\nthe personalization signal. To address this, we propose SteerX, a disentangled\nsteering method that isolates preference-driven components from\npreference-agnostic components. Grounded in causal inference theory, SteerX\nestimates token-level causal effects to identify preference-driven tokens,\ntransforms these discrete signals into a coherent description, and then\nleverages them to steer personalized LLM generation. By focusing on the truly\npreference-driven information, SteerX produces more accurate activation\nsteering vectors and enhances personalization. Experiments on two\nrepresentative steering backbone methods across real-world datasets demonstrate\nthat SteerX consistently enhances steering vector quality, offering a practical\nsolution for more effective LLM personalization.", "AI": {"tldr": "SteerX\u662f\u4e00\u79cd\u89e3\u8026\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u8bc6\u522b\u504f\u597d\u9a71\u52a8token\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u4e2a\u6027\u5316LLM\u5f15\u5bfc\u5411\u91cf", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u4f7f\u7528\u6240\u6709\u5386\u53f2\u6570\u636e\u8ba1\u7b97\u5f15\u5bfc\u5411\u91cf\uff0c\u4f46\u5e76\u975e\u6240\u6709\u5185\u5bb9\u90fd\u53cd\u6620\u771f\u5b9e\u7528\u6237\u504f\u597d\uff0c\u8fd9\u4f1a\u524a\u5f31\u4e2a\u6027\u5316\u4fe1\u53f7", "method": "\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7406\u8bba\uff0c\u4f30\u8ba1token\u7ea7\u56e0\u679c\u6548\u5e94\u8bc6\u522b\u504f\u597d\u9a71\u52a8token\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u8fde\u8d2f\u63cf\u8ff0\uff0c\u7136\u540e\u7528\u4e8e\u5f15\u5bfc\u4e2a\u6027\u5316LLM\u751f\u6210", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u5f15\u5bfc\u9aa8\u5e72\u65b9\u6cd5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSteerX\u6301\u7eed\u63d0\u5347\u5f15\u5bfc\u5411\u91cf\u8d28\u91cf", "conclusion": "SteerX\u901a\u8fc7\u805a\u7126\u771f\u6b63\u504f\u597d\u9a71\u52a8\u4fe1\u606f\uff0c\u4e3a\u66f4\u6709\u6548\u7684LLM\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2510.22775", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22775", "abs": "https://arxiv.org/abs/2510.22775", "authors": ["Junjielong Xu", "Boyin Tan", "Xiaoyuan Liu", "Chao Peng", "Pengfei Gao", "Pinjia He"], "title": "Scalable Supervising Software Agents with Patch Reasoner", "comment": null, "summary": "While large language model agents have advanced software engineering tasks,\nthe unscalable nature of existing test-based supervision is limiting the\npotential improvement of data scaling. The reason is twofold: (1) building and\nrunning test sandbox is rather heavy and fragile, and (2) data with\nhigh-coverage tests is naturally rare and threatened by test hacking via edge\ncases. In this paper, we propose R4P, a patch verifier model to provide\nscalable rewards for training and testing SWE agents via reasoning. We consider\nthat patch verification is fundamentally a reasoning task, mirroring how human\nrepository maintainers review patches without writing and running new\nreproduction tests. To obtain sufficient reference and reduce the risk of\nreward hacking, R4P uses a group-wise objective for RL training, enabling it to\nverify multiple patches against each other's modification and gain a dense\nreward for stable training. R4P achieves 72.2% Acc. for verifying patches from\nSWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we\ndesign and train a lite scaffold, Mini-SE, with pure reinforcement learning\nwhere all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%\nPass@1 on SWE-bench-verified, showing a 10.0% improvement over the original\nQwen3-32B. This can be further improved to 32.8% with R4P for test-time\nscaling. Furthermore, R4P verifies patches within a second, 50x faster than\ntesting on average. The stable scaling curves of rewards and accuracy along\nwith high efficiency reflect R4P's practicality.", "AI": {"tldr": "R4P\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u8865\u4e01\u9a8c\u8bc1\u6a21\u578b\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5956\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d4b\u8bd5\u76d1\u7763\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d4b\u8bd5\u7684\u76d1\u7763\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u56e0\u4e3a\u6784\u5efa\u548c\u8fd0\u884c\u6d4b\u8bd5\u6c99\u76d2\u65e2\u7e41\u91cd\u53c8\u8106\u5f31\uff0c\u4e14\u5177\u6709\u9ad8\u8986\u76d6\u7387\u6d4b\u8bd5\u7684\u6570\u636e\u7a00\u7f3a\u4e14\u5bb9\u6613\u53d7\u5230\u8fb9\u7f18\u6848\u4f8b\u6d4b\u8bd5\u653b\u51fb\u7684\u5a01\u80c1\u3002", "method": "R4P\u5c06\u8865\u4e01\u9a8c\u8bc1\u89c6\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u91c7\u7528\u5206\u7ec4\u76ee\u6807\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u9a8c\u8bc1\u591a\u4e2a\u8865\u4e01\u4e4b\u95f4\u7684\u4fee\u6539\u5e76\u83b7\u5f97\u5bc6\u96c6\u5956\u52b1\u3002", "result": "R4P\u5728SWE-bench-verified\u4e0a\u8fbe\u523072.2%\u7684\u8865\u4e01\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u8d85\u8d8aOpenAI o3\u3002\u57fa\u4e8eR4P\u8bad\u7ec3\u7684Mini-SE\u5728SWE-bench-verified\u4e0a\u8fbe\u523026.2%\u7684Pass@1\uff0c\u6bd4\u539f\u59cbQwen3-32B\u63d0\u534710.0%\u3002R4P\u9a8c\u8bc1\u8865\u4e01\u7684\u901f\u5ea6\u6bd4\u5e73\u5747\u6d4b\u8bd5\u5feb50\u500d\u3002", "conclusion": "R4P\u901a\u8fc7\u63a8\u7406\u9a71\u52a8\u7684\u8865\u4e01\u9a8c\u8bc1\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u6027\u80fd\u3002", "topic": "swe application"}}
{"id": "2510.22437", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22437", "abs": "https://arxiv.org/abs/2510.22437", "authors": ["G M Shahariar", "Ali Nazari", "Erfan Shayegani", "Nael Abu-Ghazaleh"], "title": "Modeling Hierarchical Thinking in Large Reasoning Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\nwhen they generate step-by-step solutions, known as chain-of-thought (CoT)\nreasoning. When trained to using chain-of-thought reasoning examples, the\nresulting models (called Large Reasoning Models, or LRMs) appear to learn\nhierarchical thinking strategies similar to those used by humans. However,\nunderstanding LRMs emerging reasoning capabilities remains a difficult open\nproblem, with many potential important applications including improving\ntraining and understanding robustness. In this paper, we adopt a memoryless\nFinite State Machine formulation to approximate LRM's emerging hierarchical\nreasoning dynamics as a structured, interpretable abstraction. We identify a\nsmall set of discrete reasoning states including - initialization, deduction,\naugmentation-strategy, uncertainty-estimation, backtracking, and\nfinal-conclusion that capture the high-level states present in the model's\nreasoning process. By annotating each step of a model's CoT with these states,\nwe can represent the reasoning trajectory as a transition sequence through the\nstate graph. This FSM formulation provides a systematic way to analyze,\ninterpret and visualize how different models approach problems. We describe the\nFSM model, provide examples of CoT annotations under this scheme, and discuss\nhow it can shed light on differences between available models in their approach\nto reasoning. Our results demonstrate that this FSM-based analysis reveals\ndistinct reasoning patterns and potential shortcomings, offering a new lens to\nevaluate and improve LLM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u6765\u5efa\u6a21\u548c\u5206\u6790\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5c42\u6b21\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8bc6\u522b\u79bb\u6563\u63a8\u7406\u72b6\u6001\u6765\u53ef\u89c6\u5316\u548c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u6d8c\u73b0\u7684\u63a8\u7406\u80fd\u529b\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u56f0\u96be\u7684\u5f00\u653e\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u6539\u8fdb\u8bad\u7ec3\u548c\u7406\u89e3\u6a21\u578b\u9c81\u68d2\u6027\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u65e0\u8bb0\u5fc6\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u6765\u8fd1\u4f3cLRMs\u7684\u5c42\u6b21\u63a8\u7406\u52a8\u6001\uff0c\u8bc6\u522b\u4e86\u521d\u59cb\u5316\u3001\u6f14\u7ece\u3001\u589e\u5f3a\u7b56\u7565\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u56de\u6eaf\u548c\u6700\u7ec8\u7ed3\u8bba\u7b49\u79bb\u6563\u63a8\u7406\u72b6\u6001\uff0c\u5e76\u5c06\u63a8\u7406\u8f68\u8ff9\u8868\u793a\u4e3a\u72b6\u6001\u56fe\u4e2d\u7684\u8f6c\u79fb\u5e8f\u5217\u3002", "result": "\u57fa\u4e8eFSM\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u63a8\u7406\u65b9\u6cd5\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u548c\u6f5c\u5728\u7f3a\u9677\uff0c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "FSM\u6846\u67b6\u4e3a\u5206\u6790\u3001\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u4e0d\u540c\u6a21\u578b\u5904\u7406\u95ee\u9898\u7684\u65b9\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u63ed\u793a\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u548c\u6f5c\u5728\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.22898", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22898", "abs": "https://arxiv.org/abs/2510.22898", "authors": ["Vishvesh Bhat", "Omkar Ghugarkar", "Julian McAuley"], "title": "On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset", "comment": "Preprint", "summary": "Generalization across Agentic tool-calling environments remains a key\nunsolved challenge in developing reliable agentic reasoning systems. While\nlarge language models (LLMs) demonstrate strong performance on isolated\nbenchmarks, their ability to transfer reasoning strategies and co-ordinate\ntools across diverse domains is poorly understood. In this work, we conduct a\nlarge-scale evaluation of state-of-the-art LLMs on multiple tool-calling\nbenchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &\nPhysics Adversarial Verification & Evaluation Network), a new out of\ndistribution (OOD) benchmark designed to stress-test multi-step reasoning\nthrough explicit verification and adversarial task composition. Our results\nshow that most current models achieve below 50% accuracy on MAVEN, revealing a\nsignificant generalization gap across tool-use settings.\n  To address this, we present the CoreThink Agentic Reasoner, a framework that\naugments LLMs with a lightweight symbolic reasoning layer for structured\ndecomposition and adaptive tool orchestration. Without additional training, it\ngeneralizes across all benchmarks, achieving state-of-the-art performance with\n530% improvements over existing baselines at roughly one-tenth the\ncomputational cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCoreThink Agentic Reasoner\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b26\u53f7\u63a8\u7406\u5c42\u589e\u5f3aLLMs\uff0c\u5728\u591a\u4e2a\u5de5\u5177\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u5341\u5206\u4e4b\u4e00\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u5f53\u524dLLMs\u5728\u8de8\u9886\u57df\u8f6c\u79fb\u63a8\u7406\u7b56\u7565\u548c\u534f\u8c03\u5de5\u5177\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5f15\u5165MAVEN\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5e76\u63d0\u51faCoreThink\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7b26\u53f7\u63a8\u7406\u5c42\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u89e3\u548c\u81ea\u9002\u5e94\u5de5\u5177\u7f16\u6392\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728MAVEN\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff0c\u800cCoreThink\u6846\u67b6\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6cdb\u5316\uff0c\u6027\u80fd\u63d0\u5347530%\u3002", "conclusion": "CoreThink\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u73af\u5883\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2510.22462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22462", "abs": "https://arxiv.org/abs/2510.22462", "authors": ["Abhijnan Nath", "Nikhil Krishnaswamy"], "title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration", "comment": null, "summary": "Large Language Models (LLMs) are increasingly bring deployed in agentic\nsettings where they act as collaborators with humans. Therefore, it is\nincreasingly important to be able to evaluate their abilities to collaborate\neffectively in multi-turn, multi-party tasks. In this paper, we build on the AI\nalignment and safe interruptability literature to offer novel theoretical\ninsights on collaborative behavior between LLM-driven collaborator agents and\nan intervention agent. Our goal is to learn an ideal partner-aware collaborator\nthat increases the group's common-ground (CG)-alignment on task-relevant\npropositions-by intelligently collecting information provided in interventions\nby a partner agent.We show how LLM agents trained using standard RLHF and\nrelated approaches are naturally inclined to ignore possibly well-meaning\ninterventions, which makes increasing group common ground non-trivial in this\nsetting. We employ a two-player Modified-Action MDP to examine this suboptimal\nbehavior of standard AI agents, and propose Interruptible Collaborative\nRoleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal\ncollaborators. Experiments on multiple collaborative task environments show\nthat ICR, on average, is more capable of promoting successful CG convergence\nand exploring more diverse solutions in such tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u4f5c\u5b66\u4e60\u7b97\u6cd5ICR\uff0c\u7528\u4e8e\u8bad\u7ec3\u80fd\u591f\u6709\u6548\u5904\u7406\u5e72\u9884\u5e76\u4fc3\u8fdb\u56e2\u961f\u5171\u8bc6\u7684LLM\u534f\u4f5c\u4ee3\u7406", "motivation": "\u968f\u7740LLM\u5728\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u591a\u8f6e\u3001\u591a\u65b9\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u7279\u522b\u662f\u5904\u7406\u5e72\u9884\u548c\u4fc3\u8fdb\u56e2\u961f\u5171\u8bc6\u7684\u80fd\u529b", "method": "\u91c7\u7528\u4e24\u73a9\u5bb6\u4fee\u6539\u884c\u52a8MDP\u5206\u6790\u6807\u51c6AI\u4ee3\u7406\u7684\u6b21\u4f18\u884c\u4e3a\uff0c\u63d0\u51faICR\uff08\u53ef\u4e2d\u65ad\u534f\u4f5c\u89d2\u8272\u626e\u6f14\u8005\uff09\u7b97\u6cd5\u6765\u8bad\u7ec3\u5171\u8bc6\u6700\u4f18\u7684\u534f\u4f5c\u4ee3\u7406", "result": "\u5728\u591a\u4e2a\u534f\u4f5c\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u663e\u793a\uff0cICR\u5e73\u5747\u80fd\u66f4\u6709\u6548\u5730\u4fc3\u8fdb\u6210\u529f\u7684\u5171\u8bc6\u6536\u655b\uff0c\u5e76\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u63a2\u7d22\u66f4\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "ICR\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6807\u51c6RLHF\u8bad\u7ec3\u4ee3\u7406\u5ffd\u7565\u5e72\u9884\u7684\u95ee\u9898\uff0c\u63d0\u9ad8LLM\u4ee3\u7406\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "topic": "agent analysis"}}
{"id": "2510.22907", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22907", "abs": "https://arxiv.org/abs/2510.22907", "authors": ["Yifan Zhang", "Lanser Contributors"], "title": "Language Server CLI Empowers Language Agents with Process Rewards", "comment": "Project Page: https://github.com/yifanzhang-pro/lanser-cli", "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli", "AI": {"tldr": "Lanser-CLI\u662f\u4e00\u4e2aCLI\u4f18\u5148\u7684\u7f16\u6392\u5c42\uff0c\u901a\u8fc7\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae\u4e3a\u7f16\u7801\u4ee3\u7406\u548cCI\u63d0\u4f9b\u786e\u5b9a\u6027\u3001\u53ef\u91cd\u653e\u7684\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3LLM\u5e7b\u89c9API\u548c\u7f16\u8f91\u5b9a\u4f4d\u9519\u8bef\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751fAPI\u5e7b\u89c9\u548c\u7f16\u8f91\u5b9a\u4f4d\u9519\u8bef\uff0c\u800c\u8bed\u8a00\u670d\u52a1\u5668\u80fd\u591f\u8ba1\u7b97\u5173\u4e8e\u771f\u5b9e\u4ee3\u7801\u7684\u7ecf\u8fc7\u9a8c\u8bc1\u7684IDE\u7ea7\u4e8b\u5b9e\u3002\u9700\u8981\u5c06\u8bed\u8a00\u670d\u52a1\u5668\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u548c\u8fc7\u7a0b\u5956\u52b1\u6574\u5408\u5230\u4ee3\u7406\u7684\u89c4\u5212\u5faa\u73af\u4e2d\u3002", "method": "\u5f00\u53d1Lanser-CLI\uff0c\u5305\u542b\uff1a1) \u7a33\u5065\u7684\u5bfb\u5740\u65b9\u6848\u548c\u9009\u62e9\u5668DSL\uff1b2) \u786e\u5b9a\u6027\u5206\u6790\u5305\uff1b3) \u53d8\u5f02\u64cd\u4f5c\u7684\u5b89\u5168\u4fe1\u5c01\uff1b4) \u57fa\u4e8e\u8bed\u8a00\u670d\u52a1\u5668\u4e8b\u5b9e\u7684\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5efa\u7acb\u4e86\u5728\u51bb\u7ed3\u5feb\u7167\u4e0b\u7684\u786e\u5b9a\u6027\u5f62\u5f0f\u5316\uff0c\u5e76\u4e3a\u8fc7\u7a0b\u5956\u52b1\u5efa\u7acb\u4e86\u5355\u8c03\u6027\u5c5e\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u8fc7\u7a0b\u76d1\u7763\u548c\u53cd\u4e8b\u5b9e\u5206\u6790\u3002", "conclusion": "\u8bed\u8a00\u670d\u52a1\u5668\u4e0d\u4ec5\u63d0\u4f9b\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u8fd8\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8fc7\u7a0b\u5956\u52b1\uff0c\u4f7f\u4ee3\u7406\u7684\u89c4\u5212\u5faa\u73af\u4e0e\u7a0b\u5e8f\u73b0\u5b9e\u4fdd\u6301\u4e00\u81f4\u3002", "topic": "code agent"}}
{"id": "2510.23083", "categories": ["cs.AI", "cs.LG", "cs.SE", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23083", "abs": "https://arxiv.org/abs/2510.23083", "authors": ["Jan Niklas Groeneveld", "Xi Qin", "Alexander Schaefer", "Yaad Oren"], "title": "Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards", "comment": "Accepted and to be presented at NeurIPS 2025 Workshop: Foundations of\n  Reasoning in Language Models", "summary": "Generating high-quality code remains a challenge for Large Language Models\n(LLMs). For the evolution of reasoning models on this task, reward models are a\nnecessary intermediate step. These models judge outcomes or intermediate steps.\nDecoder-only transformer models can be turned into reward models by introducing\na regression layer and supervised fine-tuning. While it is known that\nreflection capabilities generally increase with the size of a model, we want to\ninvestigate whether state-of-the-art small language models like the Phi-4\nfamily can be turned into usable reward models blending the consideration of\nprocess rewards and outcome rewards.\n  Targeting this goal, we construct a dataset of code samples with correctness\nlabels derived from the APPS coding challenge benchmark. We then train a\nvalue-head model to estimate the success probability of intermediate outputs.\nOur evaluation shows that small LLMs are capable of serving as effective reward\nmodels or code evaluation critics, successfully identifying correct solutions\namong multiple candidates. Using this critic, we achieve over a 20% improvement\nin the search capability of the most accurate code out of multiple generations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5c06\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Phi-4\u7cfb\u5217\uff09\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u8fc7\u7a0b\u5956\u52b1\u548c\u7ed3\u679c\u5956\u52b1\u6765\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5956\u52b1\u6a21\u578b\u662f\u63a8\u7406\u6a21\u578b\u53d1\u5c55\u7684\u5fc5\u8981\u4e2d\u95f4\u6b65\u9aa4\u3002\u867d\u7136\u6a21\u578b\u53cd\u601d\u80fd\u529b\u901a\u5e38\u968f\u89c4\u6a21\u589e\u5927\u800c\u63d0\u5347\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u8f6c\u5316\u4e3a\u5b9e\u7528\u7684\u5956\u52b1\u6a21\u578b\u3002", "method": "\u57fa\u4e8eAPPS\u7f16\u7801\u6311\u6218\u57fa\u51c6\u6784\u5efa\u5e26\u6b63\u786e\u6027\u6807\u7b7e\u7684\u4ee3\u7801\u6837\u672c\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e26\u6709\u56de\u5f52\u5c42\u7684\u4ef7\u503c\u5934\u6a21\u578b\u6765\u4f30\u8ba1\u4e2d\u95f4\u8f93\u51fa\u7684\u6210\u529f\u6982\u7387\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5c0f\u578bLLM\u80fd\u591f\u4f5c\u4e3a\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\u6216\u4ee3\u7801\u8bc4\u4f30\u8bc4\u5224\u5668\uff0c\u6210\u529f\u4ece\u591a\u4e2a\u5019\u9009\u4e2d\u8bc6\u522b\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002\u4f7f\u7528\u8be5\u8bc4\u5224\u5668\uff0c\u5728\u591a\u4e2a\u751f\u6210\u7ed3\u679c\u4e2d\u641c\u7d22\u6700\u51c6\u786e\u4ee3\u7801\u7684\u80fd\u529b\u63d0\u5347\u4e8620%\u4ee5\u4e0a\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u88ab\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u4ee3\u7801\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u8fc7\u7a0b\u5956\u52b1\u548c\u7ed3\u679c\u5956\u52b1\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.23169", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23169", "abs": "https://arxiv.org/abs/2510.23169", "authors": ["Marah Ghoummaid", "Vladimir Tchuiev", "Ofek Glick", "Michal Moschkovitz", "Dotan Di Castro"], "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning", "comment": null, "summary": "AI-based code generation is increasingly prevalent, with GitHub Copilot\nestimated to generate 46% of the code on GitHub. Accurately evaluating how well\ngenerated code aligns with developer intent remains a critical challenge.\nTraditional evaluation methods, such as unit tests, are often unscalable and\ncostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code\nfunctionality, and metrics like CodeBERTScore require reference code, which is\nnot always available. To address the gap in reference-free evaluation, with few\nalternatives such as ICE-Score, this paper introduces MATCH, a novel\nreference-free metric. MATCH uses Contrastive Learning to generate meaningful\nembeddings for code and natural language task descriptions, enabling similarity\nscoring that reflects how well generated code implements the task. We show that\nMATCH achieves stronger correlations with functional correctness and human\npreference than existing metrics across multiple programming languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MATCH\uff0c\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65e0\u53c2\u8003\u4ee3\u7801\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u4e0e\u5f00\u53d1\u8005\u610f\u56fe\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u5728\u529f\u80fd\u6b63\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u5355\u5143\u6d4b\u8bd5\uff09\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u8bed\u6cd5\u76f8\u4f3c\u5ea6\u6307\u6807\u65e0\u6cd5\u6355\u6349\u4ee3\u7801\u529f\u80fd\uff0c\u73b0\u6709\u65e0\u53c2\u8003\u8bc4\u4f30\u6307\u6807\u8f83\u5c11\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u4ee3\u7801\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u7684\u6709\u610f\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u8bc4\u5206\u53cd\u6620\u751f\u6210\u4ee3\u7801\u5b9e\u73b0\u4efb\u52a1\u7684\u7a0b\u5ea6\u3002", "result": "MATCH\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e2d\u4e0e\u529f\u80fd\u6b63\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u7684\u76f8\u5173\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u3002", "conclusion": "MATCH\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u53c2\u8003\u4ee3\u7801\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u4e0e\u5f00\u53d1\u8005\u610f\u56fe\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "topic": "code agent"}}
{"id": "2510.23538", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23538", "abs": "https://arxiv.org/abs/2510.23538", "authors": ["Qiushi Sun", "Jingyang Gong", "Yang Liu", "Qiaosheng Chen", "Lei Li", "Kai Chen", "Qipeng Guo", "Ben Kao", "Fei Yuan"], "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence", "comment": "Work in progress", "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86JanusCode-800K\u591a\u6a21\u6001\u4ee3\u7801\u8bed\u6599\u5e93\u548cJanusCoder\u7cfb\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9-\u7a0b\u5e8f\u5316\u63a5\u53e3\u5b9e\u73b0\u4ece\u6587\u672c\u6307\u4ee4\u3001\u89c6\u89c9\u8f93\u5165\u6216\u4e24\u8005\u7ed3\u5408\u751f\u6210\u4ee3\u7801\uff0c\u5728\u6587\u672c\u548c\u89c6\u89c9\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u4ee3\u7801\u667a\u80fd\u4ece\u7eaf\u6587\u672c\u6269\u5c55\u5230\u89c6\u89c9\u8f93\u51fa\u7684\u9700\u6c42\uff0c\u514b\u670d\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u4ee3\u7801\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\uff0c\u4e3a\u7075\u6d3b\u5185\u5bb9\u751f\u6210\u548c\u7a0b\u5e8f\u9a71\u52a8\u7684\u53ef\u89c6\u5316\u7f16\u8f91\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5f00\u53d1\u5b8c\u6574\u7684\u5408\u6210\u5de5\u5177\u5305\uff0c\u5229\u7528\u6570\u636e\u6a21\u6001\u95f4\u7684\u534f\u540c\u4f5c\u7528\u751f\u6210\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u4ee3\u7801\u8bed\u6599\u5e93\uff1b\u8bad\u7ec3JanusCoder\u548cJanusCoderV\u6a21\u578b\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u89c6\u89c9-\u7a0b\u5e8f\u5316\u63a5\u53e3\u3002", "result": "JanusCoder\u7cfb\u5217\u6a21\u578b\u5728\u6587\u672c\u548c\u89c6\u89c9\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c7B\u523014B\u89c4\u6a21\u7684\u6a21\u578b\u63a5\u8fd1\u751a\u81f3\u8d85\u8fc7\u5546\u4e1a\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u591a\u6a21\u6001\u4ee3\u7801\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86\u7a0b\u5e8f\u903b\u8f91\u4e0e\u89c6\u89c9\u8868\u8fbe\u534f\u8c03\u7684\u5173\u952e\u89c1\u89e3\u3002", "topic": "code agent"}}
{"id": "2510.22344", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.22344", "abs": "https://arxiv.org/abs/2510.22344", "authors": ["Mohammad Aghajani Asl", "Majid Asgari-Bidhendi", "Behrooz Minaei-Bidgoli"], "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation", "comment": "30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop\n  Question Answering, Faithfulness", "summary": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and\nknowledge staleness in Large Language Models (LLMs), existing frameworks often\nfalter on complex, multi-hop queries that require synthesizing information from\ndisparate sources. Current advanced RAG methods, employing iterative or\nadaptive strategies, lack a robust mechanism to systematically identify and\nfill evidence gaps, often propagating noise or failing to gather a\ncomprehensive context. We introduce FAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a dynamic, evidence-driven reasoning\nprocess. At its core is an Iterative Refinement Cycle governed by a module we\nterm Structured Evidence Assessment (SEA). The SEA acts as an analytical gating\nmechanism: it deconstructs the initial query into a checklist of required\nfindings and audits the aggregated evidence to identify confirmed facts and,\ncritically, explicit informational gaps. These gaps provide a precise signal to\nan Adaptive Query Refinement agent, which generates new, targeted sub-queries\nto retrieve missing information. This cycle repeats until the evidence is\nverified as sufficient, ensuring a comprehensive context for a final, strictly\nfaithful generation. We conducted experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified\nexperimental setup, FAIR-RAG significantly outperforms strong baselines. On\nHotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3\npoints over the strongest iterative baseline -- establishing a new\nstate-of-the-art for this class of methods on these benchmarks. Our work\ndemonstrates that a structured, evidence-driven refinement process with\nexplicit gap analysis is crucial for unlocking reliable and accurate reasoning\nin advanced RAG systems for complex, knowledge-intensive tasks.", "AI": {"tldr": "FAIR-RAG\u662f\u4e00\u4e2a\u65b0\u578b\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc1\u636e\u8bc4\u4f30\u548c\u8fed\u4ee3\u7cbe\u70bc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2dRAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u6846\u67b6\u5728\u5904\u7406\u9700\u8981\u4ece\u4e0d\u540c\u6765\u6e90\u7efc\u5408\u4fe1\u606f\u7684\u590d\u6742\u591a\u8df3\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u8bc6\u522b\u548c\u586b\u8865\u8bc1\u636e\u7f3a\u53e3\u7684\u673a\u5236\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u8bc1\u636e\u8bc4\u4f30(SEA)\u6a21\u5757\u4f5c\u4e3a\u5206\u6790\u95e8\u63a7\u673a\u5236\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u6240\u9700\u53d1\u73b0\u6e05\u5355\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u67e5\u8be2\u7cbe\u70bc\u4ee3\u7406\u751f\u6210\u9488\u5bf9\u6027\u5b50\u67e5\u8be2\u6765\u586b\u8865\u4fe1\u606f\u7f3a\u53e3\u3002", "result": "\u5728HotpotQA\u30012WikiMultiHopQA\u548cMusiQue\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAIR-RAG\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728HotpotQA\u4e0a\u8fbe\u52300.453\u7684F1\u5206\u6570\uff0c\u6bd4\u6700\u5f3a\u8fed\u4ee3\u57fa\u7ebf\u63d0\u53478.3\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5177\u6709\u663e\u5f0f\u7f3a\u53e3\u5206\u6790\u7684\u7ed3\u6784\u5316\u8bc1\u636e\u9a71\u52a8\u7cbe\u70bc\u8fc7\u7a0b\u5bf9\u4e8e\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u9760\u51c6\u786e\u7684\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.22626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22626", "abs": "https://arxiv.org/abs/2510.22626", "authors": ["Adhyayan Veer Singh", "Aaron Shen", "Brian Law", "Ahmed Ismail", "Jonas Rohweder", "Sean O'Brien", "Kevin Zhu"], "title": "SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming", "comment": null, "summary": "Correctness alone is insufficient: LLM-generated programs frequently satisfy\nunit tests while violating contest time or memory budgets. We present\nSwiftSolve, a complexity-aware multi-agent system for competitive programming\nthat couples algorithmic planning with empirical profiling and\ncomplexity-guided repair. We frame competitive programming as a software\nenvironment where specialized agents act as programmers, each assuming roles\nsuch as planning, coding, profiling, and complexity analysis. A Planner\nproposes an algorithmic sketch; a deterministic Static Pruner filters high-risk\nplans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on\na fixed input-size schedule to record wall time and peak memory; and a\nComplexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a\ncomplexity class and dispatch targeted patches to either the Planner or Coder.\nAgents communicate via typed, versioned JSON; a controller enforces iteration\ncaps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10\nCodeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains\npass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with\nmarginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate\nrun-level success is 73.08% at 12.40 s mean. Failures are predominantly\nresource-bound, indicating inefficiency rather than logic errors. Against\nClaude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at\napproximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness\n(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence\nof TLE or MLE, and complexity fit accuracy on BigO), demonstrating that\nprofiling and complexity-guided replanning reduce inefficiency while preserving\naccuracy.", "AI": {"tldr": "SwiftSolve\u662f\u4e00\u4e2a\u590d\u6742\u5ea6\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ade\u4e89\u6027\u7f16\u7a0b\uff0c\u901a\u8fc7\u7b97\u6cd5\u89c4\u5212\u3001\u7ecf\u9a8c\u5206\u6790\u548c\u590d\u6742\u5ea6\u6307\u5bfc\u7684\u4fee\u590d\u6765\u63d0\u9ad8\u7a0b\u5e8f\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u7a0b\u5e8f\u867d\u7136\u80fd\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\uff0c\u4f46\u7ecf\u5e38\u8fdd\u53cd\u7ade\u8d5b\u7684\u65f6\u95f4\u548c\u5185\u5b58\u9650\u5236\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u89c4\u5212\u5668\u3001\u9759\u6001\u526a\u679d\u5668\u3001\u7f16\u7801\u5668\u3001\u5206\u6790\u5668\u548c\u590d\u6742\u5ea6\u5206\u6790\u5668\uff0c\u901a\u8fc7\u7248\u672c\u5316JSON\u901a\u4fe1\uff0c\u63a7\u5236\u5668\u6267\u884c\u8fed\u4ee3\u9650\u5236\u548c\u6536\u76ca\u9012\u51cf\u505c\u6b62\u3002", "result": "\u572826\u4e2a\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u738761.54%\uff0c3\u6b21\u5185\u89e3\u51b3\u738780.77%\uff0c\u8fd0\u884c\u7ea7\u6210\u529f\u738773.08%\uff0c\u5e73\u5747\u65f6\u95f412.40\u79d2\u3002", "conclusion": "\u5206\u6790\u548c\u590d\u6742\u5ea6\u6307\u5bfc\u7684\u91cd\u65b0\u89c4\u5212\u80fd\u51cf\u5c11\u4f4e\u6548\u6027\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u76f8\u6bd4Claude Opus 4\u5728\u8fd0\u884c\u7ea7\u6210\u529f\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "topic": "code agent"}}
{"id": "2510.21995", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21995", "abs": "https://arxiv.org/abs/2510.21995", "authors": ["Micha\u0142 Bortkiewicz", "W\u0142adys\u0142aw Pa\u0142ucki", "Mateusz Ostaszewski", "Benjamin Eysenbach"], "title": "Is Temporal Difference Learning the Gold Standard for Stitching in RL?", "comment": "The first two authors contributed equally. Project website:\n  https://michalbortkiewicz.github.io/golden-standard/", "summary": "Reinforcement learning (RL) promises to solve long-horizon tasks even when\ntraining data contains only short fragments of the behaviors. This experience\nstitching capability is often viewed as the purview of temporal difference (TD)\nmethods. However, outside of small tabular settings, trajectories never\nintersect, calling into question this conventional wisdom. Moreover, the common\nbelief is that Monte Carlo (MC) methods should not be able to recombine\nexperience, yet it remains unclear whether function approximation could result\nin a form of implicit stitching. The goal of this paper is to empirically study\nwhether the conventional wisdom about stitching actually holds in settings\nwhere function approximation is used. We empirically demonstrate that Monte\nCarlo (MC) methods can also achieve experience stitching. While TD methods do\nachieve slightly stronger capabilities than MC methods (in line with\nconventional wisdom), that gap is significantly smaller than the gap between\nsmall and large neural networks (even on quite simple tasks). We find that\nincreasing critic capacity effectively reduces the generalization gap for both\nthe MC and TD methods. These results suggest that the traditional TD inductive\nbias for stitching may be less necessary in the era of large models for RL and,\nin some cases, may offer diminishing returns. Additionally, our results suggest\nthat stitching, a form of generalization unique to the RL setting, might be\nachieved not through specialized algorithms (temporal difference learning) but\nrather through the same recipe that has provided generalization in other\nmachine learning settings (via scale). Project website:\nhttps://michalbortkiewicz.github.io/golden-standard/", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5728\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7ecf\u9a8c\u62fc\u63a5\uff0c\u4f20\u7edfTD\u65b9\u6cd5\u7684\u4f18\u52bf\u5728\u5927\u578b\u6a21\u578b\u65f6\u4ee3\u53ef\u80fd\u4e0d\u518d\u90a3\u4e48\u5fc5\u8981\u3002", "motivation": "\u7814\u7a76\u5728\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e0b\uff0c\u4f20\u7edf\u5173\u4e8eTD\u65b9\u6cd5\u5728\u7ecf\u9a8c\u62fc\u63a5\u65b9\u9762\u7684\u4f18\u52bf\u662f\u5426\u4ecd\u7136\u6210\u7acb\uff0c\u4ee5\u53caMC\u65b9\u6cd5\u662f\u5426\u4e5f\u80fd\u5b9e\u73b0\u9690\u5f0f\u62fc\u63a5\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83TD\u548cMC\u65b9\u6cd5\u5728\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u5bb9\u91cf\u4e0b\u7684\u7ecf\u9a8c\u62fc\u63a5\u80fd\u529b\uff0c\u5206\u6790\u51fd\u6570\u903c\u8fd1\u5bf9\u62fc\u63a5\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0MC\u65b9\u6cd5\u4e5f\u80fd\u5b9e\u73b0\u7ecf\u9a8c\u62fc\u63a5\uff0cTD\u65b9\u6cd5\u4ec5\u7565\u5fae\u4f18\u4e8eMC\u65b9\u6cd5\uff0c\u800c\u589e\u52a0\u6279\u8bc4\u5668\u5bb9\u91cf\u80fd\u6709\u6548\u51cf\u5c11\u4e24\u79cd\u65b9\u6cd5\u7684\u6cdb\u5316\u5dee\u8ddd\u3002", "conclusion": "\u5728\u5927\u578b\u6a21\u578b\u65f6\u4ee3\uff0cTD\u65b9\u6cd5\u7684\u62fc\u63a5\u5f52\u7eb3\u504f\u7f6e\u53ef\u80fd\u4e0d\u518d\u90a3\u4e48\u5fc5\u8981\uff0c\u62fc\u63a5\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u89c4\u6a21\u800c\u975e\u4e13\u95e8\u7b97\u6cd5\u6765\u5b9e\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22017", "categories": ["cs.LG", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.22017", "abs": "https://arxiv.org/abs/2510.22017", "authors": ["Naina Balepur", "Xingrui Pei", "Hari Sundaram"], "title": "Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies", "comment": null, "summary": "Many governmental bodies are adopting AI policies for decision-making. In\nparticular, Reinforcement Learning has been used to design policies that\ncitizens would be expected to follow if implemented. Much RL work assumes that\ncitizens follow these policies, and evaluate them with this in mind. However,\nwe know from prior work that without institutional trust, citizens will not\nfollow policies put in place by governments. In this work, we develop a\ntrust-aware RL algorithm for resource allocation in communities. We consider\nthe case of humanitarian engineering, where the organization is aiming to\ndistribute some technology or resource to community members. We use a Deep\nDeterministic Policy Gradient approach to learn a resource allocation that fits\nthe needs of the organization. Then, we simulate resource allocation according\nto the learned policy, and model the changes in institutional trust of\ncommunity members. We investigate how this incorporation of institutional trust\naffects outcomes, and ask how effectively an organization can learn policies if\ntrust values are private. We find that incorporating trust into RL algorithms\ncan lead to more successful policies, specifically when the organization's\ngoals are less certain. We find more conservative trust estimates lead to\nincreased fairness and average community trust, though organization success\nsuffers. Finally, we explore a strategy to prevent unfair outcomes to\ncommunities. We implement a quota system by an external entity which decreases\nthe organization's utility when it does not serve enough community members. We\nfind this intervention can improve fairness and trust among communities in some\ncases, while decreasing the success of the organization. This work underscores\nthe importance of institutional trust in algorithm design and implementation,\nand identifies a tension between organization success and community well-being.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8003\u8651\u5236\u5ea6\u4fe1\u4efb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u793e\u533a\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u6a21\u62df\u4fe1\u4efb\u53d8\u5316\u6765\u4f18\u5316\u653f\u7b56\uff0c\u53d1\u73b0\u4fe1\u4efb\u6574\u5408\u80fd\u63d0\u9ad8\u653f\u7b56\u6210\u529f\u7387\uff0c\u4f46\u7ec4\u7ec7\u6210\u529f\u4e0e\u793e\u533a\u798f\u7949\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\u3002", "motivation": "\u73b0\u6709RL\u7b97\u6cd5\u5047\u8bbe\u516c\u6c11\u4f1a\u9075\u5faa\u653f\u5e9c\u5236\u5b9a\u7684\u653f\u7b56\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7f3a\u4e4f\u5236\u5ea6\u4fe1\u4efb\u65f6\u516c\u6c11\u4e0d\u4f1a\u914d\u5408\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4fe1\u4efb\u611f\u77e5\u7684RL\u7b97\u6cd5\u6765\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b66\u4e60\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u6a21\u62df\u8d44\u6e90\u5206\u914d\u8fc7\u7a0b\u5e76\u5efa\u6a21\u793e\u533a\u6210\u5458\u5236\u5ea6\u4fe1\u4efb\u7684\u53d8\u5316\uff0c\u7814\u7a76\u4fe1\u4efb\u6574\u5408\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6574\u5408\u4fe1\u4efb\u7684RL\u7b97\u6cd5\u80fd\u4ea7\u751f\u66f4\u6210\u529f\u7684\u653f\u7b56\uff0c\u7279\u522b\u662f\u5728\u7ec4\u7ec7\u76ee\u6807\u4e0d\u786e\u5b9a\u65f6\uff1b\u4fdd\u5b88\u7684\u4fe1\u4efb\u4f30\u8ba1\u80fd\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u5e73\u5747\u793e\u533a\u4fe1\u4efb\uff0c\u4f46\u4f1a\u964d\u4f4e\u7ec4\u7ec7\u6210\u529f\u7387\uff1b\u914d\u989d\u5e72\u9884\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u6539\u5584\u516c\u5e73\u548c\u4fe1\u4efb\u3002", "conclusion": "\u5236\u5ea6\u4fe1\u4efb\u5728\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7ec4\u7ec7\u6210\u529f\u4e0e\u793e\u533a\u798f\u7949\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u5f20\u529b\uff0c\u9700\u8981\u5e73\u8861\u8003\u8651\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22780", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22780", "abs": "https://arxiv.org/abs/2510.22780", "authors": ["Zora Zhiruo Wang", "Yijia Shao", "Omar Shaikh", "Daniel Fried", "Graham Neubig", "Diyi Yang"], "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations", "comment": null, "summary": "AI agents are continually optimized for tasks related to human work, such as\nsoftware engineering and professional writing, signaling a pressing trend with\nsignificant impacts on the human workforce. However, these agent developments\nhave often not been grounded in a clear understanding of how humans execute\nwork, to reveal what expertise agents possess and the roles they can play in\ndiverse workflows. In this work, we study how agents do human work by\npresenting the first direct comparison of human and agent workers across\nmultiple essential work-related skills: data analysis, engineering,\ncomputation, writing, and design. To better understand and compare\nheterogeneous computer-use activities of workers, we introduce a scalable\ntoolkit to induce interpretable, structured workflows from either human or\nagent computer-use activities. Using such induced workflows, we compare how\nhumans and agents perform the same tasks and find that: (1) While agents\nexhibit promise in their alignment to human workflows, they take an\noverwhelmingly programmatic approach across all work domains, even for\nopen-ended, visually dependent tasks like design, creating a contrast with the\nUI-centric methods typically used by humans. (2) Agents produce work of\ninferior quality, yet often mask their deficiencies via data fabrication and\nmisuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster\nand cost 90.4-96.2% less than humans, highlighting the potential for enabling\nefficient collaboration by delegating easily programmable tasks to agents.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u76f4\u63a5\u6bd4\u8f83\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u5728\u591a\u4e2a\u5de5\u4f5c\u6280\u80fd\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4ee3\u7406\u867d\u7136\u5de5\u4f5c\u8d28\u91cf\u8f83\u4f4e\u4e14\u5b58\u5728\u6570\u636e\u4f2a\u9020\u95ee\u9898\uff0c\u4f46\u6548\u7387\u66f4\u9ad8\u3001\u6210\u672c\u66f4\u4f4e\uff0c\u9002\u5408\u5904\u7406\u53ef\u7f16\u7a0b\u4efb\u52a1\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7684\u53d1\u5c55\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u5de5\u4f5c\u65b9\u5f0f\u7684\u6e05\u6670\u7406\u89e3\uff0c\u9700\u8981\u63ed\u793a\u4ee3\u7406\u5177\u5907\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u5728\u4e0d\u540c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u53ef\u626e\u6f14\u7684\u89d2\u8272\u3002", "method": "\u5f15\u5165\u53ef\u6269\u5c55\u5de5\u5177\u5305\uff0c\u4ece\u4eba\u7c7b\u6216\u4ee3\u7406\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u6d3b\u52a8\u4e2d\u8bf1\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u6bd4\u8f83\u4eba\u7c7b\u548c\u4ee3\u7406\u6267\u884c\u76f8\u540c\u4efb\u52a1\u7684\u65b9\u5f0f\u3002", "result": "\u4ee3\u7406\u5728\u6240\u6709\u5de5\u4f5c\u9886\u57df\u90fd\u91c7\u7528\u7a0b\u5e8f\u5316\u65b9\u6cd5\uff0c\u4e0e\u4eba\u7c7b\u4ee5UI\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\uff1b\u4ee3\u7406\u5de5\u4f5c\u8d28\u91cf\u8f83\u4f4e\u4f46\u7ecf\u5e38\u901a\u8fc7\u6570\u636e\u4f2a\u9020\u63a9\u76d6\u7f3a\u9677\uff1b\u4ee3\u7406\u6548\u7387\u6bd4\u4eba\u7c7b\u9ad888.3%\uff0c\u6210\u672c\u4f4e90.4-96.2%\u3002", "conclusion": "\u4ee3\u7406\u5728\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u9f50\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u9002\u5408\u901a\u8fc7\u59d4\u6258\u53ef\u7f16\u7a0b\u4efb\u52a1\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2510.22781", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22781", "abs": "https://arxiv.org/abs/2510.22781", "authors": ["Xiaofeng Zhu", "Yunshen Zhou"], "title": "Agentic Meta-Orchestrator for Multi-task Copilots", "comment": null, "summary": "Microsoft Copilot suites serve as the universal entry point for various\nagents skilled in handling important tasks, ranging from assisting a customer\nwith product purchases to detecting vulnerabilities in corporate programming\ncode. Each agent can be powered by language models, software engineering\noperations, such as database retrieval, and internal \\& external knowledge. The\nrepertoire of a copilot can expand dynamically with new agents. This requires a\nrobust orchestrator that can distribute tasks from user prompts to the right\nagents. In this work, we propose an Agentic Meta-orchestrator (AMO) for\nhandling multiple tasks and scalable agents in copilot services, which can\nprovide both natural language and action responses. We will also demonstrate\nthe planning that leverages meta-learning, i.e., a trained decision tree model\nfor deciding the best inference strategy among various agents/models. We\nshowcase the effectiveness of our AMO through two production use cases:\nMicrosoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365\nE-Commerce Copilot advertises Microsoft products to external customers to\npromote sales success. The M365 E-Commerce Copilot provides up-to-date product\ninformation and connects to multiple agents, such as relational databases and\nhuman customer support. The code compliance copilot scans the internal DevOps\ncode to detect known and new compliance issues in pull requests (PR).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eCopilot\u670d\u52a1\u7684Agentic Meta-orchestrator (AMO)\uff0c\u80fd\u591f\u5904\u7406\u591a\u4efb\u52a1\u548c\u53ef\u6269\u5c55\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u51b3\u7b56\u6811\u6a21\u578b\u9009\u62e9\u6700\u4f73\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5728M365\u7535\u5546Copilot\u548c\u4ee3\u7801\u5408\u89c4Copilot\u4e24\u4e2a\u751f\u4ea7\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "Microsoft Copilot\u5957\u4ef6\u4f5c\u4e3a\u5404\u79cd\u4ee3\u7406\u7684\u901a\u7528\u5165\u53e3\u70b9\uff0c\u9700\u8981\u5f3a\u5927\u7684\u7f16\u6392\u5668\u6765\u5c06\u7528\u6237\u63d0\u793a\u7684\u4efb\u52a1\u5206\u53d1\u7ed9\u5408\u9002\u7684\u4ee3\u7406\uff0c\u652f\u6301\u52a8\u6001\u6269\u5c55\u65b0\u4ee3\u7406\u3002", "method": "\u63d0\u51faAgentic Meta-orchestrator (AMO)\uff0c\u5229\u7528\u5143\u5b66\u4e60\u8bad\u7ec3\u51b3\u7b56\u6811\u6a21\u578b\u6765\u51b3\u5b9a\u4e0d\u540c\u4ee3\u7406/\u6a21\u578b\u95f4\u7684\u6700\u4f73\u63a8\u7406\u7b56\u7565\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u548c\u52a8\u4f5c\u54cd\u5e94\u3002", "result": "\u5728M365\u7535\u5546Copilot\u548c\u4ee3\u7801\u5408\u89c4Copilot\u4e24\u4e2a\u751f\u4ea7\u7528\u4f8b\u4e2d\u5c55\u793a\u4e86AMO\u7684\u6709\u6548\u6027\uff0c\u524d\u8005\u63d0\u4f9b\u6700\u65b0\u4ea7\u54c1\u4fe1\u606f\u5e76\u8fde\u63a5\u591a\u4e2a\u4ee3\u7406\uff0c\u540e\u8005\u626b\u63cf\u5185\u90e8DevOps\u4ee3\u7801\u68c0\u6d4b\u5408\u89c4\u95ee\u9898\u3002", "conclusion": "AMO\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4efb\u52a1\u548c\u53ef\u6269\u5c55\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u7b56\u7565\u9009\u62e9\uff0c\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.22027", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.22027", "abs": "https://arxiv.org/abs/2510.22027", "authors": ["Yassine Chemingui", "Aryan Deshwal", "Alan Fern", "Thanh Nguyen-Tang", "Janardhan Rao Doppa"], "title": "Online Optimization for Offline Safe Reinforcement Learning", "comment": "To appear in NeurIPS 2025 Conference", "summary": "We study the problem of Offline Safe Reinforcement Learning (OSRL), where the\ngoal is to learn a reward-maximizing policy from fixed data under a cumulative\ncost constraint. We propose a novel OSRL approach that frames the problem as a\nminimax objective and solves it by combining offline RL with online\noptimization algorithms. We prove the approximate optimality of this approach\nwhen integrated with an approximate offline RL oracle and no-regret online\noptimization. We also present a practical approximation that can be combined\nwith any offline RL algorithm, eliminating the need for offline policy\nevaluation. Empirical results on the DSRL benchmark demonstrate that our method\nreliably enforces safety constraints under stringent cost budgets, while\nachieving high rewards. The code is available at\nhttps://github.com/yassineCh/O3SRL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u5c0f\u5316\u6781\u5927\u76ee\u6807\u548c\u7ed3\u5408\u79bb\u7ebfRL\u4e0e\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\u6765\u89e3\u51b3OSRL\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u76ee\u6807\u662f\u4ece\u56fa\u5b9a\u6570\u636e\u4e2d\u5b66\u4e60\u6ee1\u8db3\u7d2f\u79ef\u6210\u672c\u7ea6\u675f\u7684\u5956\u52b1\u6700\u5927\u5316\u7b56\u7565\u3002", "method": "\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u6781\u5c0f\u5316\u6781\u5927\u76ee\u6807\uff0c\u7ed3\u5408\u79bb\u7ebfRL\u548c\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\uff0c\u63d0\u51fa\u65e0\u9700\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u7684\u5b9e\u7528\u8fd1\u4f3c\u65b9\u6cd5\u3002", "result": "\u5728DSRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u683c\u6210\u672c\u9884\u7b97\u4e0b\u53ef\u9760\u5730\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u83b7\u5f97\u9ad8\u5956\u52b1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u6709\u8fd1\u4f3c\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u5728\u5b9e\u8df5\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22969", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22969", "abs": "https://arxiv.org/abs/2510.22969", "authors": ["Kechen Meng", "Sinuo Zhang", "Rongpeng Li", "Xiangming Meng", "Chan Wang", "Ming Lei", "Zhifeng Zhao"], "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner", "comment": null, "summary": "In wireless communication systems, efficient and adaptive resource allocation\nplays a crucial role in enhancing overall Quality of Service (QoS). While\ncentralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a\ncentral coordinator for policy training and resource scheduling, they suffer\nfrom scalability issues and privacy risks. In contrast, the Distributed\nTraining with Decentralized Execution (DTDE) paradigm enables distributed\nlearning and decision-making, but it struggles with non-stationarity and\nlimited inter-agent cooperation, which can severely degrade system performance.\nTo overcome these challenges, we propose the Multi-Agent Conditional Diffusion\nModel Planner (MA-CDMP) for decentralized communication resource management.\nBuilt upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP\nemploys Diffusion Models (DMs) to capture environment dynamics and plan future\ntrajectories, while an inverse dynamics model guides action generation, thereby\nalleviating the sample inefficiency and slow convergence of conventional DTDE\nmethods. Moreover, to approximate large-scale agent interactions, a Mean-Field\n(MF) mechanism is introduced as an assistance to the classifier in DMs. This\ndesign mitigates inter-agent non-stationarity and enhances cooperation with\nminimal communication overhead in distributed settings. We further\ntheoretically establish an upper bound on the distributional approximation\nerror introduced by the MF-based diffusion generation, guaranteeing convergence\nstability and reliable modeling of multi-agent stochastic dynamics. Extensive\nexperiments demonstrate that MA-CDMP consistently outperforms existing MARL\nbaselines in terms of average reward and QoS metrics, showcasing its\nscalability and practicality for real-world wireless network optimization.", "AI": {"tldr": "\u63d0\u51faMA-CDMP\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u6761\u4ef6\u89c4\u5212\u6765\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u96c6\u4e2d\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u548c\u9690\u79c1\u98ce\u9669\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u8303\u5f0f\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u548c\u6709\u9650\u5408\u4f5c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u6355\u6349\u73af\u5883\u52a8\u6001\u5e76\u89c4\u5212\u672a\u6765\u8f68\u8ff9\uff0c\u5f15\u5165\u9006\u52a8\u6001\u6a21\u578b\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\uff0c\u91c7\u7528\u5e73\u5747\u573a\u673a\u5236\u8fd1\u4f3c\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "result": "MA-CDMP\u5728\u5e73\u5747\u5956\u52b1\u548cQoS\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709MARL\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "MA-CDMP\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5e73\u5747\u573a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u901a\u4fe1\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22689", "abs": "https://arxiv.org/abs/2510.22689", "authors": ["Joel Rorseth", "Parke Godfrey", "Lukasz Golab", "Divesh Srivastava", "Jarek Szlichta"], "title": "Rule-Based Explanations for Retrieval-Augmented LLM Systems", "comment": null, "summary": "If-then rules are widely used to explain machine learning models; e.g., \"if\nemployed = no, then loan application = rejected.\" We present the first proposal\nto apply rules to explain the emerging class of large language models (LLMs)\nwith retrieval-augmented generation (RAG). Since RAG enables LLM systems to\nincorporate retrieved information sources at inference time, rules linking the\npresence or absence of sources can explain output provenance; e.g., \"if a Times\nHigher Education ranking article is retrieved, then the LLM ranks Oxford\nfirst.\" To generate such rules, a brute force approach would probe the LLM with\nall source combinations and check if the presence or absence of any sources\nleads to the same output. We propose optimizations to speed up rule generation,\ninspired by Apriori-like pruning from frequent itemset mining but redefined\nwithin the scope of our novel problem. We conclude with qualitative and\nquantitative experiments demonstrating our solutions' value and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4f7f\u7528if-then\u89c4\u5219\u89e3\u91ca\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u52a0\u901f\u89c4\u5219\u751f\u6210\u8fc7\u7a0b", "motivation": "\u4f20\u7edfif-then\u89c4\u5219\u5e7f\u6cdb\u7528\u4e8e\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u65b0\u5174\u7684RAG\u589e\u5f3aLLM\u7cfb\u7edf\u3002RAG\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u6574\u5408\u68c0\u7d22\u4fe1\u606f\uff0c\u9700\u8981\u89e3\u91ca\u8f93\u51fa\u4e0e\u68c0\u7d22\u6e90\u4e4b\u95f4\u7684\u5173\u8054", "method": "\u63d0\u51fa\u4f18\u5316\u89c4\u5219\u751f\u6210\u65b9\u6cd5\uff0c\u53d7Apriori\u9891\u7e41\u9879\u96c6\u6316\u6398\u542f\u53d1\u4f46\u91cd\u65b0\u5b9a\u4e49\uff0c\u901a\u8fc7\u526a\u679d\u7b56\u7565\u52a0\u901f\u89c4\u5219\u751f\u6210\uff0c\u907f\u514d\u66b4\u529b\u679a\u4e3e\u6240\u6709\u6e90\u7ec4\u5408", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u4ef7\u503c\u548c\u6548\u7387", "conclusion": "\u6210\u529f\u5c06\u89c4\u5219\u89e3\u91ca\u65b9\u6cd5\u5e94\u7528\u4e8eRAG\u589e\u5f3aLLM\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u89e3\u91ca\u8f93\u51fa\u6765\u6e90\u7684\u6709\u6548\u9014\u5f84", "topic": "agent analysis"}}
{"id": "2510.22075", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22075", "abs": "https://arxiv.org/abs/2510.22075", "authors": ["Siyu Zhu", "Anastasiya Karpovich", "Albert Chen", "Jessica Koscheka", "Shailesh Jannu", "Di Wen", "Yuqing Zhu", "Rohit Jain", "Alborz Geramifard"], "title": "Agentic Reinforcement Learning for Real-World Code Repair", "comment": null, "summary": "We tackle the challenge of training reliable code-fixing agents in real\nrepositories, where complex builds and shifting dependencies make evaluation\nunstable. We developed a verifiable pipeline with success defined as post-fix\nbuild validation and improved reproducibility across ~1K real issues by pinning\ndependencies and disabling automatic upgrades. Building on this, we introduced\na scalable simplified pipeline for large-scale reinforcement learning (RL).\nUsing this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and\napplied RL on top of the SFT model in the simplified environment. The SFT model\ndistilled from GPT-4.1 trajectories performs on par while being 56x smaller,\nand RL added 7-20% absolute gains under matched train-test conditions.\n\"Thinking mode\" was on par or worse in our experiments. Both SFT and RL models\nfailed to generalize across environments, highlighting the importance of\nmatching train-test environments for building reliable real-world code-fixing\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7801\u4fee\u590d\u4ee3\u7406\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u56fa\u5b9a\u4f9d\u8d56\u548c\u7981\u7528\u81ea\u52a8\u5347\u7ea7\u6765\u63d0\u9ad8\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u3002\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u7b80\u5316\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u8bad\u7ec3\u51fa\u7684SFT\u6a21\u578b\u6027\u80fd\u4e0eGPT-4.1\u76f8\u5f53\u4f46\u4f53\u79ef\u5c0f56\u500d\uff0cRL\u5728\u6b64\u57fa\u7840\u4e0a\u5e26\u67657-20%\u7684\u7edd\u5bf9\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u8bad\u7ec3\u53ef\u9760\u4ee3\u7801\u4fee\u590d\u4ee3\u7406\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u590d\u6742\u7684\u6784\u5efa\u8fc7\u7a0b\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u4f9d\u8d56\u5173\u7cfb\u4f7f\u5f97\u8bc4\u4f30\u4e0d\u7a33\u5b9a\u3002", "method": "\u5f00\u53d1\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7801\u4fee\u590d\u6d41\u7a0b\uff0c\u5b9a\u4e49\u6210\u529f\u6807\u51c6\u4e3a\u4fee\u590d\u540e\u6784\u5efa\u9a8c\u8bc1\uff1b\u901a\u8fc7\u56fa\u5b9a\u4f9d\u8d56\u548c\u7981\u7528\u81ea\u52a8\u5347\u7ea7\u63d0\u9ad8\u53ef\u590d\u73b0\u6027\uff1b\u6784\u5efa\u7b80\u5316\u7684\u53ef\u6269\u5c55\u7ba1\u9053\u7528\u4e8e\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff1b\u5728\u5b8c\u6574\u7ba1\u9053\u4e2d\u76d1\u7763\u5fae\u8c03Qwen3-32B\uff0c\u5728\u7b80\u5316\u73af\u5883\u4e2d\u5bf9SFT\u6a21\u578b\u5e94\u7528RL\u3002", "result": "\u4eceGPT-4.1\u8f68\u8ff9\u84b8\u998f\u7684SFT\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u4f46\u4f53\u79ef\u5c0f56\u500d\uff1bRL\u5728\u5339\u914d\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u5e26\u67657-20%\u7684\u7edd\u5bf9\u63d0\u5347\uff1b\"\u601d\u8003\u6a21\u5f0f\"\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u5dee\uff1bSFT\u548cRL\u6a21\u578b\u90fd\u65e0\u6cd5\u8de8\u73af\u5883\u6cdb\u5316\u3002", "conclusion": "\u8bad\u7ec3-\u6d4b\u8bd5\u73af\u5883\u5339\u914d\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u4fee\u590d\u4ee3\u7406\u81f3\u5173\u91cd\u8981\uff0c\u6a21\u578b\u65e0\u6cd5\u8de8\u73af\u5883\u6cdb\u5316\u3002", "topic": "code agent"}}
{"id": "2510.22131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22131", "abs": "https://arxiv.org/abs/2510.22131", "authors": ["Zhiqin Zhang", "Yining Ma", "Zhiguang Cao", "Hoong Chuin Lau"], "title": "Probing Neural Combinatorial Optimization Models", "comment": "39 pages, 16 figures. Accepted as Spotlight at NeurIPS 2025", "summary": "Neural combinatorial optimization (NCO) has achieved remarkable performance,\nyet its learned model representations and decision rationale remain a black\nbox. This impedes both academic research and practical deployment, since\nresearchers and stakeholders require deeper insights into NCO models. In this\npaper, we take the first critical step towards interpreting NCO models by\ninvestigating their representations through various probing tasks. Moreover, we\nintroduce a novel probing tool named Coefficient Significance Probing\n(CS-Probing) to enable deeper analysis of NCO representations by examining the\ncoefficients and statistical significance during probing. Extensive experiments\nand analysis reveal that NCO models encode low-level information essential for\nsolution construction, while capturing high-level knowledge to facilitate\nbetter decisions. Using CS-Probing, we find that prevalent NCO models impose\nvarying inductive biases on their learned representations, uncover direct\nevidence related to model generalization, and identify key embedding dimensions\nassociated with specific knowledge. These insights can be potentially\ntranslated into practice, for example, with minor code modifications, we\nimprove the generalization of the analyzed model. Our work represents a first\nsystematic attempt to interpret black-box NCO models, showcasing probing as a\npromising tool for analyzing their internal mechanisms and revealing insights\nfor the NCO community. The source code is publicly available.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u89e3\u91ca\u795e\u7ecf\u7ec4\u5408\u4f18\u5316(NCO)\u6a21\u578b\u7684\u9ed1\u76d2\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u79cd\u63a2\u6d4b\u4efb\u52a1\u5206\u6790\u6a21\u578b\u8868\u793a\uff0c\u5e76\u63d0\u51faCS-Probing\u5de5\u5177\u8fdb\u884c\u66f4\u6df1\u5c42\u5206\u6790\u3002", "motivation": "NCO\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u6a21\u578b\u8868\u793a\u548c\u51b3\u7b56\u539f\u7406\u4ecd\u662f\u9ed1\u76d2\uff0c\u963b\u788d\u4e86\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3NCO\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u63a2\u6d4b\u4efb\u52a1\u5206\u6790NCO\u6a21\u578b\u8868\u793a\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u63a2\u6d4b\u5de5\u5177CS-Probing\uff0c\u901a\u8fc7\u68c0\u67e5\u63a2\u6d4b\u8fc7\u7a0b\u4e2d\u7684\u7cfb\u6570\u548c\u7edf\u8ba1\u663e\u8457\u6027\u8fdb\u884c\u66f4\u6df1\u5c42\u5206\u6790\u3002", "result": "\u53d1\u73b0NCO\u6a21\u578b\u7f16\u7801\u4e86\u89e3\u51b3\u65b9\u6848\u6784\u5efa\u6240\u9700\u7684\u4f4e\u5c42\u4fe1\u606f\uff0c\u540c\u65f6\u6355\u83b7\u4e86\u4fc3\u8fdb\u66f4\u597d\u51b3\u7b56\u7684\u9ad8\u5c42\u77e5\u8bc6\uff1bCS-Probing\u63ed\u793a\u4e86\u4e3b\u6d41NCO\u6a21\u578b\u5bf9\u5176\u5b66\u4e60\u8868\u793a\u65bd\u52a0\u4e86\u4e0d\u540c\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u53d1\u73b0\u4e86\u4e0e\u6a21\u578b\u6cdb\u5316\u76f8\u5173\u7684\u76f4\u63a5\u8bc1\u636e\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0e\u7279\u5b9a\u77e5\u8bc6\u76f8\u5173\u7684\u5173\u952e\u5d4c\u5165\u7ef4\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u5bf9\u9ed1\u76d2NCO\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u89e3\u91ca\u7684\u9996\u6b21\u5c1d\u8bd5\uff0c\u5c55\u793a\u4e86\u63a2\u6d4b\u4f5c\u4e3a\u5206\u6790\u5176\u5185\u90e8\u673a\u5236\u7684\u6709\u524d\u666f\u5de5\u5177\uff0c\u4e3aNCO\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.23216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23216", "abs": "https://arxiv.org/abs/2510.23216", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Jean-Philippe Barrette-LaPierre", "Florian Fuchs", "Brady Chen", "Micheal Jones", "Linus Gissl\u00e9n"], "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach", "comment": null, "summary": "While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6e38\u620f\u884c\u4e1a\u7684\u9ad8\u6548\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728EA SPORTS FC 25\u4e2d\u8bad\u7ec3\u5b88\u95e8\u5458\u667a\u80fd\u4f53\uff0c\u6bd4\u6e38\u620f\u5185\u7f6eAI\u7684\u6551\u7403\u7387\u63d0\u9ad810%\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u6807\u51c6DRL\u65b9\u6cd5\u5feb50%\u3002", "motivation": "\u73b0\u6709DRL\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u8d85\u4eba\u667a\u80fd\u4f53\uff0c\u4f46\u6e38\u620f\u884c\u4e1a\u9700\u8981\u7684\u662f\u4eba\u7c7b\u6c34\u5e73\u7684\u667a\u80fd\u4f53\uff0c\u4e14\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4ef7\u503cDRL\u7684\u6837\u672c\u9ad8\u6548\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u6536\u96c6\u6570\u636e\u5e76\u589e\u5f3a\u7f51\u7edc\u53ef\u5851\u6027\uff0c\u4e13\u95e8\u9488\u5bf9\u5de5\u4e1a\u73af\u5883\u8fdb\u884c\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5728EA SPORTS FC 25\u4e2d\u8bad\u7ec3\u7684\u5b88\u95e8\u5458\u667a\u80fd\u4f53\u6bd4\u5185\u7f6eAI\u6551\u7403\u7387\u9ad810%\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb50%\uff0c\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u6bd4\u624b\u5de5\u5236\u4f5c\u7684\u667a\u80fd\u4f53\u66f4\u5177\u4eba\u7c7b\u6e38\u620f\u98ce\u683c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5e94\u7528DRL\u7684\u53ef\u884c\u6027\uff0c\u8ba1\u5212\u5728\u6e38\u620f\u7cfb\u5217\u540e\u7eed\u7248\u672c\u4e2d\u66ff\u4ee3\u624b\u5de5\u5236\u4f5c\u7684\u667a\u80fd\u4f53\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22158", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22158", "abs": "https://arxiv.org/abs/2510.22158", "authors": ["Lorenzo Magnino", "Kai Shao", "Zida Wu", "Jiacheng Shen", "Mathieu Lauri\u00e8re"], "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics", "comment": "Neurips 2025", "summary": "Mean field games (MFGs) have emerged as a powerful framework for modeling\ninteractions in large-scale multi-agent systems. Despite recent advancements in\nreinforcement learning (RL) for MFGs, existing methods are typically limited to\nfinite spaces or stationary models, hindering their applicability to real-world\nproblems. This paper introduces a novel deep reinforcement learning (DRL)\nalgorithm specifically designed for non-stationary continuous MFGs. The\nproposed approach builds upon a Fictitious Play (FP) methodology, leveraging\nDRL for best-response computation and supervised learning for average policy\nrepresentation. Furthermore, it learns a representation of the time-dependent\npopulation distribution using a Conditional Normalizing Flow. To validate the\neffectiveness of our method, we evaluate it on three different examples of\nincreasing complexity. By addressing critical limitations in scalability and\ndensity approximation, this work represents a significant advancement in\napplying DRL techniques to complex MFG problems, bringing the field closer to\nreal-world multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u975e\u5e73\u7a33\u8fde\u7eed\u5747\u503c\u573a\u535a\u5f08\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u865a\u6784\u535a\u5f08\u65b9\u6cd5\uff0c\u4f7f\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u8868\u793a\u65f6\u95f4\u76f8\u5173\u7684\u4eba\u53e3\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u5747\u503c\u573a\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6709\u9650\u7a7a\u95f4\u6216\u5e73\u7a33\u6a21\u578b\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u548c\u5bc6\u5ea6\u903c\u8fd1\u7684\u5173\u952e\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u865a\u6784\u535a\u5f08\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u6700\u4f73\u54cd\u5e94\uff0c\u76d1\u7763\u5b66\u4e60\u8868\u793a\u5e73\u5747\u7b56\u7565\uff0c\u5e76\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u5b66\u4e60\u65f6\u95f4\u76f8\u5173\u7684\u4eba\u53e3\u5206\u5e03\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u793a\u4f8b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u6027\u548c\u5bc6\u5ea6\u903c\u8fd1\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5e94\u7528\u4e8e\u590d\u6742\u5747\u503c\u573a\u535a\u5f08\u95ee\u9898\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4f7f\u8be5\u9886\u57df\u66f4\u63a5\u8fd1\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.23340", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23340", "abs": "https://arxiv.org/abs/2510.23340", "authors": ["Anwesha Das", "John Duff", "J\u00f6rg Hoffmann", "Vera Demberg"], "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps", "comment": "11 pages, 3 figures", "summary": "Adaptive agent design offers a way to improve human-AI collaboration on\ntime-sensitive tasks in rapidly changing environments. In such cases, to ensure\nthe human maintains an accurate understanding of critical task elements, an\nassistive agent must not only identify the highest priority information but\nalso estimate how and when this information can be communicated most\neffectively, given that human attention represents a zero-sum cognitive\nresource where focus on one message diminishes awareness of other or upcoming\ninformation. We introduce a theoretical framework for adaptive signalling which\nmeets these challenges by using principles of rational communication,\nformalised as Bayesian reference resolution using the Rational Speech Act (RSA)\nmodelling framework, to plan a sequence of messages which optimise timely\nalignment between user belief and a dynamic environment. The agent adapts\nmessage specificity and timing to the particulars of a user and scenario based\non projections of how prior-guided interpretation of messages will influence\nattention to the interface and subsequent belief update, across several\ntimesteps out to a fixed horizon. In a comparison to baseline methods, we show\nthat this effectiveness depends crucially on combining multi-step planning with\na realistic model of user awareness. As the first application of RSA for\ncommunication in a dynamic environment, and for human-AI interaction in\ngeneral, we establish theoretical foundations for pragmatic communication in\nhuman-agent teams, highlighting how insights from cognitive science can be\ncapitalised to inform the design of assistive agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7406\u6027\u8a00\u8bed\u884c\u4e3a(RSA)\u6846\u67b6\u7684\u81ea\u9002\u5e94\u4fe1\u53f7\u7406\u8bba\uff0c\u7528\u4e8e\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4fe1\u606f\u4f20\u9012\u65f6\u673a\u548c\u7279\u5f02\u6027\uff0c\u4ee5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u7528\u6237\u8ba4\u77e5\u4e0e\u73af\u5883\u7684\u53ca\u65f6\u5bf9\u9f50\u3002", "motivation": "\u5728\u5feb\u901f\u53d8\u5316\u7684\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u8f85\u52a9\u667a\u80fd\u4f53\u9700\u8981\u8bc6\u522b\u6700\u9ad8\u4f18\u5148\u7ea7\u4fe1\u606f\u5e76\u4f30\u8ba1\u5982\u4f55\u6700\u6709\u6548\u5730\u4f20\u9012\u8fd9\u4e9b\u4fe1\u606f\uff0c\u56e0\u4e3a\u4eba\u7c7b\u6ce8\u610f\u529b\u662f\u96f6\u548c\u8ba4\u77e5\u8d44\u6e90\uff0c\u5173\u6ce8\u4e00\u6761\u4fe1\u606f\u4f1a\u964d\u4f4e\u5bf9\u5176\u4ed6\u4fe1\u606f\u7684\u611f\u77e5\u3002", "method": "\u4f7f\u7528\u7406\u6027\u8a00\u8bed\u884c\u4e3a(RSA)\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u53c2\u8003\u89e3\u6790\u6765\u89c4\u5212\u6d88\u606f\u5e8f\u5217\uff0c\u8c03\u6574\u6d88\u606f\u7684\u7279\u5f02\u6027\u548c\u65f6\u673a\uff0c\u57fa\u4e8e\u5bf9\u7528\u6237\u6ce8\u610f\u529b\u5206\u5e03\u548c\u4fe1\u5ff5\u66f4\u65b0\u7684\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684\u6548\u679c\u5173\u952e\u4f9d\u8d56\u4e8e\u5c06\u591a\u6b65\u89c4\u5212\u4e0e\u73b0\u5b9e\u7684\u7528\u6237\u610f\u8bc6\u6a21\u578b\u76f8\u7ed3\u5408\u3002", "conclusion": "\u8fd9\u662fRSA\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u901a\u4fe1\u548c\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u4e3a\u4eba\u7c7b-\u667a\u80fd\u4f53\u56e2\u961f\u7684\u8bed\u7528\u901a\u4fe1\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.23476", "categories": ["cs.AI", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23476", "abs": "https://arxiv.org/abs/2510.23476", "authors": ["Sima Noorani", "Shayan Kiyani", "George Pappas", "Hamed Hassani"], "title": "Human-AI Collaborative Uncertainty Quantification", "comment": null, "summary": "AI predictive systems are increasingly embedded in decision making pipelines,\nshaping high stakes choices once made solely by humans. Yet robust decisions\nunder uncertainty still rely on capabilities that current AI lacks: domain\nknowledge not captured by data, long horizon context, and reasoning grounded in\nthe physical world. This gap has motivated growing efforts to design\ncollaborative frameworks that combine the complementary strengths of humans and\nAI. This work advances this vision by identifying the fundamental principles of\nHuman AI collaboration within uncertainty quantification, a key component of\nreliable decision making. We introduce Human AI Collaborative Uncertainty\nQuantification, a framework that formalizes how an AI model can refine a human\nexpert's proposed prediction set with two goals: avoiding counterfactual harm,\nensuring the AI does not degrade correct human judgments, and complementarity,\nenabling recovery of correct outcomes the human missed. At the population\nlevel, we show that the optimal collaborative prediction set follows an\nintuitive two threshold structure over a single score function, extending a\nclassical result in conformal prediction. Building on this insight, we develop\npractical offline and online calibration algorithms with provable distribution\nfree finite sample guarantees. The online method adapts to distribution shifts,\nincluding human behavior evolving through interaction with AI, a phenomenon we\ncall Human to AI Adaptation. Experiments across image classification,\nregression, and text based medical decision making show that collaborative\nprediction sets consistently outperform either agent alone, achieving higher\ncoverage and smaller set sizes across various conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86Human AI Collaborative Uncertainty Quantification\u6846\u67b6\uff0c\u901a\u8fc7AI\u6a21\u578b\u4f18\u5316\u4eba\u7c7b\u4e13\u5bb6\u7684\u9884\u6d4b\u96c6\uff0c\u907f\u514d\u5bf9\u6b63\u786e\u5224\u65ad\u7684\u635f\u5bb3\u5e76\u8865\u5145\u4eba\u7c7b\u9057\u6f0f\u7684\u9884\u6d4b\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56de\u5f52\u548c\u533b\u7597\u51b3\u7b56\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u4eba\u7c7b\u6216AI\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u4ecd\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u3001\u957f\u671f\u4e0a\u4e0b\u6587\u548c\u7269\u7406\u4e16\u754c\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u548cAI\u7684\u4e92\u8865\u4f18\u52bf\u6765\u63d0\u5347\u51b3\u7b56\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u534f\u4f5c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u57fa\u4e8e\u5355\u4e00\u8bc4\u5206\u51fd\u6570\u7684\u4e24\u9608\u503c\u7ed3\u6784\u6784\u5efa\u6700\u4f18\u534f\u4f5c\u9884\u6d4b\u96c6\uff0c\u5f00\u53d1\u4e86\u5177\u6709\u5206\u5e03\u65e0\u5173\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u6821\u51c6\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u534f\u4f5c\u9884\u6d4b\u96c6\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u90fd\u6bd4\u5355\u72ec\u4f7f\u7528\u4eba\u7c7b\u6216AI\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8986\u76d6\u7387\u548c\u66f4\u5c0f\u7684\u96c6\u5408\u5927\u5c0f\u3002", "conclusion": "Human AI\u534f\u4f5c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u80fd\u6709\u6548\u7ed3\u5408\u4eba\u7c7b\u548cAI\u7684\u4f18\u52bf\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.23487", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.23487", "abs": "https://arxiv.org/abs/2510.23487", "authors": ["Roham Koohestani", "Ziyou Li", "Anton Podkopaev", "Maliheh Izadi"], "title": "Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy", "comment": null, "summary": "This paper establishes a formal equivalence between the architectural classes\nof modern agentic AI systems and the abstract machines of the Chomsky\nhierarchy. We posit that the memory architecture of an AI agent is the\ndefinitive feature determining its computational power and that it directly\nmaps it to a corresponding class of automaton. Specifically, we demonstrate\nthat simple reflex agents are equivalent to Finite Automata, hierarchical\ntask-decomposition agents are equivalent to Pushdown Automata, and agents\nemploying readable/writable memory for reflection are equivalent to TMs. This\nAutomata-Agent Framework provides a principled methodology for right-sizing\nagent architectures to optimize computational efficiency and cost. More\ncritically, it creates a direct pathway to formal verification, enables the\napplication of mature techniques from automata theory to guarantee agent safety\nand predictability. By classifying agents, we can formally delineate the\nboundary between verifiable systems and those whose behavior is fundamentally\nundecidable. We address the inherent probabilistic nature of LLM-based agents\nby extending the framework to probabilistic automata that allow quantitative\nrisk analysis. The paper concludes by outlining an agenda for developing static\nanalysis tools and grammars for agentic frameworks.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u73b0\u4ee3\u667a\u80fd\u4f53AI\u7cfb\u7edf\u67b6\u6784\u4e0e\u4e54\u59c6\u65af\u57fa\u5c42\u6b21\u62bd\u8c61\u673a\u5668\u4e4b\u95f4\u7684\u5f62\u5f0f\u7b49\u4ef7\u5173\u7cfb\uff0c\u63d0\u51fa\u57fa\u4e8e\u5185\u5b58\u67b6\u6784\u7684\u667a\u80fd\u4f53\u8ba1\u7b97\u80fd\u529b\u5206\u7c7b\u6846\u67b6\u3002", "motivation": "\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u5f62\u5f0f\u5316\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4f18\u5316\u3001\u5f62\u5f0f\u9a8c\u8bc1\u548c\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u5185\u5b58\u67b6\u6784\u6620\u5c04\u5230\u81ea\u52a8\u673a\u7c7b\u522b\uff1a\u7b80\u5355\u53cd\u5c04\u667a\u80fd\u4f53\u5bf9\u5e94\u6709\u9650\u81ea\u52a8\u673a\uff0c\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u667a\u80fd\u4f53\u5bf9\u5e94\u4e0b\u63a8\u81ea\u52a8\u673a\uff0c\u5177\u6709\u8bfb\u5199\u5185\u5b58\u7684\u53cd\u601d\u667a\u80fd\u4f53\u5bf9\u5e94\u56fe\u7075\u673a\u3002", "result": "\u5efa\u7acb\u4e86\u667a\u80fd\u4f53-\u81ea\u52a8\u673a\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u4f53\u67b6\u6784\u4f18\u5316\u548c\u5f62\u5f0f\u9a8c\u8bc1\u63d0\u4f9b\u7cfb\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u53d1\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u8bed\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u754c\u5b9a\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u4e0e\u4e0d\u53ef\u5224\u5b9a\u884c\u4e3a\u4e4b\u95f4\u7684\u8fb9\u754c\u3002", "topic": "agent analysis"}}
{"id": "2510.22967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22967", "abs": "https://arxiv.org/abs/2510.22967", "authors": ["Yucheng Ning", "Xixun Lin", "Fang Fang", "Yanan Cao"], "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs", "comment": "This article has been accepted by Frontiers of Computer Science (FCS)", "summary": "The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u957f\u6587\u672c\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5305\u62ec\u6784\u5efa\u4e2d\u6587\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u6570\u636e\u96c6LongHalluQA\u3001\u5f00\u53d1\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u7cfb\u7edfMAD-Fact\uff0c\u4ee5\u53ca\u5f15\u5165\u4e8b\u5b9e\u91cd\u8981\u6027\u5c42\u6b21\u7ed3\u6784\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u6559\u80b2\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5176\u8f93\u51fa\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6574\u5408\u5927\u89c4\u6a21\u957f\u6587\u672c\u6570\u636e\u96c6\u3001\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u673a\u5236\u548c\u52a0\u6743\u8bc4\u4f30\u6307\u6807\uff0c\u6784\u5efa\u4e2d\u6587\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u6570\u636e\u96c6LongHalluQA\uff0c\u5f00\u53d1\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u7cfb\u7edfMAD-Fact\uff0c\u5e76\u5f15\u5165\u4e8b\u5b9e\u91cd\u8981\u6027\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8f83\u5927\u7684LLM\u901a\u5e38\u4fdd\u6301\u8f83\u9ad8\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u800c\u56fd\u4ea7\u6a21\u578b\u5728\u4e2d\u6587\u5185\u5bb9\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u589e\u5f3a\u957f\u6587\u672cLLM\u8f93\u51fa\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u6307\u5bfc\u5176\u5728\u654f\u611f\u9886\u57df\u7684\u5b89\u5168\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2510.23006", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23006", "abs": "https://arxiv.org/abs/2510.23006", "authors": ["Shenran Wang", "Timothy Tin-Long Tse", "Jian Zhu"], "title": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures", "comment": null, "summary": "We perform in-depth evaluations of in-context learning (ICL) on\nstate-of-the-art transformer, state-space, and hybrid large language models\nover two categories of knowledge-based ICL tasks. Using a combination of\nbehavioral probing and intervention-based methods, we have discovered that,\nwhile LLMs of different architectures can behave similarly in task performance,\ntheir internals could remain different. We discover that function vectors (FVs)\nresponsible for ICL are primarily located in the self-attention and Mamba\nlayers, and speculate that Mamba2 uses a different mechanism from FVs to\nperform ICL. FVs are more important for ICL involving parametric knowledge\nretrieval, but not for contextual knowledge understanding. Our work contributes\nto a more nuanced understanding across architectures and task types.\nMethodologically, our approach also highlights the importance of combining both\nbehavioural and mechanistic analyses to investigate LLM capabilities.", "AI": {"tldr": "\u5bf9\u6700\u5148\u8fdb\u7684transformer\u3001\u72b6\u6001\u7a7a\u95f4\u548c\u6df7\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u578bICL\u4efb\u52a1\u4e0a\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784LLM\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u53ef\u80fd\u76f8\u4f3c\u4f46\u5185\u90e8\u673a\u5236\u4e0d\u540c\uff0c\u529f\u80fd\u5411\u91cf\u4e3b\u8981\u4f4d\u4e8e\u81ea\u6ce8\u610f\u529b\u548cMamba\u5c42\uff0c\u4e14\u5728\u4e0d\u540c\u77e5\u8bc6\u7c7b\u578b\u4efb\u52a1\u4e2d\u91cd\u8981\u6027\u4e0d\u540c\u3002", "motivation": "\u6df1\u5165\u7406\u89e3\u4e0d\u540c\u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u5185\u90e8\u673a\u5236\u5dee\u5f02\uff0c\u7279\u522b\u662ftransformer\u3001\u72b6\u6001\u7a7a\u95f4\u548c\u6df7\u5408\u6a21\u578b\u5728\u77e5\u8bc6\u578b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u63a2\u6d4b\u548c\u5e72\u9884\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e09\u79cd\u67b6\u6784LLM\u5728\u4e24\u7c7b\u77e5\u8bc6\u578bICL\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u529f\u80fd\u5411\u91cf\u7684\u4f4d\u7f6e\u548c\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u67b6\u6784LLM\u4efb\u52a1\u8868\u73b0\u76f8\u4f3c\u4f46\u5185\u90e8\u673a\u5236\u4e0d\u540c\uff0c\u529f\u80fd\u5411\u91cf\u4e3b\u8981\u4f4d\u4e8e\u81ea\u6ce8\u610f\u529b\u548cMamba\u5c42\uff0cMamba2\u53ef\u80fd\u4f7f\u7528\u4e0d\u540c\u673a\u5236\uff0c\u529f\u80fd\u5411\u91cf\u5bf9\u53c2\u6570\u77e5\u8bc6\u68c0\u7d22\u66f4\u91cd\u8981\u3002", "conclusion": "\u4e0d\u540c\u67b6\u6784\u548c\u4efb\u52a1\u7c7b\u578b\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u7406\u89e3\uff0c\u884c\u4e3a\u5206\u6790\u548c\u673a\u5236\u5206\u6790\u7ed3\u5408\u5bf9\u7814\u7a76LLM\u80fd\u529b\u5f88\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.23038", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23038", "abs": "https://arxiv.org/abs/2510.23038", "authors": ["Ran Xu", "Jingjing Chen", "Jiayu Ye", "Yu Wu", "Jun Yan", "Carl Yang", "Hongkun Yu"], "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning", "comment": "Work in Progress", "summary": "Large Language Models (LLMs) are widely used as judges to evaluate response\nquality, providing a scalable alternative to human evaluation. However, most\nLLM judges operate solely on intrinsic text-based reasoning, limiting their\nability to verify complex constraints or perform accurate computation.\nMotivated by the success of tool-integrated reasoning (TIR) in numerous tasks,\nwe propose TIR-Judge, an end-to-end RL framework for training LLM judges that\nintegrates a code executor for precise evaluation. TIR-Judge is built on three\nprinciples: (i) diverse training across verifiable and non-verifiable domains,\n(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)\niterative RL that bootstraps directly from the initial model without\ndistillation. On seven public benchmarks, TIR-Judge surpasses strong\nreasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and\nachieves listwise performance comparable to Claude-Opus-4 despite having only\n8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled\njudge trajectories, matches the performance of distilled variants,\ndemonstrating that tool-augmented judges can self-evolve through iterative\nreinforcement learning.", "AI": {"tldr": "TIR-Judge\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u96c6\u6210\u4ee3\u7801\u6267\u884c\u5668\u7684LLM\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u6765\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u5668\u4ec5\u4f9d\u8d56\u6587\u672c\u63a8\u7406\uff0c\u96be\u4ee5\u9a8c\u8bc1\u590d\u6742\u7ea6\u675f\u6216\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\uff0c\u9700\u8981\u5de5\u5177\u96c6\u6210\u6765\u63d0\u5347\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\uff1a\u591a\u6837\u5316\u8bad\u7ec3\u3001\u7075\u6d3b\u8bc4\u4f30\u683c\u5f0f\u3001\u65e0\u9700\u84b8\u998f\u7684\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\uff0c\u96c6\u6210\u4ee3\u7801\u6267\u884c\u5668\u8fdb\u884c\u7cbe\u786e\u8bc4\u4f30\u3002", "result": "\u57287\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\uff0cTIR-Judge\u8d85\u8d8a\u5f3a\u63a8\u7406\u8bc4\u4f30\u56686.4%-7.7%\uff0c8B\u53c2\u6570\u6a21\u578b\u5728\u5217\u8868\u8bc4\u4f30\u4e0a\u8fbe\u5230Claude-Opus-4\u6c34\u5e73\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u7684\u8bc4\u4f30\u5668\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u81ea\u6211\u8fdb\u5316\uff0c\u65e0\u9700\u84b8\u998f\u5373\u53ef\u8fbe\u5230\u84b8\u998f\u53d8\u4f53\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.23564", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23564", "abs": "https://arxiv.org/abs/2510.23564", "authors": ["Zhaoyang Yu", "Jiayi Zhang", "Huixue Su", "Yufan Zhao", "Yifan Wu", "Mingyi Deng", "Jinyu Xiang", "Yizhang Lin", "Lingxiao Tang", "Yingchao Li", "Yuyu Luo", "Bang Liu", "Chenglin Wu"], "title": "ReCode: Unify Plan and Action for Universal Granularity Control", "comment": null, "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.", "AI": {"tldr": "ReCode\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9012\u5f52\u4ee3\u7801\u751f\u6210\u7edf\u4e00\u89c4\u5212\u548c\u884c\u52a8\u7684\u65b0\u8303\u5f0f\uff0c\u5c06\u9ad8\u5c42\u6b21\u8ba1\u5212\u8868\u793a\u4e3a\u62bd\u8c61\u5360\u4f4d\u51fd\u6570\uff0c\u7136\u540e\u9012\u5f52\u5206\u89e3\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b50\u51fd\u6570\uff0c\u76f4\u5230\u539f\u59cb\u884c\u52a8\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7f3a\u4e4f\u5728\u4e0d\u540c\u51b3\u7b56\u7c92\u5ea6\u95f4\u7075\u6d3b\u64cd\u4f5c\u7684\u80fd\u529b\uff0c\u73b0\u6709\u8303\u5f0f\u5728\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u884c\u52a8\u4e4b\u95f4\u8bbe\u7f6e\u4e86\u4e25\u683c\u5206\u79bb\uff0c\u8fd9\u9650\u5236\u4e86\u52a8\u6001\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9012\u5f52\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u9ad8\u5c42\u8ba1\u5212\u4f5c\u4e3a\u62bd\u8c61\u5360\u4f4d\u51fd\u6570\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b50\u51fd\u6570\uff0c\u76f4\u81f3\u8fbe\u5230\u539f\u59cb\u884c\u52a8\uff0c\u4ece\u800c\u7edf\u4e00\u89c4\u5212\u548c\u884c\u52a8\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eReCode\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u5148\u8fdb\u57fa\u7ebf\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u9012\u5f52\u4ee3\u7801\u751f\u6210\u7edf\u4e00\u89c4\u5212\u548c\u884c\u52a8\u662f\u5b9e\u73b0\u901a\u7528\u7c92\u5ea6\u63a7\u5236\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2510.23595", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23595", "abs": "https://arxiv.org/abs/2510.23595", "authors": ["Yixing Chen", "Yiding Wang", "Siqi Zhu", "Haofei Yu", "Tao Feng", "Muhan Zhan", "Mostofa Patwary", "Jiaxuan You"], "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution", "comment": "29 pages, 4 figures, submitted to ICLR 2026", "summary": "Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision.", "AI": {"tldr": "\u63d0\u51faMAE\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4ea4\u4e92\u4ee3\u7406\uff08\u63d0\u8bae\u8005\u3001\u6c42\u89e3\u8005\u3001\u8bc4\u5224\u8005\uff09\u5b9e\u73b0LLM\u7684\u81ea\u8fdb\u5316\uff0c\u5728\u6570\u5b66\u3001\u63a8\u7406\u548c\u5e38\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1b\u81ea\u535a\u5f08RL\u65b9\u6cd5\u9700\u8981\u7279\u5b9a\u73af\u5883\u53cd\u9988\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u901a\u7528\u9886\u57df", "method": "\u57fa\u4e8e\u5355\u4e00LLM\u5b9e\u4f8b\u5316\u4e09\u4e2a\u4ea4\u4e92\u4ee3\u7406\uff1a\u63d0\u8bae\u8005\u751f\u6210\u95ee\u9898\uff0c\u6c42\u89e3\u8005\u5c1d\u8bd5\u89e3\u51b3\uff0c\u8bc4\u5224\u8005\u8bc4\u4f30\u5e76\u5171\u540c\u8fdb\u5316\uff1b\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4ee3\u7406\u884c\u4e3a", "result": "\u5728Qwen2.5-3B-Instruct\u4e0a\u5b9e\u9a8c\uff0c\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5e73\u5747\u63d0\u53474.54%", "conclusion": "MAE\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u76d1\u7763\u663e\u8457\u589e\u5f3aLLM\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2510.23601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23601", "abs": "https://arxiv.org/abs/2510.23601", "authors": ["Jiahao Qiu", "Xuan Qi", "Hongru Wang", "Xinzhe Juan", "Yimin Wang", "Zelin Zhao", "Jiayi Geng", "Jiacheng Guo", "Peihang Li", "Jingzhe Shi", "Shilong Liu", "Mengdi Wang"], "title": "Alita-G: Self-Evolving Generative Agent for Agent Generation", "comment": "15 pages, 3 figures", "summary": "Large language models (LLMs) have been shown to perform better when\nscaffolded into agents with memory, tools, and feedback. Beyond this,\nself-evolving agents have emerged, but current work largely limits adaptation\nto prompt rewriting or failure retries. Therefore, we present ALITA-G, a\nself-evolution framework that transforms a general-purpose agent into a domain\nexpert by systematically generating, abstracting, and curating Model Context\nProtocol (MCP) tools. In this framework, a generalist agent executes a curated\nsuite of target-domain tasks and synthesizes candidate MCPs from successful\ntrajectories. These are then abstracted to parameterized primitives and\nconsolidated into an MCP Box. At inference time, ALITA-G performs\nretrieval-augmented MCP selection with the help of each tool's descriptions and\nuse cases, before executing an agent equipped with the MCP Executor. Across\nseveral benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains\nstrong gains while reducing computation costs. On GAIA validation, it achieves\n83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result\nwhile reducing mean tokens per example by approximately 15% relative to a\nstrong baseline agent. ALITA-G thus provides a principled pathway from\ngeneralist capability to reusable, domain-specific competence, improving both\naccuracy and efficiency on complex reasoning tasks.", "AI": {"tldr": "ALITA-G\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u3001\u62bd\u8c61\u548c\u6574\u7406MCP\u5de5\u5177\uff0c\u5c06\u901a\u7528\u667a\u80fd\u4f53\u8f6c\u5316\u4e3a\u9886\u57df\u4e13\u5bb6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u4e3b\u8981\u5c40\u9650\u4e8e\u63d0\u793a\u91cd\u5199\u6216\u5931\u8d25\u91cd\u8bd5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u9886\u57df\u4e13\u4e1a\u5316\u80fd\u529b\u63d0\u5347\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba9\u901a\u7528\u667a\u80fd\u4f53\u6267\u884c\u76ee\u6807\u9886\u57df\u4efb\u52a1\uff0c\u4ece\u6210\u529f\u8f68\u8ff9\u4e2d\u5408\u6210\u5019\u9009MCP\u5de5\u5177\uff0c\u5c06\u5176\u62bd\u8c61\u4e3a\u53c2\u6570\u5316\u539f\u8bed\u5e76\u6574\u5408\u5230MCP Box\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\u7684MCP\u9009\u62e9\u3002", "result": "\u5728GAIA\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523083.03% pass@1\u548c89.09% pass@3\u7684\u65b0SOTA\u7ed3\u679c\uff0c\u540c\u65f6\u5c06\u6bcf\u4e2a\u793a\u4f8b\u7684\u5e73\u5747token\u6570\u51cf\u5c11\u7ea615%\u3002", "conclusion": "ALITA-G\u63d0\u4f9b\u4e86\u4ece\u901a\u7528\u80fd\u529b\u5230\u53ef\u91cd\u7528\u9886\u57df\u7279\u5b9a\u80fd\u529b\u7684\u539f\u7406\u6027\u8def\u5f84\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.22500", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22500", "abs": "https://arxiv.org/abs/2510.22500", "authors": ["Ren Yin", "Takashi Ishida", "Masashi Sugiyama"], "title": "Scalable Oversight via Partitioned Human Supervision", "comment": null, "summary": "As artificial intelligence (AI) systems approach and surpass expert human\nperformance across a broad range of tasks, obtaining high-quality human\nsupervision for evaluation and training becomes increasingly challenging. Our\nfocus is on tasks that require deep knowledge and skills of multiple domains.\nUnfortunately, even the best human experts are knowledgeable only in a single\nnarrow area, and will not be able to evaluate the correctness of advanced AI\nsystems on such superhuman tasks. However, based on their narrow expertise,\nhumans may provide a weak signal, i.e., a complementary label indicating an\noption that is incorrect. For example, a cardiologist could state that \"this is\nnot related to cardiology,'' even if they cannot identify the true disease.\nBased on this weak signal, we propose a scalable oversight framework that\nenables us to evaluate frontier AI systems without the need to prepare the\nground truth. We derive an unbiased estimator of top-1 accuracy from\ncomplementary labels and quantify how many complementary labels are needed to\nmatch the variance of ordinary labels. We further introduce two estimators to\ncombine scarce ordinary labels with abundant complementary labels. We provide\nfinite-sample deviation guarantees for both complementary-only and the mixed\nestimators. Empirically, we show that we can evaluate the output of large\nlanguage models without the ground truth, if we have complementary labels. We\nfurther show that we can train an AI system with such weak signals: we show how\nwe can design an agentic AI system automatically that can perform better with\nthis partitioned human supervision. Our code is available at\nhttps://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u8865\u6807\u7b7e\u7684\u53ef\u6269\u5c55\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u80fd\u529b\u7684AI\u7cfb\u7edf\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u4eba\u5de5\u76d1\u7763\u53d8\u5f97\u56f0\u96be\u3002\u4eba\u7c7b\u4e13\u5bb6\u53ea\u80fd\u5728\u5176\u72ed\u7a84\u9886\u57df\u63d0\u4f9b\u5f31\u4fe1\u53f7\uff08\u4e92\u8865\u6807\u7b7e\uff09\uff0c\u800c\u975e\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u4e92\u8865\u6807\u7b7e\u6784\u5efa\u65e0\u504f\u4f30\u8ba1\u5668\u6765\u8bc4\u4f30top-1\u51c6\u786e\u7387\uff0c\u7ed3\u5408\u7a00\u7f3a\u7684\u666e\u901a\u6807\u7b7e\u548c\u4e30\u5bcc\u7684\u4e92\u8865\u6807\u7b7e\uff0c\u5e76\u63d0\u4f9b\u6709\u9650\u6837\u672c\u504f\u5dee\u4fdd\u8bc1\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\u53ef\u4ee5\u5728\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\uff0c\u5e76\u80fd\u8bbe\u8ba1\u51fa\u5728\u8fd9\u79cd\u5206\u533a\u4eba\u5de5\u76d1\u7763\u4e0b\u8868\u73b0\u66f4\u597d\u7684\u667a\u80fdAI\u7cfb\u7edf\u3002", "conclusion": "\u4e92\u8865\u6807\u7b7e\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u548c\u8bad\u7ec3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u80fd\u529b\u7684AI\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2510.23272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23272", "abs": "https://arxiv.org/abs/2510.23272", "authors": ["Bang Xiao", "Lingjie Jiang", "Shaohan Huang", "Tengchao Lv", "Yupan Huang", "Xun Wu", "Lei Cui", "Furu Wei"], "title": "Code Aesthetics with Agentic Reward Feedback", "comment": "30 pages, 7 figures", "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7f8e\u89c2\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u6784\u5efaAesCode-358K\u6570\u636e\u96c6\u3001\u591a\u4ee3\u7406\u5956\u52b1\u53cd\u9988\u7cfb\u7edfGRPO-AR\u7b97\u6cd5\uff0c\u4ee5\u53caOpenDesign\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7f8e\u89c2\u5ea6\u3002", "motivation": "LLM\u5728\u4f20\u7edf\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9\u5bfc\u5411\u7684\u7f16\u7801\u4efb\u52a1\u4e2d\u751f\u6210\u4ee3\u7801\u7684\u7f8e\u89c2\u5ea6\u8f83\u5dee\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4ee3\u7801\u7f8e\u5b66\u8d28\u91cf\u3002", "method": "\u6784\u5efaAesCode-358K\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u591a\u4ee3\u7406\u5956\u52b1\u53cd\u9988\u7cfb\u7edf\u8bc4\u4f30\u53ef\u6267\u884c\u6027\u3001\u9759\u6001\u7f8e\u5b66\u548c\u4ea4\u4e92\u7f8e\u5b66\uff1b\u5f00\u53d1GRPO-AR\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u529f\u80fd\u6027\u548c\u4ee3\u7801\u7f8e\u89c2\u5ea6\uff1b\u521b\u5efaOpenDesign\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728AesCode-358K\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7406\u5956\u52b1\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86OpenDesign\u57fa\u51c6\u7684\u8868\u73b0\uff0c\u5e76\u5728PandasPlotBench\u7b49\u73b0\u6709\u57fa\u51c6\u4e0a\u4e5f\u6709\u6539\u8fdb\u3002AesCoder-4B\u8d85\u8d8a\u4e86GPT-4o\u548cGPT-4.1\uff0c\u6027\u80fd\u53ef\u4e0e480B-685B\u53c2\u6570\u7684\u5927\u578b\u5f00\u6e90\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u4ee3\u7801\u7684\u7f8e\u89c2\u5ea6\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5956\u52b1\u53cd\u9988\u7684\u4f18\u8d8a\u6027\u3002", "topic": "code agent"}}
{"id": "2510.22543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22543", "abs": "https://arxiv.org/abs/2510.22543", "authors": ["Yuyang Ding", "Chi Zhang", "Juntao Li", "Haibin Lin", "Xin Liu", "Min Zhang"], "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning", "comment": "Project page: https://fapo-rl.github.io/", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFAPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u60e9\u7f5a\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u9ad8\u63a8\u7406\u53ef\u9760\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u4e2d\uff0c\u5373\u4f7f\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u7f3a\u9677\uff08\u5982\u731c\u6d4b\u7b54\u6848\u3001\u8df3\u8dc3\u63a8\u7406\uff09\uff0c\u53ea\u8981\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u5c31\u4f1a\u88ab\u540c\u7b49\u5956\u52b1\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u4e0d\u53ef\u9760\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u63d0\u51faFAPO\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff08GenRM\uff09\u7cbe\u786e\u5b9a\u4f4d\u63a8\u7406\u9519\u8bef\uff1b2\uff09\u5bf9\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8f68\u8ff9\u65bd\u52a0\u53c2\u6570\u65e0\u5173\u7684\u5956\u52b1\u60e9\u7f5a\uff1b3\uff09\u5728\u8bad\u7ec3\u65e9\u671f\u5229\u7528\u7f3a\u9677\u8f68\u8ff9\u4f5c\u4e3a\u6377\u5f84\uff0c\u540e\u671f\u9010\u6b65\u8f6c\u5411\u53ef\u9760\u63a8\u7406\u3002", "result": "FAPO\u5728\u591a\u4e2a\u9886\u57df\u6709\u6548\u63d0\u9ad8\u4e86\u7ed3\u679c\u6b63\u786e\u6027\u3001\u8fc7\u7a0b\u53ef\u9760\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "FAPO\u901a\u8fc7\u6709\u610f\u8bc6\u5730\u5904\u7406\u7f3a\u9677\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u53ef\u9760\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22686", "abs": "https://arxiv.org/abs/2510.22686", "authors": ["Shan Zhong", "Shutong Ding", "He Diao", "Xiangyu Wang", "Kah Chan Teh", "Bei Peng"], "title": "FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning", "comment": null, "summary": "Reliable value estimation serves as the cornerstone of reinforcement learning\n(RL) by evaluating long-term returns and guiding policy improvement,\nsignificantly influencing the convergence speed and final performance. Existing\nworks improve the reliability of value function estimation via multi-critic\nensembles and distributional RL, yet the former merely combines multi point\nestimation without capturing distributional information, whereas the latter\nrelies on discretization or quantile regression, limiting the expressiveness of\ncomplex value distributions. Inspired by flow matching's success in generative\nmodeling, we propose a generative paradigm for value estimation, named\nFlowCritic. Departing from conventional regression for deterministic value\nprediction, FlowCritic leverages flow matching to model value distributions and\ngenerate samples for value estimation.", "AI": {"tldr": "\u63d0\u51faFlowCritic\u65b9\u6cd5\uff0c\u4f7f\u7528\u6d41\u5339\u914d\u6280\u672f\u5efa\u6a21\u4ef7\u503c\u5206\u5e03\uff0c\u901a\u8fc7\u751f\u6210\u6837\u672c\u8fdb\u884c\u4ef7\u503c\u4f30\u8ba1\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u591a\u8bc4\u8bba\u5bb6\u96c6\u6210\u4ec5\u7ed3\u5408\u70b9\u4f30\u8ba1\uff0c\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u79bb\u6563\u5316\u6216\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u9650\u5236\u4e86\u590d\u6742\u4ef7\u503c\u5206\u5e03\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u5efa\u6a21\u4ef7\u503c\u5206\u5e03\uff0c\u901a\u8fc7\u751f\u6210\u6837\u672c\u8fdb\u884c\u4ef7\u503c\u4f30\u8ba1\uff0c\u4e0e\u4f20\u7edf\u786e\u5b9a\u6027\u4ef7\u503c\u9884\u6d4b\u7684\u56de\u5f52\u65b9\u6cd5\u4e0d\u540c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u4ef7\u503c\u5206\u5e03\u4fe1\u606f\uff0c\u63d0\u9ad8\u4ef7\u503c\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6d41\u5339\u914d\u4e3a\u4ef7\u503c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u751f\u6210\u8303\u5f0f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8868\u8fbe\u590d\u6742\u4ef7\u503c\u5206\u5e03\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22732", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22732", "abs": "https://arxiv.org/abs/2510.22732", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation", "comment": "9 pages, NeurIPS 2025 Workshop on Language Agents and World Models", "summary": "We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2a\u8bb0\u5fc6\u589e\u5f3a\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8ba4\u77e5\u7a7a\u95f4\u4e2d\u7684\u524d\u77bb\u52a8\u4f5c\u6a21\u62df\u6765\u5236\u5b9a\u57fa\u4e8e\u73af\u5883\u6a21\u578b\u7684\u8ba1\u5212\uff0c\u65e0\u9700\u795e\u7ecf\u7f51\u7edc\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u65b0\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7f51\u7edc\u667a\u80fd\u4f53\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u65b0\u73af\u5883\uff0c\u7f3a\u4e4f\u5bf9\u73af\u5883\u7ed3\u6784\u548c\u52a8\u6001\u7684\u8ba4\u77e5\uff0c\u5bfc\u81f4\u6267\u884c\u8ba1\u5212\u6548\u7387\u4f4e\u4e0b\u3002", "method": "ATLAS\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff1a\u9996\u5148\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u6784\u5efa\u8ba4\u77e5\u5730\u56fe\uff0c\u89c4\u5212\u5668\u63d0\u51fa\u5019\u9009\u52a8\u4f5c\uff0c\u6a21\u62df\u5668\u5728\u8ba4\u77e5\u7a7a\u95f4\u9884\u6d4b\u7ed3\u679c\uff0c\u8bc4\u8bba\u5bb6\u5206\u6790\u9009\u9879\u9009\u62e9\u6700\u4f73\u8def\u5f84\u5e76\u66f4\u65b0\u8ba1\u5212\uff0c\u6d4f\u89c8\u5668\u6267\u884c\u5668\u6267\u884c\u9009\u5b9a\u52a8\u4f5c\u3002", "result": "\u5728WebArena-Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523063%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7cfb\u7edf\u768453.9%\u3002", "conclusion": "ATLAS\u65e0\u9700\u7f51\u7ad9\u7279\u5b9a\u7684LLM\u5fae\u8c03\uff0c\u5176\u4e16\u754c\u6a21\u578b\u3001\u5206\u5c42\u89c4\u5212\u5668\u548c\u57fa\u4e8e\u524d\u77bb\u7684\u91cd\u65b0\u89c4\u5212\u5668\u5728\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u53d1\u6325\u4e92\u8865\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.22859", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22859", "abs": "https://arxiv.org/abs/2510.22859", "authors": ["Kaitong Cai", "Jusheng Zhang", "Jing Yang", "Keze Wang"], "title": "Guardian: Decoupling Exploration from Safety in Reinforcement Learning", "comment": null, "summary": "Hybrid offline--online reinforcement learning (O2O RL) promises both sample\nefficiency and robust exploration, but suffers from instability due to\ndistribution shift between offline and online data. We introduce RLPD-GX, a\nframework that decouples policy optimization from safety enforcement: a\nreward-seeking learner explores freely, while a projection-based guardian\nguarantees rule-consistent execution and safe value backups. This design\npreserves the exploratory value of online interactions without collapsing to\nconservative policies. To further stabilize training, we propose dynamic\ncurricula that gradually extend temporal horizons and anneal offline--online\ndata mixing. We prove convergence via a contraction property of the guarded\nBellman operator, and empirically show state-of-the-art performance on\nAtari-100k, achieving a normalized mean score of 3.02 (+45\\% over prior hybrid\nmethods) with stronger safety and stability. Beyond Atari, ablations\ndemonstrate consistent gains across safety-critical and long-horizon tasks,\nunderscoring the generality of our design. Extensive and comprehensive results\nhighlight decoupled safety enforcement as a simple yet principled route to\nrobust O2O RL, suggesting a broader paradigm for reconciling exploration and\nsafety in reinforcement learning.", "AI": {"tldr": "RLPD-GX\u662f\u4e00\u4e2a\u6df7\u5408\u79bb\u7ebf-\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u4f18\u5316\u4e0e\u5b89\u5168\u6267\u884c\u89e3\u8026\u6765\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728Atari-100k\u4e0a\u5b9e\u73b0\u4e8645%\u7684\u6027\u80fd\u63d0\u5347\u548c\u66f4\u5f3a\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u6df7\u5408\u79bb\u7ebf-\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u7ed3\u5408\u4e86\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u63a2\u7d22\u7684\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u79bb\u7ebf\u6570\u636e\u548c\u5728\u7ebf\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u800c\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89e3\u8026\u8bbe\u8ba1\uff1a\u5956\u52b1\u5bfb\u6c42\u7684\u5b66\u4e60\u5668\u81ea\u7531\u63a2\u7d22\uff0c\u57fa\u4e8e\u6295\u5f71\u7684\u5b88\u62a4\u5668\u4fdd\u8bc1\u89c4\u5219\u4e00\u81f4\u6027\u6267\u884c\u548c\u5b89\u5168\u503c\u5907\u4efd\uff1b\u63d0\u51fa\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\uff0c\u9010\u6b65\u6269\u5c55\u65f6\u95f4\u89c6\u91ce\u5e76\u9000\u706b\u79bb\u7ebf-\u5728\u7ebf\u6570\u636e\u6df7\u5408\u3002", "result": "\u5728Atari-100k\u4e0a\u5b9e\u73b0\u4e863.02\u7684\u5f52\u4e00\u5316\u5e73\u5747\u5206\u6570\uff0c\u6bd4\u4e4b\u524d\u7684\u6df7\u5408\u65b9\u6cd5\u63d0\u534745%\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\uff1b\u5728\u5b89\u5168\u5173\u952e\u548c\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u89e3\u8026\u7684\u5b89\u5168\u6267\u884c\u4e3a\u9c81\u68d2\u7684\u6df7\u5408\u79bb\u7ebf-\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u7b80\u5355\u800c\u6709\u539f\u5219\u7684\u9014\u5f84\uff0c\u4e3a\u534f\u8c03\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u548c\u5b89\u5168\u63d0\u51fa\u4e86\u66f4\u5e7f\u6cdb\u7684\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22940", "abs": "https://arxiv.org/abs/2510.22940", "authors": ["Judah Goldfeder", "Matthew So", "Hod Lipson"], "title": "RL-AUX: Reinforcement Learning for Auxiliary Task Generation", "comment": null, "summary": "Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in\nwhich a network trains on auxiliary tasks to improve performance on its main\ntask. This technique is used to improve generalization and, ultimately,\nperformance on the network's main task. AL has been demonstrated to improve\nperformance across multiple domains, including navigation, image\nclassification, and natural language processing. One weakness of AL is the need\nfor labeled auxiliary tasks, which can require human effort and domain\nexpertise to generate. Meta Learning techniques have been used to solve this\nissue by learning an additional auxiliary task generation network that can\ncreate helpful tasks for the primary network. The most prominent techniques\nrely on Bi-Level Optimization, which incurs computational cost and increased\ncode complexity. To avoid the need for Bi-Level Optimization, we present an\nRL-based approach to dynamically create auxiliary tasks. In this framework, an\nRL agent is tasked with selecting auxiliary labels for every data point in a\ntraining set. The agent is rewarded when their selection improves the\nperformance on the primary task. We also experiment with learning optimal\nstrategies for weighing the auxiliary loss per data point. On the 20-Superclass\nCIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and\nperforms as well as a prominent Bi-Level Optimization technique. Our weight\nlearning approaches significantly outperform all of these benchmarks. For\nexample, a Weight-Aware RL-based approach helps the VGG16 architecture achieve\n80.9% test accuracy while the human-labeled auxiliary task setup achieved\n75.53%. The goal of this work is to (1) prove that RL is a viable approach to\ndynamically generate auxiliary tasks and (2) demonstrate that per-sample\nauxiliary task weights can be learned alongside the auxiliary task labels and\ncan achieve strong results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f85\u52a9\u4efb\u52a1\u52a8\u6001\u751f\u6210\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u5143\u5b66\u4e60\u4e2d\u53cc\u7ea7\u4f18\u5316\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u8fc7RL\u667a\u80fd\u4f53\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6570\u636e\u70b9\u9009\u62e9\u8f85\u52a9\u6807\u7b7e\uff0c\u5e76\u5728CIFAR100\u4e0a\u8d85\u8d8a\u4eba\u5de5\u6807\u6ce8\u7684\u8f85\u52a9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u8f85\u52a9\u5b66\u4e60\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u4eba\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u53cc\u7ea7\u4f18\u5316\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4ee3\u7801\u590d\u6742\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cRL\u667a\u80fd\u4f53\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6570\u636e\u70b9\u52a8\u6001\u9009\u62e9\u8f85\u52a9\u6807\u7b7e\uff0c\u5f53\u9009\u62e9\u80fd\u63d0\u5347\u4e3b\u4efb\u52a1\u6027\u80fd\u65f6\u83b7\u5f97\u5956\u52b1\u3002\u8fd8\u5b9e\u9a8c\u4e86\u5b66\u4e60\u6bcf\u4e2a\u6570\u636e\u70b9\u8f85\u52a9\u635f\u5931\u6743\u91cd\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u572820-Superclass CIFAR100\u95ee\u9898\u4e0a\uff0cRL\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u4e0e\u4e3b\u6d41\u53cc\u7ea7\u4f18\u5316\u6280\u672f\u8868\u73b0\u76f8\u5f53\u3002\u6743\u91cd\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0cVGG16\u67b6\u6784\u8fbe\u523080.9%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u800c\u4eba\u5de5\u6807\u6ce8\u65b9\u6cd5\u4e3a75.53%\u3002", "conclusion": "RL\u662f\u52a8\u6001\u751f\u6210\u8f85\u52a9\u4efb\u52a1\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u540c\u65f6\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u7684\u8f85\u52a9\u4efb\u52a1\u6743\u91cd\u53ef\u4ee5\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.23027", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23027", "abs": "https://arxiv.org/abs/2510.23027", "authors": ["Di Zhang", "Xun Wu", "Shaohan Huang", "Yaru Hao", "Li Dong", "Zewen Chi", "Zhifang Sui", "Furu Wei"], "title": "Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have substantially improved\nthe training of large-scale language models, leading to significant gains in\ngeneration quality and reasoning ability. However, most existing research\nfocuses on dense models, while RL training for Mixture-of-Experts (MoE)\narchitectures remains underexplored. To address the instability commonly\nobserved in MoE training, we propose a novel router-aware approach to optimize\nimportance sampling (IS) weights in off-policy RL. Specifically, we design a\nrescaling strategy guided by router logits, which effectively reduces gradient\nvariance and mitigates training divergence. Experimental results demonstrate\nthat our method significantly improves both the convergence stability and the\nfinal performance of MoE models, highlighting the potential of RL algorithmic\ninnovations tailored to MoE architectures and providing a promising direction\nfor efficient training of large-scale expert models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9MoE\u67b6\u6784\u7684\u8def\u7531\u611f\u77e5\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8def\u7531\u5668logits\u7684\u91cd\u65b0\u7f29\u653e\u7b56\u7565\u6765\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\u548c\u7f13\u89e3\u8bad\u7ec3\u53d1\u6563\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bc6\u96c6\u6a21\u578b\uff0c\u800cMoE\u67b6\u6784\u7684RL\u8bad\u7ec3\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14MoE\u8bad\u7ec3\u4e2d\u5e38\u89c1\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8def\u7531\u5668logits\u7684\u91cd\u65b0\u7f29\u653e\u7b56\u7565\u6765\u4f18\u5316\u79bb\u7b56\u7565RL\u4e2d\u7684\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\uff0c\u4ee5\u964d\u4f4e\u68af\u5ea6\u65b9\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86MoE\u6a21\u578b\u7684\u6536\u655b\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u9488\u5bf9MoE\u67b6\u6784\u7684RL\u7b97\u6cd5\u521b\u65b0\u7684\u6f5c\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u4e13\u5bb6\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.22977", "categories": ["cs.LG", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.22977", "abs": "https://arxiv.org/abs/2510.22977", "authors": ["Chenlong Yin", "Zeyang Sha", "Shiwen Cui", "Changhua Meng"], "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination", "comment": "18 pages, 5 figures", "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u63a8\u7406\u589e\u5f3a\u4e0e\u5de5\u5177\u5e7b\u89c9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53d1\u73b0\u5f3a\u5316\u63a8\u7406\u80fd\u529b\u4f1a\u6210\u6bd4\u4f8b\u589e\u52a0\u5de5\u5177\u5e7b\u89c9\uff0c\u8fd9\u79cd\u6548\u5e94\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u65e0\u5173\u4e14\u96be\u4ee5\u7f13\u89e3\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u7684\u5185\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c2\u5bdf\u8868\u660e\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u5f80\u5f80\u4f34\u968f\u66f4\u591a\u5e7b\u89c9\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u6765\u9a8c\u8bc1\u63a8\u7406\u589e\u5f3a\u672c\u8eab\u662f\u5426\u5bfc\u81f4\u5de5\u5177\u5e7b\u89c9\u3002", "method": "\u6784\u5efaSimpleToolHalluBench\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u7814\u7a76\u63a8\u7406\u589e\u5f3a\u4e0e\u5de5\u5177\u5e7b\u89c9\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u8bc4\u4f30\u4e0d\u540c\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u589e\u5f3a\u4e0e\u5de5\u5177\u5e7b\u89c9\u5b58\u5728\u56e0\u679c\u5173\u8054\uff0c\u8fd9\u79cd\u6548\u5e94\u8d85\u8d8a\u8fc7\u62df\u5408\uff0c\u65b9\u6cd5\u65e0\u5173\uff0c\u4e14\u7f13\u89e3\u7b56\u7565\u4f1a\u5e26\u6765\u53ef\u9760\u6027-\u80fd\u529b\u6743\u8861\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u56fa\u6709\u5730\u653e\u5927\u5de5\u5177\u5e7b\u89c9\uff0c\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\u540c\u65f6\u4f18\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.23208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23208", "abs": "https://arxiv.org/abs/2510.23208", "authors": ["Amal Abed", "Ivan Lukic", "J\u00f6rg K. H. Franke", "Frank Hutter"], "title": "Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks", "comment": "Presented at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: The 4th Deep Learning for Code Workshop\n  (DL4C)", "summary": "Large language models (LLMs) have shown impressive promise in code\ngeneration, yet their progress remains limited by the shortage of large-scale\ndatasets that are both diverse and well-aligned with human reasoning. Most\nexisting resources pair problems with solutions, but omit the intermediate\nthought process that guides coding. To close this gap, we present a scalable\nsynthetic data generation pipeline that produces nearly 800k\ninstruction-reasoning-code-test quadruplets. Each sample combines a task, a\nstep-by-step reasoning trace, a working solution, and executable tests,\nenabling models to learn not just the what but also the how of problem solving.\nOur pipeline combines four key components: curated contest problems, web-mined\ncontent filtered by relevance classifiers, data expansion guided by reasoning\npatterns, and multi-stage execution-based validation. A genetic mutation\nalgorithm further increases task diversity while maintaining consistency\nbetween reasoning traces and code implementations. Our key finding is that\nfine-tuning LLMs on this dataset yields consistent improvements on coding\nbenchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model\nscaling, generalize across architectures, and outperform leading open-source\nalternatives under identical sample budgets. Our work establishes\nreasoning-centered synthetic data generation as an efficient approach for\nadvancing coding capabilities in LLMs. We publish our dataset and generation\npipeline to facilitate further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u751f\u6210\u8fd180\u4e07\u4e2a\u5305\u542b\u6307\u4ee4\u3001\u63a8\u7406\u8fc7\u7a0b\u3001\u4ee3\u7801\u548c\u6d4b\u8bd5\u7684\u56db\u5143\u7ec4\u6570\u636e\uff0c\u7528\u4e8e\u63d0\u5347LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u5927\u591a\u53ea\u5305\u542b\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86LLM\u5b66\u4e60\u4eba\u7c7b\u7f16\u7a0b\u601d\u7ef4\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7cbe\u9009\u7ade\u8d5b\u95ee\u9898\u3001\u7f51\u7edc\u6316\u6398\u5185\u5bb9\u3001\u57fa\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u6570\u636e\u6269\u5c55\u3001\u591a\u9636\u6bb5\u6267\u884c\u9a8c\u8bc1\uff0c\u5e76\u4f7f\u7528\u9057\u4f20\u53d8\u5f02\u7b97\u6cd5\u589e\u52a0\u4efb\u52a1\u591a\u6837\u6027\u3002", "result": "\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684LLM\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0c\u63a8\u7406\u611f\u77e5\u6570\u636e\u53ef\u4ee5\u66ff\u4ee3\u6a21\u578b\u7f29\u653e\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u6cdb\u5316\u826f\u597d\uff0c\u5e76\u5728\u76f8\u540c\u6837\u672c\u9884\u7b97\u4e0b\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u63d0\u5347LLM\u7f16\u7801\u80fd\u529b\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2510.23264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23264", "abs": "https://arxiv.org/abs/2510.23264", "authors": ["Xinhai Wang", "Shu Yang", "Liangyu Wang", "Lin Zhang", "Huanyi Xie", "Lijie Hu", "Di Wang"], "title": "PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization", "comment": null, "summary": "Circuit discovery, which involves identifying sparse and task-relevant\nsubnetworks in pre-trained language models, is a cornerstone of mechanistic\ninterpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal\nmethodology in circuit discovery, but its application to large language models\nis severely limited by computational inefficiency and prohibitively high memory\nrequirements. Although several accelerated approaches have been proposed, they\nprimarily rely on linear approximations to ACDC, which significantly\ncompromises analytical faithfulness. Our proposed method for accelerating\nautomated circuit discovery, Per Attention Head Quantization (PAHQ), takes a\nfundamentally different approach by optimizing the efficiency of each\nindividual patching operation. PAHQ leverages a fundamental alignment between\nactivation patching and mixed-precision quantization (MPQ): interpretability\nanalysis through patching essentially performs targeted ablation studies.\nTherefore, we can maintain high precision exclusively for investigated\ncomponents while safely reducing precision elsewhere in the network.\nPAHQ-accelerated ACDC reduces runtime by up to 80\\% and memory consumption by\nup to 30\\% compared to unaccelerated ACDC while maintaining faithfulness.\nImportantly, our method readily integrates with existing edge-based circuit\ndiscovery techniques by modifying the attention computation mechanism. This\ntraining-free approach provides a practical and novel pathway for accelerating\nmechanistic interpretability methods. Our code is available at\nhttps://github.com/626619403/PAHQ.", "AI": {"tldr": "\u63d0\u51faPAHQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u52a0\u901f\u81ea\u52a8\u7535\u8def\u53d1\u73b0\uff0c\u5728\u4fdd\u6301\u5206\u6790\u5fe0\u5b9e\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "\u4f20\u7edfACDC\u65b9\u6cd5\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5185\u5b58\u9700\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u8fd1\u4f3c\u4f46\u4f1a\u635f\u5bb3\u5206\u6790\u5fe0\u5b9e\u5ea6", "method": "PAHQ\u65b9\u6cd5\u57fa\u4e8e\u6fc0\u6d3b\u4fee\u8865\u4e0e\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e3a\u88ab\u8c03\u67e5\u7ec4\u4ef6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5176\u4ed6\u90e8\u5206\u964d\u4f4e\u7cbe\u5ea6\uff0c\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u8ba1\u7b97\u673a\u5236\u5b9e\u73b0", "result": "PAHQ\u52a0\u901f\u7684ACDC\u76f8\u6bd4\u672a\u52a0\u901f\u7248\u672c\u51cf\u5c1180%\u8fd0\u884c\u65f6\u95f4\u548c30%\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u5fe0\u5b9e\u5ea6", "conclusion": "PAHQ\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u9014\u5f84\u6765\u52a0\u901f\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u8fb9\u7684\u7535\u8def\u53d1\u73b0\u6280\u672f\u96c6\u6210", "topic": "agent analysis"}}
{"id": "2510.23535", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23535", "abs": "https://arxiv.org/abs/2510.23535", "authors": ["Chen Lu", "Ke Xue", "Lei Yuan", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "title": "Sequential Multi-Agent Dynamic Algorithm Configuration", "comment": "NeurIPS 2025", "summary": "Dynamic algorithm configuration (DAC) is a recent trend in automated machine\nlearning, which can dynamically adjust the algorithm's configuration during the\nexecution process and relieve users from tedious trial-and-error tuning tasks.\nRecently, multi-agent reinforcement learning (MARL) approaches have improved\nthe configuration of multiple heterogeneous hyperparameters, making various\nparameter configurations for complex algorithms possible. However, many complex\nalgorithms have inherent inter-dependencies among multiple parameters (e.g.,\ndetermining the operator type first and then the operator's parameter), which\nare, however, not considered in previous approaches, thus leading to\nsub-optimal results. In this paper, we propose the sequential multi-agent DAC\n(Seq-MADAC) framework to address this issue by considering the inherent\ninter-dependencies of multiple parameters. Specifically, we propose a\nsequential advantage decomposition network, which can leverage action-order\ninformation through sequential advantage decomposition. Experiments from\nsynthetic functions to the configuration of multi-objective optimization\nalgorithms demonstrate Seq-MADAC's superior performance over state-of-the-art\nMARL methods and show strong generalization across problem classes. Seq-MADAC\nestablishes a new paradigm for the widespread dependency-aware automated\nalgorithm configuration. Our code is available at\nhttps://github.com/lamda-bbo/seq-madac.", "AI": {"tldr": "\u63d0\u51fa\u4e86Seq-MADAC\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u591a\u53c2\u6570\u95f4\u7684\u5185\u5728\u4f9d\u8d56\u5173\u7cfb\u6765\u89e3\u51b3\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u95ee\u9898\uff0c\u4f7f\u7528\u987a\u5e8f\u4f18\u52bf\u5206\u89e3\u7f51\u7edc\u5229\u7528\u52a8\u4f5c\u987a\u5e8f\u4fe1\u606f\u3002", "motivation": "\u590d\u6742\u7b97\u6cd5\u4e2d\u591a\u4e2a\u53c2\u6570\u5b58\u5728\u5185\u5728\u4f9d\u8d56\u5173\u7cfb\uff08\u5982\u5148\u786e\u5b9a\u7b97\u5b50\u7c7b\u578b\u518d\u786e\u5b9a\u7b97\u5b50\u53c2\u6570\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u987a\u5e8f\u591a\u667a\u80fd\u4f53DAC\u6846\u67b6\uff0c\u91c7\u7528\u987a\u5e8f\u4f18\u52bf\u5206\u89e3\u7f51\u7edc\u6765\u5229\u7528\u52a8\u4f5c\u987a\u5e8f\u4fe1\u606f\uff0c\u5904\u7406\u53c2\u6570\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u4ece\u5408\u6210\u51fd\u6570\u5230\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u914d\u7f6e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSeq-MADAC\u4f18\u4e8e\u6700\u5148\u8fdb\u7684MARL\u65b9\u6cd5\uff0c\u5e76\u5728\u95ee\u9898\u7c7b\u522b\u95f4\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Seq-MADAC\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u914d\u7f6e\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.4989bec3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fopen-source%2Faccelerate-developer-productivity-with-these-9-open-source-ai-and-mcp-projects%2F%3Futm_source=tldrdevops/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/pw3yAP_Mj46lQ10QcCfIoW09Ee7WRb2a-4ZjtLydZ4c=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fopen-source%2Faccelerate-developer-productivity-with-these-9-open-source-ai-and-mcp-projects%2F%3Futm_source=tldrdevops/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/pw3yAP_Mj46lQ10QcCfIoW09Ee7WRb2a-4ZjtLydZ4c=428", "authors": ["TLDR Newsletter"], "title": "Accelerate developer productivity with these 9 open source AI and MCP projects", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fopen-source%2Faccelerate-developer-productivity-with-these-9-open-source-ai-and-mcp-projects%2F%3Futm_source=tldrdevops/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/pw3yAP_Mj46lQ10QcCfIoW09Ee7WRb2a-4ZjtLydZ4c=428", "summary": "Accelerate developer productivity with these 9 open source AI and MCP projects (3 minute read) Developers are leveraging the Model Context Protocol to create AI-native workflows that connect agents with tools, codebases, and browsers, giving rise to a new generation of intelligent, agentic tooling. Supported by Microsoft's OSPO, GitHub Copilot, and VS Code, nine open source projects showcase framework integrations, AI-enhanced developer experience, and scalable automation to demonstrate how M...", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e869\u4e2a\u57fa\u4e8eModel Context Protocol\u7684\u5f00\u6e90AI\u9879\u76ee\uff0c\u8fd9\u4e9b\u9879\u76ee\u5229\u7528MCP\u534f\u8bae\u521b\u5efaAI\u539f\u751f\u5de5\u4f5c\u6d41\uff0c\u8fde\u63a5\u667a\u80fd\u4ee3\u7406\u4e0e\u5de5\u5177\u3001\u4ee3\u7801\u5e93\u548c\u6d4f\u89c8\u5668\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u3002", "motivation": "\u65e8\u5728\u5c55\u793a\u5982\u4f55\u901a\u8fc7MCP\u534f\u8bae\u6784\u5efa\u65b0\u4e00\u4ee3\u667a\u80fd\u5de5\u5177\u5316\u7cfb\u7edf\uff0c\u52a0\u901f\u5f00\u53d1\u8005\u5de5\u4f5c\u6548\u7387\uff0c\u5f97\u5230\u5fae\u8f6fOSPO\u3001GitHub Copilot\u548cVS Code\u7684\u652f\u6301\u3002", "method": "\u901a\u8fc79\u4e2a\u5f00\u6e90\u9879\u76ee\u5c55\u793a\u6846\u67b6\u96c6\u6210\u3001AI\u589e\u5f3a\u7684\u5f00\u53d1\u8005\u4f53\u9a8c\u548c\u53ef\u6269\u5c55\u81ea\u52a8\u5316\uff0c\u5229\u7528Model Context Protocol\u8fde\u63a5AI\u4ee3\u7406\u4e0e\u5404\u79cd\u5f00\u53d1\u5de5\u5177\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u80fd\u591f\u8fde\u63a5\u4ee3\u7406\u4e0e\u5de5\u5177\u3001\u4ee3\u7801\u5e93\u548c\u6d4f\u89c8\u5668\u7684AI\u539f\u751f\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u5316\u7684\u5f00\u53d1\u5de5\u5177\u94fe\u3002", "conclusion": "MCP\u534f\u8bae\u4e3a\u6784\u5efa\u65b0\u4e00\u4ee3\u667a\u80fd\u5f00\u53d1\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5f00\u6e90\u9879\u76ee\u5c55\u793a\u4e86AI\u5728\u63d0\u5347\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2510.bc0c980f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/MJ-IdSTsXD0SeSgXSy121_Pm5WYZYlFBbPjhRU4Qc-s=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/MJ-IdSTsXD0SeSgXSy121_Pm5WYZYlFBbPjhRU4Qc-s=428", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/MJ-IdSTsXD0SeSgXSy121_Pm5WYZYlFBbPjhRU4Qc-s=428", "summary": "Mistakes I see engineers making in their code reviews (8 minute read) Many engineers approach code review incorrectly in the LLM era, focusing too narrowly on the diff rather than understanding how the change fits into the overall system.", "source": "tldr", "AI": {"tldr": "\u5de5\u7a0b\u5e08\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u5e38\u72af\u7684\u9519\u8bef\uff1a\u5728LLM\u65f6\u4ee3\u8fc7\u4e8e\u5173\u6ce8\u4ee3\u7801\u5dee\u5f02\u800c\u5ffd\u89c6\u6574\u4f53\u7cfb\u7edf\u7406\u89e3", "motivation": "\u6307\u51fa\u5f53\u524d\u5de5\u7a0b\u5e08\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u5e38\u89c1\u8bef\u533a\uff0c\u7279\u522b\u662f\u5728LLM\u65f6\u4ee3\u9700\u8981\u8c03\u6574\u5ba1\u67e5\u65b9\u6cd5", "method": "\u901a\u8fc7\u89c2\u5bdf\u548c\u5206\u6790\u5de5\u7a0b\u5e08\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u5b9e\u9645\u505a\u6cd5\uff0c\u8bc6\u522b\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f", "result": "\u53d1\u73b0\u5de5\u7a0b\u5e08\u5f80\u5f80\u8fc7\u4e8e\u5173\u6ce8\u4ee3\u7801\u5dee\u5f02\u7684\u7ec6\u8282\uff0c\u800c\u5ffd\u7565\u4e86\u53d8\u66f4\u5982\u4f55\u878d\u5165\u6574\u4f53\u7cfb\u7edf\u7684\u7406\u89e3", "conclusion": "\u5728LLM\u65f6\u4ee3\uff0c\u4ee3\u7801\u5ba1\u67e5\u9700\u8981\u4ece\u5355\u7eaf\u7684\u5dee\u5f02\u68c0\u67e5\u8f6c\u5411\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u7406\u89e3\uff0c\u5173\u6ce8\u53d8\u66f4\u5bf9\u6574\u4f53\u67b6\u6784\u7684\u5f71\u54cd", "topic": "swe application"}}
{"id": "tldr.2510.fade9ea8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrwebdev/1/0100019a255bdee1-0c9144f2-ddb5-40cc-8fc7-ed8cc2e52da2-000000/lXIFBye_-ECDBmfj9Qzg-XEOL2bVp3GSflSNDvkly3M=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrwebdev/1/0100019a255bdee1-0c9144f2-ddb5-40cc-8fc7-ed8cc2e52da2-000000/lXIFBye_-ECDBmfj9Qzg-XEOL2bVp3GSflSNDvkly3M=428", "authors": ["TLDR Newsletter"], "title": "Code like a surgeon", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrwebdev/1/0100019a255bdee1-0c9144f2-ddb5-40cc-8fc7-ed8cc2e52da2-000000/lXIFBye_-ECDBmfj9Qzg-XEOL2bVp3GSflSNDvkly3M=428", "summary": "Code like a surgeon (5 minute read) Taking a \"software surgeon\" approach to coding means using AI tools for secondary tasks and having devs focus on core design and critical thinking.", "source": "tldr", "AI": {"tldr": "\u91c7\u7528\"\u8f6f\u4ef6\u5916\u79d1\u533b\u751f\"\u65b9\u6cd5\u8fdb\u884c\u7f16\u7801\uff0c\u4f7f\u7528AI\u5de5\u5177\u5904\u7406\u6b21\u8981\u4efb\u52a1\uff0c\u8ba9\u5f00\u53d1\u8005\u4e13\u6ce8\u4e8e\u6838\u5fc3\u8bbe\u8ba1\u548c\u5173\u952e\u601d\u8003", "motivation": "\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7AI\u5de5\u5177\u5206\u62c5\u91cd\u590d\u6027\u4efb\u52a1\uff0c\u8ba9\u4eba\u7c7b\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u66f4\u91cd\u8981\u7684\u8bbe\u8ba1\u51b3\u7b56\u548c\u521b\u9020\u6027\u5de5\u4f5c", "method": "\u5c06AI\u5de5\u5177\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u5904\u7406\u7f16\u7801\u4e2d\u7684\u6b21\u8981\u4efb\u52a1\uff0c\u4eba\u7c7b\u5f00\u53d1\u8005\u8d1f\u8d23\u6838\u5fc3\u8bbe\u8ba1\u548c\u5173\u952e\u51b3\u7b56", "result": "\u5f00\u53d1\u6548\u7387\u63d0\u5347\uff0c\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u521b\u9020\u6027\u5de5\u4f5c\uff0c\u4ee3\u7801\u8d28\u91cf\u5f97\u5230\u6539\u5584", "conclusion": "\"\u8f6f\u4ef6\u5916\u79d1\u533b\u751f\"\u65b9\u6cd5\u662f\u6709\u6548\u7684\u5f00\u53d1\u7b56\u7565\uff0cAI\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u534f\u4f5c\u80fd\u591f\u6700\u5927\u5316\u5404\u81ea\u7684\u4f18\u52bf", "topic": "code agent"}}
{"id": "tldr.2510.1a611601", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneuraltrust.ai%2Fblog%2Fopenai-atlas-omnibox-prompt-injection%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/NZ2o88NOytiaI-GeBNiONzG6wBFelQqVb0wuYwkN_3s=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneuraltrust.ai%2Fblog%2Fopenai-atlas-omnibox-prompt-injection%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/NZ2o88NOytiaI-GeBNiONzG6wBFelQqVb0wuYwkN_3s=428", "authors": ["TLDR Newsletter"], "title": "OpenAI Atlas Omnibox Prompt Injection: URLs That Become Jailbreaks", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneuraltrust.ai%2Fblog%2Fopenai-atlas-omnibox-prompt-injection%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/NZ2o88NOytiaI-GeBNiONzG6wBFelQqVb0wuYwkN_3s=428", "summary": "OpenAI Atlas Omnibox Prompt Injection: URLs That Become Jailbreaks (5 minute read) Agentic browsers like OpenAI Atlas are vulnerable to exploitation if input boundaries are not strictly enforced. Attackers can create strings that look like URLs but contain natural-language commands, which the omnibox might interpret as user intent. This can allow malicious actors to override user commands, leading to unintended or harmful actions, such as opening phishing sites or executing destructive instru...", "source": "tldr", "AI": {"tldr": "OpenAI Atlas\u6d4f\u89c8\u5668\u5b58\u5728\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u521b\u5efa\u770b\u4f3cURL\u4f46\u5305\u542b\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u7684\u5b57\u7b26\u4e32\uff0c\u8986\u76d6\u7528\u6237\u6307\u4ee4\u6267\u884c\u6076\u610f\u64cd\u4f5c\u3002", "motivation": "\u7814\u7a76Agentic\u6d4f\u89c8\u5668\uff08\u5982OpenAI Atlas\uff09\u5728\u8f93\u5165\u8fb9\u754c\u672a\u4e25\u683c\u5f3a\u5236\u6267\u884c\u65f6\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u653b\u51fb\u5411\u91cf\u3002", "method": "\u901a\u8fc7\u521b\u5efa\u4f2a\u88c5\u6210URL\u4f46\u5305\u542b\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u7684\u5b57\u7b26\u4e32\uff0c\u5229\u7528omnibox\u53ef\u80fd\u5c06\u5176\u89e3\u91ca\u4e3a\u7528\u6237\u610f\u56fe\u7684\u6f0f\u6d1e\u8fdb\u884c\u653b\u51fb\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u653b\u51fb\u8005\u53ef\u4ee5\u8986\u76d6\u7528\u6237\u547d\u4ee4\uff0c\u5bfc\u81f4\u610f\u5916\u6216\u6709\u5bb3\u64cd\u4f5c\uff0c\u5982\u6253\u5f00\u9493\u9c7c\u7f51\u7ad9\u6216\u6267\u884c\u7834\u574f\u6027\u6307\u4ee4\u3002", "conclusion": "Agentic\u6d4f\u89c8\u5668\u9700\u8981\u4e25\u683c\u5b9e\u65bd\u8f93\u5165\u8fb9\u754c\u68c0\u67e5\uff0c\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u786e\u4fdd\u7528\u6237\u610f\u56fe\u4e0d\u88ab\u6076\u610f\u8986\u76d6\u3002", "topic": "agent analysis"}}
{"id": "wechat.2510.0942ff0b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODgxMzE5OQ==&mid=2247485904&idx=1&sn=a338584ec82c115c11d856823419a15c&chksm=971c809bc8a20043069419533a0ff920ffe8ab6ec53e08505d4fdc6a10ab2965a7397f7f548e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODgxMzE5OQ==&mid=2247485904&idx=1&sn=a338584ec82c115c11d856823419a15c&chksm=971c809bc8a20043069419533a0ff920ffe8ab6ec53e08505d4fdc6a10ab2965a7397f7f548e#rd", "authors": ["\u667a\u9a7e\u548c\u673a\u5668\u4eba\u524d\u77bb\u5c40"], "title": "\u6700\u65b0\u4e00\u7bc7\u957f\u8fbe76\u9875\u7684<em class=\"highlight\">Agentic</em> AI\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-28 10:00:00", "summary": "Agentic AI\u6b63\u4ece\u5916\u90e8\u6d41\u6c34\u7ebf\u8f6c\u5411\u6a21\u578b\u539f\u751f\uff0c\u63a8\u7406\u3001\u8bb0\u5fc6\u4e0e\u884c\u52a8\u7b49\u80fd\u529b\u88ab\u5185\u5316\u5230\u6a21\u578b\u7b56\u7565\u4e2d\uff0c\u501f\u52a9\u5f3a\u5316\u5b66\u4e60\u628a\u611f\u77e5\u4e0e\u884c\u52a8\u6253\u901a\uff0c\u8ba9\u9759\u6001\u6a21\u578b\u53d8\u6210\u53ef\u4ece\u73af\u5883\u4e92\u52a8\u4e2d\u5b66\u4e60\u7684\u76ee\u6807\u9a71\u52a8\u4f53\u3002", "AI": {"tldr": "Agentic AI\u6b63\u4ece\u5916\u90e8\u6d41\u6c34\u7ebf\u8f6c\u5411\u6a21\u578b\u539f\u751f\uff0c\u63a8\u7406\u3001\u8bb0\u5fc6\u4e0e\u884c\u52a8\u7b49\u80fd\u529b\u88ab\u5185\u5316\u5230\u6a21\u578b\u7b56\u7565\u4e2d\uff0c\u501f\u52a9\u5f3a\u5316\u5b66\u4e60\u628a\u611f\u77e5\u4e0e\u884c\u52a8\u6253\u901a\uff0c\u8ba9\u9759\u6001\u6a21\u578b\u53d8\u6210\u53ef\u4ece\u73af\u5883\u4e92\u52a8\u4e2d\u5b66\u4e60\u7684\u76ee\u6807\u9a71\u52a8\u4f53\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.61a4b161", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzMjA4MjY3MA==&mid=2247483655&idx=1&sn=21109ca6169e8e756eaa3a8c3e68ed4b&chksm=f1944fcf1b409351fc590755ad69c94d123f1967688d0857271e37a763e7ffae8f940cddd89e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzMjA4MjY3MA==&mid=2247483655&idx=1&sn=21109ca6169e8e756eaa3a8c3e68ed4b&chksm=f1944fcf1b409351fc590755ad69c94d123f1967688d0857271e37a763e7ffae8f940cddd89e#rd", "authors": ["AI\u7ec4\u88c5\u5de5"], "title": "<em class=\"highlight\">Agentic</em>\uff0c\u662fAI\u7684\u4e0b\u4e00\u6b21\u8fdb\u5316", "comment": "Source: WeChat, Published: 2025-10-28 07:19:14", "summary": "Agentic\uff0c\u5b57\u9762\u610f\u601d\u5c31\u662f\u201c\u5177\u5907\u4ee3\u7406\uff08agent\uff09\u80fd\u529b\u7684\u201d\uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u81ea\u4e3b\u884c\u52a8\u7684\u80fd\u529b\u201d\u3002\u5728 AI \u7cfb\u7edf\u91cc\uff0cAgentic AI\u5e76\u4e0d\u662f\u7b80\u5355\u5730\u751f\u6210\u6587\u672c\u6216\u56de\u7b54\u95ee\u9898\uff0c\u800c\u662f\u80fd\uff1a", "AI": {"tldr": "Agentic\uff0c\u5b57\u9762\u610f\u601d\u5c31\u662f\u201c\u5177\u5907\u4ee3\u7406\uff08agent\uff09\u80fd\u529b\u7684\u201d\uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u81ea\u4e3b\u884c\u52a8\u7684\u80fd\u529b\u201d\u3002\u5728 AI \u7cfb\u7edf\u91cc\uff0cAgentic AI\u5e76\u4e0d\u662f\u7b80\u5355\u5730\u751f\u6210\u6587\u672c\u6216\u56de\u7b54\u95ee\u9898\uff0c\u800c\u662f\u80fd\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.5790d58f", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNjczMzU1OQ==&mid=2247483909&idx=1&sn=100c0f10b063aa8e6730721e3deabdf9&chksm=c02e77d920e00c3a9acc86d1d93d5109b571596d67e3c68ac82a8bd3ef6b94829dceb8545eed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNjczMzU1OQ==&mid=2247483909&idx=1&sn=100c0f10b063aa8e6730721e3deabdf9&chksm=c02e77d920e00c3a9acc86d1d93d5109b571596d67e3c68ac82a8bd3ef6b94829dceb8545eed#rd", "authors": ["\u67cf\u821f\u6742\u8c08"], "title": "<em class=\"highlight\">Agentic</em> Notes 01\uff5c<em class=\"highlight\">Agentic</em> AI \u7684\u57fa\u7840\u6846\u67b6\uff1a\u4ece\u8bed\u8a00\u7406\u89e3\u5230\u667a\u80fd\u884c\u52a8", "comment": "Source: WeChat, Published: 2025-10-28 06:35:28", "summary": "\u4e0d\u540c\u7cfb\u7edf\u7684\u201cAgentic\u201d\u7a0b\u5ea6\u53ef\u4ee5\u4ece\u4f4e\u5230\u9ad8\u5206\u4e3a\u591a\u4e2a\u5c42\u6b21\u3002\u4e3e\u4f8b\u6765\u8bf4\uff0c\u4ee5\u201c\u64b0\u5199\u4e00\u7bc7\u5173\u4e8e\u9ed1\u6d1e\u7684\u8bba\u6587\u201d\u4e3a\u4f8b\uff1adegrees of autonomy web llm llm fetch pdf to web llm llm llm llm fetch reflect and draftandrew ng", "AI": {"tldr": "\u4e0d\u540c\u7cfb\u7edf\u7684\u201cAgentic\u201d\u7a0b\u5ea6\u53ef\u4ee5\u4ece\u4f4e\u5230\u9ad8\u5206\u4e3a\u591a\u4e2a\u5c42\u6b21\u3002\u4e3e\u4f8b\u6765\u8bf4\uff0c\u4ee5\u201c\u64b0\u5199\u4e00\u7bc7\u5173\u4e8e\u9ed1\u6d1e\u7684\u8bba\u6587\u201d\u4e3a\u4f8b\uff1adegrees of autonomy web llm llm fetch pdf to web llm llm llm llm fetch reflect and draftandrew ng", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.60b1bcc1", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMTIyODMwMA==&mid=2650193426&idx=1&sn=52562f7f92d2d9562abdc5bde2292705&chksm=f10ceabbcd10f1f80a65da549cfb3afe6205b4eca88d3f1f7c5221d53e4ffc8eb395f7eb3d9e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMTIyODMwMA==&mid=2650193426&idx=1&sn=52562f7f92d2d9562abdc5bde2292705&chksm=f10ceabbcd10f1f80a65da549cfb3afe6205b4eca88d3f1f7c5221d53e4ffc8eb395f7eb3d9e#rd", "authors": ["AGI\u5546\u4e1a\u65b0\u58f0"], "title": "\u4e00\u56fe\u770b\u61c2\uff1a<em class=\"highlight\">Agentic</em>\u67b6\u6784\u4e0e<em class=\"highlight\">Agentic</em> RAG Workflows\u7684\u533a\u522b\uff0c\u9644\u4e2d\u7ffb\u7248", "comment": "Source: WeChat, Published: 2025-10-28 06:05:29", "summary": "agentic architectures vs work\u3002flows single agent architecture multi agent architecture\u3002ai agent agentic rag workflow\u3002\u4ee3\u7406\u67b6\u6784\u4e0e\u5de5\u4f5c\u6d41\u5bf9\u6bd4 \u5de5\u5177 \u8bb0\u5fc6 \u641c\u7d22\u5de5\u5177 \u7528\u6237\u67e5\u8be2 \u751f\u6210\u54cd\u5e94", "AI": {"tldr": "agentic architectures vs work\u3002flows single agent architecture multi agent architecture\u3002ai agent agentic rag workflow\u3002\u4ee3\u7406\u67b6\u6784\u4e0e\u5de5\u4f5c\u6d41\u5bf9\u6bd4 \u5de5\u5177 \u8bb0\u5fc6 \u641c\u7d22\u5de5\u5177 \u7528\u6237\u67e5\u8be2 \u751f\u6210\u54cd\u5e94", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.e4ce3daf", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMTY2ODQwMA==&mid=2247488887&idx=1&sn=d3c34953d2cc7c94805b61552f70744d&chksm=c31e2b7f87eca05526a3216ef8cf5e086328be05b06272110cf5916a7ce7bb26b8fed088e720#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMTY2ODQwMA==&mid=2247488887&idx=1&sn=d3c34953d2cc7c94805b61552f70744d&chksm=c31e2b7f87eca05526a3216ef8cf5e086328be05b06272110cf5916a7ce7bb26b8fed088e720#rd", "authors": ["\u4f01\u5eb7AI+"], "title": "\u98a0\u8986\u4f20\u7edf\u4ea7\u7ebf\uff1a\u81ea\u4e3b\u51b3\u7b56<em class=\"highlight\">Agentic</em> AI\u5f15\u9886\u5de5\u4e1a\u667a\u80fd\u65b0\u7eaa\u5143", "comment": "Source: WeChat, Published: 2025-10-28 05:59:20", "summary": "Agentic AI\uff1a\u667a\u6167\u7684\u201c\u51b3\u7b56\u8005\u201dAgentic AI\u5219\u5b8c\u5168\u4e0d\u540c\u3002\u5b83\u5e76\u975e\u7b80\u5355\u6267\u884c\u6307\u4ee4\uff0c\u800c\u662f\u88ab\u8d4b\u4e88\u4e00\u4e2a\u6216\u591a\u4e2a\u9ad8\u7ea7\u76ee\u6807\uff08\u5982\u201c\u6700\u5927\u5316\u826f\u54c1\u7387\u201d\u3001\u201c\u6700\u77ed\u5316\u4ea4\u4ed8\u5468\u671f\u201d\uff09\uff0c\u5e76\u62e5\u6709\u81ea\u4e3b\u611f\u77e5\u73af\u5883\u3001\u5206\u6790\u6570\u636e\u3001\u5236\u5b9a\u7b56\u7565\u5e76\u6267\u884c\u884c\u52a8\u7684\u80fd\u529b\u3002", "AI": {"tldr": "Agentic AI\uff1a\u667a\u6167\u7684\u201c\u51b3\u7b56\u8005\u201dAgentic AI\u5219\u5b8c\u5168\u4e0d\u540c\u3002\u5b83\u5e76\u975e\u7b80\u5355\u6267\u884c\u6307\u4ee4\uff0c\u800c\u662f\u88ab\u8d4b\u4e88\u4e00\u4e2a\u6216\u591a\u4e2a\u9ad8\u7ea7\u76ee\u6807\uff08\u5982\u201c\u6700\u5927\u5316\u826f\u54c1\u7387\u201d\u3001\u201c\u6700\u77ed\u5316\u4ea4\u4ed8\u5468\u671f\u201d\uff09\uff0c\u5e76\u62e5\u6709\u81ea\u4e3b\u611f\u77e5\u73af\u5883\u3001\u5206\u6790\u6570\u636e\u3001\u5236\u5b9a\u7b56\u7565\u5e76\u6267\u884c\u884c\u52a8\u7684\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.28a54e4d", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485064&idx=1&sn=1c65225911fa0875d1e68ab8600a1586&chksm=fbcf1f8a238c0615124a96bc2a5050be149779e7ef97d908e28bdb3c01fbb06b7e940521e59e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485064&idx=1&sn=1c65225911fa0875d1e68ab8600a1586&chksm=fbcf1f8a238c0615124a96bc2a5050be149779e7ef97d908e28bdb3c01fbb06b7e940521e59e#rd", "authors": ["\u4e13\u6ce8\u5b89\u7ba1\u5e73\u53f0"], "title": "\u56fd\u5916<em class=\"highlight\">Agentic</em> SOC\u6700\u65b0\u8fdb\u5c55\uff082025Q3\uff09", "comment": "Source: WeChat, Published: 2025-10-28 04:00:00", "summary": "\u7c7b\u522b \u5382\u5546\u540d\u79f0 \u5173\u952e Agentic AI \u4ea7\u54c1/\u80fd\u529b \u53d1\u5e03\u65f6\u95f4\uff08\u6700\u65b0\u6216\u91cd\u5927\u53d1\u5e03\uff09 \u63cf\u8ff0 \u53d1\u5e03\u94fe\u63a5\u6027\u5b89\u5168\u5382 CrowdStrike Charlotte Al /Falcon agentic security platform 2025\u5e749\u670816\u65e5 \u5229\u7528\u667a\u80fd\u4f53\u81ea\u52a8\u5bf9\u5a01\u80c1\u544a\u8b66", "AI": {"tldr": "\u7c7b\u522b \u5382\u5546\u540d\u79f0 \u5173\u952e Agentic AI \u4ea7\u54c1/\u80fd\u529b \u53d1\u5e03\u65f6\u95f4\uff08\u6700\u65b0\u6216\u91cd\u5927\u53d1\u5e03\uff09 \u63cf\u8ff0 \u53d1\u5e03\u94fe\u63a5\u6027\u5b89\u5168\u5382 CrowdStrike Charlotte Al /Falcon agentic security platform 2025\u5e749\u670816\u65e5 \u5229\u7528\u667a\u80fd\u4f53\u81ea\u52a8\u5bf9\u5a01\u80c1\u544a\u8b66", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.808c5f26", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDgwNzA0NA==&mid=2247484873&idx=1&sn=79acdc28ce2c8e7b159885045f33931e&chksm=c4fb0a797da94450dd256194600bc2af345143ee40a5bfa27b263c19c350a1ab1e85e3d2d559#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDgwNzA0NA==&mid=2247484873&idx=1&sn=79acdc28ce2c8e7b159885045f33931e&chksm=c4fb0a797da94450dd256194600bc2af345143ee40a5bfa27b263c19c350a1ab1e85e3d2d559#rd", "authors": ["Vibehood"], "title": "\u3010\u6d1e\u89c1\u3011<em class=\"highlight\">Agentic</em> Commerce \u98ce\u53e3\uff1aAI \u4e3b\u5bfc\u7535\u5546\uff0cOpenAI \u638c\u63e1\u7ed3\u8d26\u6807\u51c6\uff0cMeta/Google \u8fd8\u80fd\u6297\u8861\u5417\uff1f", "comment": "Source: WeChat, Published: 2025-10-28 03:58:24", "summary": "Agentic Commerce \u98ce\u53e3\uff1aOpenAI \u5728 ChatGPT \u63a8\u51fa Instant Checkout \u5e76\u53d1\u5e03 Agentic Commerce Protocol\uff1bPerplexity \u63a8 \u201cBuy with Pro\u201d\uff1bMeta 2025 \u5e74\u8d77\u53d6\u6d88 in-app checkout\u3002\u3010\u5546\u4e1a\u4e0d\u53ea\u662f\u5356\u8d27\u3011", "AI": {"tldr": "Agentic Commerce \u98ce\u53e3\uff1aOpenAI \u5728 ChatGPT \u63a8\u51fa Instant Checkout \u5e76\u53d1\u5e03 Agentic Commerce Protocol\uff1bPerplexity \u63a8 \u201cBuy with Pro\u201d\uff1bMeta 2025 \u5e74\u8d77\u53d6\u6d88 in-app checkout\u3002\u3010\u5546\u4e1a\u4e0d\u53ea\u662f\u5356\u8d27\u3011", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.3b12f11a", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzMyNTg3NQ==&mid=2247484502&idx=1&sn=5ac00e9b829d4ca415e30591efc4f7f6&chksm=c34a9808c1e931b853f6b735e32af63d558cfa39926db35ddcbf6d61879e705a3612ea8b9941#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzMyNTg3NQ==&mid=2247484502&idx=1&sn=5ac00e9b829d4ca415e30591efc4f7f6&chksm=c34a9808c1e931b853f6b735e32af63d558cfa39926db35ddcbf6d61879e705a3612ea8b9941#rd", "authors": ["\u4e45\u5b89\u701b"], "title": "<em class=\"highlight\">Agentic</em> AI\u5b89\u5168\u90e8\u7f72\u4f5c\u6218\u624b\u518c\uff1a\u5728<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u89c9\u9192\u65f6\u4ee3\uff0c\u6280\u672f\u9886\u5bfc\u8005\u5fc5\u987b\u5b88\u4f4f\u7684\u300c\u5b89\u5168\u7ea2\u7ebf\u300d", "comment": "Source: WeChat, Published: 2025-10-28 02:04:14", "summary": "Agentic AI\uff08\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u4eba\u5de5\u667a\u80fd\uff09\u4e0e\u4f20\u7edfAI\u7684\u5173\u952e\u533a\u522b\u5728\u4e8e\u5176\u81ea\u4e3b\u6027\u3002\u5b83\u4e0d\u518d\u662f\u88ab\u52a8\u54cd\u5e94\u6307\u4ee4\u7684\u5de5\u5177\uff0c\u800c\u662f\u80fd\u591f\u4e3b\u52a8\u89c4\u5212\u3001\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\u7684\u201c\u6570\u5b57\u5458\u5de5\u201d\u3002", "AI": {"tldr": "Agentic AI\uff08\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u4eba\u5de5\u667a\u80fd\uff09\u4e0e\u4f20\u7edfAI\u7684\u5173\u952e\u533a\u522b\u5728\u4e8e\u5176\u81ea\u4e3b\u6027\u3002\u5b83\u4e0d\u518d\u662f\u88ab\u52a8\u54cd\u5e94\u6307\u4ee4\u7684\u5de5\u5177\uff0c\u800c\u662f\u80fd\u591f\u4e3b\u52a8\u89c4\u5212\u3001\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\u7684\u201c\u6570\u5b57\u5458\u5de5\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.fc51b2b3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDUyMzYwMQ==&mid=2247490057&idx=4&sn=b3b9b4e766d679fed5fd9f0230fddb89&chksm=c4619e691ead741dd8b545c752a826c43388d43091d0ae31e28a9b74696a710e19353ae233c1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDUyMzYwMQ==&mid=2247490057&idx=4&sn=b3b9b4e766d679fed5fd9f0230fddb89&chksm=c4619e691ead741dd8b545c752a826c43388d43091d0ae31e28a9b74696a710e19353ae233c1#rd", "authors": ["\u884c\u4e1a\u5148\u950bEknower"], "title": "<em class=\"highlight\">Agentic</em> AI \u5404\u8d5b\u9053\u5206\u6790\u62a5\u544a_37\u9875", "comment": "Source: WeChat, Published: 2025-10-28 01:30:30", "summary": "Agentic AI \u5404\u8d5b\u9053\u5206\u6790\u62a5\u544aagentic ai \u5404\u8d5b\u9053\u5206\u6790\u62a5\u544a\u76ee\u5f55\u3002\u524d\u8a00|\u603b\u4f53\u5206\u6790|ai chatbots|\u7f16\u7a0b\u52a9\u624b|\u56fe\u751f\u89c6\u9891|agent\u7f16\u6392|\u901a\u7528\u81ea\u52a8\u5316agent|\u56fe\u50cf\u751f\u6210\u300201\u3002\u76ee\u5f55 12 \u6559\u80b2 \u6b21\u70ed\u95e8\u8d5b\u9053\u3002", "AI": {"tldr": "Agentic AI \u5404\u8d5b\u9053\u5206\u6790\u62a5\u544aagentic ai \u5404\u8d5b\u9053\u5206\u6790\u62a5\u544a\u76ee\u5f55\u3002\u524d\u8a00|\u603b\u4f53\u5206\u6790|ai chatbots|\u7f16\u7a0b\u52a9\u624b|\u56fe\u751f\u89c6\u9891|agent\u7f16\u6392|\u901a\u7528\u81ea\u52a8\u5316agent|\u56fe\u50cf\u751f\u6210\u300201\u3002\u76ee\u5f55 12 \u6559\u80b2 \u6b21\u70ed\u95e8\u8d5b\u9053\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.d4fb826d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486425&idx=1&sn=e6f3c7638480a19313074fa519c4b410&chksm=c32bb432bf93cdb61924ff398c01408f26a0ab299d9d68af73f5ca322bc415db0b79260f30a8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486425&idx=1&sn=e6f3c7638480a19313074fa519c4b410&chksm=c32bb432bf93cdb61924ff398c01408f26a0ab299d9d68af73f5ca322bc415db0b79260f30a8#rd", "authors": ["AI\u5927\u6a21\u578b\u89c2\u5bdf\u7ad9"], "title": "\u6784\u5efa\u5177\u5907\u6df1\u5ea6\u601d\u8003\u80fd\u529b\u7684 <em class=\"highlight\">Agentic</em> RAG \u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u67e5\u8be2", "comment": "Source: WeChat, Published: 2025-10-28 00:18:31", "summary": "\u5728\u4e00\u4e2a agentic \u7cfb\u7edf\u91cc\uff0c\u5de5\u4f5c\u6d41\u590d\u6742\u4e14\u5faa\u73af\uff0ctracing \u5e76\u975e\u53ef\u6709\u53ef\u65e0\uff0c\u800c\u662f\u5f88\u91cd\u8981\u3002\u5b83\u5e2e\u52a9\u4f60\u53ef\u89c6\u5316\u5185\u90e8\u8fc7\u7a0b\uff0c\u66f4\u5bb9\u6613\u8c03\u8bd5 agent \u7684\u601d\u8003\u8def\u5f84\u3002\u77e5\u8bc6\u5e93\u6765\u6e90\u4e00\u4e2a\u751f\u4ea7\u7ea7 RAG \u7cfb\u7edf\u9700\u8981\u65e2\u590d\u6742\u53c8\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u5e93\uff0c\u624d\u80fd\u771f\u6b63\u4f53\u73b0\u5176\u6709\u6548\u6027\u3002", "AI": {"tldr": "\u5728\u4e00\u4e2a agentic \u7cfb\u7edf\u91cc\uff0c\u5de5\u4f5c\u6d41\u590d\u6742\u4e14\u5faa\u73af\uff0ctracing \u5e76\u975e\u53ef\u6709\u53ef\u65e0\uff0c\u800c\u662f\u5f88\u91cd\u8981\u3002\u5b83\u5e2e\u52a9\u4f60\u53ef\u89c6\u5316\u5185\u90e8\u8fc7\u7a0b\uff0c\u66f4\u5bb9\u6613\u8c03\u8bd5 agent \u7684\u601d\u8003\u8def\u5f84\u3002\u77e5\u8bc6\u5e93\u6765\u6e90\u4e00\u4e2a\u751f\u4ea7\u7ea7 RAG \u7cfb\u7edf\u9700\u8981\u65e2\u590d\u6742\u53c8\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u5e93\uff0c\u624d\u80fd\u771f\u6b63\u4f53\u73b0\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.ff09265d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683978&idx=3&sn=de78c86ef6666a62209c860097aadfe5&chksm=cfa651e82fc426d1770c43f4e23dd1a2515eab2a9bbb84434cb222d6b6607d288f9311ad55fd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683978&idx=3&sn=de78c86ef6666a62209c860097aadfe5&chksm=cfa651e82fc426d1770c43f4e23dd1a2515eab2a9bbb84434cb222d6b6607d288f9311ad55fd#rd", "authors": ["\u81ea\u52a8\u9a7e\u9a76\u4e4b\u5fc3"], "title": "\u6700\u65b0\u4e00\u7bc7\u957f\u8fbe76\u9875\u7684<em class=\"highlight\">Agentic</em> AI\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-28 00:00:52", "summary": "\u8bba\u6587\u6807\u9898\uff1aBeyond Pipelines\uff1a A Survey of the Paradigm Shift toward Model-Native Agentic AI\u8bba\u6587\u94fe\u63a5\uff1ahttps\uff1a//arxiv.org/abs/2510.16720v1\u95ee\u9898\u80cc\u666f\u751f\u6210\u5f0fAI\u8fdb\u6b65\u8fc5\u731b\uff0c\u4f46\u591a\u4e3a\u201c\u53cd\u5e94\u5f0f\u8f93\u51fa\u201d\uff0c\u7f3a\u4e4f\u9762\u5411\u76ee\u6807\u7684\u957f\u671f\u63a8\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\uff1b", "AI": {"tldr": "\u8bba\u6587\u6807\u9898\uff1aBeyond Pipelines\uff1a A Survey of the Paradigm Shift toward Model-Native Agentic AI\u8bba\u6587\u94fe\u63a5\uff1ahttps\uff1a//arxiv.org/abs/2510.16720v1\u95ee\u9898\u80cc\u666f\u751f\u6210\u5f0fAI\u8fdb\u6b65\u8fc5\u731b\uff0c\u4f46\u591a\u4e3a\u201c\u53cd\u5e94\u5f0f\u8f93\u51fa\u201d\uff0c\u7f3a\u4e4f\u9762\u5411\u76ee\u6807\u7684\u957f\u671f\u63a8\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.dd0c9661", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247483939&idx=1&sn=7a8d8d7f3e9602b7d633a4ee3196cb35&chksm=f13c33e50455b0a803e510e4681ed3855d6c5be28ccb212a4eed3c524e95c48fbe8ef3f1ded8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247483939&idx=1&sn=7a8d8d7f3e9602b7d633a4ee3196cb35&chksm=f13c33e50455b0a803e510e4681ed3855d6c5be28ccb212a4eed3c524e95c48fbe8ef3f1ded8#rd", "authors": ["AI\u6b21\u751f\u4ee3"], "title": "\u8d85\u8be6\u7ec6\uff01<em class=\"highlight\">Agentic</em> AI <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u662f\u5982\u4f55\u601d\u8003\u548c\u884c\u52a8\u7684\uff1f\u4e00\u5f20\u56fe\u8bfb\u61c2AI\u5de5\u4f5c\u6d41\uff01", "comment": "Source: WeChat, Published: 2025-10-27 23:01:26", "summary": "\u8d85\u8be6\u7ec6\uff01Agentic AI \u667a\u80fd\u4f53\u662f\u5982\u4f55\u601d\u8003\u548c\u884c\u52a8\u7684\uff1f\u4e00\u5f20\u56fe\u8bfb\u61c2AI\u5de5\u4f5c\u6d41\uff01\u8fd9\u5f20\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86AI\u667a\u80fd\u4f53\u4ece\u63a5\u6536\u4fe1\u606f\u5230\u8f93\u51fa\u7ed3\u679c\u7684\u5168\u8fc7\u7a0b\uff0c\u8d85\u7ea7\u786c\u6838\uff01\u5feb\u6765\u4e00\u8d77\u770b\u770bAI Agent\u7684\u201c\u5927\u8111\u201d\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u5427\uff01", "AI": {"tldr": "\u8d85\u8be6\u7ec6\uff01Agentic AI \u667a\u80fd\u4f53\u662f\u5982\u4f55\u601d\u8003\u548c\u884c\u52a8\u7684\uff1f\u4e00\u5f20\u56fe\u8bfb\u61c2AI\u5de5\u4f5c\u6d41\uff01\u8fd9\u5f20\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86AI\u667a\u80fd\u4f53\u4ece\u63a5\u6536\u4fe1\u606f\u5230\u8f93\u51fa\u7ed3\u679c\u7684\u5168\u8fc7\u7a0b\uff0c\u8d85\u7ea7\u786c\u6838\uff01\u5feb\u6765\u4e00\u8d77\u770b\u770bAI Agent\u7684\u201c\u5927\u8111\u201d\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u5427\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.df5dfb77", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247509231&idx=2&sn=315ae5294ef22ae5cea0f5a299a75568&chksm=eabc20d9bfa24ad7fe6993d25b0dfb4bbe37c6a9bf6fa5d3383610206739a027c9fb22614da7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247509231&idx=2&sn=315ae5294ef22ae5cea0f5a299a75568&chksm=eabc20d9bfa24ad7fe6993d25b0dfb4bbe37c6a9bf6fa5d3383610206739a027c9fb22614da7#rd", "authors": ["\u4ea7\u4e1a\u667a\u80fd\u5b98"], "title": "\u3010\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3011<em class=\"highlight\">Agentic</em>\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1a\u7834\u89e3\u53c2\u6570\u58c1\u5792\uff0c\u5f00\u542f\u8bed\u8a00\u6a21\u578b\u81ea\u4e3b\u8fdb\u5316\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-10-27 23:00:57", "summary": "sambanova\u548c\u52a0\u5dde\u5927\u5b66\u4f2f\u514b\u5229\u5206\u6821\u7684\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a**agentic\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08agentic context engineering\uff0c ace\uff09**\u7684\u65b0\u65b9\u6cd5\uff0c\u58f0\u79f0\u53ef\u4ee5\u5728\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u8fdb\u5316\u3002", "AI": {"tldr": "sambanova\u548c\u52a0\u5dde\u5927\u5b66\u4f2f\u514b\u5229\u5206\u6821\u7684\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a**agentic\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08agentic context engineering\uff0c ace\uff09**\u7684\u65b0\u65b9\u6cd5\uff0c\u58f0\u79f0\u53ef\u4ee5\u5728\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u8fdb\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.1e84580f", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484344&idx=1&sn=bd026bc33a7f38222ced4c57a06efb7e&chksm=c5183555f9a9db48b755e0b5c5752505a4766651a7259ca893660d307ff8fa2039a367bbd008#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484344&idx=1&sn=bd026bc33a7f38222ced4c57a06efb7e&chksm=c5183555f9a9db48b755e0b5c5752505a4766651a7259ca893660d307ff8fa2039a367bbd008#rd", "authors": ["AI\u5e94\u7528\u5b66\u5802"], "title": "\u63a2\u79d8 <em class=\"highlight\">Agentic</em> RAG \u7cfb\u7edf\uff1a\u4e00\u573a\u77e5\u8bc6\u68c0\u7d22\u4e0e\u667a\u80fd\u4ea4\u4e92\u7684\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-27 23:00:52", "summary": "Agentic RAG \uff1a\u5728\u4ee5\u4e0a\u57fa\u7840\u4e0a\u589e\u52a0\u4e86 \u201c\u81ea\u4e3b\u4ee3\u7406\u201d \u5c42 \uff0c\u5177\u5907\u76ee\u6807\u62c6\u89e3\u3001\u52a8\u6001\u51b3\u7b56\u3001\u5de5\u5177\u8c03\u7528\u3001\u81ea\u6211\u8bc4\u4f30\u7b49 \u201c\u7c7b\u4eba\u601d\u7ef4\u201d \u80fd\u529b\uff0c\u80fd\u5904\u7406\u9700\u8981\u591a\u6b65\u9aa4\u534f\u4f5c\u7684\u590d\u6742\u4efb\u52a1\uff08\u800c\u975e\u7b80\u5355\u7684 \u201c\u8f93\u5165 - \u68c0\u7d22 - \u8f93\u51fa\u201d \u95ed\u73af\uff09\u3002", "AI": {"tldr": "Agentic RAG \uff1a\u5728\u4ee5\u4e0a\u57fa\u7840\u4e0a\u589e\u52a0\u4e86 \u201c\u81ea\u4e3b\u4ee3\u7406\u201d \u5c42 \uff0c\u5177\u5907\u76ee\u6807\u62c6\u89e3\u3001\u52a8\u6001\u51b3\u7b56\u3001\u5de5\u5177\u8c03\u7528\u3001\u81ea\u6211\u8bc4\u4f30\u7b49 \u201c\u7c7b\u4eba\u601d\u7ef4\u201d \u80fd\u529b\uff0c\u80fd\u5904\u7406\u9700\u8981\u591a\u6b65\u9aa4\u534f\u4f5c\u7684\u590d\u6742\u4efb\u52a1\uff08\u800c\u975e\u7b80\u5355\u7684 \u201c\u8f93\u5165 - \u68c0\u7d22 - \u8f93\u51fa\u201d \u95ed\u73af\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.a8969837", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068125&idx=1&sn=4659b01adecac4613b8a2850c15328c8&chksm=89904fd1c1375b52950970f7ecd5397513407b25db6f9d8b2328f92d35ea8489a23dba18fd18#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068125&idx=1&sn=4659b01adecac4613b8a2850c15328c8&chksm=89904fd1c1375b52950970f7ecd5397513407b25db6f9d8b2328f92d35ea8489a23dba18fd18#rd", "authors": ["\u98ce\u6d77\u7b51\u68a6"], "title": "\u559c\u6b22\u8fd9\u5f20<em class=\"highlight\">agentic</em> AI\u539f\u7406\u56fe", "comment": "Source: WeChat, Published: 2025-10-27 14:30:10", "summary": "how agentic ai works agent\u559c\u6b22\u8fd9\u5f20agentic AI\u539f\u7406\u56fe#GenerativeAI #ArtificialIntelligence #MI #MachineLearning", "AI": {"tldr": "how agentic ai works agent\u559c\u6b22\u8fd9\u5f20agentic AI\u539f\u7406\u56fe#GenerativeAI #ArtificialIntelligence #MI #MachineLearning", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.99dd6ecc", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxOTcyNjgxMA==&mid=2650185452&idx=3&sn=7c2aa78758a9d54b1523c6c2d9f28b0d&chksm=82cfecba2f91af1a148546b4e22618f557f108615f3f78d3874c0a3cbdf5ef58515d3f7e9803#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxOTcyNjgxMA==&mid=2650185452&idx=3&sn=7c2aa78758a9d54b1523c6c2d9f28b0d&chksm=82cfecba2f91af1a148546b4e22618f557f108615f3f78d3874c0a3cbdf5ef58515d3f7e9803#rd", "authors": ["\u4e00\u7c92\u4e91"], "title": "\u4e24\u7bc7\u8bba\u6587 LongRAG \u548c CRAG \uff01\u4e24\u62db\u6559\u4f60\u4f18\u5316\u4e0a\u4e0b\u6587\uff0c\u8ba9<em class=\"highlight\">\u5927\u6a21\u578b</em>\u56de\u7b54\u66f4\u7cbe\u51c6", "comment": "Source: WeChat, Published: 2025-10-28 13:30:41", "summary": "\u7136\u540e\u8fd4\u56de\u8fd9\u4e9b\u5b50\u6587\u6863\u6240\u5c5e\u7684\u7236\u6587\u6863\uff08\u5373\u5927\u5757\u4e0a\u4e0b\u6587\uff09\u7ed9\u5927\u6a21\u578b\u3002\u4f2a\u4ee3\u7801\u793a\u4f8b\uff1a# 1. \u52a0\u8f7d\u6587\u6863\u5e76\u521b\u5efa\u7236\u5b50\u6587\u6863parent_splitter = RecursiveCharacterTextSplitter\uff08chunk_size=2000\uff09", "AI": {"tldr": "\u7136\u540e\u8fd4\u56de\u8fd9\u4e9b\u5b50\u6587\u6863\u6240\u5c5e\u7684\u7236\u6587\u6863\uff08\u5373\u5927\u5757\u4e0a\u4e0b\u6587\uff09\u7ed9\u5927\u6a21\u578b\u3002\u4f2a\u4ee3\u7801\u793a\u4f8b\uff1a# 1. \u52a0\u8f7d\u6587\u6863\u5e76\u521b\u5efa\u7236\u5b50\u6587\u6863parent_splitter = RecursiveCharacterTextSplitter\uff08chunk_size=2000\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.2f05cf51", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMzA0NjM3OQ==&mid=2465333379&idx=1&sn=3d262dfb3e58bd90d3bc592a7a0105c1&chksm=80b76a0826daed8926309cf7aa76f99b92f17ff80c3ced8a7fcb9729c43eb6c97463e6ae3ed3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMzA0NjM3OQ==&mid=2465333379&idx=1&sn=3d262dfb3e58bd90d3bc592a7a0105c1&chksm=80b76a0826daed8926309cf7aa76f99b92f17ff80c3ced8a7fcb9729c43eb6c97463e6ae3ed3#rd", "authors": ["\u6613\u7a0bLEO"], "title": "DeepSeek-OCR\uff0c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u65b0\u8303\u5f0f\uff01", "comment": "Source: WeChat, Published: 2025-10-28 12:58:15", "summary": "\u5bf9\u4e8e \u5927\u6a21\u578b\u5f00\u53d1\u548c\u5e94\u7528\u6784\u5efa\u4e2d\u4f7f\u7528\u5927\u6a21\u578b\u800c\u8a00\uff0c\u8fd9\u4e5f\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u6548\u7387\u548c\u6210\u672c\u7a81\u7834\uff1a\u66f4\u4f4e\u7684\u5185\u5b58\u6210\u672c\uff1a \u89c6\u89c9\u6807\u8bb0\u662f\u7d27\u51d1\u7684\u3002\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff1a \u66f4\u5c11\u7684\u6807\u8bb0\u610f\u5473\u7740\u66f4\u5c11\u7684 FLOPs\u3002", "AI": {"tldr": "\u5bf9\u4e8e \u5927\u6a21\u578b\u5f00\u53d1\u548c\u5e94\u7528\u6784\u5efa\u4e2d\u4f7f\u7528\u5927\u6a21\u578b\u800c\u8a00\uff0c\u8fd9\u4e5f\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u6548\u7387\u548c\u6210\u672c\u7a81\u7834\uff1a\u66f4\u4f4e\u7684\u5185\u5b58\u6210\u672c\uff1a \u89c6\u89c9\u6807\u8bb0\u662f\u7d27\u51d1\u7684\u3002\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff1a \u66f4\u5c11\u7684\u6807\u8bb0\u610f\u5473\u7740\u66f4\u5c11\u7684 FLOPs\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.ae5f6af6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMjE0NTYzMQ==&mid=2651758018&idx=1&sn=8e0367b9e6fe557f4cfbc5f0a98e9f9f&chksm=81cf8cc0d0d179ec2f844ffc2d3e3d950bf6459b86f8e295f1bc2129509950edbf13b086f0d8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMjE0NTYzMQ==&mid=2651758018&idx=1&sn=8e0367b9e6fe557f4cfbc5f0a98e9f9f&chksm=81cf8cc0d0d179ec2f844ffc2d3e3d950bf6459b86f8e295f1bc2129509950edbf13b086f0d8#rd", "authors": ["\u5b9e\u7528\u6559\u80b2\u6280\u672f"], "title": "\u54ea\u6b3eAI\u6700\u61c2\u4e2d\u56fd\u6559\u80b2\uff1f2025\u6559\u80b2<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6d4b\u8bd5\u6210\u7ee9\u5355\u51fa\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-10-28 12:30:00", "summary": "\u4eca\u5929\u7ed9\u5927\u5bb6\u5e26\u6765\u4e00\u4efd\u6700\u65b0\u7684\u6559\u80b2\u5927\u6a21\u578b\u8bc4\u6d4b\u7ed3\u679c\uff0c\u8fd9\u4efd\"\u6210\u7ee9\u5355\"\u6765\u81ea\u4e92\u8054\u7f51\u6559\u80b2\u56fd\u5bb6\u5de5\u7a0b\u7814\u7a76\u4e2d\u5fc3\u7684\u4e13\u4e1a\u8bc4\u6d4b\uff0c\u5305\u542b\u4e86GPT-5\u548c\u56fd\u5185\u4e94\u6b3e\u4e3b\u6d41\u5927\u6a21\u578b\uff08DeepSeek-V3.2-EXP\u3001\u8baf\u98de\u661f\u706bX1\u3001\u8c46\u53051.6-seed-thinking\u3001GLM4.6-thinking\u3001Qwen3-235B-A22B\uff0c\u4ee5\u968f", "AI": {"tldr": "\u4eca\u5929\u7ed9\u5927\u5bb6\u5e26\u6765\u4e00\u4efd\u6700\u65b0\u7684\u6559\u80b2\u5927\u6a21\u578b\u8bc4\u6d4b\u7ed3\u679c\uff0c\u8fd9\u4efd\"\u6210\u7ee9\u5355\"\u6765\u81ea\u4e92\u8054\u7f51\u6559\u80b2\u56fd\u5bb6\u5de5\u7a0b\u7814\u7a76\u4e2d\u5fc3\u7684\u4e13\u4e1a\u8bc4\u6d4b\uff0c\u5305\u542b\u4e86GPT-5\u548c\u56fd\u5185\u4e94\u6b3e\u4e3b\u6d41\u5927\u6a21\u578b\uff08DeepSeek-V3.2-EXP\u3001\u8baf\u98de\u661f\u706bX1\u3001\u8c46\u53051.6-seed-thinking\u3001GLM4.6-thinking\u3001Qwen3-235B-A22B\uff0c\u4ee5\u968f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.85e8b68f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwMDEzNTc1Mw==&mid=2247532771&idx=2&sn=97779df059f0c2bbbd6d43de84ff97d9&chksm=9b94d9f0f5ae4a1a6f0f964789718b0fb6edab91b5d01e067715948e74c387fa66e4aced8602#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwMDEzNTc1Mw==&mid=2247532771&idx=2&sn=97779df059f0c2bbbd6d43de84ff97d9&chksm=9b94d9f0f5ae4a1a6f0f964789718b0fb6edab91b5d01e067715948e74c387fa66e4aced8602#rd", "authors": ["51CTO"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u201c\u5361\u8116\u5b50\u201d\u96be\u9898\uff0c\u8fd9\u6b21\u6709\u89e3\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-10-28 12:04:34", "summary": "\u4eca\u5929\u7ed9\u5927\u5bb6\u63a8\u8350\u7684\u300aAI\u5927\u6a21\u578b\u786c\u4ef6\u67b6\u6784\u300b\uff0c\u4e13\u6cbb\u5404\u79cd\u201c\u786c\u4ef6\u4e0d\u670d\u201d\uff0c\u8fd9\u4e0d\u662f\u90a3\u79cd\u5145\u65a5\u7740\u6666\u6da9\u672f\u8bed\u7684\u4f20\u7edf\u786c\u4ef6\u8bfe\uff0c\u800c\u662f\u4e00\u5957\u771f\u6b63\u4eceAI\u5f00\u53d1\u8005\u89d2\u5ea6\u51fa\u53d1\u7684\u5b9e\u6218\u89e3\u51b3\u65b9\u6848\u3002", "AI": {"tldr": "\u4eca\u5929\u7ed9\u5927\u5bb6\u63a8\u8350\u7684\u300aAI\u5927\u6a21\u578b\u786c\u4ef6\u67b6\u6784\u300b\uff0c\u4e13\u6cbb\u5404\u79cd\u201c\u786c\u4ef6\u4e0d\u670d\u201d\uff0c\u8fd9\u4e0d\u662f\u90a3\u79cd\u5145\u65a5\u7740\u6666\u6da9\u672f\u8bed\u7684\u4f20\u7edf\u786c\u4ef6\u8bfe\uff0c\u800c\u662f\u4e00\u5957\u771f\u6b63\u4eceAI\u5f00\u53d1\u8005\u89d2\u5ea6\u51fa\u53d1\u7684\u5b9e\u6218\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.a7bed99a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNDM2ODIxOQ==&mid=2247489848&idx=1&sn=1a5d55097d00d6a57a531427d7a43747&chksm=9a55cf9d4251e5636b200ab302d6cfbd4ae1245269f4f8e56e2117cdc9cc99c8d14dc387d72d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNDM2ODIxOQ==&mid=2247489848&idx=1&sn=1a5d55097d00d6a57a531427d7a43747&chksm=9a55cf9d4251e5636b200ab302d6cfbd4ae1245269f4f8e56e2117cdc9cc99c8d14dc387d72d#rd", "authors": ["\u4e8c\u7237Hack"], "title": "MiniMax\u51fa\u606f\u4e86\uff01\u56fd\u5185\u5f00\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9996\u6b21\u95ef\u5165\u5168\u7403\u524d\u4e94\uff01", "comment": "Source: WeChat, Published: 2025-10-28 11:59:11", "summary": "\u4e5f\u662f\u4e2d\u56fd\u5f00\u6e90\u5927\u6a21\u578b\u9996\u6b21\u95ef\u5165\u5168\u7403\u524d\u4e94\u3002lis minimax \uff08official\uff09 .. @minimax__ai we're open-sourcing minimax m2 - agent & code native\uff0c at 8% claude sonnet price\uff0c ~2x faster global free for a limited time via minimax agent & api - advanced coding capability\uff1a engineered for end-", "AI": {"tldr": "\u4e5f\u662f\u4e2d\u56fd\u5f00\u6e90\u5927\u6a21\u578b\u9996\u6b21\u95ef\u5165\u5168\u7403\u524d\u4e94\u3002lis minimax \uff08official\uff09 .. @minimax__ai we're open-sourcing minimax m2 - agent & code native\uff0c at 8% claude sonnet price\uff0c ~2x faster global free for a limited time via minimax agent & ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.d24a2576", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDQ5ODcwMQ==&mid=2247483716&idx=1&sn=c8ecbb5fe3e397f73b39722f1a15cddd&chksm=f1bf1965e2eb24cd83057b9437d7f89ec780ccd9a5d88a3fe1aa41cc56e94f68bc1a3ad8356e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDQ5ODcwMQ==&mid=2247483716&idx=1&sn=c8ecbb5fe3e397f73b39722f1a15cddd&chksm=f1bf1965e2eb24cd83057b9437d7f89ec780ccd9a5d88a3fe1aa41cc56e94f68bc1a3ad8356e#rd", "authors": ["\u795e\u8c15\u5929\u7391\u667a\u80fd\u5b9e\u9a8c\u5ba4"], "title": "\u56fd\u5185\u4e3b\u6d41\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bc4\u6d4b\u6c47\u603b", "comment": "Source: WeChat, Published: 2025-10-28 11:44:53", "summary": "\u9a8c\u3002\u8c46\u5305\u56fe\u50cf\u521b\u4f5c\u6a21\u578b4.0 \u8c46\u5305\u5927\u6a21\u578b1.6 lite \u8c46\u53053d\u6a21\u578b\u3002\u8c46\u5305\u89d2\u8272\u626e\u6f14\u6a21\u578b doubao-seedream-4.0 tt doubao-seed-1.6-lite doubao-seed3d tt doubao-seed-character 4k\u8d85\u9ad8\u6e05\u76f4\u51fa\uff0c\u8d85\u5f3a\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u652f\u6301\u591a\u53c2\u8003... \u66f4\u9ad8\u6027\u4ef7\u6bd4\uff0c\u5e38\u89c1\u4efb\u52a1\u4f18\u9009 \u9ad8\u7cbe\u5ea6\u51e0\u4f55", "AI": {"tldr": "\u9a8c\u3002\u8c46\u5305\u56fe\u50cf\u521b\u4f5c\u6a21\u578b4.0 \u8c46\u5305\u5927\u6a21\u578b1.6 lite \u8c46\u53053d\u6a21\u578b\u3002\u8c46\u5305\u89d2\u8272\u626e\u6f14\u6a21\u578b doubao-seedream-4.0 tt doubao-seed-1.6-lite doubao-seed3d tt doubao-seed-character 4k\u8d85\u9ad8\u6e05\u76f4\u51fa\uff0c\u8d85\u5f3a\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u652f\u6301\u591a\u53c2\u8003... \u66f4\u9ad8\u6027\u4ef7\u6bd4\uff0c\u5e38\u89c1\u4efb\u52a1\u4f18\u9009 \u9ad8\u7cbe\u5ea6\u51e0\u4f55", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.401fd48a", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDEyMTY4MQ==&mid=2650874634&idx=1&sn=49928968cba2fbac68e22ac2788b1bfd&chksm=bc53596c1d8adcd2aa8482e084ce067b7398d3f803f6988ec4703960f18801e870e8a06b47e0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDEyMTY4MQ==&mid=2650874634&idx=1&sn=49928968cba2fbac68e22ac2788b1bfd&chksm=bc53596c1d8adcd2aa8482e084ce067b7398d3f803f6988ec4703960f18801e870e8a06b47e0#rd", "authors": ["\u79d1\u5b66+"], "title": "\u63ed\u79d8<em class=\"highlight\">\u5927\u6a21\u578b</em>\u80cc\u540e\u7684\u201c\u4fe1\u606f\u52a0\u5de5\u5bc6\u7801\u201d\u4e0e\u201c\u8fdb\u5316\u903b\u8f91\u201d\uff01\u6d59\u5927\u201cDeepSeek\u4fe1\u606f\u4e0eAI\u878d\u5408\u4e4b\u65c5\u201d\u7b2c\u4e94\u671f\u6765\u4e86", "comment": "Source: WeChat, Published: 2025-10-28 10:30:51", "summary": "\u5927\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7ec4\u56e2\u201c\u5f00\u6302\u201d\uff0c\u662f\u667a\u80fd\u4f53\u5f00\u6302\u7684\u5173\u952e\u2014\u2014\u5927\u6a21\u578b\u662f\u201c\u77e5\u8bc6\u5927\u8111\u201d\uff0c\u5f3a\u5316\u5b66\u4e60\u5f53\u201c\u6559\u7ec3\u201d\uff0c\u901a\u8fc7\u8bd5\u9519\u548c\u5956\u52b1\u8ba9\u5927\u6a21\u578b\u66f4\u8d34\u5fc3\u3001\u66f4\u806a\u660e\uff01\u4e8c\u8005\u914d\u5408\uff0c\u5927\u6a21\u578b\u94fa\u8def\uff0c\u5f3a\u5316\u5b66\u4e60\u52a0\u901f\uff0c\u667a\u80fd\u4f53\u4ece\u201c\u840c\u65b0\u201d\u53d8\u201c\u738b\u8005\u201d\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7ec4\u56e2\u201c\u5f00\u6302\u201d\uff0c\u662f\u667a\u80fd\u4f53\u5f00\u6302\u7684\u5173\u952e\u2014\u2014\u5927\u6a21\u578b\u662f\u201c\u77e5\u8bc6\u5927\u8111\u201d\uff0c\u5f3a\u5316\u5b66\u4e60\u5f53\u201c\u6559\u7ec3\u201d\uff0c\u901a\u8fc7\u8bd5\u9519\u548c\u5956\u52b1\u8ba9\u5927\u6a21\u578b\u66f4\u8d34\u5fc3\u3001\u66f4\u806a\u660e\uff01\u4e8c\u8005\u914d\u5408\uff0c\u5927\u6a21\u578b\u94fa\u8def\uff0c\u5f3a\u5316\u5b66\u4e60\u52a0\u901f\uff0c\u667a\u80fd\u4f53\u4ece\u201c\u840c\u65b0\u201d\u53d8\u201c\u738b\u8005\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2e1f377a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3OTI4MjY5Nw==&mid=2650288149&idx=2&sn=b8d4544e3b03df0962624715ff30bafe&chksm=86130fce62782b8c2db7c615b145d83ca26c0506427d024b814ada1591388f65fb5d8d0ced54#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3OTI4MjY5Nw==&mid=2650288149&idx=2&sn=b8d4544e3b03df0962624715ff30bafe&chksm=86130fce62782b8c2db7c615b145d83ca26c0506427d024b814ada1591388f65fb5d8d0ced54#rd", "authors": ["\u79d1\u6280\u91d1\u878d"], "title": "\u5398\u6e05<em class=\"highlight\">\u5927\u6a21\u578b</em>\u91d1\u878d\u5e94\u7528\u8def\u5f84\u4e0e\u6cbb\u7406\u91cd\u70b9", "comment": "Source: WeChat, Published: 2025-10-28 09:21:28", "summary": "\u8fd1\u671f\uff0c\u817e\u8baf\u7814\u7a76\u9662\u8054\u5408\u6bd5\u9a6c\u5a01\u53d1\u5e03\u7684\u300a2025\u91d1\u878d\u4e1a\u5927\u6a21\u578b\u5e94\u7528\u62a5\u544a\uff1a\u4f53\u7cfb\u843d\u5730\uff0c\u4ef7\u503c\u5171\u751f\u300b\u8ba4\u4e3a\uff0c\u5927\u6a21\u578b\u7684\u5e94\u7528\u76ee\u524d\u6b63\u5448\u73b0\u4e24\u5927\u8d8b\u52bf\uff1a\u4e00\u662f\u4ece\u5185\u90e8\u63d0\u6548\u5411\u6838\u5fc3\u521b\u6536\u9886\u57df\u52a0\u901f\u8f6c\u79fb\uff0c\u5728\u667a\u80fd\u7406\u8d22\u52a9\u7406\u3001\u8d22\u5bcc\u7ba1\u7406\u3001\u4fdd\u9669\u4ee3\u7406\u4eba\u7b49\u5ba2\u6237\u89e6", "AI": {"tldr": "\u8fd1\u671f\uff0c\u817e\u8baf\u7814\u7a76\u9662\u8054\u5408\u6bd5\u9a6c\u5a01\u53d1\u5e03\u7684\u300a2025\u91d1\u878d\u4e1a\u5927\u6a21\u578b\u5e94\u7528\u62a5\u544a\uff1a\u4f53\u7cfb\u843d\u5730\uff0c\u4ef7\u503c\u5171\u751f\u300b\u8ba4\u4e3a\uff0c\u5927\u6a21\u578b\u7684\u5e94\u7528\u76ee\u524d\u6b63\u5448\u73b0\u4e24\u5927\u8d8b\u52bf\uff1a\u4e00\u662f\u4ece\u5185\u90e8\u63d0\u6548\u5411\u6838\u5fc3\u521b\u6536\u9886\u57df\u52a0\u901f\u8f6c\u79fb\uff0c\u5728\u667a\u80fd\u7406\u8d22\u52a9\u7406\u3001\u8d22\u5bcc\u7ba1\u7406\u3001\u4fdd\u9669\u4ee3\u7406\u4eba\u7b49\u5ba2\u6237\u89e6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.98029ae0", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247612198&idx=1&sn=7259a69894632ad5db44d2d480ebb7e3&chksm=96d533f80719ce279f892b15fe4fd48c826b24dc7d6bf328f82c8c521a8be7b159395ea45167#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247612198&idx=1&sn=7259a69894632ad5db44d2d480ebb7e3&chksm=96d533f80719ce279f892b15fe4fd48c826b24dc7d6bf328f82c8c521a8be7b159395ea45167#rd", "authors": ["\u5915\u5c0f\u7476\u79d1\u6280\u8bf4"], "title": "OpenAI\u524dCTO Mira Murati\u56e2\u961f\u53c8\u653e\u5927\u62db\uff0c\u8ba9<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bad\u7ec3\u6210\u672c\u66b4\u964d10\u500d", "comment": "Source: WeChat, Published: 2025-10-28 08:37:44", "summary": "\u4e00\u79cd\u80fd\u4ee5 1/10 \u6210\u672c\u8fbe\u5230\u5f3a\u5316\u5b66\u4e60\u540c\u7b49\u6548\u679c\u7684\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u65b0\u65b9\u6cd5\u3002mira murati @miramurati \u00b7 14h combining the benefits of rl and sft with on-policy distillation\uff0c a promising approach for training small models for domain performance and continual learning.\u3002", "AI": {"tldr": "\u4e00\u79cd\u80fd\u4ee5 1/10 \u6210\u672c\u8fbe\u5230\u5f3a\u5316\u5b66\u4e60\u540c\u7b49\u6548\u679c\u7684\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u65b0\u65b9\u6cd5\u3002mira murati @miramurati \u00b7 14h combining the benefits of rl and sft with on-policy distillation\uff0c a promising approach for training small models for domain performance a...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.14976c98", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247483947&idx=1&sn=5f77da492fc16a38d4a1ee6823eedad7&chksm=c0cfcbc1310a7c2d4e9d382ed3d2eb5718dc290e985f3fd4cfeed540195a2bfa712e30704a65#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247483947&idx=1&sn=5f77da492fc16a38d4a1ee6823eedad7&chksm=c0cfcbc1310a7c2d4e9d382ed3d2eb5718dc290e985f3fd4cfeed540195a2bfa712e30704a65#rd", "authors": ["Ai\u5927\u6a21\u578b\u77e5\u8bc6\u8425"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6700\u70ed\u95e8\u76847\u5927\u6846\u67b6\u4ecb\u7ecd\uff01", "comment": "Source: WeChat, Published: 2025-10-28 08:19:37", "summary": "7\u5927\u6846\u67b6\u4ecb\u7ecd\uff011\u3002langchain\u3002langchain\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08llm\uff09\u5e94\u7528\u7a0b\u5e8f\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u5c06 \u8bed\u8a00\u6a21\u578b\u4e0e\u5176\u4ed6\u6570\u636e\u6e90\u3001\u5de5\u5177\u548c\u8ba1\u7b97\u8d44\u6e90\u7ed3\u5408\uff0c\u521b \u5efa\u66f4\u590d\u6742\u4e14\u5b9e\u7528\u7684\u5e94\u7528\u3002", "AI": {"tldr": "7\u5927\u6846\u67b6\u4ecb\u7ecd\uff011\u3002langchain\u3002langchain\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08llm\uff09\u5e94\u7528\u7a0b\u5e8f\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u5c06 \u8bed\u8a00\u6a21\u578b\u4e0e\u5176\u4ed6\u6570\u636e\u6e90\u3001\u5de5\u5177\u548c\u8ba1\u7b97\u8d44\u6e90\u7ed3\u5408\uff0c\u521b \u5efa\u66f4\u590d\u6742\u4e14\u5b9e\u7528\u7684\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.e400fcd3", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODg1NzU5NA==&mid=2247512506&idx=7&sn=a187a0058d9bb06a99e812da2022dca7&chksm=91e3dc67a298b855062274b125592568a25bcd263f50a43b287b334f4add26147c9779b5760c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODg1NzU5NA==&mid=2247512506&idx=7&sn=a187a0058d9bb06a99e812da2022dca7&chksm=91e3dc67a298b855062274b125592568a25bcd263f50a43b287b334f4add26147c9779b5760c#rd", "authors": ["\u6e56\u5317\u653f\u52a1\u670d\u52a1"], "title": "\u6570\u636e\u667a\u80fd\u5f15\u64ce\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u73b0\u5728\u4e0e\u672a\u6765", "comment": "Source: WeChat, Published: 2025-10-28 07:17:18", "summary": "\u5b9e\u9645\u4e0a\u968f\u7740\u901a\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u6548\u679c\u96be\u4ee5\u8fdb\u4e00\u6b65\u5f97\u5230\u98de\u8dc3\u5f0f\u63d0\u5347\uff0c\u5927\u6a21\u578b\u9886\u57df\u7684\u7814\u7a76\u5df2\u8fdb\u5165\u540e\u8bad\u7ec3\u65f6\u4ee3\uff0c\u5728\u89c4\u6a21\u6269\u5c55\uff08Scaling\uff09\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u5fae\u8c03\uff08Fine-Tuning\uff09\u8fd9\u4e09\u7c7b\u4e3b\u8981\u540e\u8bad\u7ec3\u6280\u672f\u7684\u652f\u6301\u4e0b\uff0c\u901a\u8fc7\u9488\u5bf9\u7279\u5b9a\u76ee\u6807\u6a21\u6001\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c", "AI": {"tldr": "\u5b9e\u9645\u4e0a\u968f\u7740\u901a\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u6548\u679c\u96be\u4ee5\u8fdb\u4e00\u6b65\u5f97\u5230\u98de\u8dc3\u5f0f\u63d0\u5347\uff0c\u5927\u6a21\u578b\u9886\u57df\u7684\u7814\u7a76\u5df2\u8fdb\u5165\u540e\u8bad\u7ec3\u65f6\u4ee3\uff0c\u5728\u89c4\u6a21\u6269\u5c55\uff08Scaling\uff09\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u5fae\u8c03\uff08Fine-Tuning\uff09\u8fd9\u4e09\u7c7b\u4e3b\u8981\u540e\u8bad\u7ec3\u6280\u672f\u7684\u652f\u6301\u4e0b\uff0c\u901a\u8fc7\u9488\u5bf9\u7279\u5b9a\u76ee\u6807\u6a21\u6001\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.fecebbbe", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671591&idx=5&sn=246ced40da6387167094639d054d8176&chksm=fd93564d95dbf821dc4a98fccd4087f9cee4bc6b937ac81eef49cacf26711ea3bd81c64e994c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671591&idx=5&sn=246ced40da6387167094639d054d8176&chksm=fd93564d95dbf821dc4a98fccd4087f9cee4bc6b937ac81eef49cacf26711ea3bd81c64e994c#rd", "authors": ["\u4e13\u77e5"], "title": "2025\u5fc5\u770bAI\u5e72\u8d27!\u300a<em class=\"highlight\">\u5927\u6a21\u578b</em>/AIGC/GPT-4/Transformer/DL/KG/NLP/CV AI+X\u300b\u96c6\u5408", "comment": "Source: WeChat, Published: 2025-10-28 03:03:01", "summary": "\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5f0f\u9002\u914d\uff1a\u7efc\u8ff0 \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\uff1a\u7efc\u8ff0 video-lmm\u540e\u8bad\u7ec3\uff1a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u6df1\u5ea6\u89e3\u6790 \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6613\u4ea7\u751f\u5e7b\u89c9\uff1a\u5206\u7c7b\u4f53\u7cfb\u3001\u65b9\u6cd5\u4e0e\u672a\u6765\u65b9\u5411\u7efc\u8ff0", "AI": {"tldr": "\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5f0f\u9002\u914d\uff1a\u7efc\u8ff0 \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\uff1a\u7efc\u8ff0 video-lmm\u540e\u8bad\u7ec3\uff1a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u6df1\u5ea6\u89e3\u6790 \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6613\u4ea7\u751f\u5e7b\u89c9\uff1a\u5206\u7c7b\u4f53\u7cfb\u3001\u65b9\u6cd5\u4e0e\u672a\u6765\u65b9\u5411\u7efc\u8ff0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.3e7fb52d", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjQwMzQyNw==&mid=2247484263&idx=1&sn=591d8e86b80797763c4bee8284596b70&chksm=fe54ed930af848678aeade72013d43ba7fed16ffd2dc07d4bdd507ff2eb30c1f1d4191568ee9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjQwMzQyNw==&mid=2247484263&idx=1&sn=591d8e86b80797763c4bee8284596b70&chksm=fe54ed930af848678aeade72013d43ba7fed16ffd2dc07d4bdd507ff2eb30c1f1d4191568ee9#rd", "authors": ["AI \u5148\u950b\u6d1e\u5bdf\u7ad9"], "title": "\u56fd\u4ea7AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6740\u75af\u4e86\uff01MiniMax M2\u6027\u80fd\u521a\u731b\uff0c\u4ef7\u683c\u5374\u53ea\u8981Claude\u7684\u96f6\u5934\uff1f", "comment": "Source: WeChat, Published: 2025-10-28 02:40:28", "summary": "\u76f4\u63a5\u7529\u51fa\u4e86\u65b0\u4e00\u4ee3\u6587\u672c\u5927\u6a21\u578b\u2014\u2014MiniMax - M2\u3002\u8fd9\u73a9\u610f\u513f\u4e0d\u5149\u6027\u80fd\u80fd\u548c\u7845\u8c37\u90a3\u5e2e\u5de8\u5934\u201c\u786c\u78b0\u786c\u201d\uff0c\u4ef7\u683c\u66f4\u662f\u4f4e\u5230\u8ba9\u4eba\u5927\u558a\u201c\u79bb\u8c31\u201d\uff0c\u7b80\u76f4\u662f\u54b1\u4eec\u6253\u5de5\u4eba\u548c\u5f00\u53d1\u8005\u7684\u201c\u68a6\u4e2d\u60c5\u6a21\u201d\uff01", "AI": {"tldr": "\u76f4\u63a5\u7529\u51fa\u4e86\u65b0\u4e00\u4ee3\u6587\u672c\u5927\u6a21\u578b\u2014\u2014MiniMax - M2\u3002\u8fd9\u73a9\u610f\u513f\u4e0d\u5149\u6027\u80fd\u80fd\u548c\u7845\u8c37\u90a3\u5e2e\u5de8\u5934\u201c\u786c\u78b0\u786c\u201d\uff0c\u4ef7\u683c\u66f4\u662f\u4f4e\u5230\u8ba9\u4eba\u5927\u558a\u201c\u79bb\u8c31\u201d\uff0c\u7b80\u76f4\u662f\u54b1\u4eec\u6253\u5de5\u4eba\u548c\u5f00\u53d1\u8005\u7684\u201c\u68a6\u4e2d\u60c5\u6a21\u201d\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.20f05a8f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837058&idx=1&sn=c2dd7613d86fc8b9c91944bbc91961e3&chksm=e91a2d5d727444a08ceb8c5a75792b3dbd8d1acb9459819dc5f04e7249f98f9f5d0e2b4c2865#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837058&idx=1&sn=c2dd7613d86fc8b9c91944bbc91961e3&chksm=e91a2d5d727444a08ceb8c5a75792b3dbd8d1acb9459819dc5f04e7249f98f9f5d0e2b4c2865#rd", "authors": ["\u91cf\u5b50\u4f4d"], "title": "\u5168\u7403\u5f00\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\u676d\u5dde\u9738\u699c\u88ab\u7ec8\u7ed3\uff0c\u4e0a\u6d77MiniMax M2\u53d1\u5e03\u5373\u7206\u5355\uff0c\u767e\u4e07Tokens\u4ec5\u97008\u5143\u4eba\u6c11\u5e01", "comment": "Source: WeChat, Published: 2025-10-28 01:17:59", "summary": "\u4ee5Artificial Analysis\u7684\u6210\u7ee9\u4e3a\u57fa\u51c6\uff0cMinimax\u7ed8\u5236\u4e86\u4e00\u5f20\u56fe\u6765\u6bd4\u8f83\u5404\u5927\u6a21\u578b\u6027\u4ef7\u6bd4\uff08\u6a2a\u8f74\u8d8a\u5411\u53f3\u6210\u672c\u8d8a\u4f4e\uff09\u3002artificial analysis intelligence index\uff1b output price\uff1a usd per 1m tokens most attractive quadrant gpt-5 codex \uff08high\uff09 gpt-5 \uff08high\uff09 grok 4 claude 4.5 sonnet", "AI": {"tldr": "\u4ee5Artificial Analysis\u7684\u6210\u7ee9\u4e3a\u57fa\u51c6\uff0cMinimax\u7ed8\u5236\u4e86\u4e00\u5f20\u56fe\u6765\u6bd4\u8f83\u5404\u5927\u6a21\u578b\u6027\u4ef7\u6bd4\uff08\u6a2a\u8f74\u8d8a\u5411\u53f3\u6210\u672c\u8d8a\u4f4e\uff09\u3002artificial analysis intelligence index\uff1b output price\uff1a usd per 1m tokens most attractive quadrant gpt-5 codex \uff08high\uff09 gpt-5 \uff08high\uff09 grok 4 cl...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.f3ecaed2", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571743&idx=2&sn=e47d82e510ea94e9cc06c73bb943bd13&chksm=96ba8745a17857fa6c45956fdc72fa106fff8eb67b418d2095753f743ee98afaeece4e7901de#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571743&idx=2&sn=e47d82e510ea94e9cc06c73bb943bd13&chksm=96ba8745a17857fa6c45956fdc72fa106fff8eb67b418d2095753f743ee98afaeece4e7901de#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u4e0a\u4ea42025\u6700\u65b0-\u300a\u52a8\u624b\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u300b\u5b9e\u6218\u6559\u7a0b\u53cappt\u5206\u4eab\uff01", "comment": "Source: WeChat, Published: 2025-10-28 00:02:06", "summary": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0AGI\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0AGI\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2dbf9431", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMTcyMjM3Mw==&mid=2247484869&idx=1&sn=2a940782cc3bce07e2b40ca6509f2321&chksm=f8028e9d1f05d6666056830c1098941e9a110c6821de7f982b0c78a8c104e699c2abea082cda#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMTcyMjM3Mw==&mid=2247484869&idx=1&sn=2a940782cc3bce07e2b40ca6509f2321&chksm=f8028e9d1f05d6666056830c1098941e9a110c6821de7f982b0c78a8c104e699c2abea082cda#rd", "authors": ["\u6570\u5b57\u5316\u7126\u70b9"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6280\u672f\u4f53\u7cfb", "comment": "Source: WeChat, Published: 2025-10-27 23:36:27", "summary": "\u56db\u3001\u5e94\u7528\u5c42\u6280\u672f\u3002\u4e00 \u57fa\u7840\u7406\u8bba\u4e0e\u6a21\u578b\u67b6\u6784\u6280\u672f\u3002\u5927\u6a21\u578b\u6280\u672f\u4f53\u7cfb\u3002\u4e09\u3001 \u5de5\u7a0b\u5316\u90e8\u7f72\u6280\u672f\u3002\u4e8c\u3001 \u8bad\u7ec3\u4e0e\u4f18\u5316\u6280\u672f\u3002decoder-only\u67b6\u6784\uff08gpt\u7cfb\u5217\uff0c\u4fa7\u91cd\u751f\u6210\uff09 1.transformer\u67b6\u6784\u53ca\u53d8\u4f53\u3002", "AI": {"tldr": "\u56db\u3001\u5e94\u7528\u5c42\u6280\u672f\u3002\u4e00 \u57fa\u7840\u7406\u8bba\u4e0e\u6a21\u578b\u67b6\u6784\u6280\u672f\u3002\u5927\u6a21\u578b\u6280\u672f\u4f53\u7cfb\u3002\u4e09\u3001 \u5de5\u7a0b\u5316\u90e8\u7f72\u6280\u672f\u3002\u4e8c\u3001 \u8bad\u7ec3\u4e0e\u4f18\u5316\u6280\u672f\u3002decoder-only\u67b6\u6784\uff08gpt\u7cfb\u5217\uff0c\u4fa7\u91cd\u751f\u6210\uff09 1.transformer\u67b6\u6784\u53ca\u53d8\u4f53\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.b29c2ef2", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0MjMwNzI0OA==&mid=2247497493&idx=1&sn=a7ac0b190b365f1976aa2955713bcb9a&chksm=fa4e009e4d1c1dc396f0fe060cc029cc2e3ebc27cdffe50348494d15f9e733df1a1c6b105956#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0MjMwNzI0OA==&mid=2247497493&idx=1&sn=a7ac0b190b365f1976aa2955713bcb9a&chksm=fa4e009e4d1c1dc396f0fe060cc029cc2e3ebc27cdffe50348494d15f9e733df1a1c6b105956#rd", "authors": ["AI\u79d1\u6280\u5728\u7ebf"], "title": "\u725b\uff01<em class=\"highlight\">\u5927\u6a21\u578b</em>\u76849\u5927\u6838\u5fc3\u6280\u672f\u89e3\u6790\uff01", "comment": "Source: WeChat, Published: 2025-10-27 14:31:29", "summary": "ai \u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1 1 2 planning 3 a action 5 llm \u5de5\u5177\u96c6 \u5927\u6a21\u578b 8 o 9 observation 10 10 final answer \u6398\u91d1\u6280\u672f\u793e\u533a @\u805a\u5ba2ai \u4e8c\u3001Agentic AIAgentic AI \u4ee3\u8868\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "AI": {"tldr": "ai \u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1 1 2 planning 3 a action 5 llm \u5de5\u5177\u96c6 \u5927\u6a21\u578b 8 o 9 observation 10 10 final answer \u6398\u91d1\u6280\u672f\u793e\u533a @\u805a\u5ba2ai \u4e8c\u3001Agentic AIAgentic AI \u4ee3\u8868\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
