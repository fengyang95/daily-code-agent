{"id": "2510.11722", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11722", "abs": "https://arxiv.org/abs/2510.11722", "authors": ["Haruhiko Yoshioka", "Kazumasa Shimari", "Hidetake Uwano", "Kenichi Matsumoto"], "title": "eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis", "comment": "2 pages, 1 figure, conference", "summary": "This paper presents eye2vec, an infrastructure for analyzing software\ndevelopers' eye movements while reading source code. In common eye-tracking\nstudies in program comprehension, researchers must preselect analysis targets\nsuch as control flow or syntactic elements, and then develop analysis methods\nto extract appropriate metrics from the fixation for source code. Here,\nresearchers can define various levels of AOIs like words, lines, or code\nblocks, and the difference leads to different results. Moreover, the\ninterpretation of fixation for word/line can vary across the purposes of the\nanalyses. Hence, the eye-tracking analysis is a difficult task that depends on\nthe time-consuming manual work of the researchers. eye2vec represents\ncontinuous two fixations as transitions between syntactic elements using\ndistributed representations. The distributed representation facilitates the\nadoption of diverse data analysis methods with rich semantic interpretations.", "AI": {"tldr": "eye2vec\u662f\u4e00\u4e2a\u5206\u6790\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u9605\u8bfb\u6e90\u4ee3\u7801\u65f6\u773c\u7403\u8fd0\u52a8\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8868\u793a\u5c06\u8fde\u7eed\u6ce8\u89c6\u8868\u793a\u4e3a\u8bed\u6cd5\u5143\u7d20\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u7b80\u5316\u4e86\u773c\u52a8\u5206\u6790\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u773c\u52a8\u7814\u7a76\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u9884\u5148\u9009\u62e9\u5206\u6790\u76ee\u6807\u5e76\u5f00\u53d1\u5206\u6790\u65b9\u6cd5\uff0c\u8fd9\u79cd\u624b\u52a8\u5de5\u4f5c\u8017\u65f6\u4e14\u7ed3\u679c\u56e0\u5206\u6790\u533a\u57df\u5b9a\u4e49\u4e0d\u540c\u800c\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u8868\u793a\u5c06\u8fde\u7eed\u4e24\u6b21\u6ce8\u89c6\u8868\u793a\u4e3a\u8bed\u6cd5\u5143\u7d20\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u4fbf\u4e8e\u91c7\u7528\u591a\u6837\u5316\u7684\u6570\u636e\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u773c\u52a8\u5206\u6790\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "eye2vec\u57fa\u7840\u8bbe\u65bd\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u773c\u52a8\u5206\u6790\u4e2d\u7684\u4f9d\u8d56\u624b\u52a8\u5de5\u4f5c\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.11736", "categories": ["cs.AI", "cs.GT", "cs.LG", "91A68", "I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.11736", "abs": "https://arxiv.org/abs/2510.11736", "authors": ["Sahaj Raj Malla"], "title": "AI Agents for the Dhumbal Card Game: A Comparative Study", "comment": "10 pages, 7 figures, 6 tables", "summary": "This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a\nculturally significant multiplayer card game with imperfect information,\nthrough a systematic comparison of rule-based, search-based, and learning-based\nstrategies. We formalize Dhumbal's mechanics and implement diverse agents,\nincluding heuristic approaches (Aggressive, Conservative, Balanced,\nOpportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and\nInformation Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning\napproaches including Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO), and a random baseline. Evaluation involves within-category tournaments\nfollowed by a cross-category championship. Performance is measured via win\nrate, economic outcome, Jhyap success, cards discarded per round, risk\nassessment, and decision efficiency. Statistical significance is assessed using\nWelch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%\nconfidence intervals (CI). Across 1024 simulated rounds, the rule-based\nAggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),\noutperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of\nJhyap declarations. The study contributes a reproducible AI framework, insights\ninto heuristic efficacy under partial information, and open-source code,\nthereby advancing AI research and supporting digital preservation of cultural\ngames.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u3001\u641c\u7d22\u548c\u5b66\u4e60\u7684AI\u4ee3\u7406\u5728Dhumbal\u7eb8\u724c\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8e\u89c4\u5219\u7684\u6fc0\u8fdb\u7b56\u7565\u57281024\u8f6e\u6a21\u62df\u4e2d\u80dc\u7387\u6700\u9ad8\uff0888.3%\uff09\uff0c\u4f18\u4e8eISMCTS\u548cPPO\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540cAI\u7b56\u7565\u5728\u5177\u6709\u6587\u5316\u610f\u4e49\u7684\u4e0d\u5b8c\u5168\u4fe1\u606f\u591a\u4eba\u7eb8\u724c\u6e38\u620fDhumbal\u4e2d\u7684\u8868\u73b0\uff0c\u4e3aAI\u7814\u7a76\u63d0\u4f9b\u53ef\u590d\u73b0\u6846\u67b6\u5e76\u652f\u6301\u6587\u5316\u6e38\u620f\u7684\u6570\u5b57\u4fdd\u5b58\u3002", "method": "\u5b9e\u73b0\u591a\u79cdAI\u4ee3\u7406\uff08\u542f\u53d1\u5f0f\u3001MCTS\u3001ISMCTS\u3001DQN\u3001PPO\u3001\u968f\u673a\u57fa\u7ebf\uff09\uff0c\u901a\u8fc7\u7c7b\u522b\u5185\u9526\u6807\u8d5b\u548c\u8de8\u7c7b\u522b\u9526\u6807\u8d5b\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u6027\u80fd\u6307\u6807\u3002", "result": "\u57fa\u4e8e\u89c4\u5219\u7684\u6fc0\u8fdb\u4ee3\u7406\u80dc\u7387\u6700\u9ad8\uff0888.3%\uff09\uff0cISMCTS\u4e3a9.0%\uff0cPPO\u4e3a1.5%\u3002\u6fc0\u8fdb\u7b56\u7565\u901a\u8fc7\u6709\u6548\u5229\u7528Jhyap\u58f0\u660e\u83b7\u5f97\u4f18\u52bf\u3002", "conclusion": "\u5728Dhumbal\u6e38\u620f\u4e2d\uff0c\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u4e8e\u590d\u6742\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u7684AI\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.11822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11822", "abs": "https://arxiv.org/abs/2510.11822", "authors": ["Suryaansh Jain", "Umair Z. Ahmed", "Shubham Sahai", "Ben Leong"], "title": "Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations", "comment": null, "summary": "New Large Language Models (LLMs) become available every few weeks, and modern\napplication developers confronted with the unenviable task of having to decide\nif they should switch to a new model. While human evaluation remains the gold\nstandard, it is costly and unscalable. The state-of-the-art approach is to use\nLLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:\nLLMs exhibit a strong positive bias. We provide empirical evidence showing that\nwhile LLMs can identify valid outputs with high accuracy (i.e., True Positive\nRate 96%), they are remarkably poor at identifying invalid ones (i.e., True\nNegative Rate <25%). This systematic bias, coupled with class imbalance, often\nleads to inflated reliability scores.\n  While ensemble-based methods like majority voting can help, we show that they\nare not good enough. We introduce an optimal minority-veto strategy that is\nresilient to missing data and mitigates this bias to a large extent. For\nscenarios requiring even higher precision, we propose a novel regression-based\nframework that directly models the validator bias using a small set of\nhuman-annotated ground truth data. On a challenging code feedback task over 366\nhigh-school Python programs, our regression approach reduces the maximum\nabsolute error to just 1.2%, achieving a 2x improvement over the\nbest-performing ensemble of 14 state-of-the-art LLMs.", "AI": {"tldr": "LLM\u8bc4\u4f30\u5668\u5b58\u5728\u4e25\u91cd\u6b63\u5411\u504f\u5dee\uff0c\u96be\u4ee5\u8bc6\u522b\u65e0\u6548\u8f93\u51fa\u3002\u8bba\u6587\u63d0\u51fa\u5c11\u6570\u5426\u51b3\u7b56\u7565\u548c\u57fa\u4e8e\u56de\u5f52\u7684\u6846\u67b6\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u4ee3\u7801\u53cd\u9988\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u6b63\u5411\u504f\u5dee\uff0c\u867d\u7136\u80fd\u51c6\u786e\u8bc6\u522b\u6709\u6548\u8f93\u51fa\uff0c\u4f46\u5bf9\u65e0\u6548\u8f93\u51fa\u7684\u8bc6\u522b\u7387\u5f88\u4f4e\uff0c\u5bfc\u81f4\u53ef\u9760\u6027\u8bc4\u5206\u865a\u9ad8\u3002", "method": "\u63d0\u51fa\u5c11\u6570\u5426\u51b3\u7b56\u7565\u6765\u7f13\u89e3\u504f\u5dee\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u56de\u5f52\u7684\u6846\u67b6\u76f4\u63a5\u5efa\u6a21\u9a8c\u8bc1\u5668\u504f\u5dee\uff0c\u4f7f\u7528\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u5728366\u4e2a\u9ad8\u4e2dPython\u7a0b\u5e8f\u7684\u4ee3\u7801\u53cd\u9988\u4efb\u52a1\u4e0a\uff0c\u56de\u5f52\u65b9\u6cd5\u5c06\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u964d\u81f31.2%\uff0c\u6bd414\u4e2a\u6700\u5148\u8fdbLLM\u7684\u96c6\u6210\u65b9\u6cd5\u63d0\u53472\u500d\u3002", "conclusion": "LLM\u8bc4\u4f30\u5668\u7684\u6b63\u5411\u504f\u5dee\u95ee\u9898\u9700\u8981\u4e13\u95e8\u65b9\u6cd5\u89e3\u51b3\uff0c\u63d0\u51fa\u7684\u56de\u5f52\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u8bc4\u4f30\u7cbe\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.11769", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11769", "abs": "https://arxiv.org/abs/2510.11769", "authors": ["Ruida Wang", "Jiarui Yao", "Rui Pan", "Shizhe Diao", "Tong Zhang"], "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving", "comment": null, "summary": "Solving math problems through verifiable languages such as Lean has\nsignificantly impacted both the mathematics and computer science communities.\nCurrent state-of-the-art models are often trained with expensive online\nReinforcement Learning (RL) or expert iteration. However, these approaches rely\non fixed problem sets, which causes inefficient training and limits the model\nto tackle complex problems. To overcome these limitations, we propose GAR:\nGenerative Adversarial Reinforcement learning, a comprehensive RL training\nframework that jointly trains the problem composer and solver in an adversarial\nloop. GAR introduces an implicit curriculum learning mechanism, which aligns\ntask difficulty with the prover's evolving capability. It thereby improves the\ntraining efficiency and enables stronger performance of proving advanced\ntheorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and\nDeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of\n4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on\nProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR\nestablishes a general RL paradigm for co-evolution of problem generation and\nsolving under verifiable environments.", "AI": {"tldr": "\u63d0\u51faGAR\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u8054\u5408\u4f18\u5316\u95ee\u9898\u751f\u6210\u5668\u548c\u6c42\u89e3\u5668\uff0c\u5728\u53ef\u9a8c\u8bc1\u73af\u5883\u4e2d\u5b9e\u73b0\u95ee\u9898\u751f\u6210\u4e0e\u6c42\u89e3\u7684\u534f\u540c\u8fdb\u5316\uff0c\u63d0\u5347\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u95ee\u9898\u96c6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "GAR\u6846\u67b6\u5305\u542b\u95ee\u9898\u751f\u6210\u5668\u548c\u6c42\u89e3\u5668\u7684\u5bf9\u6297\u6027\u5faa\u73af\u8bad\u7ec3\uff0c\u5f15\u5165\u9690\u5f0f\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u4f7f\u4efb\u52a1\u96be\u5ea6\u4e0e\u8bc1\u660e\u8005\u80fd\u529b\u540c\u6b65\u63d0\u5347\u3002", "result": "\u5728MiniF2F-Test\u57fa\u51c6\u4e0a\u5e73\u5747\u76f8\u5bf9\u63d0\u53474.20%\uff0cProofNet-Test\u4e0apass@32\u4ece22.58%\u63d0\u5347\u81f325.81%\u3002", "conclusion": "GAR\u5efa\u7acb\u4e86\u53ef\u9a8c\u8bc1\u73af\u5883\u4e0b\u95ee\u9898\u751f\u6210\u4e0e\u6c42\u89e3\u534f\u540c\u8fdb\u5316\u7684\u4e00\u822c\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12082", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12082", "abs": "https://arxiv.org/abs/2510.12082", "authors": ["Huy Nguyen", "Christoph Treude", "Patanamon Thongtanunam"], "title": "Enhancing Neural Code Representation with Additional Context", "comment": "34 pages, 7 figures, 11 tables", "summary": "Automated program comprehension underpins many software engineering tasks,\nfrom code summarisation to clone detection. Recent deep learning models achieve\nstrong results but typically rely on source code alone, overlooking contextual\ninformation such as version history or structural relationships. This limits\ntheir ability to capture how code evolves and operates. We conduct an empirical\nstudy on how enriching code representations with such contextual signals\naffects neural model performance on key comprehension tasks. Two downstream\ntasks, code clone detection and code summarisation, are evaluated using SeSaMe\n(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative\nmodels (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under\ncode-only and context-augmented settings. Results show that context generally\nimproves performance: version history consistently boosts clone detection\n(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%\nMETEOR), while call-graph effects vary by model and task. Combining multiple\ncontexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100\nJava snippets confirms that context-augmented summaries are significantly\npreferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).\nThese findings highlight the potential of contextual signals to enhance code\ncomprehension and open new directions for optimising contextual encoding in\nneural SE models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4ee3\u7801\u8868\u793a\u4e2d\u878d\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u7248\u672c\u5386\u53f2\u548c\u8c03\u7528\u56fe\uff09\u80fd\u591f\u663e\u8457\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u548c\u4ee3\u7801\u6458\u8981\u7b49\u7a0b\u5e8f\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6e90\u4ee3\u7801\u672c\u8eab\uff0c\u5ffd\u7565\u4e86\u7248\u672c\u5386\u53f2\u548c\u7ed3\u6784\u5173\u7cfb\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7406\u89e3\u4ee3\u7801\u6f14\u5316\u8fc7\u7a0b\u7684\u80fd\u529b\u3002", "method": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08SeSaMe\u548cCodeSearchNet\uff09\u4e0a\u8bc4\u4f30\u4e86\u4e94\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff08CodeBERT\u3001GraphCodeBERT\u3001CodeT5\u3001PLBART\u3001ASTNN\uff09\uff0c\u6bd4\u8f83\u4e86\u4ec5\u4f7f\u7528\u4ee3\u7801\u548c\u4f7f\u7528\u4e0a\u4e0b\u6587\u589e\u5f3a\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u666e\u904d\u63d0\u5347\u6027\u80fd\uff1a\u7248\u672c\u5386\u53f2\u6301\u7eed\u6539\u5584\u514b\u9686\u68c0\u6d4b\uff08CodeT5 F1\u63d0\u534715.92%\uff09\u548c\u4ee3\u7801\u6458\u8981\uff08GraphCodeBERT METEOR\u63d0\u53475.56%\uff09\uff0c\u7ec4\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u53ef\u83b7\u5f97\u66f4\u5927\u589e\u76ca\uff08\u6700\u9ad8\u63d0\u534721.48% macro-F1\uff09\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u53f7\u6709\u6f5c\u529b\u589e\u5f3a\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u7f16\u7801\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.11977", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11977", "abs": "https://arxiv.org/abs/2510.11977", "authors": ["Sayash Kapoor", "Benedikt Stroebl", "Peter Kirgis", "Nitya Nadgir", "Zachary S Siegel", "Boyi Wei", "Tianci Xue", "Ziru Chen", "Felix Chen", "Saiteja Utpala", "Franck Ndzomga", "Dheeraj Oruganty", "Sophie Luskin", "Kangheng Liu", "Botao Yu", "Amit Arora", "Dongyoon Hahm", "Harsh Trivedi", "Huan Sun", "Juyong Lee", "Tengjun Jin", "Yifan Mai", "Yifei Zhou", "Yuxuan Zhu", "Rishi Bommasani", "Daniel Kang", "Dawn Song", "Peter Henderson", "Yu Su", "Percy Liang", "Arvind Narayanan"], "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation", "comment": null, "summary": "AI agents have been developed for complex real-world tasks from coding to\ncustomer service. But AI agent evaluations suffer from many challenges that\nundermine our understanding of how well agents really work. We introduce the\nHolistic Agent Leaderboard (HAL) to address these challenges. We make three\nmain contributions. First, we provide a standardized evaluation harness that\norchestrates parallel evaluations across hundreds of VMs, reducing evaluation\ntime from weeks to hours while eliminating common implementation bugs. Second,\nwe conduct three-dimensional analysis spanning models, scaffolds, and\nbenchmarks. We validate the harness by conducting 21,730 agent rollouts across\n9 models and 9 benchmarks in coding, web navigation, science, and customer\nservice with a total cost of about $40,000. Our analysis reveals surprising\ninsights, such as higher reasoning effort reducing accuracy in the majority of\nruns. Third, we use LLM-aided log inspection to uncover previously unreported\nbehaviors, such as searching for the benchmark on HuggingFace instead of\nsolving a task, or misusing credit cards in flight booking tasks. We share all\nagent logs, comprising 2.5B tokens of language model calls, to incentivize\nfurther research into agent behavior. By standardizing how the field evaluates\nagents and addressing common pitfalls in agent evaluation, we hope to shift the\nfocus from agents that ace benchmarks to agents that work reliably in the real\nworld.", "AI": {"tldr": "\u63d0\u51fa\u4e86HAL\uff08Holistic Agent Leaderboard\uff09\u6765\u89e3\u51b3AI\u667a\u80fd\u4f53\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3001\u4e09\u7ef4\u5206\u6790\u548cLLM\u8f85\u52a9\u65e5\u5fd7\u68c0\u67e5\uff0c\u63ed\u793a\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u6df1\u5c42\u6d1e\u5bdf\u3002", "motivation": "AI\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8bf8\u591a\u95ee\u9898\uff0c\u65e0\u6cd5\u51c6\u786e\u7406\u89e3\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u9700\u8981\u66f4\u5168\u9762\u53ef\u9760\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "1) \u5f00\u53d1\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u6570\u767e\u4e2aVM\u4e0a\u5e76\u884c\u8bc4\u4f30\uff1b2) \u8fdb\u884c\u6a21\u578b\u3001\u652f\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e09\u7ef4\u5206\u6790\uff1b3) \u4f7f\u7528LLM\u8f85\u52a9\u65e5\u5fd7\u68c0\u67e5\u53d1\u73b0\u672a\u62a5\u544a\u884c\u4e3a\u3002", "result": "\u6267\u884c\u4e8621,730\u6b21\u667a\u80fd\u4f53\u6d4b\u8bd5\uff0c\u8986\u76d69\u4e2a\u6a21\u578b\u548c9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u63a8\u7406\u52aa\u529b\u589e\u52a0\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u7387\u7b49\u53cd\u76f4\u89c9\u7ed3\u679c\uff0c\u5e76\u8bc6\u522b\u51fa\u667a\u80fd\u4f53\u5728\u4efb\u52a1\u4e2d\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u800c\u975e\u89e3\u51b3\u95ee\u9898\u7684\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u6807\u51c6\u5316\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u5e76\u89e3\u51b3\u5e38\u89c1\u9677\u9631\uff0c\u5e0c\u671b\u63a8\u52a8\u7814\u7a76\u4ece\u8ffd\u6c42\u57fa\u51c6\u6d4b\u8bd5\u9ad8\u5206\u8f6c\u5411\u5f00\u53d1\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53ef\u9760\u5de5\u4f5c\u7684\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2510.11905", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11905", "abs": "https://arxiv.org/abs/2510.11905", "authors": ["Patrick Haller", "Mark Ibrahim", "Polina Kirichenko", "Levent Sagun", "Samuel J. Bell"], "title": "LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance", "comment": null, "summary": "For Large Language Models (LLMs) to be reliable, they must learn robust\nknowledge that can be generally applied in diverse settings -- often unlike\nthose seen during training. Yet, extensive research has shown that LLM\nperformance can be brittle, with models exhibiting excessive sensitivity to\ntrivial input variations. In this work, we explore whether this brittleness is\na direct result of unstable internal knowledge representations. To explore this\nquestion, we build on previous work showing that LLM representations encode\nstatement truthfulness -- i.e., true, factual statements can be easily\nseparated from false, inaccurate ones. Specifically, we test the robustness of\nlearned knowledge by evaluating representation separability on samples that\nhave undergone superficial transformations to drive them out-of-distribution\n(OOD), such as typos or reformulations. By applying semantically-preserving\nperturbations, we study how separability degrades as statements become more\nOOD, across four LLM families, five evaluation datasets, and three knowledge\nprobing methods. Our results reveal that internal representations of statement\ntruthfulness collapse as the samples' presentations become less similar to\nthose seen during pre-training. While LLMs can often distinguish between true\nand false statements when they closely resemble the pre-training data, this\nability is highly dependent on the statement's exact surface form. These\nfindings offer a possible explanation for brittle benchmark performance: LLMs\nmay learn shallow, non-robust knowledge representations that allow for only\nlimited generalizability. Our work presents a fundamental challenge for the\nutility of truthfulness probes, and more broadly, calls for further research on\nimproving the robustness of learned knowledge representations.", "AI": {"tldr": "LLM\u5185\u90e8\u77e5\u8bc6\u8868\u793a\u5bf9\u8f93\u5165\u53d8\u5316\u654f\u611f\uff0c\u771f\u5b9e\u9648\u8ff0\u4e0e\u865a\u5047\u9648\u8ff0\u7684\u53ef\u5206\u79bb\u6027\u5728\u8bed\u4e49\u4fdd\u7559\u7684\u6270\u52a8\u4e0b\u4f1a\u5d29\u6e83\uff0c\u8868\u660eLLM\u5b66\u4e60\u7684\u662f\u6d45\u5c42\u3001\u975e\u9c81\u68d2\u7684\u77e5\u8bc6\u8868\u793a\u3002", "motivation": "\u63a2\u7d22LLM\u6027\u80fd\u8106\u5f31\u6027\u662f\u5426\u6e90\u4e8e\u4e0d\u7a33\u5b9a\u7684\u5185\u90e8\u77e5\u8bc6\u8868\u793a\uff0c\u7279\u522b\u662f\u771f\u5b9e\u9648\u8ff0\u4e0e\u865a\u5047\u9648\u8ff0\u7684\u53ef\u5206\u79bb\u6027\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u8bed\u4e49\u4fdd\u7559\u7684\u6270\u52a8\uff08\u5982\u62fc\u5199\u9519\u8bef\u3001\u91cd\u8ff0\uff09\u4f7f\u6837\u672c\u5206\u5e03\u5916\uff0c\u5728\u56db\u4e2aLLM\u5bb6\u65cf\u3001\u4e94\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u4e09\u79cd\u77e5\u8bc6\u63a2\u6d4b\u65b9\u6cd5\u4e0a\u8bc4\u4f30\u8868\u793a\u53ef\u5206\u79bb\u6027\u7684\u9000\u5316\u3002", "result": "\u5f53\u6837\u672c\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u76f8\u4f3c\u5ea6\u964d\u4f4e\u65f6\uff0cLLM\u5185\u90e8\u771f\u5b9e\u9648\u8ff0\u8868\u793a\u7684\u53ef\u5206\u79bb\u6027\u4f1a\u5d29\u6e83\uff0c\u8868\u660e\u77e5\u8bc6\u8868\u793a\u5bf9\u8868\u9762\u5f62\u5f0f\u9ad8\u5ea6\u4f9d\u8d56\u3002", "conclusion": "LLM\u5b66\u4e60\u7684\u662f\u6d45\u5c42\u3001\u975e\u9c81\u68d2\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u8fd9\u89e3\u91ca\u4e86\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5bf9\u771f\u5b9e\u6027\u63a2\u6d4b\u7684\u5b9e\u7528\u6027\u63d0\u51fa\u4e86\u6839\u672c\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.12120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.12120", "abs": "https://arxiv.org/abs/2510.12120", "authors": ["Zhenyu Mao", "Jacky Keung", "Fengji Zhang", "Shuo Liu", "Yifei Wang", "Jialong Li"], "title": "Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach", "comment": null, "summary": "The increasing demand for software development has driven interest in\nautomating software engineering (SE) tasks using Large Language Models (LLMs).\nRecent efforts extend LLMs into multi-agent systems (MAS) that emulate\ncollaborative development workflows, but these systems often fail due to three\ncore deficiencies: under-specification, coordination misalignment, and\ninappropriate verification, arising from the absence of foundational SE\nstructuring principles. This paper introduces Software Engineering Multi-Agent\nProtocol (SEMAP), a protocol-layer methodology that instantiates three core SE\ndesign principles for multi-agent LLMs: (1) explicit behavioral contract\nmodeling, (2) structured messaging, and (3) lifecycle-guided execution with\nverification, and is implemented atop Google's Agent-to-Agent (A2A)\ninfrastructure. Empirical evaluation using the Multi-Agent System Failure\nTaxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures\nacross different SE tasks. In code development, it achieves up to a 69.6%\nreduction in total failures for function-level development and 56.7% for\ndeployment-level development. For vulnerability detection, SEMAP reduces\nfailure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.", "AI": {"tldr": "\u63d0\u51faSEMAP\u534f\u8bae\u5c42\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\u51cf\u5c11\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\uff1a\u660e\u786e\u884c\u4e3a\u5951\u7ea6\u5efa\u6a21\u3001\u7ed3\u6784\u5316\u6d88\u606f\u4f20\u9012\u3001\u751f\u547d\u5468\u671f\u5f15\u5bfc\u6267\u884c\u4e0e\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u57fa\u7840SE\u7ed3\u6784\u5316\u539f\u5219\u5bfc\u81f4\u7684\u4e09\u4e2a\u6838\u5fc3\u7f3a\u9677\uff1a\u89c4\u8303\u4e0d\u8db3\u3001\u534f\u8c03\u9519\u4f4d\u548c\u4e0d\u9002\u5f53\u7684\u9a8c\u8bc1\u3002", "method": "\u5f15\u5165SEMAP\u534f\u8bae\u5c42\u65b9\u6cd5\uff0c\u5b9e\u4f8b\u5316\u4e09\u4e2a\u6838\u5fc3SE\u8bbe\u8ba1\u539f\u5219\uff1a\u660e\u786e\u884c\u4e3a\u5951\u7ea6\u5efa\u6a21\u3001\u7ed3\u6784\u5316\u6d88\u606f\u4f20\u9012\u3001\u751f\u547d\u5468\u671f\u5f15\u5bfc\u6267\u884c\u4e0e\u9a8c\u8bc1\uff0c\u5e76\u5728Google\u7684A2A\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b9e\u73b0\u3002", "result": "\u4f7f\u7528MAST\u6846\u67b6\u8bc4\u4f30\u663e\u793a\uff0cSEMAP\u663e\u8457\u51cf\u5c11\u4e0d\u540cSE\u4efb\u52a1\u4e2d\u7684\u6545\u969c\uff1a\u4ee3\u7801\u5f00\u53d1\u4e2d\u51fd\u6570\u7ea7\u5f00\u53d1\u603b\u6545\u969c\u51cf\u5c1169.6%\uff0c\u90e8\u7f72\u7ea7\u5f00\u53d1\u51cf\u5c1156.7%\uff1b\u6f0f\u6d1e\u68c0\u6d4b\u4e2dPython\u4efb\u52a1\u6545\u969c\u51cf\u5c1147.4%\uff0cC/C++\u4efb\u52a1\u51cf\u5c1128.2%\u3002", "conclusion": "SEMAP\u901a\u8fc7\u5f15\u5165\u8f6f\u4ef6\u5de5\u7a0b\u7ed3\u6784\u5316\u539f\u5219\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.12186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.12186", "abs": "https://arxiv.org/abs/2510.12186", "authors": ["Yun Peng", "Kisub Kim", "Linghan Meng", "Kui Liu"], "title": "iCodeReviewer: Improving Secure Code Review with Mixture of Prompts", "comment": null, "summary": "Code review is an essential process to ensure the quality of software that\nidentifies potential software issues at an early stage of software development.\nAmong all software issues, security issues are the most important to identify,\nas they can easily lead to severe software crashes and service disruptions.\nRecent research efforts have been devoted to automated approaches to reduce the\nmanual efforts required in the secure code review process. Despite the\nprogress, current automated approaches on secure code review, including static\nanalysis, deep learning models, and prompting approaches, still face the\nchallenges of limited precision and coverage, and a lack of comprehensive\nevaluation.\n  To mitigate these challenges, we propose iCodeReviewer, which is an automated\nsecure code review approach based on large language models (LLMs).\niCodeReviewer leverages a novel mixture-of-prompts architecture that\nincorporates many prompt experts to improve the coverage of security issues.\nEach prompt expert is a dynamic prompt pipeline to check the existence of a\nspecific security issue. iCodeReviewer also implements an effective routing\nalgorithm to activate only necessary prompt experts based on the code features\nin the input program, reducing the false positives induced by LLM\nhallucination. Experiment results in our internal dataset demonstrate the\neffectiveness of iCodeReviewer in security issue identification and\nlocalization with an F1 of 63.98%. The review comments generated by\niCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed\nin production environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86iCodeReviewer\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5b89\u5168\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u63d0\u793a\u67b6\u6784\u63d0\u9ad8\u5b89\u5168\u95ee\u9898\u7684\u8986\u76d6\u7387\uff0c\u5e76\u51cf\u5c11\u8bef\u62a5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u5b89\u5168\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\uff08\u9759\u6001\u5206\u6790\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3001\u63d0\u793a\u65b9\u6cd5\uff09\u5b58\u5728\u7cbe\u5ea6\u548c\u8986\u76d6\u7387\u6709\u9650\u3001\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6df7\u5408\u63d0\u793a\u67b6\u6784\uff0c\u5305\u542b\u591a\u4e2a\u52a8\u6001\u63d0\u793a\u4e13\u5bb6\u6765\u68c0\u67e5\u7279\u5b9a\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u6709\u6548\u7684\u8def\u7531\u7b97\u6cd5\u57fa\u4e8e\u4ee3\u7801\u7279\u5f81\u6fc0\u6d3b\u5fc5\u8981\u4e13\u5bb6\u3002", "result": "\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u5b9e\u73b063.98%\u7684F1\u5206\u6570\uff0c\u751f\u4ea7\u73af\u5883\u4e2d\u751f\u6210\u7684\u5ba1\u67e5\u8bc4\u8bba\u63a5\u53d7\u7387\u9ad8\u8fbe84%\u3002", "conclusion": "iCodeReviewer\u5728\u5b89\u5168\u95ee\u9898\u7684\u8bc6\u522b\u548c\u5b9a\u4f4d\u65b9\u9762\u6709\u6548\uff0c\u751f\u6210\u7684\u5ba1\u67e5\u8bc4\u8bba\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u9ad8\u63a5\u53d7\u7387\u3002", "topic": "swe application"}}
{"id": "2510.12294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.12294", "abs": "https://arxiv.org/abs/2510.12294", "authors": ["Gerg\u0151 Balogh", "D\u00e1vid K\u00f3sz\u00f3", "Homayoun Safarpour Motealegh Mahalegi", "L\u00e1szl\u00f3 T\u00f3th", "Bence Szak\u00e1cs", "\u00c1ron B\u00facs\u00fa"], "title": "Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening", "comment": "preprint of a paper under publication in Quality of Information and\n  Communications Technology 2025", "summary": "Understanding how software developers think, make decisions, and behave\nremains a key challenge in software engineering (SE). Verbalization techniques\n(methods that capture spoken or written thought processes) offer a lightweight\nand accessible way to study these cognitive aspects. This paper presents a\nscoping review of research at the intersection of SE and psychology (PSY),\nfocusing on the use of verbal data. To make large-scale interdisciplinary\nreviews feasible, we employed a large language model (LLM)-assisted screening\npipeline using GPT to assess the relevance of over 9,000 papers based solely on\ntitles. We addressed two questions: what themes emerge from\nverbalization-related work in SE, and how effective are LLMs in supporting\ninterdisciplinary review processes? We validated GPT's outputs against human\nreviewers and found high consistency, with a 13\\% disagreement rate. Prominent\nthemes mainly were tied to the craft of SE, while more human-centered topics\nwere underrepresented. The data also suggests that SE frequently draws on PSY\nmethods, whereas the reverse is rare.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7LLM\u8f85\u52a9\u7b5b\u90099000\u591a\u7bc7\u8bba\u6587\uff0c\u7814\u7a76\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e0e\u5fc3\u7406\u5b66\u4ea4\u53c9\u9886\u57df\u4e2d\u4f7f\u7528\u8a00\u8bed\u6570\u636e\u7684\u7814\u7a76\u4e3b\u9898\uff0c\u53d1\u73b0SE\u4e3b\u8981\u501f\u9274PSY\u65b9\u6cd5\uff0c\u800c\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u9898\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "motivation": "\u7406\u89e3\u8f6f\u4ef6\u5f00\u53d1\u8005\u7684\u601d\u7ef4\u3001\u51b3\u7b56\u548c\u884c\u4e3a\u662f\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5173\u952e\u6311\u6218\uff0c\u8a00\u8bed\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u7814\u7a76\u8fd9\u4e9b\u8ba4\u77e5\u65b9\u9762\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GPT\u6784\u5efaLLM\u8f85\u52a9\u7b5b\u9009\u7ba1\u9053\uff0c\u57fa\u4e8e\u6807\u9898\u8bc4\u4f309000\u591a\u7bc7\u8bba\u6587\u7684\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1GPT\u8f93\u51fa\u4e0e\u4eba\u5de5\u8bc4\u5ba1\u7684\u4e00\u81f4\u6027\u3002", "result": "GPT\u4e0e\u4eba\u5de5\u8bc4\u5ba1\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5206\u6b67\u7387\u4e3a13%\u3002\u4e3b\u8981\u4e3b\u9898\u4e0eSE\u5de5\u827a\u76f8\u5173\uff0c\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u9898\u4ee3\u8868\u6027\u4e0d\u8db3\uff0cSE\u9891\u7e41\u501f\u9274PSY\u65b9\u6cd5\u800c\u53cd\u5411\u7f55\u89c1\u3002", "conclusion": "LLM\u53ef\u4ee5\u6709\u6548\u652f\u6301\u8de8\u5b66\u79d1\u8bc4\u5ba1\u8fc7\u7a0b\uff0cSE\u4e0ePSY\u7684\u4ea4\u53c9\u7814\u7a76\u5b58\u5728\u4e0d\u5e73\u8861\u73b0\u8c61\u3002", "topic": "agent analysis"}}
{"id": "2510.12047", "categories": ["cs.AI", "cs.SE", "68T01", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.12047", "abs": "https://arxiv.org/abs/2510.12047", "authors": ["Soohan Lim", "Joonghyuk Hahn", "Hyunwoo Park", "Sang-Ki Ko", "Yo-Sub Han"], "title": "Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation", "comment": "21 pages, 12 figures, 3 tables", "summary": "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,\nprimarily evaluate large language models (LLMs) with pass@k on functional\ncorrectness using well-formed inputs. However, they ignore a crucial aspect of\nreal-world software: adherence to contracts-the preconditions and validity\nconstraints that dictate how ill-formed inputs must be rejected. This critical\noversight means that existing benchmarks fail to measure, and models\nconsequently fail to generate, truly robust and reliable code snippets. We\nintroduce PACT, a program assessment and contract-adherence evaluation\nframework, to bridge this gap. PACT is the first framework designed to\nsystematically evaluate and enhance contract-adherence in LLM-generated code\nsnippets alongside functional correctness. PACT's contributions are threefold:\nFirst, it provides a comprehensive test-suite corpus focused on contract\nviolations, extending HumanEval+ and MBPP+. Second, it enables a systematic\nanalysis of code generation under varied prompting conditions. This analysis\ndemonstrates that augmenting prompts with contract-violating test cases\nsignificantly enhance a model's ability to respect contracts compared to using\ncontract description alone. Finally, it introduces novel metrics to rigorously\nquantify contract adherence in both test generation and code generation. By\nrevealing critical errors that conventional benchmarks overlook, PACT provides\nthe rigorous and interpretable metrics to evaluate the robustness of\nLLM-generated code snippets in both functionality and contract-adherence.Our\ncode and data are available at https://github.com/suhanmen/PACT.", "AI": {"tldr": "PACT\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u5408\u7ea6\u9075\u4ece\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55HumanEval+\u548cMBPP+\u57fa\u51c6\uff0c\u5f15\u5165\u5408\u7ea6\u8fdd\u53cd\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u5408\u7ea6\u7684\u9075\u4ece\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u771f\u5b9e\u8f6f\u4ef6\u4e2d\u5408\u7ea6\u9075\u4ece\u6027\u8fd9\u4e00\u5173\u952e\u65b9\u9762\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8861\u91cf\u4ee3\u7801\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6269\u5c55HumanEval+\u548cMBPP+\u57fa\u51c6\uff0c\u6784\u5efa\u4e13\u6ce8\u4e8e\u5408\u7ea6\u8fdd\u53cd\u7684\u6d4b\u8bd5\u5957\u4ef6\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u7cfb\u7edf\u5206\u6790\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u91cf\u5316\u6307\u6807\u3002", "result": "\u5728\u63d0\u793a\u4e2d\u52a0\u5165\u5408\u7ea6\u8fdd\u53cd\u6d4b\u8bd5\u7528\u4f8b\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5408\u7ea6\u63cf\u8ff0\uff0c\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5bf9\u5408\u7ea6\u7684\u9075\u4ece\u80fd\u529b\u3002", "conclusion": "PACT\u901a\u8fc7\u63ed\u793a\u4f20\u7edf\u57fa\u51c6\u5ffd\u7565\u7684\u5173\u952e\u9519\u8bef\uff0c\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u5728\u529f\u80fd\u548c\u5408\u7ea6\u9075\u4ece\u6027\u65b9\u9762\u9c81\u68d2\u6027\u7684\u4e25\u683c\u53ef\u89e3\u91ca\u6307\u6807\u3002", "topic": "swe benchmark"}}
{"id": "2510.12063", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12063", "abs": "https://arxiv.org/abs/2510.12063", "authors": ["Sunzhu Li", "Zhiyu Lin", "Shuling Yang", "Jiale Zhao", "Wei Chen"], "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization", "comment": null, "summary": "Large Reasoning Models (LRMs) are powerful, but they still suffer from\ninefficient and off-target reasoning. Currently, training-free methods are\nlimited to either rigid heuristics or descriptive, non-actionable analyses. In\nthis paper, we introduce ThinkPilot, a training-free framework that\nautomatically optimizes LRMs reasoning. It uses an evolutionary process to\ngenerate think-prefixes, which are instructions that evolve driven by a\ntaxonomy of reasoning behaviors to guide models toward superior performance.\nExtensive experiments demonstrate ThinkPilot's broad effectiveness: it\nsignificantly improves the accuracy-length trade-off for efficient reasoning,\ndrastically improves safety (for example, cutting the StrongREJECT score of\nDeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction\nfollowing. It also synergizes with existing training-based methods. Our\nanalysis reveals that think-prefixes can reliably control LRMs' reasoning\nbehaviors, and that different tasks have strong preferences for specific\nbehavioral distributions. By automatically identifying and eliciting these\nbehaviors, ThinkPilot provides a generalizable framework for aligning LRMs\nreasoning with task demands. Data and code are available at\nhttps://github.com/teqkilla/ThinkPilot", "AI": {"tldr": "ThinkPilot\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u81ea\u52a8\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u63a8\u7406\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u8fc7\u7a0b\u751f\u6210think-prefixes\u6765\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u548c\u504f\u79bb\u76ee\u6807\u7684\u95ee\u9898\uff0c\u800c\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u521a\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8981\u4e48\u53ea\u80fd\u63d0\u4f9b\u63cf\u8ff0\u6027\u3001\u975e\u53ef\u64cd\u4f5c\u7684\u5206\u6790\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u8fc7\u7a0b\u751f\u6210think-prefixes\uff0c\u8fd9\u4e9b\u6307\u4ee4\u57fa\u4e8e\u63a8\u7406\u884c\u4e3a\u5206\u7c7b\u5b66\u9a71\u52a8\u8fdb\u5316\uff0c\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "result": "\u663e\u8457\u6539\u5584\u4e86\u51c6\u786e\u7387-\u957f\u5ea6\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff08\u5982\u5c06DeepSeek-R1-Distill-Qwen-32B\u7684StrongREJECT\u5206\u6570\u4ece27.0%\u964d\u81f30.7\uff09\uff0c\u589e\u5f3a\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u534f\u540c\u5de5\u4f5c\u3002", "conclusion": "think-prefixes\u80fd\u591f\u53ef\u9760\u5730\u63a7\u5236LRMs\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u4e0d\u540c\u4efb\u52a1\u5bf9\u7279\u5b9a\u884c\u4e3a\u5206\u5e03\u6709\u5f3a\u70c8\u504f\u597d\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u548c\u6fc0\u53d1\u8fd9\u4e9b\u884c\u4e3a\uff0cThinkPiot\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u6765\u4f7fLRMs\u63a8\u7406\u4e0e\u4efb\u52a1\u9700\u6c42\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2510.12487", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12487", "abs": "https://arxiv.org/abs/2510.12487", "authors": ["Evgeniy Glukhov", "Michele Conti", "Egor Bogomolov", "Yaroslav Golubev", "Alexander Bezzubov"], "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding", "comment": null, "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.", "AI": {"tldr": "Diff-XYZ\u662f\u4e00\u4e2a\u7528\u4e8e\u4ee3\u7801\u5dee\u5f02\u7406\u89e3\u7684\u7d27\u51d1\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u76d1\u7763\u4efb\u52a1\uff1a\u5e94\u7528\u5dee\u5f02\u3001\u53cd\u5e94\u7528\u5dee\u5f02\u548c\u5dee\u5f02\u751f\u6210\uff0c\u57fa\u4e8e\u771f\u5b9e\u63d0\u4ea4\u6570\u636e\u6784\u5efa\u3002", "motivation": "\u53ef\u9760\u7684\u4ee3\u7801\u5dee\u5f02\u5904\u7406\u5bf9\u4e8e\u5927\u89c4\u6a21\u7f16\u8f91\u548c\u91cd\u6784\u4ee3\u7801\u5e93\u7684\u667a\u80fd\u4f53\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5dee\u5f02\u683c\u5f0f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4eceCommitPackFT\u7684\u771f\u5b9e\u63d0\u4ea4\u4e2d\u63d0\u53d6<\u65e7\u4ee3\u7801,\u65b0\u4ee3\u7801,\u5dee\u5f02>\u4e09\u5143\u7ec4\uff0c\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u81ea\u52a8\u6307\u6807\u548c\u6e05\u6670\u8bc4\u4f30\u534f\u8bae\u8fdb\u884c\u8de8\u683c\u5f0f\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u683c\u5f0f\u5e94\u6839\u636e\u4f7f\u7528\u573a\u666f\u548c\u6a21\u578b\u5927\u5c0f\u9009\u62e9\uff0c\u4f8b\u5982\u641c\u7d22\u66ff\u6362\u683c\u5f0f\u9002\u5408\u5927\u6a21\u578b\u751f\u6210\u5dee\u5f02\uff0c\u4f46\u4e0d\u9002\u5408\u5dee\u5f02\u5206\u6790\u548c\u5c0f\u6a21\u578b\u3002", "conclusion": "Diff-XYZ\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u4e2d\u7684\u5dee\u5f02\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5dee\u5f02\u683c\u5f0f\u548c\u4ee3\u7801\u7f16\u8f91\u6a21\u578b\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2510.12066", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12066", "abs": "https://arxiv.org/abs/2510.12066", "authors": ["Alessandro Achille", "Stefano Soatto"], "title": "AI Agents as Universal Task Solvers", "comment": null, "summary": "AI reasoning agents are already able to solve a variety of tasks by deploying\ntools, simulating outcomes of multiple hypotheses and reflecting on them. In\ndoing so, they perform computation, although not in the classical sense --\nthere is no program being executed. Still, if they perform computation, can AI\nagents be universal? Can chain-of-thought reasoning solve any computable task?\nHow does an AI Agent learn to reason? Is it a matter of model size? Or training\ndataset size?\n  In this work, we reinterpret the role of learning in the context of AI\nAgents, viewing them as compute-capable stochastic dynamical systems, and\nhighlight the role of time in a foundational principle for learning to reason.\nIn doing so, we propose a shift from classical inductive learning to\ntransductive learning -- where the objective is not to approximate the\ndistribution of past data, but to capture their algorithmic structure to reduce\nthe time needed to find solutions to new tasks.\n  Transductive learning suggests that, counter to Shannon's theory, a key role\nof information in learning is about reduction of time rather than\nreconstruction error. In particular, we show that the optimal speed-up that a\nuniversal solver can achieve using past data is tightly related to their\nalgorithmic information. Using this, we show a theoretical derivation for the\nobserved power-law scaling of inference time versus training time. We then show\nthat scaling model size can lead to behaviors that, while improving accuracy on\nbenchmarks, fail any reasonable test of intelligence, let alone\nsuper-intelligence: In the limit of infinite space and time, large models can\nbehave as savants, able to brute-force through any task without any insight.\nInstead, we argue that the key quantity to optimize when scaling reasoning\nmodels is time, whose critical role in learning has so far only been indirectly\nconsidered.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8be0\u91ca\u4e86AI\u667a\u80fd\u4f53\u4e2d\u7684\u5b66\u4e60\u89d2\u8272\uff0c\u5c06\u5176\u89c6\u4e3a\u5177\u6709\u8ba1\u7b97\u80fd\u529b\u7684\u968f\u673a\u52a8\u529b\u7cfb\u7edf\uff0c\u5f3a\u8c03\u65f6\u95f4\u5728\u5b66\u4e60\u63a8\u7406\u4e2d\u7684\u57fa\u7840\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4ece\u5f52\u7eb3\u5b66\u4e60\u8f6c\u5411\u8f6c\u5bfc\u5b66\u4e60\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u63a2\u8ba8AI\u63a8\u7406\u667a\u80fd\u4f53\u662f\u5426\u5177\u6709\u901a\u7528\u6027\uff0c\u80fd\u5426\u89e3\u51b3\u4efb\u4f55\u53ef\u8ba1\u7b97\u4efb\u52a1\uff0c\u4ee5\u53ca\u5b66\u4e60\u63a8\u7406\u7684\u5173\u952e\u56e0\u7d20\u662f\u4ec0\u4e48\u2014\u2014\u662f\u6a21\u578b\u89c4\u6a21\u8fd8\u662f\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3002", "method": "\u5c06AI\u667a\u80fd\u4f53\u91cd\u65b0\u89e3\u91ca\u4e3a\u8ba1\u7b97\u80fd\u529b\u7684\u968f\u673a\u52a8\u529b\u7cfb\u7edf\uff0c\u63d0\u51fa\u8f6c\u5bfc\u5b66\u4e60\u8303\u5f0f\uff0c\u5173\u6ce8\u7b97\u6cd5\u7ed3\u6784\u800c\u975e\u6570\u636e\u5206\u5e03\uff0c\u5e76\u5206\u6790\u65f6\u95f4\u5728\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "result": "\u8bc1\u660e\u4e86\u901a\u7528\u6c42\u89e3\u5668\u4f7f\u7528\u8fc7\u53bb\u6570\u636e\u5b9e\u73b0\u7684\u6700\u4f18\u52a0\u901f\u4e0e\u5176\u7b97\u6cd5\u4fe1\u606f\u7d27\u5bc6\u76f8\u5173\uff0c\u63a8\u5bfc\u51fa\u63a8\u7406\u65f6\u95f4\u4e0e\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u53ef\u80fd\u5bfc\u81f4\u667a\u80fd\u4f53\u884c\u4e3a\u50cf\u4e13\u5bb6\u4f46\u7f3a\u4e4f\u771f\u6b63\u667a\u80fd\u3002", "conclusion": "\u5728\u6269\u5c55\u63a8\u7406\u6a21\u578b\u65f6\uff0c\u5e94\u8be5\u4f18\u5316\u7684\u5173\u952e\u91cf\u662f\u65f6\u95f4\u800c\u975e\u51c6\u786e\u6027\uff0c\u65f6\u95f4\u5728\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u81f3\u4eca\u53ea\u88ab\u95f4\u63a5\u8003\u8651\u3002", "topic": "agent analysis"}}
{"id": "2510.11967", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11967", "abs": "https://arxiv.org/abs/2510.11967", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "title": "Scaling Long-Horizon LLM Agent via Context-Folding", "comment": null, "summary": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10$\\times$\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.", "AI": {"tldr": "\u63d0\u51fa\u4e86Context-Folding\u6846\u67b6\uff0c\u8ba9LLM\u4ee3\u7406\u80fd\u591f\u4e3b\u52a8\u7ba1\u7406\u5de5\u4f5c\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5206\u652f\u5904\u7406\u5b50\u4efb\u52a1\u5e76\u5728\u5b8c\u6210\u540e\u6298\u53e0\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4ec5\u4fdd\u7559\u7ed3\u679c\u6458\u8981\uff0c\u4ece\u800c\u89e3\u51b3\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u53d7\u5230\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u6839\u672c\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7ba1\u7406\u4e0a\u4e0b\u6587\u3002", "method": "\u5f00\u53d1\u4e86Context-Folding\u6846\u67b6\u548c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6FoldGRPO\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u9f13\u52b1\u6709\u6548\u7684\u4efb\u52a1\u5206\u89e3\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u4ee3\u7406\u53ef\u4ee5\u7a0b\u5e8f\u6027\u5730\u5206\u652f\u5230\u5b50\u8f68\u8ff9\u5904\u7406\u5b50\u4efb\u52a1\uff0c\u5b8c\u6210\u540e\u6298\u53e0\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728\u590d\u6742\u957f\u89c6\u91ce\u4efb\u52a1\uff08Deep Research\u548cSWE\uff09\u4e0a\uff0c\u6298\u53e0\u4ee3\u7406\u5339\u914d\u6216\u4f18\u4e8eReAct\u57fa\u7ebf\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u6d3b\u52a8\u4e0a\u4e0b\u6587\u5c0f10\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u4f9d\u8d56\u57fa\u4e8e\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u6a21\u578b\u3002", "conclusion": "Context-Folding\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u4e3b\u52a8\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.11997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11997", "abs": "https://arxiv.org/abs/2510.11997", "authors": ["Ryan Shea", "Yunan Lu", "Liang Qiu", "Zhou Yu"], "title": "SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation", "comment": null, "summary": "Evaluating multi-turn interactive agents is challenging due to the need for\nhuman assessment. Evaluation with simulated users has been introduced as an\nalternative, however existing approaches typically model generic users and\noverlook the domain-specific principles required to capture realistic behavior.\nWe propose SAGE, a novel user Simulation framework for multi-turn AGent\nEvaluation that integrates knowledge from business contexts. SAGE incorporates\ntop-down knowledge rooted in business logic, such as ideal customer profiles,\ngrounding user behavior in realistic customer personas. We further integrate\nbottom-up knowledge taken from business agent infrastructure (e.g., product\ncatalogs, FAQs, and knowledge bases), allowing the simulator to generate\ninteractions that reflect users' information needs and expectations in a\ncompany's target market. Through empirical evaluation, we find that this\napproach produces interactions that are more realistic and diverse, while also\nidentifying up to 33% more agent errors, highlighting its effectiveness as an\nevaluation tool to support bug-finding and iterative agent improvement.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u7528\u6237\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e1a\u52a1\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08\u5982\u5ba2\u6237\u753b\u50cf\u548c\u4ea7\u54c1\u76ee\u5f55\uff09\u6765\u751f\u6210\u66f4\u771f\u5b9e\u3001\u591a\u6837\u7684\u4ea4\u4e92\uff0c\u80fd\u53d1\u73b033%\u4ee5\u4e0a\u7684\u667a\u80fd\u4f53\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8f6e\u4ea4\u4e92\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\u6216\u901a\u7528\u7528\u6237\u6a21\u62df\uff0c\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u3002", "method": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u6574\u5408\u81ea\u4e0a\u800c\u4e0b\u7684\u4e1a\u52a1\u903b\u8f91\u77e5\u8bc6\uff08\u7406\u60f3\u5ba2\u6237\u753b\u50cf\uff09\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u4e1a\u52a1\u57fa\u7840\u8bbe\u65bd\u77e5\u8bc6\uff08\u4ea7\u54c1\u76ee\u5f55\u3001FAQ\u7b49\uff09\uff0c\u6a21\u62df\u771f\u5b9e\u5ba2\u6237\u884c\u4e3a\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u4ea4\u4e92\u66f4\u771f\u5b9e\u3001\u591a\u6837\uff0c\u80fd\u8bc6\u522b\u51fa33%\u4ee5\u4e0a\u7684\u667a\u80fd\u4f53\u9519\u8bef\uff0c\u6709\u6548\u652f\u6301\u9519\u8bef\u53d1\u73b0\u548c\u667a\u80fd\u4f53\u8fed\u4ee3\u6539\u8fdb\u3002", "conclusion": "SAGE\u901a\u8fc7\u6574\u5408\u4e1a\u52a1\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u4ea4\u4e92\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.11899", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11899", "abs": "https://arxiv.org/abs/2510.11899", "authors": ["Chenliang Li", "Junyu Leng", "Jiaxiang Li", "Youbang Sun", "Shixiang Chen", "Shahin Shahrampour", "Alfredo Garcia"], "title": "ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty", "comment": null, "summary": "Robust reinforcement learning (Robust RL) seeks to handle epistemic\nuncertainty in environment dynamics, but existing approaches often rely on\nnested min--max optimization, which is computationally expensive and yields\noverly conservative policies. We propose \\textbf{Adaptive Rank Representation\n(AdaRL)}, a bi-level optimization framework that improves robustness by\naligning policy complexity with the intrinsic dimension of the task. At the\nlower level, AdaRL performs policy optimization under fixed-rank constraints\nwith dynamics sampled from a Wasserstein ball around a centroid model. At the\nupper level, it adaptively adjusts the rank to balance the bias--variance\ntrade-off, projecting policy parameters onto a low-rank manifold. This design\navoids solving adversarial worst-case dynamics while ensuring robustness\nwithout over-parameterization. Empirical results on MuJoCo continuous control\nbenchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank\nbaselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,\nParseval), but also converges toward the intrinsic rank of the underlying\ntasks. These results highlight that adaptive low-rank policy representations\nprovide an efficient and principled alternative for robust RL under model\nuncertainty.", "AI": {"tldr": "AdaRL\u662f\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u590d\u6742\u5ea6\u4e0e\u4efb\u52a1\u5185\u5728\u7ef4\u5ea6\u5bf9\u9f50\u6765\u6539\u8fdb\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\uff0c\u907f\u514d\u4f20\u7edfmin-max\u4f18\u5316\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u4fdd\u5b88\u7b56\u7565\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5d4c\u5957min-max\u4f18\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4ea7\u751f\u8fc7\u4e8e\u4fdd\u5b88\u7684\u7b56\u7565\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9c81\u68d2\u6027\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u4e0b\u5c42\u5728\u56fa\u5b9a\u79e9\u7ea6\u675f\u4e0b\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u4eceWasserstein\u7403\u4e2d\u91c7\u6837\u52a8\u6001\uff1b\u4e0a\u5c42\u81ea\u9002\u5e94\u8c03\u6574\u79e9\u4ee5\u5e73\u8861\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u5c06\u7b56\u7565\u53c2\u6570\u6295\u5f71\u5230\u4f4e\u79e9\u6d41\u5f62\u4e0a\u3002", "result": "\u5728MuJoCo\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaRL\u4e0d\u4ec5\u4f18\u4e8e\u56fa\u5b9a\u79e9\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u9c81\u68d2RL\u65b9\u6cd5\uff0c\u800c\u4e14\u6536\u655b\u5230\u4efb\u52a1\u7684\u5185\u5728\u79e9\u3002", "conclusion": "\u81ea\u9002\u5e94\u4f4e\u79e9\u7b56\u7565\u8868\u793a\u4e3a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9c81\u68d2RL\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12702", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.12702", "abs": "https://arxiv.org/abs/2510.12702", "authors": ["Cedric Richter", "Heike Wehrheim"], "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?", "comment": "under submission", "summary": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs.", "AI": {"tldr": "NL2Contract\u4efb\u52a1\u4f7f\u7528LLM\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u5305\u542b\u524d\u7f6e\u6761\u4ef6\u548c\u540e\u7f6e\u6761\u4ef6\u7684\u6b63\u5f0f\u529f\u80fd\u5951\u7ea6\uff0c\u76f8\u6bd4\u4ec5\u751f\u6210\u540e\u7f6e\u6761\u4ef6\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u9a8c\u8bc1\u5668\u4ea7\u751f\u7684\u8bef\u62a5\u3002", "motivation": "\u81ea\u52a8\u8f6f\u4ef6\u9a8c\u8bc1\u5668\u5728\u5b9e\u8df5\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u56e0\u4e3a\u771f\u5b9e\u4ee3\u7801\u4e2d\u7f3a\u4e4f\u5f62\u5f0f\u5316\u89c4\u8303\u3002LLM\u867d\u7136\u80fd\u4ece\u4ee3\u7801\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u63a8\u65ad\u540e\u7f6e\u6761\u4ef6\uff0c\u4f46\u4ec5\u4f7f\u7528\u540e\u7f6e\u6761\u4ef6\u8fdb\u884c\u9a8c\u8bc1\u4f1a\u4ea7\u751f\u8bef\u62a5\u3002", "method": "\u5f15\u5165NL2Contract\u4efb\u52a1\uff0c\u4f7f\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5305\u542b\u524d\u7f6e\u6761\u4ef6\u548c\u540e\u7f6e\u6761\u4ef6\u7684\u6b63\u5f0f\u529f\u80fd\u5951\u7ea6\u3002\u901a\u8fc7\u58f0\u97f3\u6027\u3001\u9519\u8bef\u5224\u522b\u80fd\u529b\u548c\u9a8c\u8bc1\u53ef\u7528\u6027\u7b49\u6307\u6807\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u3002", "result": "LLM\u80fd\u6709\u6548\u751f\u6210\u5bf9\u6240\u6709\u53ef\u80fd\u8f93\u5165\u90fd\u5408\u7406\u7684\u529f\u80fd\u5951\u7ea6\uff1b\u751f\u6210\u7684\u5951\u7ea6\u80fd\u6709\u6548\u533a\u5206\u9519\u8bef\u548c\u6b63\u786e\u884c\u4e3a\uff1b\u4f7f\u7528\u529f\u80fd\u5951\u7ea6\u6bd4\u4ec5\u4f7f\u7528\u540e\u7f6e\u6761\u4ef6\u4ea7\u751f\u7684\u8bef\u62a5\u66f4\u5c11\u3002", "conclusion": "LLM\u63a8\u65ad\u7684\u524d\u7f6e\u6761\u4ef6\u4e0e\u5f00\u53d1\u8005\u610f\u56fe\u4e00\u81f4\uff0c\u4f7f\u5f97\u81ea\u52a8\u8f6f\u4ef6\u9a8c\u8bc1\u5668\u80fd\u591f\u6355\u83b7\u771f\u5b9e\u4e16\u754c\u7684\u9519\u8bef\u3002", "topic": "swe application"}}
{"id": "2510.12080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12080", "abs": "https://arxiv.org/abs/2510.12080", "authors": ["Rabimba Karanjai", "Yang Lu", "Ranjith Chodavarapu", "Lei Xu", "Weidong Shi"], "title": "Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models", "comment": null, "summary": "The rapid advancement of large language model (LLM) technology has led to\ndiverse applications, many of which inherently require randomness, such as\nstochastic decision-making, gaming, scheduling, AI agents, and\ncryptography-related tasks. However, the capabilities of LLMs in handling\nrandomness, particularly in generating and utilizing random numbers\neffectively, remain unclear. This paper investigates the capacity of LLMs for\nhandling tasks that involve randomness through a series of experiments. We\ndesigned a set of experiments that consider various factors that can influence\nan LLM's performance in tasks involving randomness, such as accessibility to\nexternal tools, types of tasks, model states (fresh vs. non-fresh), and\nprompting strategies. The experiments cover a range of tasks, including\ngenerating random numbers, generating random strings such as passwords,\nshuffling items, and evaluating the quality of randomness using entropy and the\nNIST randomness test-suite. Our findings reveal that while LLMs can generate\noutputs that exhibit some degree of randomness, their performance is\ninconsistent and often deviates significantly from the expected behavior. The\nanalysis of the experimental results highlights key limitations and areas where\nimprovement is needed for the LLMs to effectively handle tasks involving\nrandomness", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u968f\u673a\u6027\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136LLM\u80fd\u751f\u6210\u5177\u6709\u4e00\u5b9a\u968f\u673a\u6027\u7684\u8f93\u51fa\uff0c\u4f46\u8868\u73b0\u4e0d\u4e00\u81f4\u4e14\u4e0e\u9884\u671f\u884c\u4e3a\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002", "motivation": "\u968f\u7740LLM\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bb8\u591a\u5e94\u7528\uff08\u5982\u968f\u673a\u51b3\u7b56\u3001\u6e38\u620f\u3001\u8c03\u5ea6\u3001AI\u4ee3\u7406\u548c\u5bc6\u7801\u5b66\u4efb\u52a1\uff09\u90fd\u9700\u8981\u968f\u673a\u6027\uff0c\u4f46LLM\u5728\u5904\u7406\u968f\u673a\u6027\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u8003\u8651\u5916\u90e8\u5de5\u5177\u53ef\u8bbf\u95ee\u6027\u3001\u4efb\u52a1\u7c7b\u578b\u3001\u6a21\u578b\u72b6\u6001\uff08\u65b0\u9c9cvs\u975e\u65b0\u9c9c\uff09\u548c\u63d0\u793a\u7b56\u7565\u7b49\u56e0\u7d20\uff0c\u6db5\u76d6\u751f\u6210\u968f\u673a\u6570\u3001\u968f\u673a\u5b57\u7b26\u4e32\u3001\u6d17\u724c\u9879\u76ee\u4ee5\u53ca\u4f7f\u7528\u71b5\u548cNIST\u968f\u673a\u6027\u6d4b\u8bd5\u5957\u4ef6\u8bc4\u4f30\u968f\u673a\u6027\u8d28\u91cf\u7b49\u4efb\u52a1\u3002", "result": "LLM\u80fd\u591f\u751f\u6210\u8868\u73b0\u51fa\u4e00\u5b9a\u968f\u673a\u6027\u7684\u8f93\u51fa\uff0c\u4f46\u6027\u80fd\u4e0d\u4e00\u81f4\uff0c\u4e14\u7ecf\u5e38\u663e\u8457\u504f\u79bb\u9884\u671f\u884c\u4e3a\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLLM\u5728\u5904\u7406\u6d89\u53ca\u968f\u673a\u6027\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u9700\u8981\u5728\u8fd9\u4e9b\u65b9\u9762\u8fdb\u884c\u6539\u8fdb\u624d\u80fd\u6709\u6548\u5904\u7406\u968f\u673a\u6027\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2510.12088", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12088", "abs": "https://arxiv.org/abs/2510.12088", "authors": ["Zaid Khan", "Archiki Prasad", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration", "comment": "Project page: https://onelife-worldmodel.github.io/; 39 pages", "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.", "AI": {"tldr": "OneLife\u6846\u67b6\u901a\u8fc7\u6982\u7387\u7f16\u7a0b\u548c\u6761\u4ef6\u6fc0\u6d3b\u7684\u7a0b\u5e8f\u5316\u6cd5\u5219\u5b66\u4e60\u590d\u6742\u968f\u673a\u73af\u5883\u7684\u4e16\u754c\u52a8\u6001\uff0c\u5728\u5355\u6b21\u63a2\u7d22\u7684\u82db\u523b\u6761\u4ef6\u4e0b\u6210\u529f\u5b66\u4e60\u5173\u952e\u73af\u5883\u52a8\u6001\uff0c\u5e76\u5728\u72b6\u6001\u6392\u5e8f\u548c\u72b6\u6001\u4fdd\u771f\u5ea6\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u968f\u673a\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u53ea\u6709\"\u4e00\u6b21\u751f\u547d\"\u6765\u63a2\u7d22\u654c\u5bf9\u73af\u5883\u4e14\u65e0\u4eba\u6307\u5bfc\u7684\u6311\u6218\u6027\u573a\u666f\uff0c\u81ea\u4e3b\u6784\u5efa\u7a0b\u5e8f\u5316\u4e16\u754c\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6fc0\u6d3b\u7684\u7a0b\u5e8f\u5316\u6cd5\u5219\u5728\u6982\u7387\u7f16\u7a0b\u6846\u67b6\u4e2d\u5efa\u6a21\u4e16\u754c\u52a8\u6001\uff0c\u6bcf\u4e2a\u6cd5\u5219\u91c7\u7528\u524d\u63d0-\u6548\u679c\u7ed3\u6784\uff0c\u5728\u76f8\u5173\u4e16\u754c\u72b6\u6001\u4e2d\u6fc0\u6d3b\uff0c\u521b\u5efa\u52a8\u6001\u8ba1\u7b97\u56fe\u4ee5\u907f\u514d\u6269\u5c55\u6311\u6218\u3002", "result": "\u5728Crafter-OO\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cOneLife\u572823\u4e2a\u573a\u666f\u4e2d\u768416\u4e2a\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u80fd\u591f\u4ece\u6700\u5c11\u65e0\u6307\u5bfc\u4ea4\u4e92\u4e2d\u6210\u529f\u5b66\u4e60\u5173\u952e\u73af\u5883\u52a8\u6001\uff0c\u6a21\u62df\u63a8\u6f14\u6210\u529f\u8bc6\u522b\u51fa\u66f4\u4f18\u7b56\u7565\u3002", "conclusion": "\u4e3a\u81ea\u4e3b\u6784\u5efa\u672a\u77e5\u590d\u6742\u73af\u5883\u7684\u7a0b\u5e8f\u5316\u4e16\u754c\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5728\u82db\u523b\u7ea6\u675f\u4e0b\u5b66\u4e60\u968f\u673a\u52a8\u6001\u7684\u53ef\u884c\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.11933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11933", "abs": "https://arxiv.org/abs/2510.11933", "authors": ["Hiroshi Nonaka", "Simon Ambrozak", "Sofia R. Miskala-Dinc", "Amedeo Ercole", "Aviva Prins"], "title": "Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning", "comment": "This paper contains 19 pages and 3 figures. To be presented at the\n  2nd Workshop on Aligning Reinforcement Learning Experimentalists and\n  Theorists (ARLET 2025) at NeurIPS 2025", "summary": "In this work, we propose three efficient restart paradigms for model-free\nnon-stationary reinforcement learning (RL). We identify two core issues with\nthe restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete\nforgetting, where all the information learned about an environment is lost\nafter a restart, and (2) scheduled restarts, in which restarts occur only at\npredefined timings, regardless of the incompatibility of the policy with the\ncurrent environment dynamics. We introduce three approaches, which we call\npartial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB\nand RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance\nin multiple different environments, decreasing dynamic regret by up to $91$%\nrelative to RestartQ-UCB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e09\u79cd\u9ad8\u6548\u7684\u91cd\u542f\u8303\u5f0f\u7528\u4e8e\u65e0\u6a21\u578b\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86RestartQ-UCB\u7b97\u6cd5\u4e2d\u7684\u5b8c\u5168\u9057\u5fd8\u548c\u56fa\u5b9a\u65f6\u95f4\u91cd\u542f\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u52a8\u6001\u9057\u61be\u3002", "motivation": "\u73b0\u6709RestartQ-UCB\u7b97\u6cd5\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5b8c\u5168\u9057\u5fd8\uff08\u91cd\u542f\u540e\u4e22\u5931\u6240\u6709\u73af\u5883\u4fe1\u606f\uff09\u548c\u56fa\u5b9a\u65f6\u95f4\u91cd\u542f\uff08\u4e0d\u8003\u8651\u5f53\u524d\u73af\u5883\u52a8\u6001\u4e0e\u7b56\u7565\u7684\u517c\u5bb9\u6027\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u7b97\u6cd5\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u4e09\u79cd\u91cd\u542f\u65b9\u6cd5\uff1a\u90e8\u5206\u91cd\u542f\u3001\u81ea\u9002\u5e94\u91cd\u542f\u548c\u9009\u62e9\u6027\u91cd\u542f\uff0c\u7528\u4e8e\u6539\u8fdbRestartQ-UCB\u548cRANDOMIZEDQ\u7b97\u6cd5\uff0c\u901a\u8fc7\u66f4\u667a\u80fd\u7684\u91cd\u542f\u673a\u5236\u6765\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u5728\u591a\u79cd\u4e0d\u540c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u76f8\u5bf9\u4e8eRestartQ-UCB\u5c06\u52a8\u6001\u9057\u61be\u964d\u4f4e\u4e86\u9ad8\u8fbe91%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u79cd\u91cd\u542f\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u91cd\u542f\u8bbe\u8ba1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12194", "abs": "https://arxiv.org/abs/2510.12194", "authors": ["Linyi Yang", "Yixuan Weng"], "title": "ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents", "comment": "EMNLP 2025 Demo, Oral", "summary": "Current deep-research agents run in a ''fire-and-forget'' mode: once started,\nthey give users no way to fix errors or add expert knowledge during execution.\nWe present ResearStudio, the first open-source framework that places real-time\nhuman control at its core. The system follows a Collaborative Workshop design.\nA hierarchical Planner-Executor writes every step to a live\n''plan-as-document,'' a fast communication layer streams each action, file\nchange, and tool call to a web interface. At any moment, the user can pause the\nrun, edit the plan or code, run custom commands, and resume -- switching\nsmoothly between AI-led, human-assisted and human-led, AI-assisted modes. In\nfully autonomous mode, ResearStudio achieves state-of-the-art results on the\nGAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These\nresults show that strong automated performance and fine-grained human control\ncan coexist. The full code, protocol, and evaluation scripts are available at\nhttps://github.com/ResearAI/ResearStudio. We will continue to update the\nrepository to encourage further work on safe and controllable research agents.\nOur live demo is publicly accessible at http://ai-researcher.net:3000/. We\nsupport the development of DeepScientist, which can be accessed at\nhttps://github.com/ResearAI/DeepScientist.", "AI": {"tldr": "ResearStudio\u662f\u9996\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5c06\u5b9e\u65f6\u4eba\u5de5\u63a7\u5236\u7f6e\u4e8e\u6838\u5fc3\uff0c\u901a\u8fc7\u534f\u4f5c\u5de5\u4f5c\u574a\u8bbe\u8ba1\u5b9e\u73b0AI\u4e3b\u5bfc\u3001\u4eba\u5de5\u8f85\u52a9\u548c\u4eba\u5de5\u4e3b\u5bfc\u3001AI\u8f85\u52a9\u6a21\u5f0f\u95f4\u7684\u5e73\u6ed1\u5207\u6362\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4ee5\"\u53d1\u5c04\u540e\u4e0d\u7ba1\"\u6a21\u5f0f\u8fd0\u884c\uff0c\u7528\u6237\u65e0\u6cd5\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u4fee\u6b63\u9519\u8bef\u6216\u6dfb\u52a0\u4e13\u5bb6\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u652f\u6301\u5b9e\u65f6\u4eba\u5de5\u63a7\u5236\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\uff0c\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5199\u5165\u5b9e\u65f6\"\u8ba1\u5212\u5373\u6587\u6863\"\uff0c\u901a\u8fc7\u5feb\u901f\u901a\u4fe1\u5c42\u5c06\u6bcf\u4e2a\u64cd\u4f5c\u3001\u6587\u4ef6\u53d8\u66f4\u548c\u5de5\u5177\u8c03\u7528\u6d41\u5f0f\u4f20\u8f93\u5230Web\u754c\u9762\uff0c\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u6682\u505c\u3001\u7f16\u8f91\u8ba1\u5212\u548c\u4ee3\u7801\u3001\u8fd0\u884c\u81ea\u5b9a\u4e49\u547d\u4ee4\u5e76\u6062\u590d\u6267\u884c\u3002", "result": "\u5728\u5b8c\u5168\u81ea\u4e3b\u6a21\u5f0f\u4e0b\uff0cResearStudio\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e86OpenAI\u7684DeepResearch\u548cManus\u7b49\u7cfb\u7edf\u3002", "conclusion": "\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u6027\u80fd\u548c\u7ec6\u7c92\u5ea6\u4eba\u5de5\u63a7\u5236\u53ef\u4ee5\u5171\u5b58\uff0c\u4e3a\u5b89\u5168\u53ef\u63a7\u7684\u7814\u7a76\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.12218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12218", "abs": "https://arxiv.org/abs/2510.12218", "authors": ["Hyunji Min", "Sangwon Jung", "Junyoung Sung", "Dosung Lee", "Leekyeung Han", "Paul Hongsuck Seo"], "title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools", "comment": "32 pages, 21 figures", "summary": "Large language models (LLMs) have recently been extended beyond traditional\ntext generation to serve as interactive agents capable of using external tools\nbased on user intent. However, current LLM agents still show limited ability to\nhandle goal-oriented queries, which require decomposing a high-level objective\ninto multiple interdependent API calls with correct planning and execution.\nCurrent approaches mainly rely on zero-shot evaluation due to the absence of\ntraining data. While proprietary closed-source models such as GPT-4 demonstrate\nstrong reasoning abilities, smaller open-source models struggle to perform\ncomplex tool use effectively. Thus, we propose a novel training framework GOAT,\nwhich enables fine-tuning of LLM agents in a human annotation-free setting.\nGOAT automatically constructs synthetic datasets of goal-oriented API execution\ntasks directly from given API documents, equipping models with the ability to\nreason over interdependent calls and generate coherent responses. Through\nextensive experiments, we show that GOAT-trained agents achieve\nstate-of-the-art performance across multiple existing goal-oriented benchmarks.\nIn addition, we introduce GOATBench, a new goal-oriented API execution\nbenchmark, and demonstrate that agents trained with GOAT also excel in this\nsetting. These results highlight GOAT as a practical path toward building\nrobust open-source LLM agents capable of complex reasoning and tool use.", "AI": {"tldr": "GOAT\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4eceAPI\u6587\u6863\u81ea\u52a8\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u80fd\u591f\u5fae\u8c03LLM\u4ee3\u7406\u5904\u7406\u76ee\u6807\u5bfc\u5411\u7684API\u6267\u884c\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u5904\u7406\u9700\u8981\u5206\u89e3\u9ad8\u5c42\u6b21\u76ee\u6807\u4e3a\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56API\u8c03\u7528\u7684\u76ee\u6807\u5bfc\u5411\u67e5\u8be2\u65f6\u80fd\u529b\u6709\u9650\uff0c\u5f00\u6e90\u6a21\u578b\u5c24\u5176\u96be\u4ee5\u6709\u6548\u6267\u884c\u590d\u6742\u5de5\u5177\u4f7f\u7528\u3002", "method": "\u63d0\u51faGOAT\u8bad\u7ec3\u6846\u67b6\uff0c\u4eceAPI\u6587\u6863\u81ea\u52a8\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03LLM\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u63a8\u7406\u76f8\u4e92\u4f9d\u8d56\u7684\u8c03\u7528\u5e76\u751f\u6210\u8fde\u8d2f\u54cd\u5e94\u3002", "result": "GOAT\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u591a\u4e2a\u73b0\u6709\u76ee\u6807\u5bfc\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u63d0\u51fa\u7684GOATBench\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "GOAT\u4e3a\u6784\u5efa\u80fd\u591f\u8fdb\u884c\u590d\u6742\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u7a33\u5065\u5f00\u6e90LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12224", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12224", "abs": "https://arxiv.org/abs/2510.12224", "authors": ["Yuechun Yu", "Han Ying", "Haoan Jin", "Wenjian Jiang", "Dong Xian", "Binghao Wang", "Zhou Yang", "Mengyue Wu"], "title": "MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs", "comment": null, "summary": "The reliable evaluation of large language models (LLMs) in medical\napplications remains an open challenge, particularly in capturing the\ncomplexity of multi-turn doctor-patient interactions that unfold in real\nclinical environments. Existing evaluation methods typically rely on post hoc\nreview of full conversation transcripts, thereby neglecting the dynamic,\ncontext-sensitive nature of medical dialogues and the evolving informational\nneeds of patients. In this work, we present MedKGEval, a novel multi-turn\nevaluation framework for clinical LLMs grounded in structured medical\nknowledge. Our approach introduces three key contributions: (1) a knowledge\ngraph-driven patient simulation mechanism, where a dedicated control module\nretrieves relevant medical facts from a curated knowledge graph, thereby\nendowing the patient agent with human-like and realistic conversational\nbehavior. This knowledge graph is constructed by integrating open-source\nresources with additional triples extracted from expert-annotated datasets; (2)\nan in-situ, turn-level evaluation framework, where each model response is\nassessed by a Judge Agent for clinical appropriateness, factual correctness,\nand safety as the dialogue progresses using a suite of fine-grained,\ntask-specific metrics; (3) a comprehensive multi-turn benchmark of eight\nstate-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle\nbehavioral flaws and safety risks that are often overlooked by conventional\nevaluation pipelines. Although initially designed for Chinese and English\nmedical applications, our framework can be readily extended to additional\nlanguages by switching the input knowledge graphs, ensuring seamless bilingual\nsupport and domain-specific applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedKGEval\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u533b\u5b66\u77e5\u8bc6\u7684\u591a\u8f6e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e34\u5e8aLLMs\u5728\u533b\u751f-\u60a3\u8005\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u533b\u5b66LLM\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5bf9\u8bdd\u8f6c\u5f55\u540e\u5206\u6790\uff0c\u5ffd\u7565\u4e86\u533b\u7597\u5bf9\u8bdd\u7684\u52a8\u6001\u6027\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u590d\u6742\u7684\u591a\u8f6e\u533b\u60a3\u4e92\u52a8\u3002", "method": "1) \u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u60a3\u8005\u6a21\u62df\u673a\u5236\uff1b2) \u5b9e\u65f6\u8f6e\u7ea7\u8bc4\u4f30\u6846\u67b6\uff0c\u7531\u6cd5\u5b98\u4ee3\u7406\u8bc4\u4f30\u6bcf\u4e2a\u6a21\u578b\u54cd\u5e94\u7684\u4e34\u5e8a\u9002\u5f53\u6027\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff1b3) \u6784\u5efa\u53cc\u8bed\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u4e2d\u82f1\u6587\u8bc4\u4f30\u3002", "result": "\u57288\u4e2a\u6700\u5148\u8fdbLLMs\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMedKGEval\u80fd\u591f\u8bc6\u522b\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u7684\u7ec6\u5fae\u884c\u4e3a\u7f3a\u9677\u548c\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "MedKGEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u52a8\u6001\u7684\u533b\u5b66LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u4e34\u5e8a\u5bf9\u8bdd\u7684\u590d\u6742\u6027\uff0c\u5e76\u652f\u6301\u591a\u8bed\u8a00\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.12264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12264", "abs": "https://arxiv.org/abs/2510.12264", "authors": ["Deyu Zou", "Yongqiang Chen", "Jianxiang Wang", "Haochen Yang", "Mufei Li", "James Cheng", "Pan Li", "Yu Gong"], "title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning", "comment": null, "summary": "Active reasoning requires large language models (LLMs) to interact with\nexternal sources and strategically gather information to solve problems.\nCentral to this process is belief tracking: maintaining a coherent\nunderstanding of the problem state and the missing information toward the\nsolution. However, due to limited reasoning capabilities, LLM-based agents\noften suffer from belief deviation: they struggle to correctly model beliefs,\nlose track of problem states, and fall into uninformative or repetitive\nactions. Once this happens, errors compound and reinforcement learning (RL)\ntraining fails to properly credit the crucial exploratory steps. To address\nthis issue, we propose to track the deviation of model beliefs and develop\n$\\mathbf{T^3}$, a simple yet effective method that detects excessive belief\ndeviation and truncates trajectories during training to remove uninformative\ntails. By preserving credit for informative prefixes, $\\mathbf{T^3}$\nsystematically improves policy optimization. Across 5 challenging tasks,\n$\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and\nfinal performance, achieving up to 30% gains while cutting rollout tokens by\nroughly 25%. These results highlight belief control as a key principle for\ndeveloping robust and generalizable LLM-based active reasoners.", "AI": {"tldr": "\u63d0\u51faT^3\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u622a\u65ad\u8fc7\u5ea6\u4fe1\u5ff5\u504f\u5dee\u7684\u8f68\u8ff9\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4fdd\u7559\u6709\u4ef7\u503c\u7684\u524d\u7f00\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u4e3b\u52a8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u56e0\u4fe1\u5ff5\u504f\u5dee\u5bfc\u81f4\u7684\u8ddf\u8e2a\u95ee\u9898\u72b6\u6001\u5931\u8d25\u3001\u9677\u5165\u65e0\u4fe1\u606f\u91cd\u590d\u884c\u4e3a\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5931\u8d25\u3002", "method": "\u5f00\u53d1T^3\u65b9\u6cd5\uff0c\u8ddf\u8e2a\u6a21\u578b\u4fe1\u5ff5\u504f\u5dee\uff0c\u68c0\u6d4b\u8fc7\u5ea6\u504f\u5dee\u5e76\u622a\u65ad\u8bad\u7ec3\u8f68\u8ff9\uff0c\u79fb\u9664\u65e0\u4fe1\u606f\u5c3e\u90e8\uff0c\u4fdd\u7559\u6709\u4ef7\u503c\u7684\u524d\u7f00\u4fe1\u606f\u7528\u4e8e\u7b56\u7565\u4f18\u5316\u3002", "result": "\u57285\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cT^3\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u4ee4\u724c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u83b7\u5f97\u9ad8\u8fbe30%\u7684\u6027\u80fd\u589e\u76ca\uff0c\u540c\u65f6\u51cf\u5c11\u7ea625%\u7684\u4ee4\u724c\u4f7f\u7528\u3002", "conclusion": "\u4fe1\u5ff5\u63a7\u5236\u662f\u5f00\u53d1\u9c81\u68d2\u548c\u53ef\u6cdb\u5316LLM\u4e3b\u52a8\u63a8\u7406\u5668\u7684\u5173\u952e\u539f\u5219\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12164", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12164", "abs": "https://arxiv.org/abs/2510.12164", "authors": ["Ziqi Wang", "Boye Niu", "Zipeng Gao", "Zhi Zheng", "Tong Xu", "Linghui Meng", "Zhongli Li", "Jing Liu", "Yilong Chen", "Chen Zhu", "Hua Wu", "Haifeng Wang", "Enhong Chen"], "title": "A Survey on Parallel Reasoning", "comment": null, "summary": "With the increasing capabilities of Large Language Models (LLMs), parallel\nreasoning has emerged as a new inference paradigm that enhances reasoning\nrobustness by concurrently exploring multiple lines of thought before\nconverging on a final answer. It has become a significant trend to explore\nparallel reasoning to overcome the fragility of standard sequential methods and\nimprove practical performance. In this paper, we aim to survey and summarize\nthe progress and challenges of parallel reasoning. We first present a formal\ndefinition of parallel reasoning and clarify its distinction from related\nconcepts like Chain-of-Thought. Then, we organize and discuss advanced\ntechniques based on a novel taxonomy, including non-interactive reasoning,\ninteractive reasoning, and efficiency-focused decoding strategies.\nAdditionally, we explore various application scenarios, such as solving complex\nproblems and enhancing the reliability of LLM outputs.Finally, we highlight the\ncore challenges of parallel reasoning and suggest potential directions for\nfuture research. We hope that our work can provide a useful roadmap for\nbeginners and encourage more research on improving parallel reasoning methods.\nRelated source can be avaliable in\nhttps://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5e76\u884c\u63a8\u7406\u8fd9\u4e00\u65b0\u5174\u63a8\u7406\u8303\u5f0f\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u9610\u660e\u4e86\u5176\u5b9a\u4e49\u3001\u5206\u7c7b\u3001\u5e94\u7528\u573a\u666f\u3001\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u5e76\u884c\u63a8\u7406\u4f5c\u4e3a\u4e00\u79cd\u589e\u5f3a\u63a8\u7406\u9c81\u68d2\u6027\u7684\u65b0\u8303\u5f0f\u51fa\u73b0\uff0c\u65e8\u5728\u514b\u670d\u6807\u51c6\u987a\u5e8f\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u5e76\u63d0\u5347\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u5e76\u884c\u63a8\u7406\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u57fa\u4e8e\u65b0\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u8ba8\u8bba\u5148\u8fdb\u6280\u672f\uff0c\u5305\u62ec\u975e\u4ea4\u4e92\u5f0f\u63a8\u7406\u3001\u4ea4\u4e92\u5f0f\u63a8\u7406\u548c\u6548\u7387\u5bfc\u5411\u7684\u89e3\u7801\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u5e76\u884c\u63a8\u7406\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u6280\u672f\u5206\u7c7b\u548c\u5e94\u7528\u573a\u666f\u5206\u6790\uff0c\u5e76\u8bc6\u522b\u4e86\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u8def\u7ebf\u56fe\uff0c\u9f13\u52b1\u66f4\u591a\u7814\u7a76\u6539\u8fdb\u5e76\u884c\u63a8\u7406\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.12399", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12399", "abs": "https://arxiv.org/abs/2510.12399", "authors": ["Yuyao Ge", "Lingrui Mei", "Zenghao Duan", "Tianhao Li", "Yujia Zheng", "Yiwei Wang", "Lexin Wang", "Jiayu Yao", "Tianyu Liu", "Yujun Cai", "Baolong Bi", "Fangda Guo", "Jiafeng Guo", "Shenghua Liu", "Xueqi Cheng"], "title": "A Survey of Vibe Coding with Large Language Models", "comment": null, "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Vibe Coding\u8303\u5f0f\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6210\u529f\u5b9e\u65bdVibe Coding\u7684\u5173\u952e\u8981\u7d20\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ece\u4ee3\u7801\u751f\u6210\u8f85\u52a9\u8f6c\u5411\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\uff0c\u50ac\u751f\u4e86\"Vibe Coding\"\u65b0\u8303\u5f0f\uff0c\u4f46\u5176\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u67901000\u591a\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u5efa\u7acb\u4e86\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u7efc\u5408\u51fa\u4e94\u79cd\u5f00\u53d1\u6a21\u578b\uff1a\u65e0\u7ea6\u675f\u81ea\u52a8\u5316\u3001\u8fed\u4ee3\u5bf9\u8bdd\u534f\u4f5c\u3001\u89c4\u5212\u9a71\u52a8\u3001\u6d4b\u8bd5\u9a71\u52a8\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u6210\u529f\u5b9e\u65bdVibe Coding\u4e0d\u4ec5\u4f9d\u8d56\u4ee3\u7406\u80fd\u529b\uff0c\u66f4\u9700\u8981\u7cfb\u7edf\u5316\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001\u5b8c\u5584\u7684\u5f00\u53d1\u73af\u5883\u548c\u4eba\u673a\u534f\u4f5c\u5f00\u53d1\u6a21\u578b\u3002", "conclusion": "Vibe Coding\u4f5c\u4e3a\u4e00\u79cd\u53d8\u9769\u6027\u5f00\u53d1\u65b9\u6cd5\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6a21\u578b\u7684\u7cfb\u7edf\u652f\u6301\uff0c\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.12167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12167", "abs": "https://arxiv.org/abs/2510.12167", "authors": ["Minghan Wang", "Thuy-Trang Vu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Towards Inference-time Scaling for Continuous Space Reasoning", "comment": null, "summary": "Inference-time scaling through multiple sample generation in combination with\nProcess- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective\nfor text-based reasoning in large language models. This paper investigates\nwhether such established techniques can be successfully adapted to reasoning in\nthe continuous space, using COCONUT (Hao et al. 2024) continuous space\nreasoning LM as the backbone. We demonstrate the feasibility of generating\ndiverse reasoning paths through dropout-based sampling. Our Pass@N analysis on\nthe generated samples reveals the potential that could enable a significant\ngain in performance akin to observed gain in the discrete space. However, we\nhighlight unique challenges faced for materializing this gain in the continuous\nthought space. In particular, working recipes for data generation and training\nPRM and ORM models in the discrete space unlocks only marginal improvements in\nthe continuous space. Through probing various aspects including geometric\nproperties and trajectory dynamics we identify the underlying reasons that\nprevent effective discrimination between correct and incorrect reasoning\n(essential for the functioning of PRM and ORM). Our findings reveal that\ncurrent limitations stem from the absence of key inductive biases in continuous\nthought representations. We argue that the training frameworks for continuous\nreasoning LMs require not only to optimize for accuracy but also to explicitly\nincorporate inductive biases that could be utilized during inference-time for\ndiscrimination of correct and incorrect thoughts.\\footnote{Our code and data\nwill be publicly available.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u79bb\u6563\u63a8\u7406\u7a7a\u95f4\u7684\u63a8\u7406\u6280\u672f\uff08\u5982\u591a\u6837\u672c\u751f\u6210\u548c\u5956\u52b1\u6a21\u578b\u91cd\u6392\u5e8f\uff09\u5e94\u7528\u4e8e\u8fde\u7eed\u63a8\u7406\u7a7a\u95f4\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u867d\u7136\u751f\u6210\u591a\u6837\u6027\u63a8\u7406\u8def\u5f84\u662f\u53ef\u884c\u7684\uff0c\u4f46\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u6709\u6548\u5956\u52b1\u6a21\u578b\u91cd\u6392\u5e8f\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u5df2\u8bc1\u660e\u5728\u79bb\u6563\u63a8\u7406\u7a7a\u95f4\u6709\u6548\u7684\u63a8\u7406\u65f6\u6269\u5c55\u6280\u672f\uff08\u591a\u6837\u672c\u751f\u6210+\u5956\u52b1\u6a21\u578b\u91cd\u6392\u5e8f\uff09\u662f\u5426\u80fd\u591f\u6210\u529f\u9002\u5e94\u8fde\u7eed\u63a8\u7406\u7a7a\u95f4\uff0c\u4ee5\u63d0\u5347\u8fde\u7eed\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528COCONUT\u8fde\u7eed\u63a8\u7406LM\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\uff0c\u901a\u8fc7dropout-based\u91c7\u6837\u751f\u6210\u591a\u6837\u6027\u63a8\u7406\u8def\u5f84\uff0c\u8fdb\u884cPass@N\u5206\u6790\uff0c\u5e76\u7814\u7a76\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8bad\u7ec3PRM\u548cORM\u6a21\u578b\u7684\u6548\u679c\uff0c\u901a\u8fc7\u51e0\u4f55\u5c5e\u6027\u548c\u8f68\u8ff9\u52a8\u6001\u5206\u6790\u8bc6\u522b\u6311\u6218\u3002", "result": "\u751f\u6210\u591a\u6837\u6027\u63a8\u7406\u8def\u5f84\u662f\u53ef\u884c\u7684\uff0cPass@N\u5206\u6790\u663e\u793a\u5b58\u5728\u663e\u8457\u6027\u80fd\u63d0\u5347\u6f5c\u529b\uff0c\u4f46\u79bb\u6563\u7a7a\u95f4\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u8fde\u7eed\u7a7a\u95f4\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u3002", "conclusion": "\u5f53\u524d\u8fde\u7eed\u63a8\u7406LM\u7684\u8bad\u7ec3\u6846\u67b6\u4e0d\u4ec5\u9700\u8981\u4f18\u5316\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u8981\u660e\u786e\u878d\u5165\u53ef\u5728\u63a8\u7406\u65f6\u7528\u4e8e\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u601d\u7ef4\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "topic": "agent analysis"}}
{"id": "2510.12096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12096", "abs": "https://arxiv.org/abs/2510.12096", "authors": ["Guozheng Ma", "Lu Li", "Zilin Wang", "Haoyu Wang", "Shengchao Hu", "Leszek Rutkowski", "Dacheng Tao"], "title": "Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning", "comment": null, "summary": "Scaling neural networks has driven breakthrough advances in machine learning,\nyet this paradigm fails in deep reinforcement learning (DRL), where larger\nmodels often degrade performance due to unique optimization pathologies such as\nplasticity loss. While recent works show that dynamically adapting network\ntopology during training can mitigate these issues, existing studies have three\ncritical limitations: (1) applying uniform dynamic training strategies across\nall modules despite encoder, critic, and actor following distinct learning\nparadigms, (2) focusing evaluation on basic architectures without clarifying\nthe relative importance and interaction between dynamic training and\narchitectural improvements, and (3) lacking systematic comparison between\ndifferent dynamic approaches including sparse-to-sparse, dense-to-sparse, and\nsparse-to-dense. Through comprehensive investigation across modules and\narchitectures, we reveal that dynamic sparse training strategies provide\nmodule-specific benefits that complement the primary scalability foundation\nestablished by architectural improvements. We finally distill these insights\ninto Module-Specific Training (MST), a practical framework that further\nexploits the benefits of architectural improvements and demonstrates\nsubstantial scalability gains across diverse RL algorithms without algorithmic\nmodifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u5757\u7279\u5b9a\u8bad\u7ec3(MST)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u6a21\u578b\u89c4\u6a21\u6269\u5927\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u6a21\u5757\u7279\u5b9a\u52a8\u6001\u8bad\u7ec3\u4e0e\u67b6\u6784\u6539\u8fdb\u7684\u4e92\u8865\u5173\u7cfb\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u6a21\u578b\u89c4\u6a21\u6269\u5927\u5f80\u5f80\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u52a8\u6001\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u5bf9\u6240\u6709\u6a21\u5757\u91c7\u7528\u7edf\u4e00\u7b56\u7565\u3001\u8bc4\u4f30\u57fa\u7840\u67b6\u6784\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e0d\u540c\u52a8\u6001\u65b9\u6cd5\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u8de8\u6a21\u5757\u548c\u67b6\u6784\u7684\u5168\u9762\u8c03\u67e5\uff0c\u5f00\u53d1\u4e86\u6a21\u5757\u7279\u5b9a\u8bad\u7ec3(MST)\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u5230\u7a00\u758f\u3001\u7a20\u5bc6\u5230\u7a00\u758f\u548c\u7a00\u758f\u5230\u7a20\u5bc6\u7b49\u52a8\u6001\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u52a8\u6001\u7a00\u758f\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u6a21\u5757\u7279\u5b9a\u6536\u76ca\uff0c\u4e0e\u67b6\u6784\u6539\u8fdb\u5f62\u6210\u4e92\u8865\uff0cMST\u6846\u67b6\u5728\u4e0d\u4fee\u6539\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cdRL\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6a21\u5757\u7279\u5b9a\u52a8\u6001\u8bad\u7ec3\u4e0e\u67b6\u6784\u6539\u8fdb\u5177\u6709\u534f\u540c\u6548\u5e94\uff0cMST\u6846\u67b6\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12462", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.12462", "abs": "https://arxiv.org/abs/2510.12462", "authors": ["Jiaxin Gao", "Chen Chen", "Yanwen Jia", "Xueluan Gong", "Kwok-Yan Lam", "Qian Wang"], "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\u7684\u5224\u65ad\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5bf9\u504f\u89c1\u8f93\u5165\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5fae\u8c03\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740LLM\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u88ab\u7528\u4e8e\u81ea\u4e3b\u8bc4\u4f30\u5185\u5bb9\u8d28\u91cf\uff0c\u8bc4\u4f30\u8005\u7684\u516c\u6b63\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff0c\u4efb\u4f55\u504f\u89c1\u90fd\u53ef\u80fd\u5f71\u54cd\u7ed3\u679c\u5e76\u635f\u5bb3\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e24\u79cdLLM\u8bc4\u4f30\u6a21\u578b\u5728\u70b9\u5f0f\u8bc4\u5206\u8bbe\u7f6e\u4e0b\u768411\u79cd\u504f\u89c1\u7c7b\u578b\uff0c\u6db5\u76d6\u9690\u5f0f\u548c\u663e\u5f0f\u504f\u89c1\u5f62\u5f0f\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u504f\u89c1\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3001\u5fae\u8c03\u5f71\u54cd\u4ee5\u53ca\u4efb\u52a1\u96be\u5ea6\u76f8\u5173\u6027\u3002", "result": "\u6700\u5148\u8fdb\u7684LLM\u8bc4\u4f30\u8005\u5bf9\u504f\u89c1\u8f93\u5165\u5177\u6709\u9c81\u68d2\u6027\uff0c\u901a\u5e38\u7ed9\u504f\u89c1\u6837\u672c\u5206\u914d\u8f83\u4f4e\u5206\u6570\uff1b\u63d0\u4f9b\u8be6\u7ec6\u8bc4\u5206\u6807\u51c6\u80fd\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u5728\u504f\u89c1\u6570\u636e\u4e0a\u5fae\u8c03\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff1b\u8bc4\u4f30\u5206\u6570\u4e0e\u4efb\u52a1\u96be\u5ea6\u76f8\u5173\u3002", "conclusion": "LLM\u8bc4\u4f30\u8005\u5b58\u5728\u5224\u65ad\u504f\u89c1\u98ce\u9669\uff0c\u9700\u8981\u91c7\u53d6\u7f13\u89e3\u7b56\u7565\u6765\u786e\u4fdd\u5b9e\u9645\u901a\u4fe1\u573a\u666f\u4e2d\u7684\u516c\u5e73\u53ef\u9760\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2510.12555", "categories": ["cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.12555", "abs": "https://arxiv.org/abs/2510.12555", "authors": ["Andries Rosseau", "Rapha\u00ebl Avalos", "Ann Now\u00e9"], "title": "Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings", "comment": "This version is a slightly updated version (e.g., added an important\n  reference) compared to the peer-reviewed versions at 'Adapative Learning\n  Agents' at AAMAS 2022 or 'From Cells to Societies' at ICLR 2022", "summary": "The competitive and cooperative forces of natural selection have driven the\nevolution of intelligence for millions of years, culminating in nature's vast\nbiodiversity and the complexity of human minds. Inspired by this process, we\npropose a novel multi-agent reinforcement learning framework where each agent\nis assigned a genotype and where reward functions are modelled after the\nconcept of inclusive fitness. An agent's genetic material may be shared with\nother agents, and our inclusive reward function naturally accounts for this. We\nstudy the resulting social dynamics in two types of network games with\nprisoner's dilemmas and find that our results align with well-established\nprinciples from biology, such as Hamilton's rule. Furthermore, we outline how\nthis framework can extend to more open-ended environments with spatial and\ntemporal structure, finite resources, and evolving populations. We hypothesize\nthe emergence of an arms race of strategies, where each new strategy is a\ngradual improvement over earlier adaptations of other agents, effectively\nproducing a multi-agent autocurriculum analogous to biological evolution. In\ncontrast to the binary team-based structures prevalent in earlier research, our\ngene-based reward structure introduces a spectrum of cooperation ranging from\nfull adversity to full cooperativeness based on genetic similarity, enabling\nunique non team-based social dynamics. For example, one agent having a mutual\ncooperative relationship with two other agents, while the two other agents\nbehave adversarially towards each other. We argue that incorporating inclusive\nfitness in agents provides a foundation for the emergence of more strategically\nadvanced and socially intelligent agents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5305\u5bb9\u6027\u9002\u5e94\u5ea6\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u56e0\u578b\u5206\u914d\u548c\u9057\u4f20\u76f8\u4f3c\u6027\u9a71\u52a8\u7684\u5956\u52b1\u51fd\u6570\uff0c\u6a21\u62df\u81ea\u7136\u9009\u62e9\u4e2d\u7684\u5408\u4f5c\u4e0e\u7ade\u4e89\u52a8\u6001\u3002", "motivation": "\u53d7\u81ea\u7136\u9009\u62e9\u4e2d\u5408\u4f5c\u4e0e\u7ade\u4e89\u529b\u91cf\u9a71\u52a8\u667a\u529b\u8fdb\u5316\u7684\u542f\u53d1\uff0c\u65e8\u5728\u521b\u5efa\u80fd\u4ea7\u751f\u7c7b\u4f3c\u751f\u7269\u8fdb\u5316\u4e2d\u7b56\u7565\u519b\u5907\u7ade\u8d5b\u7684\u591a\u667a\u80fd\u4f53\u73af\u5883\u3002", "method": "\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u5206\u914d\u57fa\u56e0\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u5305\u5bb9\u6027\u9002\u5e94\u5ea6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u56da\u5f92\u56f0\u5883\u7f51\u7edc\u6e38\u620f\u4e2d\u7814\u7a76\u793e\u4f1a\u52a8\u6001\uff0c\u57fa\u56e0\u76f8\u4f3c\u6027\u51b3\u5b9a\u5408\u4f5c\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u751f\u7269\u5b66\u539f\u7406\uff08\u5982\u6c49\u5bc6\u5c14\u987f\u6cd5\u5219\uff09\u4e00\u81f4\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u9057\u4f20\u76f8\u4f3c\u6027\u7684\u5408\u4f5c\u8c31\u7cfb\u548c\u975e\u56e2\u961f\u578b\u793e\u4f1a\u52a8\u6001\u3002", "conclusion": "\u57fa\u4e8e\u5305\u5bb9\u6027\u9002\u5e94\u5ea6\u7684\u5956\u52b1\u673a\u5236\u4e3a\u66f4\u590d\u6742\u7b56\u7565\u548c\u793e\u4f1a\u667a\u80fd\u7684\u51fa\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u4ea7\u751f\u7c7b\u4f3c\u751f\u7269\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u8bfe\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12157", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12157", "abs": "https://arxiv.org/abs/2510.12157", "authors": ["Zhongwei Yu", "Wannian Xia", "Xue Yan", "Bo Xu", "Haifeng Zhang", "Yali Du", "Jun Wang"], "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning", "comment": "Accepted by NeurIPS2025", "summary": "Advanced large language models (LLMs) frequently reflect in reasoning\nchain-of-thoughts (CoTs), where they self-verify the correctness of current\nsolutions and explore alternatives. However, given recent findings that LLMs\ndetect limited errors in CoTs, how reflection contributes to empirical\nimprovements remains unclear. To analyze this issue, in this paper, we present\na minimalistic reasoning framework to support basic self-verifying reflection\nfor small transformers without natural language, which ensures analytic clarity\nand reduces the cost of comprehensive experiments. Theoretically, we prove that\nself-verifying reflection guarantees improvements if verification errors are\nproperly bounded. Experimentally, we show that tiny transformers, with only a\nfew million parameters, benefit from self-verification in both training and\nreflective execution, reaching remarkable LLM-level performance in integer\nmultiplication and Sudoku. Similar to LLM results, we find that reinforcement\nlearning (RL) improves in-distribution performance and incentivizes frequent\nreflection for tiny transformers, yet RL mainly optimizes shallow statistical\npatterns without faithfully reducing verification errors. In conclusion,\nintegrating generative transformers with discriminative verification inherently\nfacilitates CoT reasoning, regardless of scaling and natural language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301\u5c0f\u578bTransformer\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u53cd\u601d\uff0c\u8bc1\u660e\u81ea\u6211\u9a8c\u8bc1\u80fd\u4fdd\u8bc1\u6027\u80fd\u63d0\u5347\uff0c\u5b9e\u9a8c\u663e\u793a\u5c0f\u6a21\u578b\u5728\u6574\u6570\u4e58\u6cd5\u548c\u6570\u72ec\u4efb\u52a1\u4e2d\u8fbe\u5230LLM\u7ea7\u522b\u6027\u80fd\u3002", "motivation": "\u5206\u6790LLM\u4e2d\u53cd\u601d\u673a\u5236\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u7531\u4e8eLLM\u5728CoT\u4e2d\u68c0\u6d4b\u9519\u8bef\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u660e\u786e\u53cd\u601d\u5982\u4f55\u5e26\u6765\u5b9e\u8bc1\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u6700\u5c0f\u5316\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301\u5c0f\u578bTransformer\u8fdb\u884c\u65e0\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u81ea\u6211\u9a8c\u8bc1\u53cd\u601d\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5c0f\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u9a8c\u8bc1\u5728\u8bad\u7ec3\u548c\u53cd\u601d\u6267\u884c\u4e2d\u83b7\u76ca\uff0c\u5728\u6574\u6570\u4e58\u6cd5\u548c\u6570\u72ec\u4efb\u52a1\u4e2d\u8fbe\u5230LLM\u7ea7\u522b\u6027\u80fd\uff1bRL\u4e3b\u8981\u4f18\u5316\u6d45\u5c42\u7edf\u8ba1\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u51cf\u5c11\u9a8c\u8bc1\u9519\u8bef\u3002", "conclusion": "\u751f\u6210\u5f0fTransformer\u4e0e\u5224\u522b\u5f0f\u9a8c\u8bc1\u7684\u7ed3\u5408\u672c\u8d28\u4e0a\u4fc3\u8fdb\u4e86CoT\u63a8\u7406\uff0c\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u81ea\u7136\u8bed\u8a00\u65e0\u5173\u3002", "topic": "agent analysis"}}
{"id": "2510.12635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12635", "abs": "https://arxiv.org/abs/2510.12635", "authors": ["Yuxiang Zhang", "Jiangming Shu", "Ye Ma", "Xueyuan Lin", "Shangxi Wu", "Jitao Sang"], "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks", "comment": null, "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86Memory-as-Action\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u5185\u5b58\u7ba1\u7406\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u7684\u5185\u90e8\u80fd\u529b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u7406\u4e3b\u52a8\u6267\u884c\u5185\u5b58\u7f16\u8f91\u64cd\u4f5c\uff0c\u5e76\u5f00\u53d1\u4e86Dynamic Context Policy Optimization\u7b97\u6cd5\u6765\u89e3\u51b3\u8f68\u8ff9\u65ad\u88c2\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u91ce\u4ee3\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u5185\u5b58\u9650\u5236\uff0c\u5bb9\u6613\u88ab\u65e0\u5173\u4e0a\u4e0b\u6587\u6df9\u6ca1\u3002\u73b0\u6709\u5de5\u4f5c\u5185\u5b58\u65b9\u6cd5\u4f9d\u8d56\u4e0e\u6838\u5fc3\u7b56\u7565\u89e3\u8026\u7684\u5916\u90e8\u542f\u53d1\u5f0f\u673a\u5236\uff0c\u9700\u8981\u5c06\u5185\u5b58\u7ba1\u7406\u4f5c\u4e3a\u5185\u5728\u53ef\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faMemory-as-Action\u6846\u67b6\uff0c\u4ee3\u7406\u901a\u8fc7\u7edf\u4e00\u7b56\u7565\u6267\u884c\u663e\u5f0f\u5185\u5b58\u7f16\u8f91\u64cd\u4f5c\uff1b\u5f00\u53d1Dynamic Context Policy Optimization\u7b97\u6cd5\uff0c\u5728\u5185\u5b58\u64cd\u4f5c\u70b9\u5206\u5272\u8f68\u8ff9\u5e76\u5e94\u7528\u8f68\u8ff9\u7ea7\u4f18\u52bf\u3002", "result": "\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u4efb\u52a1\u63a8\u7406\u548c\u5185\u5b58\u7ba1\u7406\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8ba1\u7b97\u6d88\u8017\uff0c\u8fd8\u63d0\u9ad8\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u901a\u8fc7\u9002\u5e94\u6a21\u578b\u5185\u5728\u80fd\u529b\u7684\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u5b9e\u73b0\u3002", "conclusion": "\u5c06\u5185\u5b58\u7ba1\u7406\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u7684\u5185\u90e8\u80fd\u529b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u5185\u5b58\u7ba1\u7406\u548c\u957f\u671f\u4efb\u52a1\u76ee\u6807\uff0c\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12693", "abs": "https://arxiv.org/abs/2510.12693", "authors": ["Hanyang Chen", "Mark Zhao", "Rui Yang", "Qinwei Ma", "Ke Yang", "Jiarui Yao", "Kangrui Wang", "Hao Bai", "Zhenhailong Wang", "Rui Pan", "Mengchao Zhang", "Jose Barreiros", "Aykut Onol", "ChengXiang Zhai", "Heng Ji", "Manling Li", "Huan Zhang", "Tong Zhang"], "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning", "comment": null, "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\n\\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, \\textit{Embodied Prior Learning}, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.", "AI": {"tldr": "ERA\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u9a8c\u77e5\u8bc6\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u7f3a\u4e4f\u5fc5\u8981\u77e5\u8bc6\u548c\u6280\u80fd\u7684\u95ee\u9898\uff0c\u5f25\u5408\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u5177\u8eab\u5148\u9a8c\u5b66\u4e60\uff0c\u4ece\u4e09\u7c7b\u6570\u636e\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff1a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u542b\u81ea\u603b\u7ed3\u3001\u5bc6\u96c6\u5956\u52b1\u5851\u9020\u548c\u56de\u5408\u7ea7\u7b56\u7565\u4f18\u5316\u3002", "result": "ERA-3B\u5728EB-ALFRED\u548cEB-Manipulation\u4efb\u52a1\u4e0a\u5206\u522b\u6bd4GPT-4o\u63d0\u53478.4%\u548c19.4%\uff0c\u5e76\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ERA\u4e3a\u53ef\u6269\u5c55\u7684\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12697", "abs": "https://arxiv.org/abs/2510.12697", "authors": ["Tianyu Hu", "Zhen Tan", "Song Wang", "Huaizhi Qu", "Tianlong Chen"], "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection", "comment": null, "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6cd5\u5b98\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u5347LLM\u81ea\u52a8\u5224\u65ad\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5f15\u5165\u7a33\u5b9a\u6027\u68c0\u6d4b\u673a\u5236\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u6cd5\u5b98\u7684\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u805a\u5408\uff08\u5982\u591a\u6570\u6295\u7968\uff09\uff0c\u5373\u4f7f\u4e2a\u4f53\u667a\u80fd\u4f53\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u4e5f\u53ef\u80fd\u5931\u8d25\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u534f\u4f5c\u5224\u65ad\u673a\u5236\u3002", "method": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6cd5\u5b98\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u534f\u4f5c\u63a8\u7406\u5e76\u8fed\u4ee3\u4f18\u5316\u56de\u7b54\uff1b\u6570\u5b66\u5f62\u5f0f\u5316\u8fa9\u8bba\u8fc7\u7a0b\uff1b\u5f15\u5165\u57fa\u4e8e\u65f6\u95f4\u53d8\u5316Beta-Binomial\u6df7\u5408\u7684\u7a33\u5b9a\u6027\u68c0\u6d4b\u673a\u5236\uff0c\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u8fdb\u884c\u81ea\u9002\u5e94\u505c\u6b62\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u591a\u6570\u6295\u7968\u63d0\u9ad8\u4e86\u5224\u65ad\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fa9\u8bba\u6846\u67b6\u80fd\u653e\u5927\u6b63\u786e\u6027\uff0c\u7a33\u5b9a\u6027\u68c0\u6d4b\u673a\u5236\u6709\u6548\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.12367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12367", "abs": "https://arxiv.org/abs/2510.12367", "authors": ["Rui Li", "Jia-Chen Gu", "Po-Nien Kung", "Heming Xia", "Junfeng liu", "Xiangwen Kong", "Zhifang Sui", "Nanyun Peng"], "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has inspired\nresearchers to integrate them extensively into the academic workflow,\npotentially reshaping how research is practiced and reviewed. While previous\nstudies highlight the potential of LLMs in supporting research and peer review,\ntheir dual roles in the academic workflow and the complex interplay between\nresearch and review bring new risks that remain largely underexplored. In this\nstudy, we focus on how the deep integration of LLMs into both peer-review and\nresearch processes may influence scholarly fairness, examining the potential\nrisks of using LLMs as reviewers by simulation. This simulation incorporates a\nresearch agent, which generates papers and revises, alongside a review agent,\nwhich assesses the submissions. Based on the simulation results, we conduct\nhuman annotations and identify pronounced misalignment between LLM-based\nreviews and human judgments: (1) LLM reviewers systematically inflate scores\nfor LLM-authored papers, assigning them markedly higher scores than\nhuman-authored ones; (2) LLM reviewers persistently underrate human-authored\npapers with critical statements (e.g., risk, fairness), even after multiple\nrevisions. Our analysis reveals that these stem from two primary biases in LLM\nreviewers: a linguistic feature bias favoring LLM-generated writing styles, and\nan aversion toward critical statements. These results highlight the risks and\nequity concerns posed to human authors and academic research if LLMs are\ndeployed in the peer review cycle without adequate caution. On the other hand,\nrevisions guided by LLM reviews yield quality gains in both LLM-based and human\nevaluations, illustrating the potential of the LLMs-as-reviewers for\nearly-stage researchers and enhancing low-quality papers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u5ba1\u7a3f\u4eba\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff1a\u504f\u5411LLM\u751f\u6210\u7684\u8bba\u6587\u98ce\u683c\uff0c\u8d2c\u4f4e\u5305\u542b\u6279\u5224\u6027\u9648\u8ff0\u7684\u4eba\u7c7b\u8bba\u6587\uff0c\u8fd9\u7ed9\u5b66\u672f\u516c\u5e73\u5e26\u6765\u98ce\u9669\u3002", "motivation": "\u63a2\u7d22LLM\u6df1\u5ea6\u6574\u5408\u5230\u5b66\u672f\u5de5\u4f5c\u6d41\u7a0b\uff08\u7814\u7a76\u548c\u8bc4\u5ba1\uff09\u4e2d\u53ef\u80fd\u5e26\u6765\u7684\u65b0\u98ce\u9669\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u672f\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\uff0c\u4f7f\u7528\u7814\u7a76\u4ee3\u7406\u751f\u6210\u548c\u4fee\u8ba2\u8bba\u6587\uff0c\u540c\u65f6\u4f7f\u7528\u8bc4\u5ba1\u4ee3\u7406\u8bc4\u4f30\u63d0\u4ea4\u7684\u8bba\u6587\uff0c\u5e76\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u3002", "result": "LLM\u5ba1\u7a3f\u4eba\u7cfb\u7edf\u6027\u62ac\u9ad8LLM\u64b0\u5199\u8bba\u6587\u7684\u5206\u6570\uff0c\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u64b0\u5199\u8bba\u6587\uff1b\u6301\u7eed\u4f4e\u4f30\u5305\u542b\u6279\u5224\u6027\u9648\u8ff0\u7684\u4eba\u7c7b\u8bba\u6587\uff1b\u5b58\u5728\u8bed\u8a00\u7279\u5f81\u504f\u89c1\u548c\u6279\u5224\u6027\u9648\u8ff0\u538c\u6076\u4e24\u79cd\u4e3b\u8981\u504f\u89c1\u3002", "conclusion": "LLM\u4f5c\u4e3a\u5ba1\u7a3f\u4eba\u5b58\u5728\u4e25\u91cd\u504f\u89c1\u98ce\u9669\uff0c\u9700\u8c28\u614e\u90e8\u7f72\uff1b\u4f46LLM\u6307\u5bfc\u7684\u4fee\u8ba2\u80fd\u63d0\u5347\u8bba\u6587\u8d28\u91cf\uff0c\u5bf9\u65e9\u671f\u7814\u7a76\u4eba\u5458\u548c\u4f4e\u8d28\u91cf\u8bba\u6587\u6709\u76ca\u3002", "topic": "agent analysis"}}
{"id": "2510.12233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12233", "abs": "https://arxiv.org/abs/2510.12233", "authors": ["Bowen Fan", "Zhilin Guo", "Xunkai Li", "Yihan Zhou", "Bing Zhou", "Zhenjun Li", "Rong-Hua Li", "Guoren Wang"], "title": "Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs", "comment": "12 pages, 4 figures", "summary": "Graph Neural Networks (GNNs) have become a pivotal framework for modeling\ngraph-structured data, enabling a wide range of applications from social\nnetwork analysis to molecular chemistry. By integrating large language models\n(LLMs), text-attributed graphs (TAGs) enhance node representations with rich\ntextual semantics, significantly boosting the expressive power of graph-based\nlearning. However, this sophisticated synergy introduces critical\nvulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both\ntheir structural topology and textual attributes. Although specialized attack\nmethods have been designed for each of these aspects, no work has yet unified\nthem into a comprehensive approach. In this work, we propose the Interpretable\nMulti-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial\nattack framework designed to orchestrate multi-level perturbations across both\ngraph structure and textual features. IMDGA utilizes three tightly integrated\nmodules to craft attacks that balance interpretability and impact, enabling a\ndeeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical\nanalysis and comprehensive empirical evaluations on diverse datasets and\narchitectures, IMDGA demonstrates superior interpretability, attack\neffectiveness, stealthiness, and robustness compared to existing methods. By\nexposing critical weaknesses in TAG representation learning, this work uncovers\na previously underexplored semantic dimension of vulnerability in Graph-LLMs,\noffering valuable insights for improving their resilience. Our code and\nresources are publicly available at\nhttps://anonymous.4open.science/r/IMDGA-7289.", "AI": {"tldr": "\u63d0\u51fa\u4e86IMDGA\u6846\u67b6\uff0c\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u591a\u7ef4\u56fe\u653b\u51fb\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u5bf9\u56fe\u7ed3\u6784\u548c\u6587\u672c\u5c5e\u6027\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u63ed\u793a\u4e86Graph-LLM\u7684\u8bed\u4e49\u7ef4\u5ea6\u6f0f\u6d1e\u3002", "motivation": "Graph-LLM\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u53ea\u9488\u5bf9\u7ed3\u6784\u6216\u6587\u672c\u5355\u4e00\u65b9\u9762\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7efc\u5408\u653b\u51fb\u6846\u67b6\u3002", "method": "IMDGA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7d27\u5bc6\u96c6\u6210\u7684\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u534f\u8c03\u591a\u5c42\u6b21\u7684\u56fe\u7ed3\u6784\u548c\u6587\u672c\u7279\u5f81\u6270\u52a8\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIMDGA\u5728\u53ef\u89e3\u91ca\u6027\u3001\u653b\u51fb\u6548\u679c\u3001\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86Graph-LLM\u4e2d\u5148\u524d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u8bed\u4e49\u7ef4\u5ea6\u6f0f\u6d1e\uff0c\u4e3a\u63d0\u9ad8\u5176\u6297\u653b\u51fb\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.12460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12460", "abs": "https://arxiv.org/abs/2510.12460", "authors": ["Linfeng Gao", "Baolong Bi", "Zheng Yuan", "Le Wang", "Zerui Chen", "Zhimin Wei", "Shenghua Liu", "Qinggang Zhang", "Jinsong Su"], "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLEAR\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6765\u5b9a\u4f4d\u77e5\u8bc6\u51b2\u7a81\uff0c\u5e76\u91c7\u7528\u51b2\u7a81\u611f\u77e5\u5fae\u8c03\u6765\u63d0\u5347RAG\u7cfb\u7edf\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5b58\u5728\u4e0d\u5fe0\u5b9e\u95ee\u9898\uff0c\u6a21\u578b\u54cd\u5e94\u4e0e\u68c0\u7d22\u8bc1\u636e\u76f8\u77db\u76fe\u3002\u73b0\u6709\u65b9\u6cd5\u5c06LLM\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u89c6\u4e86LLM\u5185\u90e8\u5982\u4f55\u6574\u5408\u68c0\u7d22\u8bc1\u636e\u4e0e\u53c2\u6570\u5316\u8bb0\u5fc6\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "1) \u5bf9LLM\u9690\u85cf\u72b6\u6001\u8868\u793a\u8fdb\u884c\u63a2\u6d4b\u5206\u6790\uff1b2) \u5c06\u4e0a\u4e0b\u6587\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u53e5\u5b50\u7ea7\u77e5\u8bc6\uff1b3) \u4f7f\u7528\u9690\u85cf\u72b6\u6001\u63a2\u6d4b\u5b9a\u4f4d\u51b2\u7a81\u77e5\u8bc6\uff1b4) \u5f15\u5165\u51b2\u7a81\u611f\u77e5\u5fae\u8c03\u6307\u5bfc\u6a21\u578b\u51c6\u786e\u6574\u5408\u68c0\u7d22\u8bc1\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCLEAR\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\uff0c\u5728\u4e0d\u540c\u51b2\u7a81\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CLEAR\u6846\u67b6\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u8868\u793a\u6765\u5b9a\u4f4d\u77e5\u8bc6\u51b2\u7a81\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u5fe0\u5b9e\u6027\uff0c\u4e3a\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.12253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12253", "abs": "https://arxiv.org/abs/2510.12253", "authors": ["Changfu Xu", "Jianxiong Guo", "Yuzhu Liang", "Haiyang Huang", "Haodong Zou", "Xi Zheng", "Shui Yu", "Xiaowen Chu", "Jiannong Cao", "Tian Wang"], "title": "Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development", "comment": "Under Review", "summary": "Diffusion Models (DMs), as a leading class of generative models, offer key\nadvantages for reinforcement learning (RL), including multi-modal\nexpressiveness, stable training, and trajectory-level planning. This survey\ndelivers a comprehensive and up-to-date synthesis of diffusion-based RL. We\nfirst provide an overview of RL, highlighting its challenges, and then\nintroduce the fundamental concepts of DMs, investigating how they are\nintegrated into RL frameworks to address key challenges in this research field.\nWe establish a dual-axis taxonomy that organizes the field along two orthogonal\ndimensions: a function-oriented taxonomy that clarifies the roles DMs play\nwithin the RL pipeline, and a technique-oriented taxonomy that situates\nimplementations across online versus offline learning regimes. We also provide\na comprehensive examination of this progression from single-agent to\nmulti-agent domains, thereby forming several frameworks for DM-RL integration\nand highlighting their practical utility. Furthermore, we outline several\ncategories of successful applications of diffusion-based RL across diverse\ndomains, discuss open research issues of current methodologies, and highlight\nkey directions for future research to advance the field. Finally, we summarize\nthe survey to identify promising future development directions. We are actively\nmaintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for\npapers and other related resources to apply DMs for RL.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5168\u9762\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u53cc\u8f74\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4e86\u4ece\u5355\u667a\u80fd\u4f53\u5230\u591a\u667a\u80fd\u4f53\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9886\u5148\u7684\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u8868\u8fbe\u80fd\u529b\u3001\u7a33\u5b9a\u8bad\u7ec3\u548c\u8f68\u8ff9\u7ea7\u89c4\u5212\u7b49\u5173\u952e\u4f18\u52bf\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "method": "\u5efa\u7acb\u529f\u80fd\u5bfc\u5411\u548c\u6280\u672f\u5bfc\u5411\u7684\u53cc\u8f74\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u6269\u6563\u6a21\u578b\u5728RL\u6d41\u7a0b\u4e2d\u7684\u4e0d\u540c\u89d2\u8272\uff0c\u4ee5\u53ca\u5728\u5728\u7ebf\u4e0e\u79bb\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u5f62\u6210\u4e86\u591a\u4e2aDM-RL\u96c6\u6210\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u6548\u7528\uff0c\u5e76\u7ef4\u62a4\u4e86\u76f8\u5173\u8d44\u6e90\u7684GitHub\u4ed3\u5e93\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u95ee\u9898\u548c\u590d\u6742\u89c4\u5212\u4efb\u52a1\u65b9\u9762\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2503.20934", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2503.20934", "abs": "https://arxiv.org/abs/2503.20934", "authors": ["Fraol Batole", "Abhiram Bellur", "Malinda Dilhara", "Mohammed Raihan Ullah", "Yaroslav Zharov", "Timofey Bryksin", "Kai Ishikawa", "Haifeng Chen", "Masaharu Morimoto", "Shota Motoura", "Takeo Hosomi", "Tien N. Nguyen", "Hridesh Rajan", "Nikolaos Tsantalis", "Danny Dig"], "title": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring", "comment": "12 pages, 2 figures", "summary": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful.", "AI": {"tldr": "MOVEMETHOD\u662f\u4e00\u79cd\u91cd\u8981\u7684\u4ee3\u7801\u91cd\u6784\u64cd\u4f5c\u3002\u7814\u7a76\u8005\u5f00\u53d1\u4e86MM-assist\u5de5\u5177\uff0c\u5229\u7528LLM\u3001IDE\u9759\u6001\u5206\u6790\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u81ea\u52a8\u5316MOVEMETHOD\u91cd\u6784\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u6784\u5efa\u8bae\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u91cd\u6784\u5de5\u5177\u63a8\u8350\u4e0e\u4e13\u5bb6\u5f00\u53d1\u8005\u7684\u5b9e\u9645\u64cd\u4f5c\u4e0d\u7b26\uff0c\u867d\u7136LLM\u80fd\u7ed9\u51fa\u4e13\u5bb6\u7ea7\u5efa\u8bae\u4f46\u5b58\u5728\u9ad8\u8fbe80%\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u52a8\u5316\u91cd\u6784\u52a9\u624b\u3002", "method": "\u8bbe\u8ba1\u4e86MM-assist\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u3001IDE\u9759\u6001\u5206\u6790\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u4f7f\u7528\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\u3001\u6279\u5224\u548c\u6392\u5e8f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u91c7\u7528\u91cd\u6784\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u89e3\u51b3LLM\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cRecall@1\u548cRecall@3\u63d0\u5347\u4e861.7\u500d\uff1b\u5728210\u4e2a\u5f00\u6e90\u8f6f\u4ef6\u91cd\u6784\u6848\u4f8b\u4e2d\uff0c\u53ec\u56de\u7387\u81f3\u5c11\u63d0\u5347\u4e862.4\u500d\uff1b\u7528\u6237\u7814\u7a76\u4e2d82.8%\u7684\u5efa\u8bae\u83b7\u5f97\u79ef\u6781\u8bc4\u4ef7\u3002", "conclusion": "MM-assist\u901a\u8fc7\u534f\u540c\u5229\u7528LLM\u3001IDE\u3001\u9759\u6001\u5206\u6790\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86MOVEMETHOD\u91cd\u6784\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u662f\u6709\u6548\u4e14\u6709\u7528\u7684\u91cd\u6784\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "2510.12312", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12312", "abs": "https://arxiv.org/abs/2510.12312", "authors": ["Florent Delgrange", "Raphael Avalos", "Willem R\u00f6pke"], "title": "Deep SPI: Safe Policy Improvement via World Models", "comment": "10 pages main text, 17 pages appendix (excluding references)", "summary": "Safe policy improvement (SPI) offers theoretical control over policy updates,\nyet existing guarantees largely concern offline, tabular reinforcement learning\n(RL). We study SPI in general online settings, when combined with world model\nand representation learning. We develop a theoretical framework showing that\nrestricting policy updates to a well-defined neighborhood of the current policy\nensures monotonic improvement and convergence. This analysis links transition\nand reward prediction losses to representation quality, yielding online, \"deep\"\nanalogues of classical SPI theorems from the offline RL literature. Building on\nthese results, we introduce DeepSPI, a principled on-policy algorithm that\ncouples local transition and reward losses with regularised policy updates. On\nthe ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including\nPPO and DeepMDPs, while retaining theoretical guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DeepSPI\u7b97\u6cd5\uff0c\u5c06\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u7406\u8bba\u6269\u5c55\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u573a\u666f\uff0c\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u548c\u8868\u793a\u5b66\u4e60\uff0c\u786e\u4fdd\u7b56\u7565\u66f4\u65b0\u7684\u5355\u8c03\u6539\u8fdb\u548c\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u79bb\u7ebf\u3001\u8868\u683c\u5316\u5f3a\u5316\u5b66\u4e60\uff0c\u7f3a\u4e4f\u5728\u5728\u7ebf\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6846\u67b6\uff0c\u9650\u5236\u7b56\u7565\u66f4\u65b0\u5728\u5f53\u524d\u7b56\u7565\u7684\u90bb\u57df\u5185\uff0c\u7ed3\u5408\u5c40\u90e8\u8f6c\u79fb\u548c\u5956\u52b1\u635f\u5931\u4e0e\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\uff0c\u63d0\u51faDeepSPI\u7b97\u6cd5\u3002", "result": "\u5728ALE-57\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeepSPI\u5339\u914d\u6216\u8d85\u8d8a\u4e86PPO\u548cDeepMDPs\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u6210\u529f\u5c06\u7ecf\u5178\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u7406\u8bba\u6269\u5c55\u5230\u5728\u7ebf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5efa\u7acb\u4e86\u8868\u793a\u8d28\u91cf\u4e0e\u9884\u6d4b\u635f\u5931\u7684\u7406\u8bba\u8054\u7cfb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12334", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12334", "abs": "https://arxiv.org/abs/2510.12334", "authors": ["Rui Hu", "Yu Chen", "Longbo Huang"], "title": "Finite-time Convergence Analysis of Actor-Critic with Evolving Reward", "comment": null, "summary": "Many popular practical reinforcement learning (RL) algorithms employ evolving\nreward functions-through techniques such as reward shaping, entropy\nregularization, or curriculum learning-yet their theoretical foundations remain\nunderdeveloped. This paper provides the first finite-time convergence analysis\nof a single-timescale actor-critic algorithm in the presence of an evolving\nreward function under Markovian sampling. We consider a setting where the\nreward parameters may change at each time step, affecting both policy\noptimization and value estimation. Under standard assumptions, we derive\nnon-asymptotic bounds for both actor and critic errors. Our result shows that\nan $O(1/\\sqrt{T})$ convergence rate is achievable, matching the best-known rate\nfor static rewards, provided the reward parameters evolve slowly enough. This\nrate is preserved when the reward is updated via a gradient-based rule with\nbounded gradient and on the same timescale as the actor and critic, offering a\ntheoretical foundation for many popular RL techniques. As a secondary\ncontribution, we introduce a novel analysis of distribution mismatch under\nMarkovian sampling, improving the best-known rate by a factor of $\\log^2T$ in\nthe static-reward case.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u4e0b\u5177\u6709\u6f14\u5316\u5956\u52b1\u51fd\u6570\u7684\u5355\u65f6\u95f4\u5c3a\u5ea6actor-critic\u7b97\u6cd5\u8fdb\u884c\u4e86\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u5956\u52b1\u53c2\u6570\u7f13\u6162\u53d8\u5316\u65f6\u4ecd\u80fd\u8fbe\u5230O(1/\u221aT)\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u8bb8\u591a\u5b9e\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f7f\u7528\u6f14\u5316\u5956\u52b1\u51fd\u6570\uff08\u5982\u5956\u52b1\u5851\u9020\u3001\u71b5\u6b63\u5219\u5316\u3001\u8bfe\u7a0b\u5b66\u4e60\uff09\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u7684\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5145\u5206\u53d1\u5c55\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u6765\u652f\u6301\u8fd9\u4e9b\u6d41\u884c\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5355\u65f6\u95f4\u5c3a\u5ea6actor-critic\u7b97\u6cd5\uff0c\u5728\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u4e0b\u5206\u6790\u5956\u52b1\u53c2\u6570\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\uff0c\u8003\u8651\u5956\u52b1\u53c2\u6570\u53d8\u5316\u5bf9\u7b56\u7565\u4f18\u5316\u548c\u4ef7\u503c\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\uff0c\u63a8\u5bfc\u4e86actor\u548ccritic\u8bef\u5dee\u7684\u975e\u6e10\u8fd1\u754c\uff0c\u8bc1\u660e\u5f53\u5956\u52b1\u53c2\u6570\u6f14\u5316\u8db3\u591f\u6162\u65f6\uff0c\u53ef\u4ee5\u8fbe\u5230O(1/\u221aT)\u7684\u6536\u655b\u901f\u7387\uff0c\u4e0e\u9759\u6001\u5956\u52b1\u7684\u6700\u4f73\u5df2\u77e5\u901f\u7387\u76f8\u5339\u914d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bb8\u591a\u6d41\u884c\u7684RL\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8868\u660e\u5728\u5956\u52b1\u51fd\u6570\u6f14\u5316\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6536\u655b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12720", "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12720", "abs": "https://arxiv.org/abs/2510.12720", "authors": ["Ziyang Ma", "Ruiyang Xu", "Zhenghao Xing", "Yunfei Chu", "Yuxuan Wang", "Jinzheng He", "Jin Xu", "Pheng-Ann Heng", "Kai Yu", "Junyang Lin", "Eng Siong Chng", "Xie Chen"], "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception", "comment": "https://github.com/ddlBoJack/Omni-Captioner", "summary": "Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.", "AI": {"tldr": "\u63d0\u51fa\u4e86Omni-Detective\u6570\u636e\u751f\u6210\u7ba1\u9053\u548cOmni-Cloze\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891-\u89c6\u89c9\u7ec6\u7c92\u5ea6\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u7ec6\u8282\u4e0e\u5e7b\u89c9\u5171\u589e\u957f\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u548c\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u7ec6\u8282\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5b58\u5728\u7ec6\u8282\u4e0e\u5e7b\u89c9\u5171\u589e\u957f\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86Omni-Detective\u4ee3\u7406\u5f0f\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u96c6\u6210\u5de5\u5177\u8c03\u7528\u6765\u81ea\u4e3b\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\uff1b\u8bad\u7ec3\u4e86Audio-Captioner\u548cOmni-Captioner\u6a21\u578b\uff1b\u8bbe\u8ba1\u4e86Omni-Cloze\u586b\u7a7a\u5f0f\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "Audio-Captioner\u5728MMAU\u548cMMAR\u57fa\u51c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8d85\u8d8aGemini 2.5 Flash\uff0c\u4e0eGemini 2.5 Pro\u76f8\u5f53\uff1bOmni-Captioner\u5728VDC\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u5728video-SALMONN 2\u4e0a\u5b9e\u73b0\u7ec6\u8282\u4e0e\u5e7b\u89c9\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "Omni-Detective\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u8be6\u7ec6\u63cf\u8ff0\uff0cOmni-Cloze\u4e3a\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u611f\u77e5\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.12773", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12773", "abs": "https://arxiv.org/abs/2510.12773", "authors": ["Ahmed Heakl", "Martin Gubri", "Salman Khan", "Sangdoo Yun", "Seong Joon Oh"], "title": "Dr.LLM: Dynamic Layer Routing in LLMs", "comment": "17 pages, Under submission", "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.", "AI": {"tldr": "Dr.LLM\u662f\u4e00\u4e2a\u52a8\u6001\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u51b3\u5b9a\u8df3\u8fc7\u3001\u6267\u884c\u6216\u91cd\u590dTransformer\u5c42\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edfLLM\u5bf9\u6240\u6709token\u90fd\u7ecf\u8fc7\u6240\u6709Transformer\u5c42\uff0c\u5bfc\u81f4\u7b80\u5355\u67e5\u8be2\u8ba1\u7b97\u6d6a\u8d39\uff0c\u590d\u6742\u63a8\u7406\u6df1\u5ea6\u4e0d\u8db3\u3002\u73b0\u6709\u81ea\u9002\u5e94\u6df1\u5ea6\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u63a8\u7406\u641c\u7d22\u6216\u67b6\u6784\u4fee\u6539\uff0c\u4e14\u5f80\u5f80\u727a\u7272\u7cbe\u5ea6", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u63a8\u5bfc\u9ad8\u8d28\u91cf\u5c42\u914d\u7f6e\uff0c\u5728\u8ba1\u7b97\u9884\u7b97\u4e0b\u4fdd\u6301\u6216\u63d0\u9ad8\u7cbe\u5ea6\u3002\u91c7\u7528\u7a97\u53e3\u6c60\u5316\u7a33\u5b9a\u8def\u7531\u3001\u7126\u70b9\u635f\u5931\u4e0e\u7c7b\u522b\u5e73\u8861\u3001\u74f6\u9888MLP\u8def\u7531\u5668\u7b49\u6280\u672f", "result": "\u5728ARC\u548cDART\u4efb\u52a1\u4e0a\u7cbe\u5ea6\u63d0\u5347\u8fbe+3.4%\uff0c\u5e73\u5747\u6bcf\u6837\u672c\u8282\u77015\u5c42\u3002\u8def\u7531\u5668\u6cdb\u5316\u5230\u591a\u4e2a\u9886\u57df\u4efb\u52a1\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.85%\u540c\u65f6\u4fdd\u6301\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u8fbe+7.7%", "conclusion": "Dr.LLM\u8bc1\u660e\u663e\u5f0f\u76d1\u7763\u7684\u8def\u7531\u5668\u53ef\u4ee5\u6539\u9020\u51bb\u7ed3LLM\uff0c\u5b9e\u73b0\u9884\u7b97\u611f\u77e5\u3001\u7cbe\u5ea6\u9a71\u52a8\u7684\u63a8\u7406\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6743\u91cd", "topic": "agent analysis"}}
{"id": "2510.12624", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12624", "abs": "https://arxiv.org/abs/2510.12624", "authors": ["Yuta Kobayashi", "Zilin Jing", "Jiayu Yao", "Hongseok Namkoong", "Shalmali Joshi"], "title": "Learning-To-Measure: In-context Active Feature Acquisition", "comment": null, "summary": "Active feature acquisition (AFA) is a sequential decision-making problem\nwhere the goal is to improve model performance for test instances by adaptively\nselecting which features to acquire. In practice, AFA methods often learn from\nretrospective data with systematic missingness in the features and limited\ntask-specific labels. Most prior work addresses acquisition for a single\npredetermined task, limiting scalability. To address this limitation, we\nformalize the meta-AFA problem, where the goal is to learn acquisition policies\nacross various tasks. We introduce Learning-to-Measure (L2M), which consists of\ni) reliable uncertainty quantification over unseen tasks, and ii) an\nuncertainty-guided greedy feature acquisition agent that maximizes conditional\nmutual information. We demonstrate a sequence-modeling or autoregressive\npre-training approach that underpins reliable uncertainty quantification for\ntasks with arbitrary missingness. L2M operates directly on datasets with\nretrospective missingness and performs the meta-AFA task in-context,\neliminating per-task retraining. Across synthetic and real-world tabular\nbenchmarks, L2M matches or surpasses task-specific baselines, particularly\nunder scarce labels and high missingness.", "AI": {"tldr": "\u63d0\u51fa\u5143\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\uff08meta-AFA\uff09\u95ee\u9898\uff0c\u5f00\u53d1Learning-to-Measure\uff08L2M\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u9884\u8bad\u7ec3\u5b9e\u73b0\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u65e0\u9700\u9010\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8de8\u4efb\u52a1\u5b66\u4e60\u7279\u5f81\u83b7\u53d6\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u9884\u5b9a\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u4ece\u5177\u6709\u7cfb\u7edf\u6027\u7f3a\u5931\u7279\u5f81\u548c\u6709\u9650\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u7684\u56de\u987e\u6027\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u8de8\u4efb\u52a1\u5b66\u4e60\u7684\u5143AFA\u65b9\u6cd5\u3002", "method": "L2M\u5305\u542b\uff1a1\uff09\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff1b2\uff09\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u8d2a\u5a6a\u7279\u5f81\u83b7\u53d6\u4ee3\u7406\uff0c\u6700\u5927\u5316\u6761\u4ef6\u4e92\u4fe1\u606f\u3002\u91c7\u7528\u5e8f\u5217\u5efa\u6a21\u6216\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u5177\u6709\u56de\u987e\u6027\u7f3a\u5931\u7684\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\uff0c\u8fdb\u884c\u4e0a\u4e0b\u6587\u4e2d\u7684\u5143AFA\u4efb\u52a1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8868\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cL2M\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u7f3a\u5931\u7387\u9ad8\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "L2M\u6210\u529f\u89e3\u51b3\u4e86\u5143AFA\u95ee\u9898\uff0c\u80fd\u591f\u8de8\u4efb\u52a1\u5b66\u4e60\u7279\u5f81\u83b7\u53d6\u7b56\u7565\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u7f3a\u5931\u7387\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u65e0\u9700\u9010\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12633", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12633", "abs": "https://arxiv.org/abs/2510.12633", "authors": ["Guangming Sheng", "Yuxuan Tong", "Borui Wan", "Wang Zhang", "Chaobo Jia", "Xibin Wu", "Yuqi Wu", "Xiang Li", "Chi Zhang", "Yanghua Peng", "Haibin Lin", "Xin Liu", "Chuan Wu"], "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework", "comment": null, "summary": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time.", "AI": {"tldr": "Laminar\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5f02\u6b65\u548c\u5b8c\u5168\u89e3\u8026\u67b6\u6784\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u7684GPU\u5229\u7528\u7387\u95ee\u9898\uff0c\u57281024-GPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u4e865.48\u500d\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709RL\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u53d7\u9650\uff0c\u56e0\u4e3aRL\u8f68\u8ff9\u751f\u6210\u7684\u6781\u7aef\u957f\u5c3e\u504f\u659c\u5bfc\u81f4\u4e25\u91cd\u7684GPU\u5229\u7528\u7387\u4e0d\u8db3\u3002\u5168\u5c40\u6743\u91cd\u540c\u6b65\u673a\u5236\u4e0d\u9002\u5408RL\u8bad\u7ec3\u4e2d\u9ad8\u5ea6\u504f\u659c\u548c\u6f14\u5316\u7684\u8f68\u8ff9\u751f\u6210\u5ef6\u8fdf\u5206\u5e03\u3002", "method": "\u63d0\u51faLaminar\u7cfb\u7edf\uff1a1\uff09\u7528\u4e2d\u7ee7\u5de5\u4f5c\u8005\u5c42\u7ea7\u66ff\u4ee3\u5168\u5c40\u66f4\u65b0\uff0c\u5b9e\u73b0\u5f02\u6b65\u7ec6\u7c92\u5ea6\u6743\u91cd\u540c\u6b65\uff1b2\uff09\u52a8\u6001\u91cd\u65b0\u6253\u5305\u673a\u5236\u5c06\u957f\u5c3e\u8f68\u8ff9\u6574\u5408\u5230\u4e13\u7528rollout\u4e0a\uff1b3\uff09\u5b8c\u5168\u89e3\u8026\u67b6\u6784\u9694\u79bb\u6545\u969c\u3002", "result": "\u57281024-GPU\u96c6\u7fa4\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLaminar\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.48\u500d\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u52a0\u901f\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6a21\u578b\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "\u8f68\u8ff9\u7ea7\u5f02\u6b65\u548c\u5b8c\u5168\u89e3\u8026\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3RL\u540e\u8bad\u7ec3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12638", "abs": "https://arxiv.org/abs/2510.12638", "authors": ["Arip Asadulaev", "Fakhri Karray", "Martin Takac"], "title": "Expert or not? assessing data quality in offline reinforcement learning", "comment": null, "summary": "Offline reinforcement learning (RL) learns exclusively from static datasets,\nwithout further interaction with the environment. In practice, such datasets\nvary widely in quality, often mixing expert, suboptimal, and even random\ntrajectories. The choice of algorithm therefore depends on dataset fidelity.\nBehavior cloning can suffice on high-quality data, whereas mixed- or\nlow-quality data typically benefits from offline RL methods that stitch useful\nbehavior across trajectories. Yet in the wild it is difficult to assess dataset\nquality a priori because the data's provenance and skill composition are\nunknown. We address the problem of estimating offline dataset quality without\ntraining an agent. We study a spectrum of proxies from simple cumulative\nrewards to learned value based estimators, and introduce the Bellman\nWasserstein distance (BWD), a value aware optimal transport score that measures\nhow dissimilar a dataset's behavioral policy is from a random reference policy.\nBWD is computed from a behavioral critic and a state conditional OT\nformulation, requiring no environment interaction or full policy optimization.\nAcross D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance\nscore that aggregates multiple offline RL algorithms, enabling efficient\nprediction of how well standard agents will perform on a given dataset. Beyond\nprediction, integrating BWD as a regularizer during policy optimization\nexplicitly pushes the learned policy away from random behavior and improves\nreturns. These results indicate that value aware, distributional signals such\nas BWD are practical tools for triaging offline RL datasets and policy\noptimization.", "AI": {"tldr": "\u63d0\u51faBellman Wasserstein\u8ddd\u79bb(BWD)\u4f5c\u4e3a\u79bb\u7ebf\u6570\u636e\u96c6\u8d28\u91cf\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u65e0\u9700\u8bad\u7ec3\u667a\u80fd\u4f53\u5373\u53ef\u9884\u6d4b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\uff0c\u4f46\u96be\u4ee5\u9884\u5148\u8bc4\u4f30\uff0c\u56e0\u4e3a\u6570\u636e\u6765\u6e90\u548c\u6280\u80fd\u7ec4\u6210\u672a\u77e5\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u667a\u80fd\u4f53\u5c31\u80fd\u4f30\u8ba1\u6570\u636e\u96c6\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4ece\u7b80\u5355\u7d2f\u79ef\u5956\u52b1\u5230\u57fa\u4e8e\u5b66\u4e60\u4ef7\u503c\u4f30\u8ba1\u5668\u7684\u591a\u79cd\u4ee3\u7406\u6307\u6807\uff0c\u5f15\u5165BWD\u2014\u2014\u4e00\u79cd\u4ef7\u503c\u611f\u77e5\u7684\u6700\u4f18\u4f20\u8f93\u8bc4\u5206\uff0c\u901a\u8fc7\u884c\u4e3a\u8bc4\u8bba\u5bb6\u548c\u72b6\u6001\u6761\u4ef6\u6700\u4f18\u4f20\u8f93\u516c\u5f0f\u8ba1\u7b97\uff0c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u6216\u5b8c\u6574\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728D4RL MuJoCo\u4efb\u52a1\u4e2d\uff0cBWD\u4e0e\u591a\u4e2a\u79bb\u7ebfRL\u7b97\u6cd5\u7684oracle\u6027\u80fd\u5f97\u5206\u5f3a\u76f8\u5173\uff0c\u80fd\u6709\u6548\u9884\u6d4b\u6807\u51c6\u667a\u80fd\u4f53\u5728\u7ed9\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u4f7f\u7528\u65f6\uff0cBWD\u80fd\u63d0\u5347\u56de\u62a5\u3002", "conclusion": "BWD\u7b49\u4ef7\u503c\u611f\u77e5\u7684\u5206\u5e03\u4fe1\u53f7\u662f\u7b5b\u9009\u79bb\u7ebfRL\u6570\u636e\u96c6\u548c\u4f18\u5316\u7b56\u7565\u7684\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.9aa2dcb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-plugins%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/OHDWYGKeEqsJC8yVWCokJCsVg5tKCbXqvRU56_UKybI=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-plugins%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/OHDWYGKeEqsJC8yVWCokJCsVg5tKCbXqvRU56_UKybI=426", "authors": ["TLDR Newsletter"], "title": "Claude Code Plugins Now in Public Beta", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-plugins%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/OHDWYGKeEqsJC8yVWCokJCsVg5tKCbXqvRU56_UKybI=426", "summary": "Claude Code Plugins Now in Public Beta (3 minute read) Anthropic has introduced plugin support in Claude Code, allowing users to install custom slash commands, agents, MCP servers, and hooks with a single command. These modular extensions simplify setup sharing and enable flexible customizations for development workflows.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Code\u63d2\u4ef6\u516c\u5f00\u6d4b\u8bd5\u7248\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u5355\u4e00\u547d\u4ee4\u5b89\u88c5\u81ea\u5b9a\u4e49\u659c\u6760\u547d\u4ee4\u3001\u4ee3\u7406\u3001MCP\u670d\u52a1\u5668\u548c\u94a9\u5b50", "motivation": "\u7b80\u5316\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u5b9a\u5236\u5316\u8bbe\u7f6e\uff0c\u4f7f\u63d2\u4ef6\u914d\u7f6e\u66f4\u6613\u4e8e\u5206\u4eab\u548c\u4f7f\u7528", "method": "\u5728Claude Code\u4e2d\u5f15\u5165\u63d2\u4ef6\u652f\u6301\u7cfb\u7edf\uff0c\u5141\u8bb8\u6a21\u5757\u5316\u6269\u5c55\u529f\u80fd", "result": "\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u8f7b\u677e\u5b89\u88c5\u548c\u7ba1\u7406\u5404\u79cd\u81ea\u5b9a\u4e49\u5f00\u53d1\u5de5\u5177\u548c\u529f\u80fd", "conclusion": "\u63d2\u4ef6\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86Claude Code\u7684\u7075\u6d3b\u6027\u548c\u5b9a\u5236\u80fd\u529b", "topic": "swe application"}}
{"id": "tldr.2510.dd3586e3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2509.25140%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/BlPQKo70UKj1yrRD4ZpOT48ouUuiIxz_6DBSirTUTa4=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2509.25140%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/BlPQKo70UKj1yrRD4ZpOT48ouUuiIxz_6DBSirTUTa4=426", "authors": ["TLDR Newsletter"], "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2509.25140%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/BlPQKo70UKj1yrRD4ZpOT48ouUuiIxz_6DBSirTUTa4=426", "summary": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory (1 minute read) ReasoningBank is a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. Agents can store and retrieve memories from ReasoningBank to inform their decisions. They can integrate new learnings and become more capable over time. The better memory enables more effective scaling. ReasoningBank consistently outperforms existing memory mech...", "source": "tldr", "AI": {"tldr": "ReasoningBank\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u667a\u80fd\u4f53\u81ea\u6211\u8bc4\u4f30\u7684\u6210\u529f\u548c\u5931\u8d25\u7ecf\u9a8c\u4e2d\u63d0\u70bc\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u7b56\u7565\u6765\u6269\u5c55\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u8bb0\u5fc6\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u6301\u7eed\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u7684\u8bb0\u5fc6\u673a\u5236\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u8bb0\u5fc6\u6846\u67b6\uff0c\u5b58\u50a8\u548c\u68c0\u7d22\u667a\u80fd\u4f53\u7684\u6210\u529f\u4e0e\u5931\u8d25\u7ecf\u9a8c\uff0c\u4ece\u4e2d\u63d0\u70bc\u901a\u7528\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5141\u8bb8\u667a\u80fd\u4f53\u6574\u5408\u65b0\u5b66\u4e60\u5185\u5bb9\u968f\u65f6\u95f4\u53d8\u5f97\u66f4\u5f3a\u3002", "result": "ReasoningBank\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u6269\u5c55\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u8bb0\u5fc6\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u548c\u81ea\u6211\u8fdb\u5316\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.d6e2b5d9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.08558%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/klv_KJsUItzBiBTFn0NkYvYaZVToYnIYXl1ECSESBPE=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.08558%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/klv_KJsUItzBiBTFn0NkYvYaZVToYnIYXl1ECSESBPE=426", "authors": ["TLDR Newsletter"], "title": "Meta's Agent Learning", "comment": "Source: TLDR Newsletter, Date: 2025-10-13, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.08558%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/klv_KJsUItzBiBTFn0NkYvYaZVToYnIYXl1ECSESBPE=426", "summary": "Meta's Agent Learning (19 minute read) Meta has introduced \"early experience,\" a training approach using data from an agent's own interactions without external rewards. It improves policy learning via implicit world modeling and self-reflection.", "source": "tldr", "AI": {"tldr": "Meta\u63d0\u51fa\"\u65e9\u671f\u7ecf\u9a8c\"\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u667a\u80fd\u4f53\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\u65e0\u9700\u5916\u90e8\u5956\u52b1\uff0c\u901a\u8fc7\u9690\u5f0f\u4e16\u754c\u5efa\u6a21\u548c\u81ea\u6211\u53cd\u601d\u6539\u8fdb\u7b56\u7565\u5b66\u4e60", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5229\u7528\u667a\u80fd\u4f53\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u66f4\u6709\u6548\u7684\u7b56\u7565\u5b66\u4e60", "method": "\u4f7f\u7528\u667a\u80fd\u4f53\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\uff0c\u7ed3\u5408\u9690\u5f0f\u4e16\u754c\u5efa\u6a21\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\u6765\u8bad\u7ec3\u7b56\u7565", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6539\u8fdb\u7b56\u7565\u5b66\u4e60\u6548\u679c\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u4fe1\u53f7", "conclusion": "\u65e9\u671f\u7ecf\u9a8c\u65b9\u6cd5\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.ba0b825d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkix.dev%2Ftwo-things-llm-coding-agents-are-still-bad-at%3Futm_source=tldrwebdev/1/01000199e267d218-3779c78d-8eba-471e-92b4-20c4b8a12ef4-000000/M1mxzN9gUwf0Gut4BiW6hUoIzBL-zrerpmIvokysNwQ=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkix.dev%2Ftwo-things-llm-coding-agents-are-still-bad-at%3Futm_source=tldrwebdev/1/01000199e267d218-3779c78d-8eba-471e-92b4-20c4b8a12ef4-000000/M1mxzN9gUwf0Gut4BiW6hUoIzBL-zrerpmIvokysNwQ=426", "authors": ["TLDR Newsletter"], "title": "Two things LLM coding agents are still bad at", "comment": "Source: TLDR Newsletter, Date: 2025-10-14, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkix.dev%2Ftwo-things-llm-coding-agents-are-still-bad-at%3Futm_source=tldrwebdev/1/01000199e267d218-3779c78d-8eba-471e-92b4-20c4b8a12ef4-000000/M1mxzN9gUwf0Gut4BiW6hUoIzBL-zrerpmIvokysNwQ=426", "summary": "Two things LLM coding agents are still bad at (2 minute read) LLM coding agents still feel awkward to work with due to two main issues: they don't copy-paste code like humans do (instead \"remembering\" and rewriting code from memory), and they're terrible at asking clarifying questions, preferring to make assumptions and brute-force solutions. These quirks make LLMs feel more like \"weird, overconfident interns\" rather than replacements for human developers.", "source": "tldr", "AI": {"tldr": "LLM\u7f16\u7a0b\u4ee3\u7406\u4ecd\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u590d\u5236\u7c98\u8d34\u4ee3\u7801\uff08\u800c\u662f\u4ece\u8bb0\u5fc6\u4e2d\u91cd\u5199\uff09\uff0c\u4ee5\u53ca\u4e0d\u5584\u4e8e\u63d0\u95ee\u6f84\u6e05\u95ee\u9898\uff08\u503e\u5411\u4e8e\u5047\u8bbe\u548c\u66b4\u529b\u6c42\u89e3\uff09\u3002", "motivation": "\u5206\u6790\u5f53\u524dLLM\u7f16\u7a0b\u4ee3\u7406\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u5de5\u4f5c\u65b9\u5f0f\u7684\u5dee\u5f02\uff0c\u5e2e\u52a9\u7406\u89e3\u4e3a\u4f55LLM\u4ee3\u7406\u5c1a\u672a\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u5f00\u53d1\u8005\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u548c\u5206\u6790LLM\u7f16\u7a0b\u4ee3\u7406\u5728\u5b9e\u9645\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8bc6\u522b\u5176\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u5de5\u4f5c\u4e60\u60ef\u7684\u5173\u952e\u5dee\u5f02\u3002", "result": "\u53d1\u73b0LLM\u7f16\u7a0b\u4ee3\u7406\u5728\u4ee3\u7801\u91cd\u7528\u65b9\u5f0f\uff08\u8bb0\u5fc6\u91cd\u5199\u800c\u975e\u590d\u5236\u7c98\u8d34\uff09\u548c\u95ee\u9898\u6f84\u6e05\uff08\u5047\u8bbe\u800c\u975e\u63d0\u95ee\uff09\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u7f16\u7a0b\u4ee3\u7406\u76ee\u524d\u66f4\u50cf\u662f\"\u5947\u602a\u4e14\u8fc7\u5ea6\u81ea\u4fe1\u7684\u5b9e\u4e60\u751f\"\uff0c\u800c\u975e\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u66ff\u4ee3\u54c1\uff0c\u9700\u8981\u5728\u8fd9\u4e9b\u5173\u952e\u884c\u4e3a\u6a21\u5f0f\u4e0a\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.eb916bab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faccented.dev%2F%3Futm_source=tldrdesign/1/01000199e2aa9543-0c6e671b-eb4b-48e0-9806-903a3a29535e-000000/umnJRO1hIoBmi0biRcE-ksbWA5EK5osQdUCwXAo8Lew=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faccented.dev%2F%3Futm_source=tldrdesign/1/01000199e2aa9543-0c6e671b-eb4b-48e0-9806-903a3a29535e-000000/umnJRO1hIoBmi0biRcE-ksbWA5EK5osQdUCwXAo8Lew=426", "authors": ["TLDR Newsletter"], "title": "Library for Accessibility Testing", "comment": "Source: TLDR Newsletter, Date: 2025-10-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faccented.dev%2F%3Futm_source=tldrdesign/1/01000199e2aa9543-0c6e671b-eb4b-48e0-9806-903a3a29535e-000000/umnJRO1hIoBmi0biRcE-ksbWA5EK5osQdUCwXAo8Lew=426", "summary": "Library for Accessibility Testing (Website) Accented is a frontend library for continuous accessibility testing and issue highlighting. Add a few lines of code to your web app, and you'll see interactive callouts appear next to elements with accessibility issues.", "source": "tldr", "AI": {"tldr": "Accented\u662f\u4e00\u4e2a\u524d\u7aef\u5e93\uff0c\u7528\u4e8e\u6301\u7eed\u8fdb\u884c\u53ef\u8bbf\u95ee\u6027\u6d4b\u8bd5\u548c\u95ee\u9898\u9ad8\u4eae\u663e\u793a\u3002\u53ea\u9700\u5728Web\u5e94\u7528\u4e2d\u6dfb\u52a0\u51e0\u884c\u4ee3\u7801\uff0c\u5373\u53ef\u5728\u5b58\u5728\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u7684\u5143\u7d20\u65c1\u8fb9\u663e\u793a\u4ea4\u4e92\u5f0f\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3Web\u5e94\u7528\u5f00\u53d1\u4e2d\u53ef\u8bbf\u95ee\u6027\u6d4b\u8bd5\u7684\u590d\u6742\u6027\u548c\u6301\u7eed\u6027\u95ee\u9898\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u8f7b\u677e\u8bc6\u522b\u548c\u4fee\u590d\u53ef\u8bbf\u95ee\u6027\u7f3a\u9677\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u524d\u7aefJavaScript\u5e93\uff0c\u901a\u8fc7\u4ee3\u7801\u6ce8\u5165\u5728\u8fd0\u884c\u65f6\u68c0\u6d4b\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u5e76\u5728\u95ee\u9898\u5143\u7d20\u65c1\u663e\u793a\u4ea4\u4e92\u5f0f\u6807\u6ce8\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5e93\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u5e76\u9ad8\u4eae\u663e\u793aWeb\u5e94\u7528\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u53cd\u9988\u673a\u5236\u3002", "conclusion": "Accented\u7b80\u5316\u4e86Web\u53ef\u8bbf\u95ee\u6027\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u65e0\u969c\u788d\u7684Web\u5e94\u7528\u3002", "topic": "swe application"}}
