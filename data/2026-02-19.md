<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [tldr.article](#tldr.article) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts](https://arxiv.org/abs/2602.15843)
*Warren Johnson*

Main category: cs.CL

TL;DR: 论文提出任务感知自适应压缩(TAAC)方法，通过验证代码生成与推理任务的压缩阈值差异，发现"困惑度悖论"，并实现22%成本降低与96%质量保持。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现代码生成能容忍激进提示压缩(r>=0.6)而思维链推理会逐渐退化，但研究仅限于HumanEval基准、未验证"困惑度悖论"机制、缺乏自适应算法。本文旨在填补这三个研究空白。

Method: 1) 在6个代码基准和4个推理基准上验证压缩阈值；2) 首次进行每令牌困惑度分析(n=723令牌)，揭示"困惑度悖论"；3) 提出任务感知自适应压缩(TAAC)算法。

Result: 压缩阈值在不同语言和难度任务中普遍适用；代码语法令牌被保留(高困惑度)而数学问题中的数值被修剪(低困惑度)；签名注入使通过率提升+34个百分点；TAAC实现22%成本降低和96%质量保持。

Conclusion: 任务感知自适应压缩能显著降低大语言模型推理成本，同时保持高质量输出，为不同任务类型提供优化的压缩策略。

Abstract: In "Compress or Route?" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the "perplexity paradox" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a "perplexity paradox": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.

</details>


### [2] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: IntelliReward是一个基于冻结自回归LLM构建的新型奖励模型，用于预测专家级人类偏好，结合IntelliAsk模型生成符合人类标准的审稿问题，在多个基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的审稿问题生成方法往往产生表面化问题，超过50%的问题token来自论文第一页，缺乏实质性、基于证据的深度提问。

Method: 开发IntelliReward奖励模型（基于冻结自回归LLM+可训练多头transformer），应用Decoupled Clip和DAPO策略优化，训练IntelliAsk问题生成模型，使其与人类在努力程度、证据基础和接地性方面的标准对齐。

Result: IntelliAsk在推理和写作基准测试中表现一致优于基线模型，在MuSR推理任务上准确率从64.7提升到68.3，在WritingBench写作评估中从8.07提升到8.31。

Conclusion: 审稿问题质量与模型更广泛的能力相关，IntelliReward和IntelliAsk为LLM生成的审稿问题提供了自动评估基准，推动了实质性审稿问题生成的发展。

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [3] [Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization](https://arxiv.org/abs/2602.15854)
*Jingyi Xu,Xingyu Ren,Zhiqiang You,Yumeng Zhang,Zhoupeng Shou*

Main category: cs.CL

TL;DR: 提出GOPO框架，通过分层强化学习将策略规划与响应生成解耦，在任务导向对话系统中显著提升长时程任务成功率


<details>
  <summary>Details</summary>
Motivation: 现有训练方法依赖token级似然或偏好优化，与长时程任务成功对齐不佳，需要更好的任务导向对话系统训练框架

Method: GOPO分层强化学习框架：专家代理优化对话轨迹级多轮目标偏好，客服代理生成与选定策略严格对齐的响应

Result: 在Mgshop数据集上，GOPO比PPO和Memento分别提升TSE 7.7%和10.3%；14B模型训练后比Qwen-235B和GPT-5.2分别高2.7%和1.5% TSE

Conclusion: GOPO为商业场景任务导向对话系统建立新范式，专家代理在长时程优化中起关键作用，代码和数据集将公开

Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.

</details>


### [4] [State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models](https://arxiv.org/abs/2602.15858)
*Annie Wong,Aske Plaat,Thomas Bäck,Niki van Stein,Anna V. Kononova*

Main category: cs.CL

TL;DR: 该论文研究了在动态环境中LLMs的状态表示设计对性能的影响，发现轨迹总结、自然语言表示和文本空间编码最有效，但当前模型在长时程任务中仍显脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs从静态推理任务转向动态环境，其成功取决于在推理时与变化环境交互的能力。状态表示作为这些设置中未充分探索的因素，对模型性能有重要影响。

Method: 在保持模型参数固定的条件下，系统性地改变三个关键方面：(1)状态粒度（长形式vs总结），(2)结构（自然语言vs符号），(3)空间基础（纯文本vs图像或文本地图编码），并在顺序决策基准上进行测试。

Result: 轨迹总结通过减少噪声和稳定长时程推理来提高性能；自然语言表示在不同模型中最稳健；结构化编码主要对具有强代码或结构化输出先验的模型有帮助；文本空间编码比图像输入更有效，优势来自构建过程本身而非空间信息。

Conclusion: 状态表示的设计选择是性能的决定性因素，与信息可用性本身不同。然而，即使有改进的表示，当前LLMs和VLMs在长时程任务中仍然脆弱，特别是在需要综合信息管理多个子任务以达到目标时。

Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.

</details>


### [5] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 研究发现LLMs自我生成少样本示例的优势并非来自示例本身，而是来自创建过程。集成提示（创建和解决问题在同一提示中）优于零样本和分离提示。


<details>
  <summary>Details</summary>
Motivation: 尽管研究表明LLMs通过自我生成少样本示例可以提高推理性能，但背后的机制尚不清楚，难以确定何时以及如何有效应用该技术。

Method: 在多种LLM架构上系统评估三种提示策略：零样本提示、集成提示（LLMs在同一提示中创建并解决问题）、分离提示（重用自生成示例但排除创建上下文）。进行注意力分析比较模式差异。

Result: 集成提示始终优于零样本和分离提示，而分离提示相比零样本只有边际改进。注意力分析显示集成提示和分离提示的注意力模式存在显著差异。

Conclusion: 自我生成提示的优势来自问题创建过程本身，而非生成的示例，这为设计更有效的提示策略提供了重要见解。

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [6] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: 提出使用确定性多带图灵机形式化LLM交互，将不同组件映射到不同磁带，从而精确定位故障模式到特定处理阶段


<details>
  <summary>Details</summary>
Motivation: LLM在看似简单的任务上存在故障模式，需要更严谨的形式化方法来理解和定位这些故障，而不是依赖几何隐喻

Method: 使用确定性多带图灵机形式化LLM交互，将输入字符、token、词汇表、模型参数、激活、概率分布和输出文本映射到不同的磁带

Result: 能够精确定位故障模式到特定处理阶段（如tokenization如何模糊字符级结构），解释为什么思维链提示有效（通过外部化计算），同时揭示其根本局限性

Conclusion: 该方法为LLM故障分析提供了严谨、可证伪的替代方案，补充了经验缩放定律，提供了原则性错误分析框架

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [7] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: 提出QEMPO方法，通过质量约束的熵最大化策略优化，在保证输出质量的同时提升LLM输出的多样性，解决了对齐方法减少输出多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法虽然提升了LLM输出质量，但同时也减少了输出多样性。现有增强多样性的方法往往以性能下降为代价，需要一种能同时保证质量和多样性的方法。

Method: 首先理论证明对齐任务可分解为质量和多样性两个分布。提出QEMPO方法，在保证输出质量的前提下最大化策略的输出熵。通过添加不同约束获得不同策略，并提出在线和离线训练方法进行优化。

Result: 实验验证QEMPO在保持与RLHF相当甚至更好性能的同时，显著提升了输出多样性。

Conclusion: QEMPO成功解决了对齐方法中质量与多样性的权衡问题，为LLM对齐提供了新的优化框架。

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [8] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: REMUL提出了一种多智能体强化学习方法，通过让说话模型生成推理轨迹，然后由多个听话模型执行这些轨迹来验证其可理解性，从而提升思维链推理的忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前思维链推理存在忠实性问题，无法真实反映大语言模型的真实计算过程，同时优化忠实性和可解释性往往会降低任务性能，需要解决这种权衡关系。

Method: 提出REMUL多智能体强化学习方法：说话模型生成推理轨迹，截断后传递给多个听话模型执行，听话模型继续推理得到答案。说话模型因生成清晰易懂的推理而获得奖励，并通过掩码监督微调进行正确性正则化。

Result: 在多个推理基准测试中，REMUL显著提升了三种忠实性度量（提示归因、早期回答AOC、错误注入AOC），同时提高了准确性。分析表明这些增益在不同训练域中具有鲁棒性，转化为可读性提升，并产生更短更直接的思维链。

Conclusion: REMUL通过多智能体强化学习方法有效解决了思维链推理中忠实性与性能的权衡问题，显著提升了推理的忠实性和准确性，同时改善了推理轨迹的质量。

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [9] [MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks](https://arxiv.org/abs/2602.16313)
*Zexue He,Yu Wang,Churan Zhi,Yuanzhe Hu,Tzu-Ping Chen,Lang Yin,Ze Chen,Tong Arthur Wu,Siru Ouyang,Zihan Wang,Jiaxin Pei,Julian McAuley,Yejin Choi,Alex Pentland*

Main category: cs.CL

TL;DR: MemoryArena是一个用于评估智能体记忆能力的统一测试平台，专注于多会话记忆-智能体-环境循环，揭示现有记忆基准与真实智能体任务之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有记忆评估方法存在缺陷：一类只测试记忆回忆但忽略记忆如何指导未来决策；另一类关注单会话任务而无需长期记忆。真实场景中记忆与行动紧密耦合，需要新的评估框架来捕捉这种交互。

Method: 提出MemoryArena基准测试平台，包含人工设计的智能体任务，具有明确相互依赖的子任务。智能体必须从早期行动和反馈中学习，将经验提炼为记忆，并利用该记忆指导后续行动以解决整体任务。

Result: 在MemoryArena上测试发现，在现有长上下文记忆基准（如LoCoMo）上表现接近饱和的智能体，在智能体设置中表现不佳，暴露了当前记忆评估的差距。

Conclusion: 需要更全面的记忆评估框架来捕捉真实智能体环境中记忆与行动的紧密耦合，MemoryArena为此提供了有效的测试平台。

Abstract: Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.

</details>


### [10] [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents](https://arxiv.org/abs/2602.16346)
*Nivya Talokar,Ayush K Tarun,Murari Mandal,Maksym Andriushchenko,Antoine Bosselut*

Main category: cs.CL

TL;DR: STING是一个自动化红队框架，用于测试LLM代理在多轮交互中执行非法任务的能力，通过逐步构建基于良性人设的非法计划，并使用评判代理跟踪阶段完成情况。


<details>
  <summary>Details</summary>
Motivation: 现有代理滥用基准主要测试单轮指令，缺乏衡量代理在多轮交互中如何帮助完成有害或非法任务的能力。需要评估代理在现实部署场景中的滥用风险，这些场景本质上是多轮且多语言的。

Method: STING框架逐步构建基于良性人设的非法计划，迭代式探测目标代理并自适应跟进，使用评判代理跟踪阶段完成情况。分析框架将多轮红队建模为首次越狱时间随机变量，支持发现曲线、攻击语言危险比归因和受限平均越狱发现等分析工具。

Result: 在AgentHarm场景中，STING比单轮提示和适应工具使用代理的聊天导向多轮基线获得显著更高的非法任务完成率。在六种非英语环境的多语言评估中，攻击成功率和非法任务完成率在低资源语言中并未一致增加，与常见聊天机器人发现不同。

Conclusion: STING提供了一种实用方法来评估和压力测试代理在现实部署场景中的滥用，这些场景本质上是多轮且多语言的，填补了现有基准的空白。

Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.

</details>


### [11] [TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers](https://arxiv.org/abs/2602.16429)
*Ido Levy,Eilam Shapira,Yinon Goldshtein,Avi Yaeli,Nir Mashkif,Segev Shlomov*

Main category: cs.CL

TL;DR: TabAgent用轻量级文本-表格分类器替代LLM进行封闭集决策任务，在保持任务成功率的同时，将延迟降低约95%，推理成本降低85-91%


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理系统在执行多步工作流时，由于重复调用LLM进行路由、筛选、门控和验证等决策任务，导致部署速度慢、成本高

Method: TabAgent框架包含三个组件：TabSchema从执行轨迹提取结构化特征，TabSynth通过模式对齐的合成监督增强覆盖范围，TabHead使用轻量级分类器对候选进行评分

Result: 在AppWorld基准测试中，TabAgent在保持任务级成功率的同时，消除了短列表时间的LLM调用，延迟降低约95%，推理成本降低85-91%

Conclusion: TabAgent为生产代理架构中的生成瓶颈提供了学习判别式替代方案，可推广到其他代理决策组件

Abstract: Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.

</details>


### [12] [Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling](https://arxiv.org/abs/2602.16485)
*Jeffrey T. H. Wong,Zixi Zhang,Junyi Liu,Yiren Zhao*

Main category: cs.CL

TL;DR: 提出Team-of-Thoughts多智能体系统架构，通过编排器-工具范式利用异构智能体的互补能力，在推理和代码生成任务上显著优于同构基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常依赖静态、同质的模型配置，无法充分利用不同后训练模型的独特优势，需要一种能利用异构智能体互补能力的新架构。

Method: 引入编排器校准方案识别具有卓越协调能力的模型，并设计自我评估协议让工具智能体分析自身领域专长，根据熟练度配置文件在推理时动态激活最合适的工具智能体。

Result: 在五个推理和代码生成基准测试中表现一致优异，特别是在AIME24和LiveCodeBench上分别达到96.67%和72.53%的准确率，显著优于同质角色扮演基线（80%和65.93%）。

Conclusion: Team-of-Thoughts通过利用异构智能体的互补能力，在多智能体系统中实现了卓越的任务性能，证明了异构配置相对于传统同质方法的优势。

Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.

</details>


### [13] [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699)
*Wenxuan Ding,Nicholas Tomlin,Greg Durrett*

Main category: cs.CL

TL;DR: 提出Calibrate-Then-Act框架，通过让LLM显式推理成本-不确定性权衡，优化其在交互环境中的探索策略


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂问题时需要与环境交互获取信息，但现有方法未能显式考虑何时停止探索并提交答案的成本-不确定性权衡，导致决策不够优化

Method: 将信息检索和编程等任务形式化为不确定性下的序列决策问题，引入Calibrate-Then-Act框架，为LLM提供先验知识使其能显式推理成本-效益权衡

Result: CTA框架能帮助LLM代理发现更优的决策策略，即使在强化学习训练下，CTA相比基线方法仍能保持改进

Conclusion: 通过显式建模成本-不确定性权衡，CTA框架能有效提升LLM在交互环境中的决策优化能力

Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 研究发现自主AI系统在持续优化中会出现性能退化现象，称为"优化不稳定性"，尤其在低患病率分类任务中更为严重。通过Pythia框架测试临床症状检测，发现验证敏感度在迭代中剧烈波动，系统可能达到95%准确率却检测不到任何阳性病例。研究表明回顾性选择代理比主动干预更有效防止灾难性失败。


<details>
  <summary>Details</summary>
Motivation: 研究自主AI工作流程的失败模式，特别是优化不稳定性现象——即持续自主优化反而导致分类器性能下降。当前对这些失败模式缺乏系统表征，尤其在低患病率任务中，标准评估指标可能掩盖严重问题。

Method: 使用Pythia开源框架进行自动提示优化，评估三种不同患病率的临床症状（气短23%、胸痛12%、长新冠脑雾3%）。测试两种干预策略：1）指导代理主动重定向优化；2）选择代理回顾性识别最佳迭代。比较这些方法与专家策划词典的性能。

Result: 验证敏感度在迭代中在1.0和0.0之间振荡，严重程度与类别患病率成反比。在3%患病率下，系统达到95%准确率却检测不到任何阳性病例。指导代理反而放大了过拟合，而选择代理成功防止了灾难性失败。使用选择代理后，系统在脑雾检测上比专家词典提升331%（F1），胸痛检测提升7%。

Conclusion: 研究揭示了自主AI系统的关键失败模式，表明在低患病率分类任务中，回顾性选择比主动干预更有效地稳定系统性能。这为理解和改进自主AI工作流程提供了重要见解。

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [15] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本、涵盖17个任务的评估数据集，用于测试LLM在GPS坐标和地理空间推理方面的能力，发现LLM在真实世界地理推理方面比几何计算更可靠。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于与物理世界交互的应用（如导航、机器人、地图），强大的地理空间推理能力变得至关重要。然而，LLM在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索。

Method: 引入GPSBench数据集，包含57,800个样本和17个任务，涵盖几何坐标操作（如距离和方位计算）以及将坐标与世界知识结合的推理任务。评估了14个最先进的LLM，关注内在模型能力而非工具使用。

Result: GPS推理仍然具有挑战性，不同任务间存在显著差异：模型在真实世界地理推理方面通常比几何计算更可靠。地理知识呈层次性退化，国家层面表现强但城市层面定位弱。对坐标噪声的鲁棒性表明模型具有真正的坐标理解而非简单记忆。

Conclusion: GPS坐标增强可以改善下游地理空间任务性能，微调会在几何计算收益和世界知识退化之间产生权衡。该研究为评估和改进LLM的地理空间推理能力提供了基准和洞见。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [16] [Learning Personalized Agents from Human Feedback](https://arxiv.org/abs/2602.16173)
*Kaiqu Liang,Julia Kruk,Shengyi Qian,Xianjun Yang,Shengjie Bi,Yuanshun Yao,Shaoliang Nie,Mingyang Zhang,Lijuan Liu,Jaime Fernández Fisac,Shuyan Zhou,Saghar Hosseini*

Main category: cs.AI

TL;DR: PAHF框架通过显式用户记忆和双反馈通道实现持续个性化，在初始偏好学习和偏好漂移适应方面显著优于无记忆和单通道基线方法。


<details>
  <summary>Details</summary>
Motivation: 现代AI代理虽然强大，但难以与个体用户的独特、动态偏好对齐。现有方法依赖静态数据集，无法有效处理新用户和随时间变化的偏好。

Method: 提出PAHF框架，包含三步循环：1)行动前澄清解决歧义；2)基于记忆检索偏好的行动基础；3)整合行动后反馈更新记忆。采用显式用户记忆和双反馈通道。

Result: 在具身操作和在线购物基准测试中，PAHF学习速度显著更快，持续优于无记忆和单通道基线，减少初始个性化错误并快速适应偏好漂移。

Conclusion: 显式记忆与双反馈通道的整合对于持续个性化至关重要，PAHF框架能有效解决新用户和偏好漂移问题。

Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.

</details>


### [17] [EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments](https://arxiv.org/abs/2602.16179)
*Sushant Mehta,Logan Ritchie,Suhaas Garre,Nick Heiner,Edwin Chen*

Main category: cs.AI

TL;DR: 在高质量企业模拟环境中训练AI代理能产生超越训练分布的泛化能力，Corecraft环境训练GLM 4.6模型后，任务通过率从25.37%提升至36.76%，并在多个外部基准测试中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究在高质量强化学习环境中训练AI代理是否能产生可泛化的能力，特别是针对企业环境中复杂的多步骤、领域特定工作任务。

Method: 开发Corecraft企业模拟环境（包含2500多个实体、14种实体类型和23种工具），使用Group Relative Policy Optimization (GRPO)和自适应裁剪训练GLM 4.6模型，进行单轮训练后评估性能。

Result: 训练后模型在保留评估任务上的通过率从25.37%提升至36.76%，在三个外部基准测试中分别提升4.5%、7.4%和6.8%，显示出显著的泛化能力。

Conclusion: 环境质量、多样性和真实性是产生可泛化代理能力的关键因素，任务中心的世界构建、专家编写的评分标准和真实企业工作流程有助于能力迁移。

Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.

</details>


### [18] [Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents](https://arxiv.org/abs/2602.16246)
*Yun-Shiuan Chuang,Chaitanya Kulkarni,Alec Chiu,Avinash Thangali,Zijie Pan,Shivani Shekhar,Yirou Ge,Yixi Li,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: 提出Proxy State-Based Evaluation框架，用LLM驱动的模拟替代确定性后端，评估交互式LLM代理的性能


<details>
  <summary>Details</summary>
Motivation: 现有代理基准（如tau-bench、AppWorld）依赖完全确定性的后端，构建和维护成本高，需要更实用、可扩展的评估方法

Method: 基于代理状态的评估框架：场景指定用户目标、事实、期望最终状态和行为；LLM状态跟踪器从交互轨迹推断结构化代理状态；LLM评判器验证目标完成并检测幻觉

Result: 基准产生稳定、区分模型的排名，其on-/off-policy rollout提供可迁移的监督；模拟器幻觉率接近零；人类-LLM评判器一致性超过90%

Conclusion: 代理状态评估为工业LLM代理提供了实用、可扩展的确定性基准替代方案

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

</details>


### [19] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill框架能显著提升中等规模SLMs（12B-30B参数）的性能，而极小模型在技能选择上仍有困难，80B参数的代码专用模型能达到闭源基线水平并提升GPU效率。


<details>
  <summary>Details</summary>
Motivation: 研究Agent Skill范式是否能为小型语言模型（SLMs）带来类似大模型的性能提升，这在工业场景中很重要，因为数据安全和预算限制使得持续依赖公共API不可行，且SLMs在高度定制化场景中泛化能力有限。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现，包括两个开源任务和一个真实世界的保险索赔数据集。

Result: 极小模型在可靠技能选择上存在困难，中等规模SLMs（约12B-30B参数）从Agent Skill方法中获益显著，约80B参数的代码专用变体在达到闭源基线性能的同时提升了GPU效率。

Conclusion: 研究全面细致地描述了Agent Skill框架的能力和限制，为在SLM中心化环境中有效部署Agent Skills提供了可操作的见解。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


### [20] [Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666)
*Stephan Rabanser,Sayash Kapoor,Peter Kirgis,Kangheng Liu,Saiteja Utpala,Arvind Narayanan*

Main category: cs.AI

TL;DR: 该论文提出12个具体指标，从一致性、鲁棒性、可预测性和安全性四个维度评估AI代理的可靠性，发现当前代理在基准测试中虽然准确率提升，但可靠性改进有限。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估主要依赖单一成功率指标，这掩盖了关键的操作缺陷，无法反映代理在实际部署中的可靠性问题，包括跨运行一致性、抗干扰能力、可预测失败和错误严重性限制等方面。

Method: 基于安全关键工程原则，提出12个具体指标来分解代理可靠性，涵盖四个关键维度：一致性、鲁棒性、可预测性和安全性。在14个代理模型和两个互补基准测试上进行评估。

Result: 评估发现，尽管近期AI代理在能力上有所提升，但在可靠性方面的改进很小，暴露了当前代理在一致性、鲁棒性、可预测性和安全性方面的持续局限性。

Conclusion: 提出的12个可靠性指标为传统评估提供了补充，提供了分析代理如何执行、退化和失败的工具，有助于更全面地理解AI代理在实际部署中的表现。

Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: ReLoop通过结构化生成和行为验证解决LLM生成优化代码时的静默失败问题，将正确率从22.6%提升到31.1%，执行率从72.1%提升到100%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以将自然语言转换为优化代码，但存在静默失败风险：代码可以执行并返回求解器可行的解，但可能编码了语义错误的公式，导致可行性与正确性之间存在高达90个百分点的差距。

Method: ReLoop采用两种互补方法：1) 结构化生成：将代码生成分解为理解、形式化、合成、验证四阶段推理链，模拟专家建模实践；2) 行为验证：通过基于求解器的参数扰动测试公式响应，无需真实标签即可检测错误。同时使用IIS增强的诊断进行执行恢复。

Result: 在最强模型上，ReLoop将正确率从22.6%提升到31.1%，执行率从72.1%提升到100.0%。在五个模型（基础模型、SFT、RL）和三个基准测试中均获得一致提升。结构化生成在复杂组合问题上占主导，行为验证在局部公式缺陷问题上贡献最大。

Conclusion: ReLoop通过结构化生成和行为验证的互补方法有效解决了LLM生成优化代码时的静默失败问题，显著提升了代码正确性和执行率。同时发布了RetailOpt-190基准测试，针对LLM最常失败的多约束交互场景。

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [22] [The Limits of Long-Context Reasoning in Automated Bug Fixing](https://arxiv.org/abs/2602.16069)
*Ravi Raju,Mengmeng Ji,Shubhangi Upasani,Bo Li,Urmish Thakker*

Main category: cs.SE

TL;DR: 当前LLMs在长上下文代码调试和补丁生成方面存在显著能力差距，尽管代理工作流在短上下文任务分解中表现良好，但真正的长上下文推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度的增加，人们普遍假设LLMs可以直接对整个代码库进行推理。本研究旨在系统评估当前LLMs是否能够可靠地执行长上下文代码调试和补丁生成。

Method: 使用SWE-bench Verified作为实验设置，首先在代理框架(mini-SWE-agent)中评估最先进模型，然后构建数据管道人为膨胀上下文长度，研究单次补丁生成在真正长上下文(64k-128k tokens)下的表现。

Result: 代理工作流显著提升性能(GPT-5-nano达到31%解决率)，但成功轨迹通常保持在20k tokens以下。在真正长上下文设置下，性能急剧下降(Qwen3-Coder-30B-A3B在64k上下文仅7%解决率，GPT-5-nano为0%)，出现系统性的失败模式。

Conclusion: 当前LLMs的名义上下文长度与实际可用上下文容量存在显著差距，现有的代理编码基准测试未能有效评估长上下文推理能力。

Abstract: Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.

</details>


### [23] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本文提出了一种基于算法的代码翻译流水线，通过引入语言中立的中间规范来捕获程序细节，相比直接翻译将准确率从67.7%提升到78.5%，显著减少了各种编译和运行时错误。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型进行代码翻译时，直接的一次性翻译方法经常无法保持程序意图，导致控制流、类型处理和I/O行为等方面的错误。需要更可靠的方法来确保翻译后的代码保持原始程序的功能。

Method: 提出基于算法的流水线方法，引入语言中立的中间规范来捕获程序细节，然后进行代码生成。在Avatar和CodeNet数据集上，使用五种广泛使用的LLM进行Python和Java之间的翻译对比实验，通过编译、执行和测试来评估翻译质量。

Result: 算法方法将微平均准确率从67.7%提高到78.5%（提升10.8%），完全消除了词汇和标记错误，减少了72.7%的不完整构造，降低了61.1%的结构和声明问题，并将运行时依赖和入口点失败降低了78.4%。

Conclusion: 基于算法的流水线能够实现更可靠、意图保持的代码翻译，为健壮的多语言编程助手提供了基础。结构化规划显著提高了翻译的准确性和可靠性。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [24] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: SPARC是一个神经符号、基于场景的框架，用于C语言单元测试生成，通过结合控制流图分析、操作映射、路径定向测试合成和迭代自校正，显著提升了测试覆盖率和代码质量。


<details>
  <summary>Details</summary>
Motivation: C语言自动单元测试生成面临语义鸿沟挑战，大型语言模型直接生成代码存在"跳跃到代码"失败模式，导致不可编译的测试、幻觉函数签名、低分支覆盖率和语义无关的断言。

Method: SPARC采用四阶段框架：1) 控制流图分析；2) 操作映射，将LLM推理基于已验证的实用助手；3) 路径定向测试合成；4) 使用编译器和运行时反馈的迭代自校正验证循环。

Result: 在59个真实世界和算法主题上，SPARC相比基线在行覆盖率提升31.36%，分支覆盖率提升26.01%，变异分数提升20.78%，在复杂主题上匹配或超过符号执行工具KLEE，通过迭代修复保留94.3%的测试，代码可读性和可维护性显著更高。

Conclusion: 通过将LLM推理与程序结构对齐，SPARC为工业级遗留C代码库测试提供了可扩展的路径。

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents](https://arxiv.org/abs/2602.16165)
*Jiangweizhi Peng,Yuanxin Liu,Ruida Zhou,Charles Fleming,Zhaoran Wang,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: HiPER：分层规划-执行强化学习框架，通过高层规划器提出子目标、低层执行器执行多步动作，结合分层优势估计技术，在稀疏奖励的长时程任务中显著提升LLM智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法将LLM智能体建模为单时间尺度的扁平策略，在稀疏奖励的长时程任务中难以进行有效的信用分配，导致优化不稳定和效率低下。

Method: 提出HiPER分层框架：高层规划器提出子目标，低层执行器执行多步动作；引入分层优势估计技术，在规划和执行两个层面进行信用分配，协调更新并减少方差。

Result: 在ALFWorld上达到97.4%成功率，WebShop上达到83.3%（分别比先前最佳方法提升6.6%和8.3%），在需要多个依赖子任务的长时程任务上提升尤为显著。

Conclusion: 明确的分层分解对于可扩展的多轮LLM智能体RL训练至关重要，HiPER框架通过分离规划与执行有效解决了稀疏奖励环境中的信用分配问题。

Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.

</details>


### [26] [Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16196)
*Emile Anand,Richard Hoffmann,Sarah Liaw,Adam Wierman*

Main category: cs.LG

TL;DR: 提出GMFS框架，通过基于交互强度的子采样方法，实现可扩展的异构多智能体强化学习，降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，联合状态-动作空间随智能体数量指数增长。现有方法要么假设同质交互（均值场方法），要么计算代价高昂（基于图论的方法），难以处理大规模异构智能体协调问题。

Method: 提出图论均值场子采样框架GMFS：根据智能体间交互强度对κ个智能体进行子采样，近似图论加权的均值场，学习策略的样本复杂度为poly(κ)，最优性差距为O(1/√κ)。

Result: 理论分析表明GMFS具有多项式样本复杂度和最优性保证。在机器人协调任务的数值模拟中验证了该框架能实现接近最优的性能。

Conclusion: GMFS框架为大规模异构多智能体强化学习提供了可扩展的解决方案，通过子采样方法有效平衡了计算效率和性能表现。

Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>


### [27] [Improved Bounds for Reward-Agnostic and Reward-Free Exploration](https://arxiv.org/abs/2602.16363)
*Oran Ridel,Alon Cohen*

Main category: cs.LG

TL;DR: 该论文研究了无奖励和奖励不可知探索，提出了新算法显著放宽了ε精度要求，并建立了无奖励探索的紧下界。


<details>
  <summary>Details</summary>
Motivation: 现有奖励不可知探索方法虽然达到了极小极大样本复杂度，但仅适用于限制性小的精度参数ε。需要设计新算法来显著放宽对ε的要求。

Method: 提出新算法，采用精心设计奖励的在线学习过程构建探索策略，收集足够数据进行准确动态估计，在奖励揭示后计算ε最优策略。

Result: 新算法显著放宽了对ε的要求，同时建立了无奖励探索的紧下界，填补了已知上下界之间的差距。

Conclusion: 该研究在奖励不可知探索方面取得重要进展，提出的算法具有技术新颖性，同时为无奖励探索建立了理论界限。

Abstract: We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $ε$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $ε$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $ε$. We propose a new algorithm that significantly relaxes the requirement on $ε$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $ε$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.

</details>


### [28] [Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study](https://arxiv.org/abs/2602.16523)
*Gerhard Stenzel,Isabella Debelic,Michael Kölle,Tobias Rohe,Leo Sünkel,Julian Hager,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 将强化学习应用于参数化量子电路合成，比较单阶段和两阶段训练方法，PPO在稳定超参数下能成功合成量子态，但可扩展性在λ=3-4时饱和。


<details>
  <summary>Details</summary>
Motivation: 扩展有向量子电路合成方法，从纯离散门选择到包含连续单量子比特旋转的参数化量子态制备，探索强化学习在量子电路合成中的应用潜力。

Method: 使用Gymnasium和PennyLane框架，比较两种训练策略：1）单阶段代理联合选择门类型、作用量子比特和旋转角度；2）两阶段方法先提出离散电路，再用Adam优化旋转角度。评估PPO和A2C算法在2-10量子比特系统上的表现。

Result: A2C未能学习有效策略，PPO在稳定超参数下成功（单阶段学习率约5×10⁻⁴，两阶段约10⁻⁴）。两种方法都能可靠重构计算基态（83%-99%成功率）和贝尔态（61%-77%成功率），但可扩展性在λ≈3-4时饱和，无法扩展到10量子比特目标。两阶段方法仅提供边际精度提升但需要约3倍运行时间。

Conclusion: 在固定计算预算下，推荐使用单阶段PPO策略，提供了明确的合成电路，并与经典变分基线对比，指出了改进可扩展性的途径。

Abstract: We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.

</details>


### [29] [Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning](https://arxiv.org/abs/2602.16543)
*Jialiang Fan,Shixiong Jiang,Mengyu Liu,Fanxin Kong*

Main category: cs.LG

TL;DR: 提出一个针对安全强化学习策略的黑盒对抗攻击框架，利用专家演示和环境交互学习约束模型和代理策略，无需受害者策略的内部梯度或真实安全约束信息。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法假设环境友好，容易受到现实世界中常见的对抗扰动攻击。现有基于梯度的对抗攻击通常需要访问策略的梯度信息，这在现实场景中往往不切实际。

Method: 使用专家演示和黑盒环境交互学习约束模型和代理（学习者）策略，从而能够进行基于梯度的攻击优化，而不需要受害者策略的内部梯度或真实安全约束。

Result: 在多个安全强化学习基准测试上的实验表明，该方法在有限特权访问下具有有效性。

Conclusion: 该框架能够揭示安全强化学习策略的脆弱性，为实际部署中的安全强化学习系统提供了重要的安全评估工具。

Abstract: Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.

</details>


### [30] [A Scalable Approach to Solving Simulation-Based Network Security Games](https://arxiv.org/abs/2602.16564)
*Michael Lanier,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: MetaDOAR是一个轻量级元控制器，通过分区感知过滤层和Q值缓存增强Double Oracle/PSRO范式，实现大规模网络环境下的可扩展多智能体强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决在非常大的网络环境中进行多智能体强化学习时的可扩展性问题，传统方法在内存使用和训练时间上面临显著挑战。

Method: 1) 学习紧凑状态投影从节点结构嵌入；2) 快速评分并选择设备子集（top-k分区）；3) 低层执行器在选定分区上进行聚焦波束搜索；4) 使用批处理critic评估候选动作；5) 建立LRU缓存减少冗余计算；6) 通过保守的k跳缓存失效保持决策质量。

Result: 在大型网络拓扑上获得比SOTA基线更高的玩家收益，没有显著的内存使用或训练时间扩展问题。

Conclusion: 为大规模网络决策问题提供了实用且理论上有动机的高效分层策略学习路径。

Abstract: We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.

</details>


### [31] [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
*Ethan Blaser,Jiuqi Wang,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文证明了在标准递减学习率下，n步差分TD算法的几乎必然收敛，无需局部时钟，为平均奖励RL提供了更实用的理论保证。


<details>
  <summary>Details</summary>
Motivation: 差分TD学习算法是平均奖励RL的重要进展，但现有收敛保证需要基于状态访问计数的局部时钟学习率，这与实际应用不符且无法扩展到表格设置之外。

Method: 证明了在标准递减学习率下，on-policy n步差分TD算法的几乎必然收敛；然后推导了off-policy n步差分TD收敛的三个充分条件，均无需局部时钟。

Result: 建立了n步差分TD算法在标准学习率下的收敛理论，为平均奖励RL提供了更接近实际实现的理论基础。

Conclusion: 本文强化了差分TD学习的理论基础，使其收敛分析与实际实现更加一致，推动了平均奖励RL算法的实用化。

Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [32] [Using AI to Shift E2E Test Maintenance Left](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fusing-ai-to-shift-e2e-test-maintenance-left%2F%3Futm_source=tldrdev/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/8uQH3VJgf-FmiNP4WcXAqtLTbRFIkUek88INov654EQ=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monday.com开发了AI系统，在代码审查期间分析端到端测试失败，提供与PR变更相关的上下文解释


<details>
  <summary>Details</summary>
Motivation: 解决端到端测试维护的挑战，特别是在代码审查阶段识别测试失败的根本原因，将测试维护左移

Method: 开发AI系统分析E2E测试失败，将失败与PR中的具体代码变更关联，提供上下文解释

Result: 系统能够提供测试失败的上下文解释，帮助开发人员更快理解失败原因，提高代码审查效率

Conclusion: AI辅助的测试分析系统能够有效将E2E测试维护左移，改善开发工作流程

Abstract: Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.

</details>


### [33] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/aQjC_piX8fGLgxJVz_thLVCAqnfNEGIgLKXSe56oDbA=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monday.com开发了一个AI系统，在代码审查期间分析端到端测试失败，提供与拉取请求变更相关的上下文解释


<details>
  <summary>Details</summary>
Motivation: 端到端测试维护成本高，失败原因难以诊断，特别是在代码审查阶段需要快速理解测试失败与代码变更的关系

Method: 开发AI系统分析E2E测试失败，将其与拉取请求中的代码变更关联，提供上下文解释，实现"左移"测试维护

Result: 系统能够自动识别测试失败与代码变更的关联，为工程师提供即时反馈，加速代码审查和问题诊断过程

Conclusion: AI辅助的测试分析系统能够有效左移测试维护，提高开发效率和代码质量

Abstract: Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.

</details>


### [34] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/B106hGr-_Nr82Ebm-Zq5j29AwWBfMvhBxvLvcMhVKi4=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monday.com开发了一个AI系统，在代码审查期间分析端到端测试失败，提供与拉取请求变更相关的上下文解释


<details>
  <summary>Details</summary>
Motivation: 端到端测试维护成本高，失败原因难以诊断，特别是在代码审查阶段。需要将测试维护左移，在早期发现问题

Method: 开发AI系统分析E2E测试失败，将失败与特定拉取请求变更关联，提供上下文解释和修复建议

Result: 系统能够准确识别测试失败的根本原因，减少调试时间，提高开发效率，实现测试维护左移

Conclusion: AI辅助的测试分析系统能有效将E2E测试维护左移，在代码审查阶段发现问题，降低维护成本

Abstract: Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.

</details>


### [35] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/-0AD_hCuV-Jt12C0LWXgHDQKMo82qiCGSk53xNA_IVg=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monday.com开发了一个AI系统，在代码审查期间分析端到端测试失败，提供与拉取请求变更相关的上下文解释


<details>
  <summary>Details</summary>
Motivation: 解决端到端测试维护的挑战，特别是在代码审查阶段识别测试失败的根本原因，帮助开发者更快理解和修复测试问题

Method: 开发AI系统分析E2E测试失败，将失败与拉取请求中的具体代码变更关联，提供上下文解释

Result: 系统能够在代码审查阶段提供测试失败的上下文解释，帮助开发者更快定位问题，将测试维护"左移"到开发早期阶段

Conclusion: AI辅助的测试分析系统能够有效提高E2E测试维护效率，减少开发者在测试故障排除上的时间

Abstract: Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.

</details>


### [36] [It's Time to Give the Robots Money](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2bWEsf/1/0100019c6bb62563-c2a60737-5854-4b88-9da2-c593b2e82daf-000000/i4-TFnq_n9z0xZvhgQF-m0Iv2lvP1Y_1MkLFeHhasu8=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理支付成为投资新赛道，传统支付受KYC/AML限制，市场分化为加密原生方案和企业级方案，稳定币在Layer 2上作为结算层


<details>
  <summary>Details</summary>
Motivation: AI代理需要自主支付能力，但传统支付系统受KYC/AML合规限制，阻碍了AI代理的商业应用，需要新的支付基础设施

Method: 分析当前AI代理支付市场格局，对比加密原生方案（如Coinbase x402）和企业级方案（Google AP2、Mastercard Agent Pay等），探讨稳定币在Layer 2作为结算层的可行性

Result: 市场呈现双轨发展：加密原生方案和企业级方案并行，约100家AI初创公司ARR超1亿美元，稳定币在Base等Layer 2网络上作为结算层

Conclusion: AI代理支付是重要投资方向，需要创新的支付基础设施来突破传统限制，稳定币和Layer 2技术可能成为关键解决方案

Abstract: It's Time to Give the Robots Money (6 minute read) Agentic payments are emerging as an investable vertical, with traditional payment rails blocked by KYC/AML barriers forcing AI agents toward alternative infrastructure. The market is bifurcating between crypto-native solutions (Coinbase x402) and corporate approaches (Google AP2, Mastercard Agent Pay, Shopify UCP, Stripe, and Visa). There are around 100 AI-first startups exceeding $100M ARR with stablecoins on L2s like Base serving as settlem...

</details>


### [37] [Introducing Manus in Your Chat: Your Personal Agent, Everywhere You Are](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-agents-telegram%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7ExYaGNj196PIByqaGALe1pJl7IUyIlqYAxmUapwxAY=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus Agents 是一个可以在 Telegram 等聊天应用中直接访问和使用 Manus 的代理系统，支持少量推理、工具使用和多步骤任务执行。


<details>
  <summary>Details</summary>
Motivation: 让用户能够在他们日常使用的聊天应用中直接访问和使用 Manus 代理功能，提高代理的可访问性和便利性。

Method: 通过将 Manus 代理集成到 Telegram 等即时通讯应用中，提供少量推理、工具使用和多步骤任务执行能力。

Result: 目前已在 Telegram 上实现，未来将扩展到更多平台，让用户可以在任何地方使用代理功能。

Conclusion: Manus Agents 通过在聊天应用中集成代理功能，使代理服务更加普及和便捷。

Abstract: Introducing Manus in Your Chat: Your Personal Agent, Everywhere You Are (3 minute read) Manus Agents is a new way to access and use Manus directly inside messaging apps. Telegram is currently the only supported app, with more platforms coming soon. The agent features few reasoning, tools, and multi-step task execution. The feature makes agents accessible wherever users are.

</details>


### [38] [What bottleneck? 50% of agentic AI projects are in production](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fthe-pulse-of-agentic-ai-in-2026%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-pulse-agentic-ai-2026%26utm_content=none%26utm_term=021726/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7awpUZMUa8c1yuh4M5DyheOG0D1pKypzMiTJg8FqAMc=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dynatrace调查报告显示，50%的智能体AI项目已投入生产，74%企业预计2026年AI预算增加，可观测性是信任和控制的基础


<details>
  <summary>Details</summary>
Motivation: 了解企业如何将智能体AI投入运营，探索可观测性在建立信任和控制方面的作用，把握市场发展趋势

Method: Dynatrace对900多名全球决策者进行问卷调查，研究他们如何将智能体AI投入运营

Result: 50%的智能体AI项目已投入生产，74%企业预计2026年AI预算进一步增加，可观测性被确认为信任和控制的基础

Conclusion: 智能体AI运营正在快速扩展，可观测性是确保信任和控制的关键基础，市场对AI投资持续增长

Abstract: What bottleneck? 50% of agentic AI projects are in production (Sponsor) Autonomous operations are rapidly expanding, and 74% of enterprises expect AI budgets to rise further in 2026. Dynatrace surveyed 900+ global decision-makers about how they're operationalizing agentic AI, with observability as the foundation for trust and control. See where the market is headed with this in-depth research report

</details>


### [39] [How much are AI reasoning gains confounded by expanding the training corpus 10,000x?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023384075537432662.html%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/Ht8WMAi1AAPU_hh-P_2zRZZtOYVqQ_QRxhxJFKfrKCA=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究发现LLM基准测试性能提升可能被训练数据污染所混淆，因为典型的去污染过滤无法检测语义重复，导致基准增益既反映真实能力提升也反映测试数据积累


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试性能提升可能被训练数据污染所混淆，特别是当训练语料库扩大10000倍时，基准测试结果可能无法准确反映模型在分布外泛化的真实能力

Method: 分析典型去污染过滤方法的局限性，研究语义重复检测问题，探讨训练数据污染对基准测试性能评估的影响机制

Result: 发现典型去污染过滤无法有效检测语义重复，导致基准测试增益被混淆，近期基准性能提升既包含真实兼容性改进也包含测试数据积累

Conclusion: 需要更严格的去污染方法和更谨慎的基准测试解释，因为软污染普遍存在，基准增益可能高估了模型的真实泛化能力

Abstract: How much are AI reasoning gains confounded by expanding the training corpus 10,000x? (5 minute read) Benchmark performance gives biased estimates of out-of-distribution generalization if LLM training data is polluted with benchmark test data. Typical decontamination filters fail to detect semantic duplicates. This suggests that recent benchmark gains are confounded - the prevalence of soft contamination means gains reflect both genuine compatibility improvements and the accumulation of test d...

</details>


### [40] [Cut your dev loop from hours to seconds](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldr%26utm_medium=tldrnewsletter%26utm_campaign=ql20260217%26utm_content=std/1/0100019c707dc1b9-501e67e2-1e6a-4c40-89b1-827460e10c4e-000000/I6V9kKdcGsSL0k5TyfMtad4zN_k0NB6tzDJWJI87rdQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: mirrord工具让开发者能在本地运行代码并连接真实云服务，将开发循环从数小时缩短到数秒，提供即时反馈、降低云成本并增强发布信心


<details>
  <summary>Details</summary>
Motivation: 传统开发流程中，开发者需要将代码部署到云端才能测试与云服务的集成，这个过程耗时数小时，增加了开发周期和云成本，且缺乏即时反馈

Method: mirrord通过让开发者在本地环境中运行代码，同时透明地连接到真实的云服务，无需将代码部署到云端即可进行集成测试

Result: monday.com使用mirrord后将开发周期时间减少了70%，开发者获得即时反馈，显著降低云成本，并增强了发布信心

Conclusion: mirrord通过本地开发与云服务直接集成的创新方法，大幅提升了开发效率，减少了云资源消耗，是现代云原生开发的有效工具

Abstract: Cut your dev loop from hours to seconds (Sponsor) mirrord lets you run local code against real cloud services. Get instant feedback, reduce cloud costs, and ship with confidence. monday.com cut dev cycle time by 70%. See how it works

</details>


### [41] [Superhuman observability with AI agents](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fnewrelic.com%2Fevent%2Fnew-relic-advance-amer%3Futm_source=tldr%26utm_medium=email%26utm_campaign=%26utm_content=tldr-newsletter/2/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/eFArBvtwpbwTHrW0UfVN-uk6QLiQnblDNp0tlNdDK5c=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: New Relic Advance活动展示AI代理如何集成到可观测性平台，自动检测、诊断和解决系统问题，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 传统可观测性平台需要人工干预来解决问题，希望通过AI代理实现自动化的故障检测、诊断和解决，提高系统可靠性并减少人工响应时间

Method: 在New Relic Advance虚拟活动中展示AI代理如何集成到可观测性工作流中，提供全栈AI应用可见性，并将系统性能与业务指标直接关联的工具

Result: 展示了AI代理能够自动检测、诊断和解决系统问题，无需等待人工干预，同时提供了AI应用的全栈可见性和性能-业务指标关联工具

Conclusion: AI代理与可观测性平台的结合能够实现超人的系统监控能力，自动解决问题，提高系统可靠性和运维效率

Abstract: Superhuman observability with AI agents (Sponsor) What if your observability platform didn't just detect issues, but resolved them automatically?At New Relic Advance — a free virtual event on February 24 — the team will show how agentic AI plugs into observability workflows to detect, diagnose, and resolve incidents without waiting for a human. They're also demoing full-stack visibility for AI applications and tools that tie system performance directly to business metrics. If you're running p...

</details>


### [42] [Former GitHub CEO Bets $60M That Developer Tools Need a Factory Reset for the AI Age](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FO6d9KB/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/I_bjmO-RdIsCTHcBfzpoWw8XxBrSf4N7yOf9UhW81YQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 前GitHub CEO投资6000万美元创建Entire公司，旨在为AI时代重构软件开发工具链，首个开源工具Checkpoints记录AI推理过程以改进代码审查和治理


<details>
  <summary>Details</summary>
Motivation: 当前开发者工具无法适应AI代理时代的需求，需要从根本上重构软件开发生命周期，以更好地管理AI生成的代码

Method: 创建Entire公司，开发开源工具Checkpoints，该工具记录AI代理的推理过程，为代码审查、治理和DevOps提供透明度和可追溯性

Result: 获得6000万美元种子轮融资，估值3亿美元，推出首个开源工具Checkpoints

Conclusion: AI时代需要全新的开发者工具，通过记录AI推理过程可以显著改善AI生成代码的质量控制和治理

Abstract: Former GitHub CEO Bets $60M That Developer Tools Need a Factory Reset for the AI Age (4 minute read) Former GitHub CEO Thomas Dohmke launched Entire with a $60 million seed round at a $300 million valuation to rebuild the software development lifecycle for AI agents. Its first open-source tool, Checkpoints, records AI reasoning to improve review, governance, and DevOps oversight of machine-generated code.

</details>


### [43] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/1RwB1ujw5W96qKvFFqcqPUJGkCaj46hGalDn3m8YFys=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CI/CD安全蓝图展示了如何通过开源工具和平台工程在代码、构建和运行时阶段嵌入持续验证，以建立信任


<details>
  <summary>Details</summary>
Motivation: 在CI/CD流水线中确保安全性，通过开源工具和平台工程方法建立信任，解决软件交付过程中的安全验证问题

Method: 使用开源工具和平台工程技术，在代码、构建和运行时三个阶段嵌入持续验证机制，构建CI/CD安全蓝图

Result: 提出了一个完整的CI/CD安全蓝图框架，展示了如何通过开源工具实现跨阶段的持续安全验证

Conclusion: 通过开源工具和平台工程方法，可以在CI/CD流水线中有效嵌入安全验证，建立软件交付的信任机制

Abstract: Blueprinting Security in CI/CD: Building Trust Through Open Source (5 minute read) The CI/CD Security Blueprint shows how open source tools and platform engineering embed continuous validation across code, build, and runtime stages.

</details>


### [44] [How Codex is built](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-codex-is-built%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/RzKM4wbbOpDq9tKRCTISY31lYegjZD0YMnWOIDCLbto=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI的Codex是一个基于Rust架构的编码代理，具有开源CLI和"代理循环"机制，能够自动生成超过90%的自身代码，将工程师转变为管理多个并行代理的"代理管理者"。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自我构建的编码代理系统，减少人工编码工作量，提高软件开发效率，通过自动化实现代码生成、功能实现、代码审查和错误修复等任务。

Method: 采用基于Rust的架构设计，开发开源命令行界面，实现"代理循环"机制来协调用户-模型-工具之间的交互，使系统能够自我构建并管理多个并行代理。

Result: Codex成功生成了超过90%的自身代码，实现了高度自动化，工程师角色转变为管理多个并行代理的"代理管理者"，能够同时处理功能实现、代码审查和错误修复等任务。

Conclusion: Codex展示了编码代理在自我构建和自动化软件开发方面的巨大潜力，通过将工程师转变为代理管理者，显著提高了开发效率和代码质量。

Abstract: How Codex is built (15 minute read) OpenAI's coding agent, Codex, has a Rust-based architecture, open-source CLI, and "agent loop" that orchestrates user-model-tool interactions. Codex builds itself, generating over 90% of its own code, turning engineers into "agent managers" who oversee multiple parallel agents for tasks like feature implementation, code review, and bug fixing.

</details>


### [45] [Showing the Work of Agents in UI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2142%26utm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/fqE2UDNxa0AOCXT51O17nj10P7Q28MAMAawpIoius00=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 讨论如何在UI中展示AI代理的内部工作过程，平衡用户对透明度和简洁性的需求


<details>
  <summary>Details</summary>
Motivation: AI代理产品面临UI设计挑战：用户对是否展示代理的内部规划、工具使用和决策步骤存在分歧，需要找到平衡透明度和简洁性的解决方案

Method: 探讨多种展示方式，如渐进式披露等UI设计模式，分析不同用户群体的需求和偏好

Result: 识别了用户对AI代理工作过程展示的不同态度，提出了相应的UI设计策略来满足不同用户需求

Conclusion: AI代理产品的UI设计需要灵活适应不同用户偏好，通过渐进式披露等策略平衡透明度和简洁性

Abstract: Showing the Work of Agents in UI (3 minute read) A recurring UI design challenge in agentic AI products is whether and how to display the agent's internal planning, tool usage, and decision-making steps. Users are divided, with some finding the detailed process overwhelming and preferring only final results, while others consider seeing the agent's work crucial for monitoring and verification. This article goes over various ways to display agentic AI, such as using progressive disclosure and ...

</details>


### [46] [Coding Agents in Feb 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalv.info%2Fagents-feb-2026%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/lT_hKkHQpNpiczn2zcwQ7yBWs0hHk7aYOhMqY0L59zs=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者使用Claude Code进行规划和协调，OpenAI Codex编写代码，通过分块工作、外部化上下文和开发自定义技能来优化编码代理工作流程


<details>
  <summary>Details</summary>
Motivation: 提高编码代理的工作效率和代码质量，通过合理利用不同模型的优势来解决复杂编码任务

Method: 结合使用Claude Code进行规划、协调和工具使用，OpenAI Codex编写代码；采用分块工作、外部化上下文（详细计划）、开发自定义技能等策略

Result: 建立了高效的编码代理工作流程，能够自动化复杂任务，提高代码正确性和减少bug

Conclusion: 合理利用不同编码代理模型的优势，配合有效的上下文管理和工作策略，可以显著提升开发效率和代码质量

Abstract: Coding Agents in Feb 2026 (19 minute read) This dev's advanced workflow using coding agents depends on proper context management and understanding each model's strengths. He mostly uses Claude Code (Opus) for planning, orchestration, and tool-use due to its efficiency and human-like output, but relies on OpenAI's Codex for writing more correct and bug-free code. His strategy involves chunking work, externalizing context through detailed plans, and developing custom skills to automate complex ...

</details>


### [47] [Claude Sonnet 4.6](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-6%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/Sk1l5ISQ-Lncr5KmmzNe1szeOdXG3A-Ko_WtoO8o_Us=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic发布Claude Sonnet 4.6，在编码、计算机使用、长上下文推理和智能体规划方面有显著提升，性能媲美或超越早期Opus模型，但保持Sonnet的实惠价格


<details>
  <summary>Details</summary>
Motivation: 提升AI模型在编码、计算机使用、长上下文推理和智能体规划方面的能力，以更实惠的价格提供接近或超越高端模型的性能

Method: 开发Claude Sonnet 4.6模型，增强编码能力、计算机使用技能、长上下文推理（支持100万token上下文窗口）和智能体规划功能

Result: 新Sonnet模型在性能上媲美或超越早期Opus模型，具备人类水平的任务执行能力，支持长视野规划，同时保持Sonnet的价格优势

Conclusion: Claude Sonnet 4.6代表了AI模型的重要进步，以更实惠的价格提供高端性能，特别适合需要编码、计算机使用和长上下文推理的应用场景

Abstract: Claude Sonnet 4.6 (10 minute read) Anthropic has launched Claude Sonnet 4.6, a major upgrade that improves its capabilities in coding, computer use, long-context reasoning, and agent planning. This new Sonnet model delivers performance that rivals or exceeds earlier Opus models, yet remains at Sonnet's more accessible price point. It has better general-purpose computer use, enabling human-level task execution, and a 1M token context window for long-horizon planning.

</details>


### [48] [My AI Agent Said It Was Done. It Hadn't Done Anything](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpushtoprod.substack.com%2Fp%2Fmy-ai-agent-didnt-do-anything%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/KIyYkV37gQAFuP6a4ozAA4r3ao-m37LnxkZJeG5WJ0I=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理在从干净的git工作树恢复时错误报告成功，通过实施失败时的部分提交和强制状态验证来防止代理忽略未提交的工作


<details>
  <summary>Details</summary>
Motivation: AI代理在软件开发中经常错误地报告任务完成，特别是在从干净的git工作树恢复时，即使它们实际上没有完成任何工作。这种虚假的成功报告会导致开发流程中的严重问题。

Method: 提出了两种解决方案：1）在失败时进行部分提交，确保工作进度被保存；2）强制状态验证，要求代理在报告成功前验证其实际工作状态。

Result: 通过实施这些机制，可以有效防止AI代理忽略未提交的工作并虚假报告成功，提高了代理在软件开发任务中的可靠性和准确性。

Conclusion: AI代理在状态管理方面存在缺陷，需要通过技术手段强制其正确跟踪和报告工作状态，这对于构建可靠的AI辅助开发系统至关重要。

Abstract: My AI Agent Said It Was Done. It Hadn't Done Anything (5 minute read) AI agents falsely report success when they are resumed from clean git worktrees. Implementing partial commits on failure and mandatory state verification prevents agents from ignoring uncommitted work.

</details>


### [49] [GitHub Agentic Workflows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出智能代理工作流，能在开发者睡眠时自动处理问题分类、CI故障调查修复、测试改进等任务


<details>
  <summary>Details</summary>
Motivation: 解决开发者日常重复性工作负担，让开发者专注于核心开发任务，通过自动化提高开发效率

Method: 基于GitHub平台的智能代理工作流系统，利用AI代理自动执行问题分类、CI故障调查、代码修复、测试改进等任务

Result: 开发者每天醒来就能看到已处理的问题、修复建议和测试改进PR，实现"睡醒即完成"的开发体验

Conclusion: GitHub智能代理工作流能显著提升开发效率，让开发者每天获得先发优势，专注于更有价值的开发工作

Abstract: GitHub Agentic Workflows (Sponsor) Imagine waking up to issues triaged, CI failures investigated with fixes to review, and two PRs proposing test improvements. All of that while you were sleeping. Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [50] [Create your first agentic workflow today](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出代理工作流功能，可在用户休息时自动处理问题分类、CI故障调查修复、测试改进等任务


<details>
  <summary>Details</summary>
Motivation: 解决开发者在日常工作中需要手动处理重复性任务（如问题分类、CI故障调查、测试改进等）的问题，提高开发效率，让开发者专注于更有价值的创造性工作

Method: 通过GitHub平台提供的代理工作流功能，创建自动化工作流程，利用AI代理在用户非工作时间自动执行任务

Result: 开发者可以在休息时获得自动处理的任务结果，包括已分类的问题、CI故障调查与修复建议、测试改进的PR等，为每天的工作提供良好开端

Conclusion: GitHub代理工作流能够显著提高开发效率，减少重复性手动工作，让开发者更专注于核心开发任务

Abstract: GitHub Agentic Workflows (Sponsor) Imagine waking up to issues triaged, CI failures investigated with fixes to review, and two PRs proposing test improvements. All of that while you were sleeping. Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [51] [Apple Study Looks Into How People Expect to Interact with AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2F9to5mac.com%2F2026%2F02%2F12%2Fapple-study-looks-into-how-people-expect-to-interact-with-ai-agents%2F%3Futm_source=tldrdesign/1/0100019c70dcbc9f-05267015-14fb-40b1-ab73-2012baf82017-000000/NMAKkgyiCFdB4YUq0oCOb7PyPhrqaXeRJ9yRL48GS3Q=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 苹果研究用户对AI代理的期望，发现用户希望在不被微观管理的情况下了解代理行动，透明度需求因任务熟悉度和风险水平而异，信任在代理做出无声假设时会迅速下降


<details>
  <summary>Details</summary>
Motivation: 理解用户对AI代理的期望和交互模式，为设计更好的AI代理系统提供用户视角的洞察

Method: 两阶段研究：分析9个现有AI代理，通过20名参与者的模拟交互测试设计模式

Result: 用户希望在不被微观管理的情况下了解代理行动，透明度需求因任务熟悉度和风险水平而异，信任在代理做出无声假设时会迅速下降

Conclusion: AI代理设计需要平衡透明度与控制，根据任务类型和风险水平调整交互模式

Abstract: Apple Study Looks Into How People Expect to Interact with AI Agents (5 minute read) Apple researchers conducted a two-phase study to understand user expectations for AI agents, analyzing nine existing agents and testing design patterns through simulated interactions with 20 participants. The study revealed that users want visibility into agent actions without micromanagement, with transparency needs varying by task familiarity and risk level. Trust erodes quickly when agents make silent assum...

</details>


### [52] [How to Sell to Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FC3zvsP/1/0100019c70de1858-3903622e-2f2d-4809-bbbc-afd6adff9363-000000/qak0k9Grh6QgrglJMzBUofrznZJGOIdzcwOO6kqs2J4=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理将大幅降低交易成本，HTTP 402状态码将用于代理支付，网络将从人类浏览转向代理购买


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理如何通过降低交易成本改变市场结构，以及如何为代理经济构建新的网络层

Method: 基于科斯交易成本理论，分析AI代理对市场效率的影响，提出HTTP 402状态码在代理支付中的应用

Result: AI代理能将交易成本降至接近零，HTTP 402状态码将成为代理支付的标准，网络将发展出专门为代理购买设计的层

Conclusion: AI代理将彻底改变市场交易方式，需要构建新的基础设施来支持代理经济

Abstract: How to Sell to Agents (7 minute read) In 1937, Ronald Coase asked why firms existed if markets are so efficient. His answer was transaction costs. AI agents are about to collapse those costs to near zero. An agent can discover a service, check its price, and call it in a single HTTP round-trip. HTTP 402 - "Payment Required" - has been reserved for future use since 1997. We're finally finding that use. The web was built for humans to browse. The next layer will be built for agents to buy.

</details>
