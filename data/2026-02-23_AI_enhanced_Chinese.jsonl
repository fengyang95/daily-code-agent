{"id": "2602.17838", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17838", "abs": "https://arxiv.org/abs/2602.17838", "authors": ["Lara Khatib", "Micheal Pu", "Bogdan Vasilescu", "Meiyappan Nagappan"], "title": "Examining LLMs Ability to Summarize Code Through Mutation-Analysis", "comment": null, "summary": "As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as \"bugs\", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5f02\u6d4b\u8bd5\u7684\u65b9\u6cd5\u8bc4\u4f30LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\uff0c\u800c\u975e\u8868\u9762\u610f\u56fe", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u4f9d\u8d56LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u8fdb\u884c\u6587\u6863\u3001\u6d4b\u8bd5\u548c\u5ba1\u67e5\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u3002LLM\u7ecf\u5e38\u81ea\u4fe1\u5730\u63cf\u8ff0\u4ee3\u7801\u770b\u8d77\u6765\u5e94\u8be5\u505a\u4ec0\u4e48\uff08\u610f\u56fe\uff09\uff0c\u4f46\u5ffd\u7565\u4e86\u5b9a\u4e49\u5b9e\u9645\u884c\u4e3a\u7684\u7ec6\u5fae\u8fb9\u754c\u60c5\u51b5\u6216\u903b\u8f91\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5f02\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1a\u751f\u6210\u6458\u8981\u2192\u5728\u4ee3\u7801\u4e2d\u6ce8\u5165\u76ee\u6807\u53d8\u5f02\u2192\u68c0\u67e5LLM\u662f\u5426\u66f4\u65b0\u6458\u8981\u4ee5\u53cd\u6620\u65b0\u884c\u4e3a\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1\uff0912\u4e2a\u53d7\u63a7\u5408\u6210\u7a0b\u5e8f\u7684324\u4e2a\u53d8\u5f02\uff1b2\uff09LBPP\u6570\u636e\u96c6\u4e2d50\u4e2a\u4eba\u5de5\u7f16\u5199\u7a0b\u5e8f\u7684150\u4e2a\u53d8\u5f02\u6837\u672c\uff1b3\uff09\u6bd4\u8f83GPT-4\u548cGPT-5.2\u7684\u6027\u80fd\u3002", "result": "\u6458\u8981\u51c6\u786e\u6027\u968f\u590d\u6742\u5ea6\u6025\u5267\u4e0b\u964d\uff1a\u5355\u51fd\u657076.5%\u2192\u591a\u7ebf\u7a0b\u7cfb\u7edf17.3%\uff1b\u4eba\u7c7b\u7f16\u5199\u7a0b\u5e8f\u6458\u8981\u51c6\u786e\u738749.3%\uff1bGPT-5.2\u76f8\u6bd4GPT-4\u6709\u663e\u8457\u63d0\u5347\uff0849.3%\u219285.3%\uff09\uff0c\u4f46\u4e24\u8005\u4ecd\u96be\u4ee5\u533a\u5206\u5b9e\u73b0\u7ec6\u8282\u4e0e\u6807\u51c6\u7b97\u6cd5\u6a21\u5f0f\u3002", "conclusion": "\u5efa\u7acb\u53d8\u5f02\u5206\u6790\u4f5c\u4e3a\u8bc4\u4f30LLM\u751f\u6210\u6458\u8981\u662f\u5426\u53cd\u6620\u7a0b\u5e8f\u884c\u4e3a\u800c\u975e\u8868\u9762\u6587\u672c\u6a21\u5f0f\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.17955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17955", "abs": "https://arxiv.org/abs/2602.17955", "authors": ["Imgyeong Lee", "Tayyib Ul Hassan", "Abram Hindle"], "title": "Mining Type Constructs Using Patterns in AI-Generated Code", "comment": null, "summary": "Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.", "AI": {"tldr": "AI\u4ee3\u7406\u5728TypeScript\u9879\u76ee\u4e2d\u6bd4\u4eba\u7c7b\u66f4\u9891\u7e41\u4f7f\u7528'any'\u5173\u952e\u5b57\u548c\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\uff0c\u4f46PR\u63a5\u53d7\u7387\u5374\u6bd4\u4eba\u7c7b\u9ad81.8\u500d", "motivation": "\u7814\u7a76AI\u5728\u7c7b\u578b\u76f8\u5173\u7f16\u7a0b\u4efb\u52a1\u4e2d\u662f\u5426\u672c\u8d28\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4ee5\u53caAI\u4ee3\u7406\u5728\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\u4e0b\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u8fc7\u5ea6\u4f7f\u7528\u6216\u8bef\u7528\u7c7b\u578b\u6784\u9020", "method": "\u5728TypeScript\u9879\u76ee\u9886\u57df\u8fdb\u884c\u9996\u6b21\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83AI\u4ee3\u7406\u548c\u4eba\u7c7b\u5728\u7c7b\u578b\u6784\u9020\u4f7f\u7528\u4e0a\u7684\u5dee\u5f02", "result": "AI\u4ee3\u7406\u6bd4\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u4f7f\u7528'any'\u5173\u952e\u5b57\uff089\u500d\uff09\uff0c\u66f4\u9891\u7e41\u4f7f\u7528\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\u548c\u5ffd\u7565\u7c7b\u578b\u68c0\u67e5\u7684\u6784\u9020\uff1b\u4f46AI\u4ee3\u7406\u7684PR\u63a5\u53d7\u7387\u6bd4\u4eba\u7c7b\u9ad81.8\u500d", "conclusion": "AI\u4ee3\u7406\u5728\u7c7b\u578b\u5b89\u5168\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u4f46PR\u63a5\u53d7\u7387\u66f4\u9ad8\uff1b\u5efa\u8bae\u5f00\u53d1\u8005\u5728\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u65f6\u4ed4\u7ec6\u786e\u8ba4\u4ee3\u7801\u5e93\u7684\u7c7b\u578b\u5b89\u5168\u6027", "topic": "agent analysis"}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u667a\u80fd\u4f53\u7684\u5b89\u5168\u95ee\u9898\u6e90\u4e8e\u6a21\u578b\u8bef\u8bbe\u800c\u975e\u8bad\u7ec3\u8bef\u5dee\uff0c\u901a\u8fc7\u7ecf\u6d4e\u5b66\u7406\u8bba\u5efa\u7acb\u4e3b\u89c2\u6a21\u578b\u5de5\u7a0b\u6846\u67b6\uff0c\u8bc1\u660e\u5b89\u5168\u884c\u4e3a\u662f\u79bb\u6563\u76f8\u800c\u975e\u5956\u52b1\u8fde\u7eed\u51fd\u6570\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u548cAI\u667a\u80fd\u4f53\u5728\u5173\u952e\u9886\u57df\u90e8\u7f72\u9762\u4e34\u884c\u4e3a\u75c5\u7406\u95ee\u9898\uff08\u5982\u5949\u627f\u3001\u5e7b\u89c9\u3001\u6218\u7565\u6b3a\u9a97\uff09\uff0c\u73b0\u6709\u5b89\u5168\u8303\u5f0f\u5c06\u8fd9\u4e9b\u89c6\u4e3a\u8bad\u7ec3\u4f2a\u5f71\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5176\u4ea7\u751f\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5c06\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\u5e94\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\uff0c\u5efa\u7acb\u667a\u80fd\u4f53\u5728\u9519\u8bef\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u4e0b\u4f18\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u8bc1\u660e\u4e0d\u5b89\u5168\u884c\u4e3a\u662f\u7ed3\u6784\u5fc5\u7136\u6027\uff1a\u4f5c\u4e3a\u7a33\u5b9a\u9519\u4f4d\u5747\u8861\u6216\u632f\u8361\u5faa\u73af\u51fa\u73b0\uff0c\u6218\u7565\u6b3a\u9a97\u4f5c\u4e3a\"\u9501\u5b9a\"\u5747\u8861\u6216\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6301\u7eed\u5b58\u5728\u3002\u5b89\u5168\u884c\u4e3a\u662f\u79bb\u6563\u76f8\uff0c\u7531\u667a\u80fd\u4f53\u8ba4\u77e5\u5148\u9a8c\u51b3\u5b9a\u3002", "conclusion": "\u5b89\u5168\u9700\u8981\u4e3b\u89c2\u6a21\u578b\u5de5\u7a0b\uff08\u8bbe\u8ba1\u667a\u80fd\u4f53\u5185\u90e8\u4fe1\u5ff5\u7ed3\u6784\uff09\u800c\u975e\u4ec5\u64cd\u7eb5\u73af\u5883\u5956\u52b1\uff0c\u6807\u5fd7\u7740\u4ece\u5956\u52b1\u64cd\u7eb5\u5230\u5851\u9020\u667a\u80fd\u4f53\u73b0\u5b9e\u89e3\u91ca\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u795e\u7ecf\u540c\u6b65\u73b0\u8c61\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u793e\u4ea4\u5927\u8111\u6d3b\u52a8\u76f8\u4f3c\uff0c\u4e3a\u8bc4\u4f30LLM\u7684\"\u793e\u4ea4\u5fc3\u667a\"\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u793e\u4ea4\u5fc3\u667a\u662f\u751f\u7269\u4f53\u7684\u4e13\u5c5e\u7279\u6027\uff0c\u867d\u7136LLM\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u662f\u4eba\u7c7b\u884c\u4e3a\u7684\u5f3a\u5927\u8fd1\u4f3c\uff0c\u4f46\u5173\u4e8eLLM\u662f\u5426\u5177\u6709\u53ef\u6bd4\u62df\u4eba\u7c7b\u793e\u4ea4\u5fc3\u667a\u7684\u80fd\u529b\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u540c\u6b65\u73b0\u8c61\u4e3a\u8fd9\u4e00\u4e89\u8bba\u63d0\u4f9b\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u540c\u6b65\u4f5c\u4e3a\u5206\u6790LLM\u793e\u4ea4\u6027\u7684\u65b0\u4ee3\u7406\u6307\u6807\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b9e\u9a8c\uff0c\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u6d4b\u91cfLLM\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\uff0c\u8bc4\u4f30\u5176\u53cd\u6620\u793e\u4ea4\u53c2\u4e0e\u5ea6\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\u53ef\u9760\u5730\u53cd\u6620\u4e86\u5b83\u4eec\u7684\u793e\u4ea4\u4e92\u52a8\u7279\u5f81\uff0c\u4e14\u795e\u7ecf\u540c\u6b65\u4e0eLLM\u7684\u793e\u4ea4\u8868\u73b0\u5448\u5f3a\u76f8\u5173\uff0c\u63ed\u793a\u4e86LLM\u5185\u90e8\u52a8\u6001\u4e0e\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u7684\u60ca\u4eba\u76f8\u4f3c\u6027\u3002", "conclusion": "LLM\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u795e\u7ecf\u540c\u6b65\u73b0\u8c61\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30LLM\u7684\"\u793e\u4ea4\u5fc3\u667a\"\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eLLM\u793e\u4ea4\u4e92\u52a8\u5185\u90e8\u52a8\u6001\u7684\u76f8\u4f3c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.18142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18142", "abs": "https://arxiv.org/abs/2602.18142", "authors": ["Sebastian Dingler", "Frederik Boenke"], "title": "Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing", "comment": null, "summary": "Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u53cd\u9988\u9a71\u52a8\u5de5\u4f5c\u6d41\u7684\u865a\u62dfECU\u6d4b\u8bd5\u73af\u5883\uff0c\u901a\u8fc7\u81ea\u52a8\u5dee\u5206\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u4fee\u6b63\u6765\u964d\u4f4eCPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u7684\u6280\u672f\u98ce\u9669\uff0c\u5b9e\u73b0\u786c\u4ef6\u53ef\u7528\u524d\u7684\u65e9\u671f\u8f6f\u4ef6\u96c6\u6210\u6d4b\u8bd5\u3002", "motivation": "\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u4e2d\uff0c\u8f6f\u4ef6\u5b8c\u6210\u5f80\u5f80\u65e9\u4e8e\u786c\u4ef6\u53ef\u7528\uff0c\u5bfc\u81f4\u540e\u671f\u96c6\u6210\u548c\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u6210\u4e3a\u74f6\u9888\u3002\u9700\u8981\u865a\u62df\u6d4b\u8bd5\u73af\u5883\u5728\u7269\u7406\u786c\u4ef6\u53ef\u7528\u524d\u8fd0\u884c\u771f\u5b9e\u8f6f\u4ef6\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u53cd\u9988\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7GDB\u8fde\u63a5\u5230\u53c2\u8003\u6a21\u62df\u5668\uff0c\u751f\u6210SystemC/TLM 2.0\u6307\u4ee4\u7ea7\u7cbe\u786e\u7684\u5904\u7406\u5668\u6a21\u578b\u3002\u901a\u8fc7\u81ea\u52a8\u5dee\u5206\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u4fee\u6b63\u6765\u786e\u4fddCPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u8868\u660e\uff0c\u6700\u5173\u952e\u7684CPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u6280\u672f\u98ce\u9669\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5dee\u5206\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u4fee\u6b63\u6765\u964d\u4f4e\u3002\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u6d4b\u8bd5\u3001\u975e\u4fb5\u5165\u5f0f\u8ffd\u8e2a\u548c\u7b26\u5408\u5b89\u5168\u6807\u51c6\u7684\u6545\u969c\u6ce8\u5165\u3002", "conclusion": "\u867d\u7136\u4e91\u89c4\u6a21\u90e8\u7f72\u548c\u5b8c\u6574\u5de5\u5177\u94fe\u96c6\u6210\u4ecd\u9700\u672a\u6765\u5de5\u4f5c\uff0c\u4f46\u539f\u578b\u5c55\u793a\u4e86\u865a\u62dfECU\u5b6a\u751f\u7684\u53ef\u884c\u5de6\u79fb\u8def\u5f84\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u53ef\u7528\u524d\u8fdb\u884c\u65e9\u671f\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u96c6\u6210\u3002", "topic": "code agent"}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "\u63d0\u51faEl Agente Gr\u00e1fico\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u578b\u5b89\u5168\u6267\u884c\u73af\u5883\u548c\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5c06LLM\u51b3\u7b56\u5d4c\u5165\u79d1\u5b66\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u7684\u4ee3\u7406\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u53ef\u8ffd\u6eaf\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dLLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u4e0e\u5f02\u6784\u8ba1\u7b97\u5de5\u5177\u7684\u96c6\u6210\u65b9\u5f0f\u4e34\u65f6\u4e14\u8106\u5f31\uff0c\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u4ee3\u7406\u65b9\u6cd5\u4ea7\u751f\u5927\u91cf\u4fe1\u606f\uff0c\u96be\u4ee5\u8ffd\u8e2a\u51b3\u7b56\u6765\u6e90\u548c\u5ba1\u8ba1\u3002", "method": "\u5f00\u53d1\u5355\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u79d1\u5b66\u6982\u5ff5\u7684\u7ed3\u6784\u5316\u62bd\u8c61\u548c\u5bf9\u8c61-\u56fe\u8c31\u6620\u5c04\u5668\uff0c\u5c06\u8ba1\u7b97\u72b6\u6001\u8868\u793a\u4e3a\u7c7b\u578b\u5316Python\u5bf9\u8c61\uff0c\u5b58\u50a8\u5728\u5185\u5b58\u6216\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u901a\u8fc7\u7c7b\u578b\u5316\u7b26\u53f7\u6807\u8bc6\u7b26\u800c\u975e\u539f\u59cb\u6587\u672c\u7ba1\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u91cf\u5b50\u5316\u5b66\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4ee3\u7406\u7ed3\u5408\u53ef\u9760\u6267\u884c\u5f15\u64ce\u80fd\u591f\u7a33\u5065\u6267\u884c\u590d\u6742\u3001\u591a\u6b65\u9aa4\u548c\u5e76\u884c\u8ba1\u7b97\uff1b\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u6784\u8c61\u96c6\u6210\u751f\u6210\u548c\u91d1\u5c5e\u6709\u673a\u6846\u67b6\u8bbe\u8ba1\uff0c\u77e5\u8bc6\u56fe\u8c31\u540c\u65f6\u4f5c\u4e3a\u8bb0\u5fc6\u548c\u63a8\u7406\u57fa\u7840\u3002", "conclusion": "\u62bd\u8c61\u5316\u548c\u7c7b\u578b\u5b89\u5168\u4e3a\u8d85\u8d8a\u63d0\u793a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u4ee3\u7406\u79d1\u5b66\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2602.18306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18306", "abs": "https://arxiv.org/abs/2602.18306", "authors": ["Dongming Jin", "Zhi Jin", "Zheng Fang", "Linyu Li", "XiaoTian Yang", "Yuanpeng He", "Xiaohong Chen"], "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation", "comment": "22page, 7 figures", "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReqElicitGym\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u4e2d\u8bbf\u8c08\u80fd\u529b\u7684\u4ea4\u4e92\u5f0f\u81ea\u52a8\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b101\u4e2a\u7f51\u7ad9\u9700\u6c42\u573a\u666f\u6570\u636e\u96c6\u3001\u6a21\u62df\u7528\u6237\u548c\u4efb\u52a1\u8bc4\u4f30\u5668\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u5f53\u524dLLM\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u968f\u7740LLM\u7f16\u7801\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\uff0cLLM\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u74f6\u9888\u4ece\u751f\u6210\u6b63\u786e\u4ee3\u7801\u8f6c\u5411\u83b7\u53d6\u7528\u6237\u9700\u6c42\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5c11\u91cf\u573a\u666f\u3001\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u548c\u4e3b\u89c2\u4eba\u5de5\u8bc4\u5206\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u91cf\u5316\u7684\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReqElicitGym\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b\uff1a1\uff09101\u4e2a\u7f51\u7ad9\u9700\u6c42\u83b7\u53d6\u573a\u666f\u7684\u6570\u636e\u96c6\uff1b2\uff09\u4ea4\u4e92\u5f0f\u6a21\u62df\u7528\u6237\uff08oracle user\uff09\uff1b3\uff09\u4efb\u52a1\u8bc4\u4f30\u5668\u3002\u8be5\u73af\u5883\u652f\u6301\u4efb\u4f55\u81ea\u52a8\u5316\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u65b9\u6cd5\uff08\u5982LLM\u4ee3\u7406\uff09\u8fdb\u884c\u53ef\u91cd\u590d\u3001\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u5bf97\u4e2a\u4ee3\u8868\u6027LLM\u7684\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff1a1\uff09\u5f53\u524dLLM\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4ec5\u80fd\u83b7\u53d6\u4e0d\u5230\u4e00\u534a\u7684\u9690\u6027\u9700\u6c42\uff1b2\uff09\u6709\u6548\u7684\u83b7\u53d6\u95ee\u9898\u5f80\u5f80\u51fa\u73b0\u5728\u5bf9\u8bdd\u540e\u671f\uff1b3\uff09LLM\u80fd\u83b7\u53d6\u4ea4\u4e92\u548c\u5185\u5bb9\u76f8\u5173\u7684\u9690\u6027\u9700\u6c42\uff0c\u4f46\u5728\u98ce\u683c\u76f8\u5173\u9700\u6c42\u4e0a\u6301\u7eed\u56f0\u96be\u3002", "conclusion": "ReqElicitGym\u80fd\u591f\u4fc3\u8fdb\u81ea\u52a8\u5316\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\uff0c\u5f53\u524dLLM\u5728\u8bbf\u8c08\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u3002", "topic": "agent analysis"}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "APEMO\u662f\u4e00\u79cd\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u60c5\u611f\u4fe1\u53f7\u4f18\u5316\u8ba1\u7b97\u5206\u914d\uff0c\u63d0\u5347\u957f\u65f6\u7a0b\u81ea\u4e3b\u4ee3\u7406\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u7684\u8f68\u8ff9\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u8f93\u51fa\uff0c\u4f46\u81ea\u4e3b\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u9700\u8981\u5728\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u4fdd\u6301\u6301\u7eed\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u5e76\u63d0\u5347\u8f68\u8ff9\u7ea7\u8d28\u91cf\u3002", "method": "APEMO\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u64cd\u4f5c\u65f6\u95f4\u60c5\u611f\u4fe1\u53f7\u6765\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u3002\u5b83\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\uff0c\u800c\u662f\u901a\u8fc7\u884c\u4e3a\u4ee3\u7406\u68c0\u6d4b\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5173\u952e\u7247\u6bb5\uff08\u5982\u5cf0\u503c\u65f6\u523b\u548c\u7ed3\u675f\u65f6\u523b\uff09\u8fdb\u884c\u4fee\u590d\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u57fa\u4e8eLLM\u7684\u89c4\u5212-\u6267\u884c\u6d41\u7a0b\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cAPEMO\u5728\u8f68\u8ff9\u7ea7\u8d28\u91cf\u548c\u91cd\u7528\u6982\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u7ed3\u6784\u5316\u7f16\u6392\u5668\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u65f6\u95f4\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u5f39\u6027\u7684\u5de5\u7a0b\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "CodeScaler\u662f\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u6269\u5c55\uff0c\u901a\u8fc7\u8bed\u6cd5\u611f\u77e5\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6301\u6709\u6548\u6027\u7684\u5956\u52b1\u5851\u9020\uff0c\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684RL\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f9d\u8d56\u4e8e\u5355\u5143\u6d4b\u8bd5\u7684\u6267\u884c\u53cd\u9988\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u53d7\u5230\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u7684\u6839\u672c\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\u6765\u6269\u5c55\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "method": "\u63d0\u51faCodeScaler\uff0c\u4e00\u4e2a\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u57fa\u4e8e\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u4ee3\u7801\u95ee\u9898\u7cbe\u5fc3\u7b56\u5212\u7684\u504f\u597d\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u8bed\u6cd5\u611f\u77e5\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6301\u6709\u6548\u6027\u7684\u5956\u52b1\u5851\u9020\uff0c\u786e\u4fdd\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u7f16\u7801\u57fa\u51c6\u4e0a\uff0cCodeScaler\u5c06Qwen3-8B-Base\u5e73\u5747\u63d0\u534711.72\u5206\uff0c\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684RL\u65b9\u6cd51.82\u5206\uff0c\u5e76\u80fd\u5728\u65e0\u9700\u6d4b\u8bd5\u7528\u4f8b\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u3002\u63a8\u7406\u65f6\uff0cCodeScaler\u4f5c\u4e3a\u6709\u6548\u7684\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e\u5355\u5143\u6d4b\u8bd5\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u964d\u4f4e10\u500d\u5ef6\u8fdf\u3002\u5728RM-Bench\u4e0a\uff0cCodeScaler\u4e0d\u4ec5\u5728\u4ee3\u7801\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u5956\u52b1\u6a21\u578b3.3\u5206\uff0c\u5728\u901a\u7528\u548c\u63a8\u7406\u9886\u57df\u4e5f\u5e73\u5747\u63d0\u53472.7\u5206\u3002", "conclusion": "CodeScaler\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\u7684\u6307\u4ee4\u4f18\u5316\u65b9\u6cd5\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u63d0\u793a\u6280\u672f\uff0c\u53d1\u73b0\u6307\u4ee4\u4f18\u5316\u80fd\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u4e0d\u540c\u63d0\u793a\u6280\u672f\u6709\u4e0d\u540c\u6548\u679c\u3002", "motivation": "\u6307\u4ee4\u4f18\u5316\u4e3a\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u6307\u4ee4\u4f18\u5316\u65b9\u6cd5\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u6548\u679c\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u63d0\u793a\u6280\u672f\uff1a\u76f4\u63a5\u9884\u6d4b\u3001\u601d\u7ef4\u94fe(CoT)\u3001\u5e26SQL\u5de5\u5177\u7684ReAct\u3001\u5e26Python\u6267\u884c\u7684CodeAct\u3002\u7814\u7a76\u4e86\u4e09\u79cdDSPy\u4f18\u5316\u5668\uff08COPRO\u3001MiPROv2\u3001SIMBA\uff09\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6307\u4ee4\u4f18\u5316\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff1aMiPROv2\u5bf9CoT\u63d0\u4f9b\u6700\u7a33\u5b9a\u7684\u589e\u76ca\uff0cSIMBA\u5bf9ReAct\u667a\u80fd\u4f53\u63d0\u4f9b\u6700\u5927\u6536\u76ca\uff08\u5c24\u5176\u5728\u66f4\u5927\u6a21\u578b\u89c4\u6a21\u65f6\uff09\u3002\u884c\u4e3a\u5206\u6790\u663e\u793aSIMBA\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u9f13\u52b1\u66f4\u76f4\u63a5\u7684\u63a8\u7406\u8def\u5f84\uff0c\u63d0\u5347CoT\u4e2d\u7684\u6570\u503c\u6bd4\u8f83\u80fd\u529b\uff0c\u5e2e\u52a9ReAct\u667a\u80fd\u4f53\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "\u5bf9\u4e8e\u8868\u683c\u4e8b\u5b9e\u68c0\u67e5\uff0cCoT\u4ecd\u7136\u6709\u6548\uff08\u5c24\u5176\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff09\u3002\u867d\u7136\u57fa\u4e8e\u66f4\u5927\u6a21\u578b\u7684ReAct\u667a\u80fd\u4f53\u53ef\u4ee5\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u7684\u6307\u4ee4\u4f18\u5316\u3002\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u4e0d\u540c\u63d0\u793a\u6280\u672f\u6709\u7279\u5b9a\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "AnchorTree\u6846\u67b6\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u5f15\u5bfc\u6269\u6563\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u4f18\u5148\u89e3\u6790\u8bed\u6cd5\u548c\u8bed\u4e49\u5173\u952e\u6807\u8bb0\u6765\u786e\u4fdd\u7a0b\u5e8f\u7ed3\u6784\u6b63\u786e\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u53ef\u6267\u884c\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65f6\u672a\u80fd\u5c0a\u91cd\u7f16\u7a0b\u8bed\u8a00\u7684\u521a\u6027\u7ed3\u6784\uff0c\u7ecf\u5e38\u4ea7\u751f\u65e0\u6cd5\u6267\u884c\u7684\u7834\u788e\u7a0b\u5e8f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u786e\u4fdd\u4ee3\u7801\u7ed3\u6784\u6b63\u786e\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAnchorTree\u6846\u67b6\uff0c\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u4f5c\u4e3a\u7ed3\u6784\u5316\u5148\u9a8c\u6765\u951a\u5b9a\u6269\u6563\u8fc7\u7a0b\uff0c\u4f18\u5148\u89e3\u6790\u8bed\u6cd5\u548c\u8bed\u4e49\u5173\u952e\u6807\u8bb0\uff08\u5982\u5173\u952e\u5b57\u3001\u6807\u8bc6\u7b26\uff09\uff0c\u5efa\u7acb\u7ed3\u6784\u652f\u67b6\u6765\u5f15\u5bfc\u540e\u7eed\u751f\u6210\u3002", "result": "\u901a\u8fc7AnCoder\u6a21\u578b\u7cfb\u5217\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u951a\u5b9a\u6269\u6563\u65b9\u6cd5\uff0c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u751f\u6210\u3002", "conclusion": "\u7ed3\u6784\u5316\u951a\u5b9a\u6269\u6563\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u786e\u4fdd\u7a0b\u5e8f\u7684\u7ed3\u6784\u6b63\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2602.18029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18029", "abs": "https://arxiv.org/abs/2602.18029", "authors": ["Ali El Filali", "In\u00e8s Bedar"], "title": "Towards More Standardized AI Evaluation: From Models to Agents", "comment": "19 pages, 3 figures", "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u57fa\u51c6\u548c\u805a\u5408\u5206\u6570\u7684\u8bc4\u4f30\u65b9\u6cd5\u5df2\u4e0d\u9002\u5e94AI\u4ee3\u7406\u65f6\u4ee3\uff0c\u8bc4\u4f30\u5e94\u6210\u4e3a\u63a7\u5236AI\u7cfb\u7edf\u884c\u4e3a\u3001\u5efa\u7acb\u4fe1\u4efb\u7684\u6838\u5fc3\u529f\u80fd\u800c\u975e\u6700\u7ec8\u68c0\u67e5\u70b9", "motivation": "AI\u7cfb\u7edf\u5df2\u4ece\u9759\u6001\u6a21\u578b\u6f14\u53d8\u4e3a\u590d\u5408\u578b\u3001\u4f7f\u7528\u5de5\u5177\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f46\u5927\u591a\u6570\u8bc4\u4f30\u5b9e\u8df5\u4ecd\u505c\u7559\u5728\u6a21\u578b\u4e2d\u5fc3\u65f6\u4ee3\u7684\u5047\u8bbe\u4e2d\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8d8a\u6765\u8d8a\u96be\u4ee5\u63ed\u793a\u7cfb\u7edf\u771f\u5b9e\u884c\u4e3a\uff0c\u53cd\u800c\u53ef\u80fd\u63a9\u76d6\u6545\u969c\u6a21\u5f0f", "method": "\u5206\u6790\u8bc4\u4f30\u6d41\u7a0b\u672c\u8eab\u5982\u4f55\u5f15\u5165\u9759\u9ed8\u6545\u969c\u6a21\u5f0f\uff0c\u63a2\u8ba8\u9ad8\u57fa\u51c6\u5206\u6570\u4e3a\u4f55\u7ecf\u5e38\u8bef\u5bfc\u56e2\u961f\uff0c\u7814\u7a76\u4ee3\u7406\u7cfb\u7edf\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u6027\u80fd\u6d4b\u91cf\u7684\u610f\u4e49\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u6307\u6807\u6216\u66f4\u96be\u57fa\u51c6", "result": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5728\u4ee3\u7406\u65f6\u4ee3\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u8bc4\u4f30\u5e94\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u6d4b\u91cf\u5b66\u79d1\uff0c\u7528\u4e8e\u5efa\u7acb\u5bf9\u975e\u786e\u5b9a\u6027\u7cfb\u7edf\u7684\u4fe1\u4efb\u3001\u8fed\u4ee3\u548c\u6cbb\u7406\uff0c\u800c\u975e\"\u6027\u80fd\u5267\u573a\"", "conclusion": "\u8bc4\u4f30\u9700\u8981\u4ece\u6700\u7ec8\u68c0\u67e5\u70b9\u8f6c\u53d8\u4e3aAI\u65f6\u4ee3\u7684\u6838\u5fc3\u63a7\u5236\u529f\u80fd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u8bc4\u4f30\u5e94\u4f5c\u4e3a\u6d4b\u91cf\u5b66\u79d1\u6765\u786e\u4fdd\u7cfb\u7edf\u5728\u53d8\u5316\u548c\u89c4\u6a21\u4e0b\u7684\u9884\u671f\u884c\u4e3a", "topic": "agent analysis"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OMAD\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u7684\u95ee\u9898\uff0c\u5728MPE\u548cMAMuJoCo\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e862.5-5\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u63d0\u5347\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u5df2\u5c55\u73b0\u51fa\u8272\u7684\u8868\u8fbe\u80fd\u529b\u548c\u591a\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5728\u5728\u7ebfMARL\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3b\u8981\u969c\u788d\u662f\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u963b\u788d\u4e86\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51faOMAD\u6846\u67b6\uff1a1\uff09\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\uff0c\u5b9e\u73b0\u6709\u6548\u63a2\u7d22\u800c\u4e0d\u4f9d\u8d56\u53ef\u5904\u7406\u4f3c\u7136\uff1b2\uff09\u5728CTDE\u8303\u5f0f\u4e0b\uff0c\u4f7f\u7528\u8054\u5408\u5206\u5e03\u503c\u51fd\u6570\u4f18\u5316\u53bb\u4e2d\u5fc3\u5316\u6269\u6563\u7b56\u7565\uff1b3\uff09\u5229\u7528\u53ef\u5904\u7406\u7684\u71b5\u589e\u5f3a\u76ee\u6807\u6307\u5bfc\u6269\u6563\u7b56\u7565\u7684\u540c\u6b65\u66f4\u65b0\uff0c\u786e\u4fdd\u7a33\u5b9a\u534f\u8c03\u3002", "result": "\u5728MPE\u548cMAMuJoCo\u768410\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0cOMAD\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e862.5\u500d\u52305\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "OMAD\u6210\u529f\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebfMARL\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u548c\u8054\u5408\u5206\u5e03\u503c\u51fd\u6570\u8bbe\u8ba1\uff0c\u514b\u670d\u4e86\u6269\u6563\u6a21\u578b\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6837\u672c\u6548\u7387\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18092", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18092", "abs": "https://arxiv.org/abs/2602.18092", "authors": ["Matthew DiGiuseppe", "Joshua Robison"], "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities", "comment": "39 pages, 10 figures", "summary": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u7528\u6237\u8ba4\u4e3aLLM\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u65f6\uff0cChatGPT\u7684\u7ea0\u6b63\u9519\u8bef\u4fe1\u606f\u6548\u679c\u4f1a\u964d\u4f4e28%\uff0c\u8868\u660eAI\u8bf4\u670d\u529b\u7684\u653f\u6cbb\u4e2d\u7acb\u6027\u611f\u77e5\u81f3\u5173\u91cd\u8981", "motivation": "\u968f\u7740LLM\u8fdb\u5165\u515a\u6d3e\u51b2\u7a81\uff0c\u7cbe\u82f1\u9636\u5c42\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u5b83\u4eec\u63cf\u7ed8\u6210\u610f\u8bc6\u5f62\u6001\u5bf9\u9f50\u7684\u5de5\u5177\u3002\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5\u8fd9\u4e9b\u53ef\u4fe1\u5ea6\u653b\u51fb\u662f\u5426\u4f1a\u964d\u4f4e\u57fa\u4e8eLLM\u7684\u8bf4\u670d\u6548\u679c\uff0c\u63a2\u7a76AI\u8bf4\u670d\u529b\u7684\u653f\u6cbb\u6761\u4ef6\u6027", "method": "\u5728\u7f8e\u56fd\u8fdb\u884c\u9884\u6ce8\u518c\u8c03\u67e5\u5b9e\u9a8c(N=2144)\uff0c\u53c2\u4e0e\u8005\u4e0eChatGPT\u8fdb\u884c\u4e09\u8f6e\u5bf9\u8bdd\uff0c\u7ea0\u6b63\u4e2a\u4eba\u6301\u6709\u7684\u7ecf\u6d4e\u653f\u7b56\u8bef\u89e3\u3002\u5b9e\u9a8c\u7ec4\u6536\u5230\u7b80\u77ed\u6d88\u606f\u8868\u660eLLM\u5bf9\u53c2\u4e0e\u8005\u6240\u5c5e\u515a\u6d3e\u5b58\u5728\u504f\u89c1\uff0c\u5bf9\u7167\u7ec4\u4e3a\u4e2d\u6027\u6761\u4ef6", "result": "\u4e0e\u4e2d\u6027\u5bf9\u7167\u7ec4\u76f8\u6bd4\uff0c\u8868\u660eLLM\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u7684\u8b66\u544a\u4f7f\u8bf4\u670d\u6548\u679c\u964d\u4f4e28%\u3002\u6587\u672c\u5206\u6790\u663e\u793a\u8b66\u544a\u6539\u53d8\u4e86\u4e92\u52a8\uff1a\u53d7\u8bbf\u8005\u66f4\u9891\u7e41\u5730\u53cd\u9a73\uff0c\u53c2\u4e0e\u5ea6\u66f4\u4f4e\uff0c\u63a5\u53d7\u5ea6\u66f4\u5dee", "conclusion": "\u5bf9\u8bddAI\u7684\u8bf4\u670d\u6548\u679c\u5177\u6709\u653f\u6cbb\u6761\u4ef6\u6027\uff0c\u53d7\u515a\u6d3e\u5bf9\u9f50\u611f\u77e5\u7684\u9650\u5236\u3002\u5f53\u7528\u6237\u8ba4\u4e3aAI\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u65f6\uff0c\u5176\u7ea0\u6b63\u9519\u8bef\u4fe1\u606f\u7684\u80fd\u529b\u4f1a\u663e\u8457\u964d\u4f4e", "topic": "agent analysis"}}
{"id": "2602.18262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18262", "abs": "https://arxiv.org/abs/2602.18262", "authors": ["Aaron Louis Eidt", "Nils Feldhus"], "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA", "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA", "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.", "AI": {"tldr": "ELIA\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0fWeb\u5e94\u7528\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5206\u6790\u6280\u672f\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u7b80\u5316\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u7406\u89e3\u590d\u6742\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u867d\u7136\u5f00\u53d1\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u6765\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\uff0c\u4f46\u5176\u590d\u6742\u6027\u9020\u6210\u4e86\u53ef\u8bbf\u95ee\u6027\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u5de5\u5177\u53ea\u80fd\u88ab\u4e13\u5bb6\u4f7f\u7528\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u6613\u7528\u7684\u5de5\u5177\u6765\u6269\u5927\u53d7\u4f17\u8303\u56f4\u3002", "method": "\u8bbe\u8ba1\u3001\u6784\u5efa\u548c\u8bc4\u4f30ELIA\u4ea4\u4e92\u5f0fWeb\u5e94\u7528\uff0c\u6574\u5408\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a\u5f52\u56e0\u5206\u6790\u3001\u51fd\u6570\u5411\u91cf\u5206\u6790\u548c\u7535\u8def\u8ffd\u8e2a\uff0c\u5e76\u5f15\u5165\u65b0\u65b9\u6cd5\uff1a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u4ea7\u751f\u7684\u590d\u6742\u53ef\u89c6\u5316\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u7528\u6237\u660e\u663e\u504f\u597d\u4ea4\u4e92\u5f0f\u3001\u53ef\u63a2\u7d22\u7684\u754c\u9762\u800c\u975e\u7b80\u5355\u7684\u9759\u6001\u53ef\u89c6\u5316\u3002AI\u9a71\u52a8\u7684\u89e3\u91ca\u5e2e\u52a9\u975e\u4e13\u5bb6\u5f25\u5408\u77e5\u8bc6\u5dee\u8ddd\uff0c\u7edf\u8ba1\u5206\u6790\u663e\u793a\u7528\u6237\u7684\u5148\u9a8cLLM\u7ecf\u9a8c\u4e0e\u7406\u89e3\u5206\u6570\u4e4b\u95f4\u65e0\u663e\u8457\u76f8\u5173\u6027\uff0c\u8868\u660e\u7cfb\u7edf\u964d\u4f4e\u4e86\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u7406\u89e3\u969c\u788d\u3002", "conclusion": "AI\u7cfb\u7edf\u786e\u5b9e\u53ef\u4ee5\u7b80\u5316\u590d\u6742\u7684\u6a21\u578b\u5206\u6790\uff0c\u4f46\u5176\u771f\u6b63\u6f5c\u529b\u5728\u4e8e\u4e0e\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4f18\u5148\u8003\u8651\u4ea4\u4e92\u6027\u3001\u7279\u5f02\u6027\u548c\u53d9\u4e8b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2602.17930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17930", "abs": "https://arxiv.org/abs/2602.17930", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance", "comment": "International Conference on Learning Representations (ICLR'26)", "summary": "Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/", "AI": {"tldr": "MIRA\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u6574\u5408LLM\u5148\u9a8c\u77e5\u8bc6\uff0c\u51cf\u5c11\u5bf9\u5b9e\u65f6LLM\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\u6837\u672c\u6548\u7387\u4f4e\uff0c\u800c\u8fc7\u5ea6\u4f9d\u8d56LLM\u76d1\u7763\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u4e0d\u53ef\u9760\u4fe1\u53f7\u95ee\u9898", "method": "\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u5b58\u50a8\u9ad8\u56de\u62a5\u7ecf\u9a8c\u8f68\u8ff9\u548cLLM\u8f93\u51fa\u7684\u5b50\u76ee\u6807\u7ed3\u6784\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u4fe1\u53f7\u8f6f\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\uff0c\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u6548\u7528\u9879\u8870\u51cf", "result": "MIRA\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u4f18\u4e8eRL\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e0e\u9891\u7e41LLM\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u56de\u62a5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5728\u7ebfLLM\u67e5\u8be2\u6b21\u6570", "conclusion": "\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u80fd\u6709\u6548\u6574\u5408LLM\u5148\u9a8c\u77e5\u8bc6\u52a0\u901f\u65e9\u671f\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u6536\u655b\u4fdd\u8bc1\uff0c\u51cf\u5c11\u5bf9\u5b9e\u65f6LLM\u76d1\u7763\u7684\u4f9d\u8d56", "topic": "agentic reinforcement learning"}}
{"id": "2602.17832", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17832", "abs": "https://arxiv.org/abs/2602.17832", "authors": ["Hang Liu", "Sangli Teng", "Maani Ghaffari"], "title": "MePoly: Max Entropy Polynomial Policy Optimization", "comment": null, "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.", "AI": {"tldr": "MePoly\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u7684\u65b0\u578b\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u63d0\u4f9b\u663e\u5f0f\u3001\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u5e76\u5b9e\u73b0\u7cbe\u786e\u71b5\u6700\u5927\u5316\u3002", "motivation": "\u4f20\u7edf\u53c2\u6570\u5316\u7b56\u7565\u96be\u4ee5\u8868\u793a\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u800c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u867d\u7136\u65e8\u5728\u6062\u590d\u591a\u6a21\u6001\uff0c\u4f46\u7f3a\u4e4f\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\uff0c\u8fd9\u4f7f\u5f97\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u53d8\u5f97\u590d\u6742\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u8868\u793a\u591a\u6a21\u6001\u53c8\u5177\u6709\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\u7684\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "method": "MePoly\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u8fdb\u884c\u7b56\u7565\u53c2\u6570\u5316\uff0c\u63d0\u4f9b\u663e\u5f0f\u3001\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\u3002\u8be5\u65b9\u6cd5\u5efa\u7acb\u5728\u7ecf\u5178\u77e9\u95ee\u9898\u7406\u8bba\u4e0a\uff0c\u5229\u7528\u591a\u9879\u5f0f\u5bf9\u4efb\u610f\u5206\u5e03\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\u3002", "result": "MePoly\u80fd\u591f\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u975e\u51f8\u6d41\u5f62\uff0c\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MePoly\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u80fd\u8868\u793a\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u53c8\u5177\u6709\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\u7684\u6709\u6548\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17931", "abs": "https://arxiv.org/abs/2602.17931", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning", "comment": "Association for the Advancement of Artificial Intelligence (AAAI)", "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LLM\u5f15\u5bfc\u548c\u667a\u80fd\u4f53\u7ecf\u9a8c\u7684\u5185\u5b58\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u6548\u7528\u51fd\u6570\u8bc4\u4f30\u8f68\u8ff9\u4e0e\u6210\u529f\u7b56\u7565\u7684\u5339\u914d\u5ea6\uff0c\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u6837\u672c\u6548\u7387", "motivation": "\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u5bfc\u81f4\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3002\u867d\u7136LLM\u53ef\u7528\u4e8e\u5b50\u76ee\u6807\u53d1\u73b0\u548c\u8f68\u8ff9\u5f15\u5bfc\uff0c\u4f46\u9891\u7e41\u8c03\u7528LLM\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898", "method": "\u6784\u5efa\u7f16\u7801LLM\u5f15\u5bfc\u548c\u667a\u80fd\u4f53\u6210\u529f\u8f68\u8ff9\u7684\u5185\u5b58\u56fe\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u51fd\u6570\u8bc4\u4f30\u8f68\u8ff9\u4e0e\u5148\u524d\u6210\u529f\u7b56\u7565\u7684\u5339\u914d\u5ea6\uff0c\u5c06\u6548\u7528\u878d\u5165\u4f18\u52bf\u51fd\u6570\u4e3acritic\u63d0\u4f9b\u989d\u5916\u6307\u5bfc\u800c\u4e0d\u6539\u53d8\u5956\u52b1", "result": "\u5728\u57fa\u51c6\u73af\u5883\u4e2d\u7684\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebfRL\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u65e9\u671f\u5b66\u4e60\uff0c\u6700\u7ec8\u56de\u62a5\u4e0e\u9700\u8981\u9891\u7e41LLM\u4ea4\u4e92\u7684\u65b9\u6cd5\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u8f93\u5165\u548c\u5076\u5c14\u5728\u7ebf\u67e5\u8be2\u7684\u5185\u5b58\u56fe\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u8fde\u7eedLLM\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6837\u672c\u6548\u7387", "topic": "agentic reinforcement learning"}}
{"id": "2602.17867", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17867", "abs": "https://arxiv.org/abs/2602.17867", "authors": ["Jo\u00e3o N. Cardoso", "Arlindo L. Oliveira", "Bruno Martins"], "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization", "comment": null, "summary": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.", "AI": {"tldr": "ADAPT\u662f\u4e00\u79cd\u7ed3\u5408\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u7a81\u53d8\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6587\u672c\u8f93\u5165\u4ee5\u6700\u5927\u5316\u6fc0\u6d3bLLM\u4e2d\u7684\u7279\u5b9a\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u79bb\u6563\u6027\u548c\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\u3002", "motivation": "\u7406\u89e3LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u5230\u7684\u65b9\u5411\u7f16\u7801\u4e86\u54ea\u4e9b\u7279\u5f81\u9700\u8981\u627e\u5230\u80fd\u5f3a\u70c8\u6fc0\u6d3b\u8fd9\u4e9b\u65b9\u5411\u7684\u8f93\u5165\u3002\u7279\u5f81\u53ef\u89c6\u5316\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u6765\u6700\u5927\u5316\u6fc0\u6d3b\u76ee\u6807\u65b9\u5411\uff0c\u4f46\u6587\u672c\u7684\u79bb\u6563\u6027\u548c\u73b0\u6709\u63d0\u793a\u4f18\u5316\u6280\u672f\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u9650\u5236\u4e86\u5728LLM\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faADAPT\u6df7\u5408\u65b9\u6cd5\uff1a\u7ed3\u5408\u675f\u641c\u7d22\u521d\u59cb\u5316\u4e0e\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u7a81\u53d8\uff0c\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u79bb\u6563\u6027\u548c\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\u8bbe\u8ba1\u3002\u5728Gemma 2 2B\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8bc4\u4f30\uff0c\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u96c6\u6fc0\u6d3b\u7edf\u8ba1\u7684\u5ea6\u91cf\u6807\u51c6\u8fdb\u884c\u4e25\u683c\u6bd4\u8f83\u3002", "result": "ADAPT\u5728\u4e0d\u540c\u5c42\u548c\u6f5c\u5728\u7c7b\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8bc1\u660eLLM\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u8be5\u9886\u57df\u91cf\u8eab\u5b9a\u5236\u7684\u8bbe\u8ba1\u5047\u8bbe\u3002", "conclusion": "LLM\u7684\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u79bb\u6563\u6027\u548c\u4f18\u5316\u6311\u6218\u8bbe\u8ba1\u7684\u7b97\u6cd5\u3002ADAPT\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u7406\u89e3LLM\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.18008", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18008", "abs": "https://arxiv.org/abs/2602.18008", "authors": ["Zihan Guan", "Rituparna Datta", "Mengxuan Hu", "Shunshun Liu", "Aiying Zhang", "Prasanna Balachandran", "Sheng Li", "Anil Vullikanti"], "title": "NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs", "comment": "19 pages, 6 figures", "summary": "Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.", "AI": {"tldr": "\u63d0\u51faNIMM\u8bc4\u4f30\u6846\u67b6\u548cNIMMgen\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30\u548c\u751f\u6210LLM\u6784\u5efa\u7684\u673a\u5236\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u90e8\u5206\u89c2\u6d4b\u548c\u591a\u6837\u5316\u4efb\u52a1\u76ee\u6807\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5236\u6a21\u578b\u6784\u5efa\u65b9\u6cd5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\uff08\u90e8\u5206\u89c2\u6d4b\u3001\u591a\u6837\u5316\u4efb\u52a1\u76ee\u6807\uff09\u7684\u53ef\u9760\u6027\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u6846\u67b6\u3002", "method": "\u63d0\u51faNIMM\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff0c\u7136\u540e\u8bbe\u8ba1NIMMgen\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u589e\u5f3a\u4ee3\u7801\u6b63\u786e\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793aNIMMgen\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u673a\u5236\u6a21\u578b\u652f\u6301\u53cd\u4e8b\u5b9e\u5e72\u9884\u6a21\u62df\u3002", "conclusion": "NIMM\u6846\u67b6\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6839\u672c\u6311\u6218\uff0cNIMMgen\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u673a\u5236\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.17978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17978", "abs": "https://arxiv.org/abs/2602.17978", "authors": ["Daqian Shao"], "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees", "comment": "A thesis submitted for the degree of DPhil in Computer Science at Oxford", "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5b58\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u51b3\u7b56\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u6269\u5c55\u5230\u6a21\u4eff\u5b66\u4e60\u548c\u65f6\u5e8f\u903b\u8f91\u76ee\u6807\u5b66\u4e60\uff0c\u5177\u6709\u6536\u655b\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff0c\u8fd9\u5728\u6210\u672c\u9ad8\u3001\u5371\u9669\u6216\u4e0d\u53ef\u884c\u7684\u573a\u666f\u4e2d\u5b58\u5728\u95ee\u9898\u3002\u79bb\u7ebf\u5b66\u4e60\u9762\u4e34\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\u7684\u6311\u6218\uff0c\u53ef\u80fd\u8bef\u5bfc\u667a\u80fd\u4f53\u91c7\u53d6\u6b21\u4f18\u6216\u5bf9\u6297\u6027\u884c\u52a8\u3002", "method": "1. \u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u4f5c\u4e3a\u6761\u4ef6\u77e9\u9650\u5236\u95ee\u9898\uff1b2. \u53d7\u53cc\u91cd/\u53bb\u504f\u673a\u5668\u5b66\u4e60\u542f\u53d1\uff0c\u5f00\u53d1\u5177\u6709\u6536\u655b\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u7684\u6837\u672c\u9ad8\u6548\u7b97\u6cd5\uff1b3. \u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\uff1b4. \u5f00\u53d1\u5b66\u4e60\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u7684\u9ad8\u5c42\u76ee\u6807\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210/\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4e14\u5177\u6709\u6536\u655b\u7387\u548c\u6700\u4f18\u6027\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5b58\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u51b3\u7b56\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5de5\u5177\u53d8\u91cf\u65b9\u6cd5\u548c\u6761\u4ef6\u77e9\u9650\u5236\u95ee\u9898\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18037", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18037", "abs": "https://arxiv.org/abs/2602.18037", "authors": ["Johannes Ackermann", "Michael Noukhovitch", "Takashi Ishida", "Masashi Sugiyama"], "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards", "comment": "25 pages, 15 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u68af\u5ea6\u6b63\u5219\u5316(GR)\u65b9\u6cd5\u6765\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5bfc\u7b56\u7565\u66f4\u65b0\u5230\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u533a\u57df\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684KL\u60e9\u7f5a\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "motivation": "RLHF/RLVR\u8bad\u7ec3\u4e2d\u5e38\u89c1\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff1a\u7b56\u7565\u53ef\u80fd\u5229\u7528\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u6027\u5b66\u4e60\u5230\u975e\u9884\u671f\u884c\u4e3a\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528KL\u60e9\u7f5a\u9650\u5236\u7b56\u7565\u66f4\u65b0\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u4e0d\u540c\u7684\u601d\u8def\u3002", "method": "1. \u7406\u8bba\u63a8\u5bfc\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u6536\u655b\u65f6\u6700\u4f18\u89e3\u5e73\u5766\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b2. \u4f7f\u7528\u68af\u5ea6\u6b63\u5219\u5316(GR)\u5f15\u5bfc\u8bad\u7ec3\u5230\u66f4\u5e73\u5766\u533a\u57df\u4ee5\u4fdd\u6301\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\uff1b3. \u63d0\u51fa\u9ad8\u6548\u7684\u6709\u9650\u5dee\u5206\u4f30\u8ba1\u5b9e\u73b0\u663e\u5f0fGR\u3002", "result": "1. \u68af\u5ea6\u8303\u6570\u4e0e\u5956\u52b1\u51c6\u786e\u6027\u5728RLHF\u4e2d\u7ecf\u9a8c\u76f8\u5173\uff1b2. \u53c2\u8003\u91cd\u7f6e\u7684KL\u60e9\u7f5a\u9690\u5f0f\u4f7f\u7528GR\u627e\u5230\u66f4\u5e73\u5766\u533a\u57df\uff1b3. \u663e\u5f0fGR\u5728\u591a\u6837\u5316\u7684LM\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u4e2d\u4f18\u4e8eKL\u60e9\u7f5a\uff1b4. \u5728RLHF\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684GPT\u8bc4\u5224\u80dc\u7387\uff1b5. \u907f\u514d\u8fc7\u5ea6\u5173\u6ce8\u57fa\u4e8e\u89c4\u5219\u7684\u6570\u5b66\u5956\u52b1\u683c\u5f0f\uff1b6. \u9632\u6b62\u5728LLM-as-a-Judge\u6570\u5b66\u4efb\u52a1\u4e2d\u9ed1\u5ba2\u8bc4\u5224\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u51b3RLHF\u4e2d\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u8bad\u7ec3\u5230\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u5e73\u5766\u533a\u57df\uff0c\u6bd4\u4f20\u7edfKL\u60e9\u7f5a\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18297", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18297", "abs": "https://arxiv.org/abs/2602.18297", "authors": ["Usman Anwar", "Tim Bakker", "Dana Kianfar", "Cristina Pinneri", "Christos Louizos"], "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "comment": "First two authors contributed equally", "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790CoT\u76d1\u63a7\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u76d1\u63a7\u51c6\u786e\u6027\u5e76\u9632\u6b62CoT\u9000\u5316", "motivation": "\u7814\u7a76CoT\u76d1\u63a7\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4fe1\u606f\u63d0\u53d6\u548c\u6700\u4f18\u76d1\u63a7\u51fd\u6570\u903c\u8fd1\u7684\u8bef\u5dee\u95ee\u9898", "method": "1. \u4fe1\u606f\u8bba\u5206\u6790CoT\u4e0e\u8f93\u51fa\u95f4\u4e92\u4fe1\u606f\u5bf9\u76d1\u63a7\u7684\u5fc5\u8981\u6027\uff1b2. \u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u57fa\u4e8eoracle\u7684\u76f4\u63a5\u5956\u52b1\u6cd5\u548c\u65e0\u6807\u7b7e\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6cd5", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u79cd\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u76d1\u63a7\u51c6\u786e\u6027\uff0c\u9632\u6b62CoT\u9000\u5316\uff0c\u7f13\u89e3\u4efb\u52a1\u5956\u52b1\u4e0d\u5b8c\u7f8e\u65f6\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898", "conclusion": "CoT\u76d1\u63a7\u53ef\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u76ee\u6807\u7cfb\u7edf\u6027\u5730\u6539\u8fdb\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u76d1\u63a7\u6027\u80fd\u5e76\u4fdd\u6301CoT\u8d28\u91cf", "topic": "agent analysis"}}
{"id": "2602.18015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18015", "abs": "https://arxiv.org/abs/2602.18015", "authors": ["Jongseong Chae", "Jongeui Park", "Yongjae Shin", "Gyeongmin Kim", "Seungyul Han", "Youngchul Sung"], "title": "Flow Actor-Critic for Offline Reinforcement Learning", "comment": "Accepted to ICLR 2026", "summary": "The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.", "AI": {"tldr": "\u63d0\u51faFlow Actor-Critic\u65b9\u6cd5\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u540c\u65f6\u4f7f\u7528\u6d41\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u548c\u4fdd\u5b88\u7684\u8bc4\u8bba\u5bb6\uff0c\u4ee5\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u96c6\u5e76\u9632\u6b62Q\u503c\u7206\u70b8\uff0c\u5728D4RL\u548cOGBench\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u96c6\u5206\u5e03\u901a\u5e38\u5448\u73b0\u590d\u6742\u591a\u6a21\u6001\u7279\u6027\uff0c\u9700\u8981\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u7b56\u7565\u6765\u6355\u6349\u8fd9\u4e9b\u5206\u5e03\uff0c\u800c\u4f20\u7edf\u7684\u9ad8\u65af\u7b56\u7565\u65e0\u6cd5\u5145\u5206\u5904\u7406\u6b64\u7c7b\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faFlow Actor-Critic\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u6d41\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\uff08actor\uff09\uff1b2) \u5229\u7528\u6d41\u6a21\u578b\u8fdb\u884c\u4fdd\u5b88\u8bc4\u8bba\u5bb6\uff08critic\uff09\u83b7\u53d6\uff0c\u9632\u6b62\u5728\u6570\u636e\u5916\u533a\u57df\u7684Q\u503c\u7206\u70b8\uff1b3) \u57fa\u4e8e\u6d41\u884c\u4e3a\u4ee3\u7406\u6a21\u578b\u8bbe\u8ba1\u65b0\u7684\u8bc4\u8bba\u5bb6\u6b63\u5219\u5316\u5668\uff0c\u8be5\u6a21\u578b\u662f\u6d41\u57fa\u7b56\u7565\u8bbe\u8ba1\u7684\u526f\u4ea7\u54c1\u3002", "result": "\u5728\u79bb\u7ebfRL\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ecD4RL\u548c\u6700\u8fd1\u7684OGBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u6d41\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u548c\u4fdd\u5b88\u8bc4\u8bba\u5bb6\uff0cFlow Actor-Critic\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u96c6\u5206\u5e03\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18117", "abs": "https://arxiv.org/abs/2602.18117", "authors": ["Yongjae Shin", "Jongseong Chae", "Jongeui Park", "Youngchul Sung"], "title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "comment": "ICLR 2026 camera-ready", "summary": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "AI": {"tldr": "FINO\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u589e\u5f3a\u63a2\u7d22\uff0c\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u6709\u9650\u5728\u7ebf\u9884\u7b97\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebfRL\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6269\u5c55\u5230\u5728\u7ebf\u5fae\u8c03\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5728\u7ebf\u5fae\u8c03\u89c6\u4e3a\u79bb\u7ebf\u9884\u8bad\u7ec3\u7684\u76f4\u63a5\u5ef6\u7eed\uff0c\u672a\u80fd\u89e3\u51b3\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u63a2\u7d22\u79bb\u7ebf\u6570\u636e\u96c6\u4e4b\u5916\u7684\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "\u63d0\u51faFINO\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7b56\u7565\uff1b2\uff09\u5728\u7b56\u7565\u8bad\u7ec3\u4e2d\u6ce8\u5165\u566a\u58f0\u4ee5\u9f13\u52b1\u63a2\u7d22\u79bb\u7ebf\u6570\u636e\u96c6\u4e4b\u5916\u7684\u52a8\u4f5c\uff1b3\uff09\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f7f\u7b56\u7565\u80fd\u5728\u5728\u7ebf\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u8c03\u6574\u884c\u4e3a\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFINO\u5728\u6709\u9650\u7684\u5728\u7ebf\u9884\u7b97\u4e0b\u59cb\u7ec8\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u79bb\u7ebf\u5230\u5728\u7ebfRL\u4e2d\u7684\u9ad8\u6548\u6837\u672c\u5229\u7528\u80fd\u529b\u3002", "conclusion": "FINO\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u71b5\u5f15\u5bfc\u91c7\u6837\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5230\u5728\u7ebfRL\u4e2d\u7684\u63a2\u7d22\u6311\u6218\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u5728RL\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18182", "abs": "https://arxiv.org/abs/2602.18182", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Lorenzo Pacchiardi", "Hugo Save", "Siddhesh Milind Pawar", "Behzad Mehrbakhsh", "Pablo Antonio Moreno Casares", "Ben Slater", "Paolo Bova", "Peter Romero", "Zachary R. Tyler", "Jonathan Prunty", "Luning Sun", "Jose Hernandez-Orallo"], "title": "Capabilities Ain't All You Need: Measuring Propensities in AI", "comment": null, "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aAI\u503e\u5411\u6027\u6d4b\u91cf\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u903b\u8f91\u516c\u5f0f\u548c\u7406\u60f3\u5e26\u6982\u5ff5\uff0c\u7ed3\u5408\u503e\u5411\u6027\u4e0e\u80fd\u529b\u8bc4\u4f30\u53ef\u66f4\u597d\u9884\u6d4bAI\u884c\u4e3a", "motivation": "\u4f20\u7edfAI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u80fd\u529b\u6d4b\u91cf\uff0c\u4f46\u503e\u5411\u6027\uff08\u6a21\u578b\u5c55\u73b0\u7279\u5b9a\u884c\u4e3a\u7684\u8d8b\u52bf\uff09\u5bf9\u6027\u80fd\u548c\u5b89\u5168\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfIRT\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u503e\u5411\u6027\u8bc4\u4f30\uff0c\u56e0\u4e3a\u503e\u5411\u6027\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u90fd\u53ef\u80fd\u6709\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53cc\u903b\u8f91\u516c\u5f0f\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u6210\u529f\u6982\u7387\u5efa\u6a21\u4e3a\u5f53\u6a21\u578b\u503e\u5411\u6027\u5904\u4e8e\"\u7406\u60f3\u5e26\"\u5185\u65f6\u6982\u7387\u8f83\u9ad8\u3002\u4f7f\u7528\u65b0\u5f00\u53d1\u7684\u4efb\u52a1\u65e0\u5173\u8bc4\u5206\u6807\u51c6\u548cLLM\u4f30\u8ba1\u7406\u60f3\u5e26\u8fb9\u754c\u3002\u57286\u4e2aLLM\u6a21\u578b\u5bb6\u65cf\u4e0a\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u6d4b\u91cf\u503e\u5411\u6027\u504f\u79fb\u53ca\u5176\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528\u4e00\u4e2a\u57fa\u51c6\u4f30\u8ba1\u7684\u503e\u5411\u6027\u53ef\u6210\u529f\u9884\u6d4b\u4fdd\u7559\u4efb\u52a1\u7684\u884c\u4e3a\u3002\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u8bc4\u4f30\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u79cd\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\u3002\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u8fdb\u884c\u4e25\u683c\u7684\u503e\u5411\u6027\u6d4b\u91cf\u53ca\u5176\u76f8\u5bf9\u4e8e\u5355\u7eaf\u80fd\u529b\u8bc4\u4f30\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u6b63\u5f0f\u7684AI\u503e\u5411\u6027\u6d4b\u91cf\u6846\u67b6\uff0c\u8bc1\u660e\u503e\u5411\u6027\u6d4b\u91cf\u5bf9\u4e8e\u9884\u6d4bAI\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u8bc4\u4f30\u53ef\u63d0\u4f9b\u66f4\u5168\u9762\u7684AI\u884c\u4e3a\u9884\u6d4b\u3002", "topic": "agent analysis"}}
{"id": "2602.18277", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18277", "abs": "https://arxiv.org/abs/2602.18277", "authors": ["Finn van der Knaap", "Kejiang Qian", "Zheng Xu", "Fengxiang He"], "title": "PRISM: Parallel Reward Integration with Symmetry for MORL", "comment": null, "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.", "AI": {"tldr": "PRISM\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u53cd\u5c04\u5bf9\u79f0\u6027\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u89e3\u51b3\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u76ee\u6807\u65f6\u95f4\u9891\u7387\u5dee\u5f02\u5bfc\u81f4\u7684\u7a00\u758f\u5956\u52b1\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u5e15\u7d2f\u6258\u8986\u76d6\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4e0d\u540c\u76ee\u6807\u7684\u65f6\u95f4\u9891\u7387\u5dee\u5f02\u5de8\u5927\uff08\u5bc6\u96c6\u76ee\u6807vs\u7a00\u758f\u957f\u65f6\u7a0b\u5956\u52b1\uff09\uff0c\u5bfc\u81f4\u5bc6\u96c6\u76ee\u6807\u4e3b\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u7a00\u758f\u5956\u52b1\u7684\u4fe1\u7528\u5206\u914d\u5f31\uff0c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faPRISM\u7b97\u6cd5\uff1a1\uff09ReSymNet\u6a21\u578b\u901a\u8fc7\u6b8b\u5dee\u5757\u5b66\u4e60\u7f29\u653e\u673a\u4f1a\u4ef7\u503c\uff0c\u534f\u8c03\u76ee\u6807\u95f4\u7684\u65f6\u95f4\u9891\u7387\u4e0d\u5339\u914d\uff1b2\uff09SymReg\u53cd\u5c04\u7b49\u53d8\u6027\u6b63\u5219\u5316\u5668\uff0c\u5f3a\u5236\u667a\u80fd\u4f53\u955c\u50cf\u5bf9\u79f0\uff0c\u5c06\u7b56\u7565\u641c\u7d22\u7ea6\u675f\u5728\u53cd\u5c04\u7b49\u53d8\u5b50\u7a7a\u95f4\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRISM\u6301\u7eed\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\u57fa\u7ebf\u548c\u5168\u5bc6\u96c6\u5956\u52b1\u7684oracle\u6a21\u578b\uff1a\u8d85\u4f53\u79ef\u589e\u76ca\u8d85\u8fc7\u57fa\u7ebf100%\uff0c\u6bd4oracle\u63d0\u5347\u8fbe32%\uff0c\u6539\u5584\u4e86\u5e15\u7d2f\u6258\u8986\u76d6\u548c\u5206\u5e03\u5e73\u8861\u3002", "conclusion": "PRISM\u901a\u8fc7\u53cd\u5c04\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u9891\u7387\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u957f\u65f6\u7a0b\u5956\u52b1\u7684\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.18266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18266", "abs": "https://arxiv.org/abs/2602.18266", "authors": ["Stefan Wahl", "Raphaela Schenk", "Ali Farnoud", "Jakob H. Macke", "Daniel Gedon"], "title": "A Probabilistic Framework for LLM-Based Model Discovery", "comment": null, "summary": "Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.", "AI": {"tldr": "\u5c06\u6a21\u578b\u53d1\u73b0\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6982\u7387\u63a8\u65ad\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5e8f\u8d2f\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684ModelSMC\u7b97\u6cd5\uff0c\u4f7f\u7528LLM\u8fed\u4ee3\u63d0\u51fa\u548c\u7cbe\u70bc\u5019\u9009\u6a21\u578b", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6a21\u578b\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6982\u7387\u516c\u5f0f\u5316\u6846\u67b6\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u7edf\u4e00\u6027\u548c\u7406\u8bba\u57fa\u7840", "method": "\u5c06\u6a21\u578b\u53d1\u73b0\u89c6\u4e3a\u4ece\u672a\u77e5\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6982\u7387\u63a8\u65ad\u95ee\u9898\uff0c\u63d0\u51faModelSMC\u7b97\u6cd5\uff0c\u4f7f\u7528\u5e8f\u8d2f\u8499\u7279\u5361\u6d1b\u91c7\u6837\uff0c\u5c06\u5019\u9009\u6a21\u578b\u8868\u793a\u4e3a\u7c92\u5b50\uff0c\u7531LLM\u8fed\u4ee3\u63d0\u51fa\u548c\u7cbe\u70bc\uff0c\u57fa\u4e8e\u4f3c\u7136\u51c6\u5219\u52a0\u6743", "result": "\u5728\u771f\u5b9e\u79d1\u5b66\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u5177\u6709\u53ef\u89e3\u91ca\u673a\u5236\u7684\u6a21\u578b\uff0c\u5e76\u6539\u5584\u4e86\u540e\u9a8c\u9884\u6d4b\u68c0\u67e5", "conclusion": "\u6982\u7387\u89c6\u89d2\u4e3a\u7406\u89e3\u548c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u6a21\u578b\u53d1\u73b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0cModelSMC\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027", "topic": "agent analysis"}}
{"id": "tldr.2602.0f3e59fd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cogent.com%2F%3Futm_source=tldrinfosec/1/0100019c7652a59c-eabf2861-2d32-46bb-90d2-ef03eb240b3a-000000/YXO5jDiFLbavtAbCEjoiJUuUPVfoGUJs9KDIRtXJr18=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cogent.com%2F%3Futm_source=tldrinfosec/1/0100019c7652a59c-eabf2861-2d32-46bb-90d2-ef03eb240b3a-000000/YXO5jDiFLbavtAbCEjoiJUuUPVfoGUJs9KDIRtXJr18=445", "authors": ["TLDR Newsletter"], "title": "Cogent Security", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cogent.com%2F%3Futm_source=tldrinfosec/1/0100019c7652a59c-eabf2861-2d32-46bb-90d2-ef03eb240b3a-000000/YXO5jDiFLbavtAbCEjoiJUuUPVfoGUJs9KDIRtXJr18=445", "summary": "Cogent Security (Product Launch) Cogent Security is an agentic AI platform for vulnerability management that autonomously investigates, prioritizes, and orchestrates remediation of security issues.", "source": "tldr", "AI": {"tldr": "Cogent Security\u662f\u4e00\u4e2a\u7528\u4e8e\u6f0f\u6d1e\u7ba1\u7406\u7684\u667a\u80fdAI\u5e73\u53f0\uff0c\u80fd\u591f\u81ea\u4e3b\u8c03\u67e5\u3001\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u7f16\u6392\u5b89\u5168\u95ee\u9898\u7684\u4fee\u590d", "motivation": "\u4f20\u7edf\u6f0f\u6d1e\u7ba1\u7406\u6d41\u7a0b\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u9700\u8981\u81ea\u52a8\u5316\u3001\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u5b89\u5168\u95ee\u9898\u7684\u8bc6\u522b\u3001\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u4fee\u590d", "method": "\u91c7\u7528\u667a\u80fdAI\u4ee3\u7406\u5e73\u53f0\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u8c03\u67e5\u3001\u667a\u80fd\u4f18\u5148\u7ea7\u7b97\u6cd5\u548c\u4fee\u590d\u7f16\u6392\u529f\u80fd\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6f0f\u6d1e\u7ba1\u7406\u81ea\u52a8\u5316", "result": "\u63a8\u51fa\u4e86Cogent Security\u4ea7\u54c1\uff0c\u8fd9\u662f\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5904\u7406\u6f0f\u6d1e\u7ba1\u7406\u7684AI\u5e73\u53f0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u56e2\u961f\u7684\u5de5\u4f5c\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6", "conclusion": "\u667a\u80fdAI\u4ee3\u7406\u5e73\u53f0\u5728\u6f0f\u6d1e\u7ba1\u7406\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5b89\u5168\u6d41\u7a0b\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898", "topic": "code agent"}}
{"id": "tldr.2602.11c1909e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropleaf.app%2Fd%2FAlXez8scbd%3Futm_source=tldrmarketing/1/0100019c7af165da-9fe91d0a-475d-40bc-a120-716381622763-000000/A2FbghIUesEd7gKLEQ_EZ91lzL3Lso3_bWdJkASbAHc=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropleaf.app%2Fd%2FAlXez8scbd%3Futm_source=tldrmarketing/1/0100019c7af165da-9fe91d0a-475d-40bc-a120-716381622763-000000/A2FbghIUesEd7gKLEQ_EZ91lzL3Lso3_bWdJkASbAHc=445", "authors": ["TLDR Newsletter"], "title": "AI Taxonomy: An Operational Framework for Precision in AI Discourse", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropleaf.app%2Fd%2FAlXez8scbd%3Futm_source=tldrmarketing/1/0100019c7af165da-9fe91d0a-475d-40bc-a120-716381622763-000000/A2FbghIUesEd7gKLEQ_EZ91lzL3Lso3_bWdJkASbAHc=445", "summary": "AI Taxonomy: An Operational Framework for Precision in AI Discourse (3 minute read) This AI taxonomy defines AI by what it does and outlines 6 roles. Analytical AI predicts and scores. Semantic AI understands context and relationships. Generative AI creates content. Agentic AI plans and executes workflows. Perceptive AI interprets vision, speech, and documents. Physical AI operates in the physical world. It also separates capability from interface and stresses that strong products combine mul...", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u5206\u7c7b\u6846\u67b6\uff0c\u5b9a\u4e49\u4e866\u79cdAI\u89d2\u8272\uff1a\u5206\u6790\u578b\u3001\u8bed\u4e49\u578b\u3001\u751f\u6210\u578b\u3001\u4ee3\u7406\u578b\u3001\u611f\u77e5\u578b\u548c\u7269\u7406\u578b\uff0c\u5f3a\u8c03\u5c06\u80fd\u529b\u4e0e\u63a5\u53e3\u5206\u79bb\uff0c\u5e76\u6307\u51fa\u5f3a\u5927\u4ea7\u54c1\u9700\u8981\u591a\u79cdAI\u7c7b\u578b\u7ec4\u5408\u3002", "motivation": "\u5f53\u524dAI\u8ba8\u8bba\u4e2d\u672f\u8bed\u6df7\u4e71\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\u6765\u7cbe\u786e\u63cf\u8ff0AI\u7cfb\u7edf\u7684\u529f\u80fd\u548c\u89d2\u8272\uff0c\u5bfc\u81f4\u6c9f\u901a\u4e0d\u51c6\u786e\u548c\u4ea7\u54c1\u8bbe\u8ba1\u4e0d\u6e05\u6670\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u64cd\u4f5c\u6027AI\u5206\u7c7b\u6cd5\uff0c\u57fa\u4e8eAI\u7684\u529f\u80fd\u5b9a\u4e496\u79cd\u89d2\u8272\u7c7b\u578b\uff0c\u5e76\u533a\u5206AI\u80fd\u529b\u4e0e\u7528\u6237\u63a5\u53e3\uff0c\u5f3a\u8c03\u4ea7\u54c1\u8bbe\u8ba1\u4e2d\u9700\u8981\u591a\u79cdAI\u7c7b\u578b\u7684\u7ec4\u5408\u3002", "result": "\u5efa\u7acb\u4e86\u6e05\u6670\u7684AI\u5206\u7c7b\u6846\u67b6\uff0c\u5305\u62ec6\u79cd\u660e\u786e\u7684AI\u89d2\u8272\u5b9a\u4e49\uff0c\u4e3aAI\u4ea7\u54c1\u8bbe\u8ba1\u3001\u6280\u672f\u8ba8\u8bba\u548c\u884c\u4e1a\u6c9f\u901a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bed\u8a00\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6846\u67b6\u80fd\u591f\u63d0\u9ad8AI\u8ba8\u8bba\u7684\u7cbe\u786e\u6027\uff0c\u6307\u5bfc\u4ea7\u54c1\u8bbe\u8ba1\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522bAI\u7cfb\u7edf\u7684\u6838\u5fc3\u529f\u80fd\u4e0e\u7528\u6237\u754c\u9762\u7684\u533a\u522b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.8873ebaf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fvs-code-becomes-multi-agent-command-center-for-developers%2F%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/js0dedmmkOMw0Ml-V4mCOhbAT2poTabC9Zu77j_KJxk=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fvs-code-becomes-multi-agent-command-center-for-developers%2F%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/js0dedmmkOMw0Ml-V4mCOhbAT2poTabC9Zu77j_KJxk=445", "authors": ["TLDR Newsletter"], "title": "VS Code becomes multi-agent command center for developers", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fvs-code-becomes-multi-agent-command-center-for-developers%2F%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/js0dedmmkOMw0Ml-V4mCOhbAT2poTabC9Zu77j_KJxk=445", "summary": "VS Code becomes multi-agent command center for developers (5 minute read) Visual Studio Code v1.109 introduces multi-agent orchestration with support for Anthropic Claude and OpenAI Codex alongside GitHub Copilot, unified session management, parallel subagents, and MCP Apps. The release enhances context retention, security sandboxing, performance, and positions VS Code as a universal AI interface.", "source": "tldr", "AI": {"tldr": "VS Code v1.109\u5f15\u5165\u591a\u667a\u80fd\u4f53\u7f16\u6392\u529f\u80fd\uff0c\u652f\u6301Anthropic Claude\u3001OpenAI Codex\u548cGitHub Copilot\uff0c\u63d0\u4f9b\u7edf\u4e00\u4f1a\u8bdd\u7ba1\u7406\u3001\u5e76\u884c\u5b50\u667a\u80fd\u4f53\u548cMCP\u5e94\u7528\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u4fdd\u7559\u3001\u5b89\u5168\u6c99\u7bb1\u548c\u6027\u80fd\u3002", "motivation": "\u5c06VS Code\u4ece\u4f20\u7edf\u4ee3\u7801\u7f16\u8f91\u5668\u8f6c\u53d8\u4e3a\u5f00\u53d1\u8005\u7684\u591a\u667a\u80fd\u4f53\u6307\u6325\u4e2d\u5fc3\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2aAI\u52a9\u624b\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u7f16\u7a0b\u8f85\u52a9\u529f\u80fd\u3002", "method": "\u5728VS Code v1.109\u7248\u672c\u4e2d\u5f15\u5165\u591a\u667a\u80fd\u4f53\u7f16\u6392\u67b6\u6784\uff0c\u652f\u6301\u591a\u4e2aAI\u6a21\u578b\uff08Claude\u3001Codex\u3001Copilot\uff09\u5e76\u884c\u5de5\u4f5c\uff0c\u63d0\u4f9b\u7edf\u4e00\u4f1a\u8bdd\u7ba1\u7406\u3001\u5e76\u884c\u5b50\u667a\u80fd\u4f53\u534f\u8c03\u548cMCP\u5e94\u7528\u96c6\u6210\u3002", "result": "VS Code\u6210\u4e3a\u901a\u7528AI\u63a5\u53e3\uff0c\u5177\u5907\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u4fdd\u7559\u80fd\u529b\u3001\u5b89\u5168\u6c99\u7bb1\u4fdd\u62a4\u3001\u6027\u80fd\u4f18\u5316\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u591a\u667a\u80fd\u4f53\u7f16\u7a0b\u8f85\u52a9\u73af\u5883\u3002", "conclusion": "VS Code v1.109\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7f16\u6392\u529f\u80fd\u6210\u529f\u8f6c\u578b\u4e3a\u5f00\u53d1\u8005\u7684AI\u6307\u6325\u4e2d\u5fc3\uff0c\u4e3a\u7f16\u7a0b\u5de5\u4f5c\u6d41\u5e26\u6765\u9769\u547d\u6027\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "tldr.2602.d935a596", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fobra%2Fsuperpowers%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/rvNtxpZ9OvgocahvUXFHTKfGJUTjpyJcmMigHHb3uNE=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fobra%2Fsuperpowers%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/rvNtxpZ9OvgocahvUXFHTKfGJUTjpyJcmMigHHb3uNE=445", "authors": ["TLDR Newsletter"], "title": "Superpowers", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fobra%2Fsuperpowers%3Futm_source=tldrdevops/1/0100019c7af21329-6561766d-3516-4027-a78a-1d90240e1d26-000000/rvNtxpZ9OvgocahvUXFHTKfGJUTjpyJcmMigHHb3uNE=445", "summary": "Superpowers (GitHub Repo) Superpowers gives AI coding agents like Claude a structured workflow for software development, guiding them through design validation, test-driven development, and subagent-driven implementation that can run autonomously for up to two hours at a time. The framework works as a plugin for platforms like Claude Code and Cursor, automatically triggering composable \"skills\" that enforce workflows like breaking projects into 2-5 minute tasks, following strict red-green-ref...", "source": "tldr", "AI": {"tldr": "Superpowers\u662f\u4e00\u4e2a\u4e3aAI\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u548c\u5b50\u4ee3\u7406\u9a71\u52a8\u5b9e\u73b0\u6765\u6307\u5bfc\u8f6f\u4ef6\u5f00\u53d1\uff0c\u80fd\u591f\u81ea\u4e3b\u8fd0\u884c\u957f\u8fbe\u4e24\u5c0f\u65f6\u3002", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u81ea\u4e3b\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u6307\u5bfcAI\u4ee3\u7406\u5b8c\u6210\u4ece\u8bbe\u8ba1\u5230\u5b9e\u73b0\u7684\u5b8c\u6574\u5f00\u53d1\u8fc7\u7a0b\u3002", "method": "\u4f5c\u4e3aClaude Code\u548cCursor\u7b49\u5e73\u53f0\u7684\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u52a8\u89e6\u53d1\u53ef\u7ec4\u5408\u7684\"\u6280\u80fd\"\u6765\u5f3a\u5236\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u5c06\u9879\u76ee\u5206\u89e3\u4e3a2-5\u5206\u949f\u7684\u4efb\u52a1\u3001\u4e25\u683c\u9075\u5faa\u7ea2\u7eff\u91cd\u6784\u6d41\u7a0b\u3001\u8bbe\u8ba1\u9a8c\u8bc1\u548c\u5b50\u4ee3\u7406\u9a71\u52a8\u7684\u5b9e\u73b0\u3002", "result": "\u8be5\u6846\u67b6\u4f7fAI\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u8fd0\u884c\u957f\u8fbe\u4e24\u5c0f\u65f6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u652f\u6301\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u548c\u7cfb\u7edf\u5316\u8bbe\u8ba1\u9a8c\u8bc1\u3002", "conclusion": "Superpowers\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u80fd\u89e6\u53d1\u548c\u4efb\u52a1\u5206\u89e3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2602.70ce5463", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgeguimaraes.com%2Fyour-agent-orchestrator-is-just-a-bad-clone-of-elixir%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/br0j__duLhnLZDFcQdJT9nW2MBMjsyKE1nMESPZykNw=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgeguimaraes.com%2Fyour-agent-orchestrator-is-just-a-bad-clone-of-elixir%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/br0j__duLhnLZDFcQdJT9nW2MBMjsyKE1nMESPZykNw=445", "authors": ["TLDR Newsletter"], "title": "Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgeguimaraes.com%2Fyour-agent-orchestrator-is-just-a-bad-clone-of-elixir%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/br0j__duLhnLZDFcQdJT9nW2MBMjsyKE1nMESPZykNw=445", "summary": "Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI (15 minute read) Modern AI agent frameworks in Python and TypeScript are similar to the Elixir/Erlang BEAM virtual machine, which solved similar concurrency and fault-recovery challenges for telecom in 1986. The BEAM's actor model has features like isolated lightweight processes, message passing, and supervision trees, well-suited for the long-lived, non-deterministic interactions of AI agents.", "source": "tldr", "AI": {"tldr": "\u73b0\u4ee3AI\u4ee3\u7406\u6846\u67b6\u4e0e1986\u5e74Elixir/Erlang BEAM\u865a\u62df\u673a\u7684\u5e76\u53d1\u6a21\u578b\u76f8\u4f3c\uff0c\u540e\u8005\u4e3a\u7535\u4fe1\u884c\u4e1a\u89e3\u51b3\u4e86\u7c7b\u4f3c\u7684\u5e76\u53d1\u548c\u5bb9\u9519\u6311\u6218", "motivation": "\u5f53\u524dPython\u548cTypeScript\u4e2d\u7684AI\u4ee3\u7406\u6846\u67b6\u5728\u5e76\u53d1\u5904\u7406\u548c\u5bb9\u9519\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u800cElixir/Erlang\u7684BEAM\u865a\u62df\u673a\u65e9\u57281986\u5e74\u5c31\u4e3a\u7535\u4fe1\u884c\u4e1a\u89e3\u51b3\u4e86\u7c7b\u4f3c\u95ee\u9898", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u73b0\u4ee3AI\u4ee3\u7406\u6846\u67b6\u4e0eBEAM\u865a\u62df\u673a\u7684\u67b6\u6784\uff0c\u6307\u51faBEAM\u7684actor\u6a21\u578b\uff08\u5305\u542b\u9694\u79bb\u7684\u8f7b\u91cf\u7ea7\u8fdb\u7a0b\u3001\u6d88\u606f\u4f20\u9012\u548c\u76d1\u7763\u6811\uff09\u66f4\u9002\u5408AI\u4ee3\u7406\u7684\u957f\u751f\u547d\u5468\u671f\u3001\u975e\u786e\u5b9a\u6027\u4ea4\u4e92\u9700\u6c42", "result": "\u53d1\u73b0\u73b0\u4ee3AI\u4ee3\u7406\u6846\u67b6\u5728\u5e76\u53d1\u548c\u5bb9\u9519\u65b9\u9762\u53ea\u662fBEAM\u865a\u62df\u673a\u7684\u4f4e\u8d28\u91cf\u590d\u5236\u54c1\uff0cBEAM\u7684actor\u6a21\u578b\u7279\u6027\u66f4\u9002\u5408\u5904\u7406AI\u4ee3\u7406\u7684\u590d\u6742\u4ea4\u4e92", "conclusion": "AI\u4ee3\u7406\u6846\u67b6\u5f00\u53d1\u8005\u5e94\u8be5\u4eceElixir/Erlang\u7684BEAM\u865a\u62df\u673a\u4e2d\u5b66\u4e60\u5e76\u53d1\u548c\u5bb9\u9519\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u53d1\u660e\u8f6e\u5b50", "topic": "agent analysis"}}
{"id": "tldr.2602.0ecb6771", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fexpo.dev%2Fblog%2Fwhat-our-web-team-learned-using-claude-code-for-a-month%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=34911156-NDR%2520-%2520Starter%2520Plan%2520Growth%2520%2526%2520Retention%26utm_content=claude/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/tWxtCMNG2wLUYlZ7xEglwIXXinpVwDth-PXFwDgrTFE=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fexpo.dev%2Fblog%2Fwhat-our-web-team-learned-using-claude-code-for-a-month%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=34911156-NDR%2520-%2520Starter%2520Plan%2520Growth%2520%2526%2520Retention%26utm_content=claude/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/tWxtCMNG2wLUYlZ7xEglwIXXinpVwDth-PXFwDgrTFE=445", "authors": ["TLDR Newsletter"], "title": "Expo went all-in on Claude Code for a month. Here's what happened", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fexpo.dev%2Fblog%2Fwhat-our-web-team-learned-using-claude-code-for-a-month%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=34911156-NDR%2520-%2520Starter%2520Plan%2520Growth%2520%2526%2520Retention%26utm_content=claude/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/tWxtCMNG2wLUYlZ7xEglwIXXinpVwDth-PXFwDgrTFE=445", "summary": "Expo went all-in on Claude Code for a month. Here's what happened (Sponsor) Expo's web team committed 100% to Claude Code for a month and ended up with wins, losses, and lessons you can read about in this blog. They also added the ability to prompt Builds and Workflows with Claude with Expo MCP Server. Try the new MCP server", "source": "tldr", "AI": {"tldr": "Expo\u56e2\u961f\u5168\u9762\u4f7f\u7528Claude Code\u4e00\u4e2a\u6708\uff0c\u5206\u4eab\u4e86\u4f7f\u7528\u4f53\u9a8c\u3001\u6210\u679c\u548c\u6559\u8bad\uff0c\u5e76\u63a8\u51fa\u4e86\u65b0\u7684MCP\u670d\u52a1\u5668\u529f\u80fd", "motivation": "\u63a2\u7d22Claude Code\u5728\u5b9e\u9645\u5f00\u53d1\u9879\u76ee\u4e2d\u7684\u6548\u679c\uff0c\u6d4b\u8bd5\u5176\u4f5c\u4e3a\u4e3b\u8981\u5f00\u53d1\u5de5\u5177\u7684\u751f\u4ea7\u529b\u5f71\u54cd", "method": "\u56e2\u961f\u5b8c\u5168\u91c7\u7528Claude Code\u8fdb\u884c\u4e3a\u671f\u4e00\u4e2a\u6708\u7684\u5f00\u53d1\u5de5\u4f5c\uff0c\u8bb0\u5f55\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u7684\u4f53\u9a8c\u548c\u7ed3\u679c", "result": "\u83b7\u5f97\u4e86\u6210\u529f\u548c\u5931\u8d25\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684MCP\u670d\u52a1\u5668\u529f\u80fd\uff0c\u652f\u6301\u901a\u8fc7Claude\u63d0\u793a\u6784\u5efa\u548c\u5de5\u4f5c\u6d41", "conclusion": "Claude Code\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u6709\u4ef7\u503c\u4f46\u9700\u8981\u9002\u5e94\uff0c\u56e2\u961f\u5206\u4eab\u4e86\u5b9e\u8df5\u7ecf\u9a8c\u5e76\u63a8\u51fa\u4e86\u76f8\u5173\u5de5\u5177\u6539\u8fdb", "topic": "swe application"}}
{"id": "tldr.2602.6fb4f325", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstatic.stepfun.com%2Fblog%2Fstep-3.5-flash%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/Pf2eAuwCnMAFS-4vbw-feQM7Fg1RQNVEkBiyM0VI-N8=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstatic.stepfun.com%2Fblog%2Fstep-3.5-flash%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/Pf2eAuwCnMAFS-4vbw-feQM7Fg1RQNVEkBiyM0VI-N8=445", "authors": ["TLDR Newsletter"], "title": "Step 3.5 Flash", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: 21 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstatic.stepfun.com%2Fblog%2Fstep-3.5-flash%2F%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/Pf2eAuwCnMAFS-4vbw-feQM7Fg1RQNVEkBiyM0VI-N8=445", "summary": "Step 3.5 Flash (21 minute read) Step 3.5 Flash is a frontier open-source foundation model by StepFun that offers efficient, high-speed reasoning and agentic capabilities with tool use, long context, and local deployment.", "source": "tldr", "AI": {"tldr": "Step 3.5 Flash\u662f\u4e00\u4e2a\u524d\u6cbf\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u4f9b\u9ad8\u6548\u9ad8\u901f\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u652f\u6301\u5de5\u5177\u4f7f\u7528\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u672c\u5730\u90e8\u7f72", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u5177\u5907\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u652f\u6301\u5de5\u5177\u4f7f\u7528\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u6ee1\u8db3\u672c\u5730\u90e8\u7f72\u9700\u6c42", "method": "\u57fa\u4e8eStepFun\u5f00\u53d1\u7684Step 3.5 Flash\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u63a8\u7406\u6548\u7387\u548c\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u96c6\u6210\u5de5\u5177\u4f7f\u7528\u529f\u80fd\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u524d\u6cbf\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u5177\u5907\u9ad8\u6548\u63a8\u7406\u901f\u5ea6\u3001\u667a\u80fd\u4f53\u80fd\u529b\u3001\u5de5\u5177\u4f7f\u7528\u652f\u6301\u3001\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u548c\u672c\u5730\u90e8\u7f72\u80fd\u529b", "conclusion": "Step 3.5 Flash\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u5728\u63a8\u7406\u6548\u7387\u3001\u667a\u80fd\u4f53\u80fd\u529b\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272", "topic": "code agent"}}
{"id": "tldr.2602.be64901e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fresearch%2Fmeasuring-agent-autonomy%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/qSDyVNZDg7RRXaKnpTE3F2tiXAJKMJWIR08ubLXd9zQ=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fresearch%2Fmeasuring-agent-autonomy%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/qSDyVNZDg7RRXaKnpTE3F2tiXAJKMJWIR08ubLXd9zQ=445", "authors": ["TLDR Newsletter"], "title": "Measuring AI agent autonomy in practice", "comment": "Source: TLDR Newsletter, Date: 2026-02-20, Reading time: 31 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fresearch%2Fmeasuring-agent-autonomy%3Futm_source=tldrdev/1/0100019c7af524aa-0bef8790-6ff4-4b41-afc4-34296c120b38-000000/qSDyVNZDg7RRXaKnpTE3F2tiXAJKMJWIR08ubLXd9zQ=445", "summary": "Measuring AI agent autonomy in practice (31 minute read) Anthropic's research analyzed millions of human-AI agent interactions across Claude Code and its public API to understand how agents are used in real-world contexts. It found that agents like Claude Code operate autonomously for longer periods, and experienced users grant them more independence (auto-approving more). Claude Code also proactively pauses for clarification more often than humans interrupt it on complex tasks.", "source": "tldr", "AI": {"tldr": "Anthropic\u7814\u7a76\u5206\u6790\u4e86\u6570\u767e\u4e07\u6b21\u4eba\u7c7b-AI\u4ee3\u7406\u4ea4\u4e92\uff0c\u53d1\u73b0Claude Code\u7b49\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u51fa\u66f4\u957f\u7684\u81ea\u4e3b\u64cd\u4f5c\u65f6\u95f4\uff0c\u7ecf\u9a8c\u7528\u6237\u7ed9\u4e88\u66f4\u591a\u81ea\u4e3b\u6743\uff0c\u4e14\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u66f4\u4e3b\u52a8\u6682\u505c\u5bfb\u6c42\u6f84\u6e05\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3AI\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\uff0c\u7279\u522b\u662f\u4ee3\u7406\u81ea\u4e3b\u6027\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4ee5\u53ca\u4eba\u7c7b\u7528\u6237\u5982\u4f55\u4e0eAI\u4ee3\u7406\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u5206\u6790Claude Code\u53ca\u5176\u516c\u5171API\u4e0a\u7684\u6570\u767e\u4e07\u6b21\u4eba\u7c7b-AI\u4ee3\u7406\u4ea4\u4e92\u6570\u636e\uff0c\u7814\u7a76\u4ee3\u7406\u5728\u771f\u5b9e\u4e0a\u4e0b\u6587\u4e2d\u7684\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0Claude Code\u7b49\u4ee3\u7406\u80fd\u591f\u957f\u65f6\u95f4\u81ea\u4e3b\u64cd\u4f5c\uff1b\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7528\u6237\u66f4\u503e\u5411\u4e8e\u81ea\u52a8\u6279\u51c6\u4ee3\u7406\u884c\u52a8\uff1b\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u6bd4\u4eba\u7c7b\u66f4\u4e3b\u52a8\u6682\u505c\u5bfb\u6c42\u6f84\u6e05\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u81ea\u4e3b\u6027\uff0c\u7528\u6237\u4fe1\u4efb\u5ea6\u968f\u7ecf\u9a8c\u589e\u957f\uff0c\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8c28\u614e\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "topic": "agent analysis"}}
