{"id": "2511.21788", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21788", "abs": "https://arxiv.org/abs/2511.21788", "authors": ["Md. Raihan Tapader", "Md. Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe"], "title": "Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings", "comment": null, "summary": "In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00\u4ee3\u7801\u91cd\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u6307\u4ee4\u5fae\u8c03\u63d0\u5347\u91cd\u6784\u6548\u679c\uff0c\u5728Java\u4e0a\u8fbe\u523099.99%\u6b63\u786e\u7387\u548c94.78%\u53ef\u7f16\u8bd1\u6027", "motivation": "\u73b0\u6709\u4ee3\u7801\u91cd\u6784\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u7f16\u7801\u98ce\u683c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u8de8\u8bed\u8a00\u5de5\u4f5c\u7684\u81ea\u52a8\u5316\u91cd\u6784\u65b9\u6848", "method": "\u4f7f\u7528\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u5fae\u8c03\u6a21\u578b\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u652f\u6301C\u3001C++\u3001C#\u3001Python\u3001Java\u591a\u8bed\u8a00\u91cd\u6784\uff0c\u7814\u7a76\u6e29\u5ea6\u53c2\u6570\u548c\u4e0d\u540c\u6837\u672c\u7b97\u6cd5\u7684\u5f71\u54cd", "result": "Java\u572810-shot\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u9ad8\u6b63\u786e\u738799.99%\uff0c\u5e73\u5747\u53ef\u7f16\u8bd1\u602794.78%\uff0c\u76f8\u4f3c\u5ea653-54%\uff1bPython\u5728\u6240\u6709shot\u4e2d\u7ed3\u6784\u8ddd\u79bb\u6700\u4f4e(277-294)\uff0c\u76f8\u4f3c\u5ea644-48%", "conclusion": "LLM-based\u6846\u67b6\u80fd\u6709\u6548\u8fdb\u884c\u591a\u8bed\u8a00\u4ee3\u7801\u91cd\u6784\uff0cJava\u8868\u73b0\u6700\u4f73\uff0cPython\u91cd\u6784\u6539\u52a8\u6700\u5c0f\uff0c\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u5bf9\u91cd\u6784\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd", "topic": "code agent"}}
{"id": "2511.21877", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21877", "abs": "https://arxiv.org/abs/2511.21877", "authors": ["Nenad Petrovic", "Norbert Kroth", "Axel Torschmied", "Yinglei Song", "Fengjunjie Pan", "Vahid Zolfaghari", "Nils Purschke", "Sven Kirchner", "Chengdong Wu", "Andre Schamschurko", "Yi Zhang", "Alois Knoll"], "title": "LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems", "comment": null, "summary": "This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u94fe\u9a71\u52a8\u3001LLM\u8d4b\u80fd\u7684\u5de5\u4f5c\u6d41\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6c7d\u8f66\u4ee3\u7801\uff0c\u901a\u8fc7RAG\u68c0\u7d22VSS\u4fe1\u53f7\u786e\u4fdd\u67b6\u6784\u6b63\u786e\u6027\uff0c\u65e0\u9700LLM\u91cd\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u4fe1\u53f7\u4f7f\u7528\u548c\u4e00\u81f4\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u6c7d\u8f66\u4ee3\u7801\u5f00\u53d1\u9762\u4e34\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5230\u4ee3\u7801\u8f6c\u6362\u7684\u6311\u6218\uff0c\u9700\u8981\u786e\u4fdd\u4fe1\u53f7\u4f7f\u7528\u7684\u6b63\u786e\u6027\u3001\u51cf\u5c11LLM\u5e7b\u89c9\uff0c\u5e76\u4fdd\u6301\u67b6\u6784\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u578b\u4e14\u4e0d\u65ad\u6f14\u5316\u7684\u8f66\u8f86\u4fe1\u53f7\u89c4\u8303(VSS)\u76ee\u5f55\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u94fe\u9a71\u52a8\u7684LLM\u5de5\u4f5c\u6d41\uff1a1) RAG\u5c42\u4eceVSS\u76ee\u5f55\u68c0\u7d22\u76f8\u5173\u4fe1\u53f7\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff1b2) \u4fe1\u53f7\u6620\u5c04\u548c\u9a8c\u8bc1\uff1b3) \u8f6c\u6362\u4e3a\u7f16\u7801\u56e0\u679c\u548c\u65f6\u5e8f\u7ea6\u675f\u7684\u4e8b\u4ef6\u94fe\uff1b4) \u4e8b\u4ef6\u94fe\u6307\u5bfcLLM\u4ee3\u7801\u5408\u6210\uff0c\u786e\u4fdd\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "result": "\u5728\u7d27\u6025\u5236\u52a8\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4fe1\u53f7\u4f7f\u7528\u548c\u4e00\u81f4\u7684\u4ee3\u7801\u751f\u6210\uff0c\u65e0\u9700LLM\u91cd\u8bad\u7ec3\u3002\u80fd\u591f\u51cf\u5c11\u5e7b\u89c9\u5e76\u786e\u4fdd\u67b6\u6784\u6b63\u786e\u6027\u3002", "conclusion": "\u4e8b\u4ef6\u94fe\u9a71\u52a8\u3001LLM\u8d4b\u80fd\u7684\u5de5\u4f5c\u6d41\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6c7d\u8f66\u4ee3\u7801\uff0c\u901a\u8fc7RAG\u68c0\u7d22\u548c\u4e8b\u4ef6\u94fe\u7ea6\u675f\u786e\u4fdd\u4fe1\u53f7\u6b63\u786e\u6027\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u4e3a\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2511.21878", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21878", "abs": "https://arxiv.org/abs/2511.21878", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "comment": null, "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.", "AI": {"tldr": "TRAM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7c7b\u578b\u89e3\u6790\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u9694\u79bb\u9a8c\u8bc1\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5728Java\u5230Python\u7ffb\u8bd1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u9a8c\u8bc1\u7ffb\u8bd1\u7ed3\u679c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u4ee3\u7406\u8fdb\u884c\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\uff0c\u8981\u4e48\u4f9d\u8d56\u8bed\u8a00\u4e92\u64cd\u4f5c\u6027\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\u3002", "method": "TRAM\u7ed3\u5408\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7c7b\u578b\u89e3\u6790\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u9694\u79bb\u9a8c\u8bc1\u3002\u9996\u5148\u68c0\u7d22API\u6587\u6863\u548c\u4e0a\u4e0b\u6587\u4ee3\u7801\u4fe1\u606f\uff0c\u4f7f\u7528LLM\u89e3\u6790\u8de8\u8bed\u8a00\u7c7b\u578b\u6620\u5c04\uff1b\u7136\u540e\u901a\u8fc7\u81ea\u5b9a\u4e49\u5e8f\u5217\u5316/\u53cd\u5e8f\u5217\u5316\u5de5\u4f5c\u6d41\u81ea\u52a8\u6784\u5efa\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u7b49\u6548\u6a21\u62df\u5bf9\u8c61\uff0c\u5b9e\u73b0\u65b9\u6cd5\u7247\u6bb5\u7684\u9694\u79bb\u9a8c\u8bc1\u3002", "result": "TRAM\u5728Java\u5230Python\u7ffb\u8bd1\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176RAG-based\u7c7b\u578b\u89e3\u6790\u4e0e\u53ef\u9760\u9694\u79bb\u9a8c\u8bc1\u96c6\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "TRAM\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7c7b\u578b\u89e3\u6790\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u9694\u79bb\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ffb\u8bd1\uff0c\u89e3\u51b3\u4e86\u7ffb\u8bd1\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2511.21920", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21920", "abs": "https://arxiv.org/abs/2511.21920", "authors": ["Apu Kumar Chakroborti", "Yi Ding", "Lipeng Wan"], "title": "Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code", "comment": null, "summary": "As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.", "AI": {"tldr": "\u8bc4\u4f30\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u751f\u6210Python\u4ee3\u7801\u7684\u53ef\u4fe1\u5ea6\uff0c\u53d1\u73b0\u65e0\u4eba\u5de5\u5e72\u9884\u65f6\u53ef\u9760\u6027\u6709\u9650\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u7b56\u7565\u5e76\u6784\u5efa\u4e86\u53ef\u590d\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u6570\u636e\u65e5\u76ca\u590d\u6742\uff0c\u9886\u57df\u79d1\u5b66\u5bb6\u7f3a\u4e4f\u7f16\u7a0b\u6280\u80fd\u6210\u4e3a\u5206\u6790\u5927\u89c4\u6a21\u6570\u636e\u7684\u969c\u788d\u3002\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4ee3\u7801\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e2d\u7684\u53ef\u4fe1\u5ea6\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u53cd\u6620\u771f\u5b9e\u7814\u7a76\u4efb\u52a1\u7684\u9886\u57df\u542f\u53d1\u5f0f\u63d0\u793a\u57fa\u51c6\u5957\u4ef6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6267\u884c\u6027\u548c\u6b63\u786e\u6027\u3002\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e09\u79cd\u7b56\u7565\uff1a\u6570\u636e\u611f\u77e5\u63d0\u793a\u6d88\u6b67\u3001\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u6539\u8fdb\u548c\u8fed\u4ee3\u9519\u8bef\u4fee\u590d\u3002", "result": "\u65e0\u4eba\u5de5\u5e72\u9884\u65f6\uff0cLLM\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\u6709\u9650\uff0c\u5931\u8d25\u5e38\u7531\u6a21\u7cca\u63d0\u793a\u548c\u6a21\u578b\u5bf9\u9886\u57df\u7279\u5b9a\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u5bfc\u81f4\u3002\u4e09\u79cd\u6539\u8fdb\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u6210\u529f\u7387\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5f53\u524d\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u9886\u57df\u77e5\u8bc6\u6574\u5408\u3002\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u5305\u5bb9\u3001\u53ef\u8bbf\u95ee\u548c\u53ef\u4fe1\u7684AI\u8f85\u52a9\u7814\u7a76\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u884c\u6280\u672f\u548c\u53ef\u590d\u7528\u57fa\u51c6\u3002", "topic": "code agent"}}
{"id": "2511.21964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21964", "abs": "https://arxiv.org/abs/2511.21964", "authors": ["Ali Sayedsalehi", "Peter C. Rigby", "Audris Mockus"], "title": "DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction", "comment": "8 pages, 4 figures, includes system architecture diagrams, Web UI screenshots, GitHub App examples, and an appendix with API endpoints. Full replication package and demo materials available", "summary": "In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.", "AI": {"tldr": "DRS-OSS\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u4f7f\u7528\u5fae\u8c03\u7684Llama 3.1 8B\u6a21\u578b\u5206\u6790\u4ee3\u7801\u53d8\u66f4\u98ce\u9669\uff0c\u5e2e\u52a9\u4f18\u5148\u5ba1\u67e5\u9ad8\u98ce\u9669PR\uff0c\u9632\u6b62\u7f3a\u9677\u5f15\u5165\u3002", "motivation": "\u5927\u89c4\u6a21\u5f00\u6e90\u9879\u76ee\u4e2d\u6bcf\u5929\u6709\u5927\u91cfPR\u5408\u5e76\uff0c\u6bcf\u4e2a\u90fd\u53ef\u80fd\u5f15\u5165\u56de\u5f52\u7f3a\u9677\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u4ee3\u7801\u53d8\u66f4\u7684\u98ce\u9669\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u8fdb\u884c\u5ba1\u67e5\u4f18\u5148\u7ea7\u6392\u5e8f\u3001\u6d4b\u8bd5\u89c4\u5212\u548cCI/CD\u95e8\u63a7\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684Llama 3.1 8B\u5e8f\u5217\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u63d0\u4ea4\u4fe1\u606f\u3001\u7ed3\u6784\u5316\u5dee\u5f02\u548c\u53d8\u66f4\u6307\u6807\u7684\u957f\u4e0a\u4e0b\u6587\u8868\u793a\u3002\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u9002\u914d\u30014\u4f4dQLoRA\u548cDeepSpeed ZeRO-3 CPU\u5378\u8f7d\u6280\u672f\uff0c\u5728\u5355\u5f2020GB GPU\u4e0a\u8bad\u7ec322k\u4ee4\u724c\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728ApacheJIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff08F1=0.64\uff0cROC-AUC=0.89\uff09\u3002\u6a21\u62df\u663e\u793a\uff0c\u4ec5\u95e8\u63a7\u98ce\u9669\u6700\u9ad8\u768430%\u63d0\u4ea4\u5c31\u80fd\u9632\u6b62\u9ad8\u8fbe86.4%\u7684\u7f3a\u9677\u5f15\u5165\u53d8\u66f4\u3002", "conclusion": "DRS-OSS\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5f00\u6e90\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u901a\u8fc7API\u3001Web UI\u548cGitHub\u63d2\u4ef6\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u4e2d\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u7f3a\u9677\u5f15\u5165\u3002", "topic": "swe application"}}
{"id": "2511.22074", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.22074", "abs": "https://arxiv.org/abs/2511.22074", "authors": ["Dasheng Bi", "Yubin Hu", "Mohammed N. Nasir"], "title": "Real-Time Procedural Learning From Experience for AI Agents", "comment": null, "summary": "Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.", "AI": {"tldr": "PRAXIS\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u8bad\u7ec3\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u5b58\u50a8\u52a8\u4f5c\u7ed3\u679c\u5e76\u6839\u636e\u73af\u5883\u72b6\u6001\u68c0\u7d22\u8fc7\u5f80\u7ecf\u9a8c\uff0c\u589e\u5f3aLLM\u667a\u80fd\u4f53\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7f3a\u4e4f\u90e8\u7f72\u540e\u83b7\u53d6\u7a0b\u5e8f\u6027\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u50cf\u751f\u7269\u667a\u80fd\u90a3\u6837\u901a\u8fc7\u8bd5\u9519\u5b9e\u65f6\u5b66\u4e60\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u8ba9AI\u667a\u80fd\u4f53\u5728\u5feb\u901f\u6f14\u5316\u7684\u72b6\u6001\u5316\u73af\u5883\u4e2d\u6709\u6548\u5b66\u4e60\u65b0\u7a0b\u5e8f", "method": "\u63d0\u51faPRAXIS\u673a\u5236\uff1a\u5b58\u50a8\u52a8\u4f5c\u7ed3\u679c\uff0c\u901a\u8fc7\u8054\u5408\u5339\u914d\u73af\u5883\u548c\u5185\u90e8\u72b6\u6001\u6765\u68c0\u7d22\u8fc7\u5f80\u7ecf\u9a8c\uff0c\u5c06\u68c0\u7d22\u5230\u7684\u72b6\u6001-\u52a8\u4f5c-\u7ed3\u679c\u793a\u4f8b\u5b9e\u65f6\u751f\u6210\u5e76\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u52a8\u4f5c\u9009\u62e9", "result": "\u5728REAL\u7f51\u9875\u6d4f\u89c8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRAXIS\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\u3001\u53ef\u9760\u6027\u548c\u6210\u672c\u6548\u7387\uff0c\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u4e0a\u90fd\u6709\u6548\uff0c\u5e76\u5728\u76f8\u4f3c\u73af\u5883\u4e2d\u5bf9\u672a\u89c1\u4efb\u52a1\u663e\u793a\u51fa\u521d\u6b65\u6cdb\u5316\u80fd\u529b", "conclusion": "PRAXIS\u901a\u8fc7\u5e2e\u52a9\u667a\u80fd\u4f53\u6709\u6548\u5b66\u4e60\u65b0\u7a0b\u5e8f\uff0c\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u5728\u5feb\u901f\u6f14\u5316\u7684\u72b6\u6001\u5316\u73af\u5883\u4e2d\u5f97\u5230\u5b9e\u9645\u5e94\u7528", "topic": "agent analysis"}}
{"id": "2511.22118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22118", "abs": "https://arxiv.org/abs/2511.22118", "authors": ["Yihan Dai", "Dimitrios Stamatios Bouras", "Haoxiang Jia", "Sergey Mechtaev"], "title": "Statistical Independence Aware Caching for LLM Workflows", "comment": null, "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.", "AI": {"tldr": "Mnimi\u662f\u4e00\u4e2aLLM\u7f13\u5b58\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c01\u88c5\u7edf\u8ba1\u7ea6\u675f\u5728LLM\u5f15\u7528\u7c7b\u578b\u4e2d\uff0c\u786e\u4fdd\u7ec4\u4ef6\u7ea7\u7edf\u8ba1\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4ee3\u7801\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u53ef\u590d\u73b0\u6027\u548c\u6548\u7387\u3002", "motivation": "LLM\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\uff0c\u672c\u5730\u7f13\u5b58\u53ef\u4ee5\u964d\u4f4e\u6210\u672c\u5ef6\u8fdf\uff0c\u4f46\u73b0\u6709\u7f13\u5b58\u7cfb\u7edf\u65e0\u6cd5\u4fdd\u8bc1\u7edf\u8ba1\u72ec\u7acb\u6027\uff0c\u800c\u7edf\u8ba1\u72ec\u7acb\u6027\u5bf9\u4e8e\u4ee3\u7801\u4ee3\u7406\u4e2d\u7684\u6027\u80fd\u6307\u6807\uff08\u5982Pass@k\uff09\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4ee5\u53ca\u7a0b\u5e8f\u4fee\u590d\u5faa\u73af\u7b49\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faMnimi\u7f13\u5b58\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5c06\u7edf\u8ba1\u7ea6\u675f\u5c01\u88c5\u5728LLM\u5f15\u7528\u7c7b\u578b\u4e2d\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u7b97\u6cd5\u8303\u56f4\u548c\u9700\u6c42\u7ba1\u7406\u548c\u8f6c\u6362\u8fd9\u4e9b\u7c7b\u578b\u3002\u5728Python\u4e2d\u4f7f\u7528\u88c5\u9970\u5668\u548c\u65e0\u9650\u5e8f\u5217\u8fed\u4ee3\u5668\u5b9e\u73b0\u3002", "result": "\u5728SpecFix\uff08\u81ea\u52a8\u7a0b\u5e8f\u89c4\u8303\u4fee\u590d\u7cfb\u7edf\uff09\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cMnimi\u63d0\u9ad8\u4e86\u53ef\u590d\u73b0\u6027\u3001\u8c03\u8bd5\u4fbf\u5229\u6027\u3001\u65f6\u95f4\u548c\u6210\u672c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7edf\u8ba1\u6b63\u786e\u6027\u3002", "conclusion": "Mnimi\u4e3a\u6a21\u5757\u5316LLM\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u652f\u6301\u7edf\u8ba1\u5b8c\u6574\u6027\u7684\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f13\u5b58\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u8ba1\u72ec\u7acb\u6027\u7ea6\u675f\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2511.22076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22076", "abs": "https://arxiv.org/abs/2511.22076", "authors": ["Yue Zhong", "Yongju Tong", "Jiawen Kang", "Minghui Dai", "Hong-Ning Dai", "Zhou Su", "Dusit Niyato"], "title": "Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents", "comment": null, "summary": "The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u5c42\u7684\u667a\u80fd\u4f53\u7269\u8054\u7f51(IoA)\u4efb\u52a1\u5378\u8f7d\u4f18\u5316\u65b9\u6848\uff0c\u7b2c\u4e00\u5c42\u4f7f\u7528Stackelberg\u535a\u5f08\u6a21\u578b\u5904\u7406\u79fb\u52a8\u667a\u80fd\u4f53\u4e0e\u56fa\u5b9a\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8d44\u6e90\u5b9a\u4ef7\u548c\u4efb\u52a1\u5378\u8f7d\uff0c\u7b2c\u4e8c\u5c42\u91c7\u7528\u53cc\u8377\u5170\u62cd\u5356\u6a21\u578b\u5904\u7406\u56fa\u5b9a\u667a\u80fd\u4f53\u5411\u7a7a\u4e2d\u667a\u80fd\u4f53\u7684\u8fdb\u4e00\u6b65\u4efb\u52a1\u5378\u8f7d\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6c42\u89e3\u3002", "motivation": "\u667a\u80fd\u4f53\u7269\u8054\u7f51(IoA)\u4f5c\u4e3a\u4e92\u8054\u667a\u80fd\u7cfb\u7edf\u7684\u57fa\u7840\u67b6\u6784\uff0c\u9700\u8981\u5904\u7406\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u667a\u80fd\u4f53(WAs)\u7684\u8ba1\u7b97\u5bc6\u96c6\u578bAI\u670d\u52a1\u5378\u8f7d\u95ee\u9898\u3002\u56fa\u5b9a\u667a\u80fd\u4f53(FAs)\u7531\u4e8e\u5730\u7406\u4f4d\u7f6e\u56fa\u5b9a\u548c\u8fde\u63a5\u7a33\u5b9a\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9760\u7684\u901a\u4fe1\u7f51\u5173\u548c\u4efb\u52a1\u805a\u5408\u70b9\uff0c\u4f46\u53ef\u80fd\u51fa\u73b0\u8fc7\u8f7d\u60c5\u51b5\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5411\u7a7a\u4e2d\u667a\u80fd\u4f53(AAs)\u5378\u8f7d\u4efb\u52a1\u3002", "method": "1. \u7b2c\u4e00\u5c42\uff1a\u591a\u9886\u5bfc\u8005\u591a\u8ffd\u968f\u8005Stackelberg\u535a\u5f08\u6a21\u578b\uff0c\u79fb\u52a8\u667a\u80fd\u4f53(MAs)\u548c\u56fa\u5b9a\u667a\u80fd\u4f53(FAs)\u4f5c\u4e3a\u9886\u5bfc\u8005\u8bbe\u5b9a\u8d44\u6e90\u4ef7\u683c\uff0c\u65e0\u7ebf\u667a\u80fd\u4f53(WAs)\u4f5c\u4e3a\u8ffd\u968f\u8005\u786e\u5b9a\u4efb\u52a1\u5378\u8f7d\u6bd4\u4f8b\n2. \u7b2c\u4e8c\u5c42\uff1a\u53cc\u8377\u5170\u62cd\u5356\u6a21\u578b\uff0c\u8fc7\u8f7d\u7684\u56fa\u5b9a\u667a\u80fd\u4f53(FAs)\u4f5c\u4e3a\u4e70\u5bb6\u8bf7\u6c42\u8d44\u6e90\uff0c\u7a7a\u4e2d\u667a\u80fd\u4f53(AAs)\u4f5c\u4e3a\u5356\u5bb6\u63d0\u4f9b\u8d44\u6e90\n3. \u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6c42\u89e3\u6574\u4e2a\u4f18\u5316\u6a21\u578b", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u4fc3\u8fdb\u4efb\u52a1\u5378\u8f7d\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u4e24\u5c42\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u7269\u8054\u7f51\u4e2d\u8d44\u6e90\u53d7\u9650\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u5378\u8f7d\u95ee\u9898\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u548c\u62cd\u5356\u673a\u5236\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u5206\u914d\u548c\u8d44\u6e90\u5229\u7528\u3002", "topic": "agent analysis"}}
{"id": "2511.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22409", "abs": "https://arxiv.org/abs/2511.22409", "authors": ["Polydoros Giannouris", "Sophia Ananiadou"], "title": "NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements", "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.", "AI": {"tldr": "NOMAD\u662f\u4e00\u4e2a\u8ba4\u77e5\u542f\u53d1\u7684\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5c06UML\u56fe\u751f\u6210\u5206\u89e3\u4e3a\u89d2\u8272\u4e13\u4e1a\u5316\u7684\u5b50\u4efb\u52a1\uff0c\u5728UML\u7c7b\u56fe\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u5206\u7c7b\u4e86LLM\u751f\u6210UML\u56fe\u7684\u9519\u8bef\u7c7b\u578b\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u751f\u6210UML\u56fe\u7b49\u7ed3\u6784\u5316\u5de5\u4ef6\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9UML\u751f\u6210\u8fc7\u7a0b\u7684\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faNOMAD\u6846\u67b6\uff0c\u5c06UML\u751f\u6210\u5206\u89e3\u4e3a\u5b9e\u4f53\u63d0\u53d6\u3001\u5173\u7cfb\u5206\u7c7b\u3001\u56fe\u5408\u6210\u7b49\u89d2\u8272\u4e13\u4e1a\u5316\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u6df7\u5408\u8bc4\u4f30\u8bbe\u8ba1\uff1a\u5927\u578b\u6848\u4f8b\u7814\u7a76\uff08Northwind\uff09\u8fdb\u884c\u6df1\u5ea6\u63a2\u6d4b\u548c\u9519\u8bef\u5206\u6790\uff0c\u4ee5\u53ca\u4eba\u5de5\u7f16\u5199\u7684UML\u7ec3\u4e60\u8fdb\u884c\u5e7f\u5ea6\u548c\u771f\u5b9e\u6027\u8bc4\u4f30\u3002", "result": "NOMAD\u5728\u6240\u6709\u9009\u5b9a\u57fa\u7ebf\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63d0\u53d6\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\u3002\u9996\u6b21\u63d0\u51fa\u4e86LLM\u751f\u6210UML\u56fe\u7684\u7cfb\u7edf\u6027\u9519\u8bef\u5206\u7c7b\u6cd5\uff08\u7ed3\u6784\u3001\u5173\u7cfb\u3001\u8bed\u4e49/\u903b\u8f91\u9519\u8bef\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u9a8c\u8bc1\u4f5c\u4e3a\u8bbe\u8ba1\u63a2\u9488\u7684\u6df7\u5408\u6548\u679c\u3002", "conclusion": "NOMAD\u65e2\u662fUML\u7c7b\u56fe\u751f\u6210\u7684\u6709\u6548\u6846\u67b6\uff0c\u4e5f\u4e3a\u53ef\u9760\u8bed\u8a00\u5230\u6a21\u578b\u5de5\u4f5c\u6d41\u7a0b\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u6311\u6218\u63d0\u4f9b\u4e86\u89c6\u89d2\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u662f\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2511.22226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22226", "abs": "https://arxiv.org/abs/2511.22226", "authors": ["Alexander Meulemans", "Rajai Nasser", "Maciej Wo\u0142czyk", "Marissa A. Weis", "Seijin Kobayashi", "Blake Richards", "Guillaume Lajoie", "Angelika Steger", "Marcus Hutter", "James Manyika", "Rif A. Saurous", "Jo\u00e3o Sacramento", "Blaise Ag\u00fcera y Arcas"], "title": "Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning", "comment": "203 pages, 3 figures", "summary": "The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u81ea\u6211\u9884\u6d4b\u7684\u5d4c\u5165\u5f0f\u667a\u80fd\u4f53\u6570\u5b66\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u5206\u79bb\u5047\u8bbe\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u524d\u77bb\u6027\u5b66\u4e60\u548c\u5d4c\u5165\u5f0f\u667a\u80fd\u3002", "motivation": "\u4f20\u7edf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u73af\u5883\u662f\u9759\u6001\u7684\uff0c\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u5206\u79bb\u3002\u4f46\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u5bfc\u81f4\u975e\u5e73\u7a33\u6027\uff0c\u9700\u8981\u57fa\u4e8e\u9884\u6d4b\u6a21\u578b\u7684\u524d\u77bb\u6027\u5b66\u4e60\u3002\u667a\u80fd\u4f53\u9700\u8981\u5efa\u6a21\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u800c\u5176\u4ed6\u667a\u80fd\u4f53\u4e5f\u5728\u5efa\u6a21\u5b83\uff0c\u8fd9\u4fc3\u4f7f\u667a\u80fd\u4f53\u5c06\u81ea\u5df1\u4f5c\u4e3a\u73af\u5883\u7684\u4e00\u90e8\u5206\u6765\u5efa\u6a21\u3002", "method": "\u57fa\u4e8e\u901a\u7528\u4eba\u5de5\u667a\u80fd(AIXI)\u57fa\u7840\u5de5\u4f5c\uff0c\u5f15\u5165\u4ee5\u81ea\u6211\u9884\u6d4b\u4e3a\u4e2d\u5fc3\u7684\u6570\u5b66\u6846\u67b6\u3002\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u540c\u65f6\u9884\u6d4b\u672a\u6765\u611f\u77e5\u8f93\u5165\u548c\u81ea\u5df1\u7684\u884c\u52a8\uff0c\u5fc5\u987b\u89e3\u51b3\u5173\u4e8e\u81ea\u8eab\u4f5c\u4e3a\u5b87\u5b99\u4e00\u90e8\u5206\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u6269\u5c55AIXI\u7406\u8bba\uff0c\u7814\u7a76\u4eceSolomonoff\u5148\u9a8c\u5f00\u59cb\u7684\u901a\u7528\u667a\u80fd\u5d4c\u5165\u5f0f\u667a\u80fd\u4f53\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u81ea\u6211\u9884\u6d4b\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u63a8\u7406\u8fd0\u884c\u76f8\u4f3c\u7b97\u6cd5\u7684\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u4ea7\u751f\u65b0\u7684\u535a\u5f08\u8bba\u89e3\u6982\u5ff5\u548c\u4f20\u7edf\u5206\u79bb\u667a\u80fd\u4f53\u65e0\u6cd5\u5b9e\u73b0\u7684\u65b0\u578b\u5408\u4f5c\u5f62\u5f0f\u3002\u7406\u60f3\u5316\u7684\u667a\u80fd\u4f53\u53ef\u4ee5\u5f62\u6210\u4e00\u81f4\u7684\u76f8\u4e92\u9884\u6d4b\uff0c\u5b9e\u73b0\u65e0\u9650\u9636\u5fc3\u667a\u7406\u8bba\u3002", "conclusion": "\u81ea\u6211\u9884\u6d4b\u6846\u67b6\u4e3a\u5d4c\u5165\u5f0f\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u8bbe\u5b9a\u4e86\u9ec4\u91d1\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u7406\u8bba\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u4e00\u4f53\u5316\u5efa\u6a21\u3002", "topic": "agent analysis"}}
{"id": "2511.22235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22235", "abs": "https://arxiv.org/abs/2511.22235", "authors": ["Zehao Deng", "Tianjie Ju", "Zheng Wu", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation", "comment": null, "summary": "The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.", "AI": {"tldr": "\u63d0\u51faCES\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u5668\u3001\u6267\u884c\u5668\u548c\u72b6\u6001\u8ddf\u8e2a\u5668\u7684\u5206\u5de5\u5408\u4f5c\uff0c\u7ed3\u5408\u6267\u884c\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347GUI\u667a\u80fd\u4f53\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dGUI\u667a\u80fd\u4f53\u5728\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u5355\u667a\u80fd\u4f53\u6a21\u578b\u96be\u4ee5\u5e73\u8861\u9ad8\u5c42\u89c4\u5212\u80fd\u529b\u548c\u5e95\u5c42\u6267\u884c\u80fd\u529b\uff0c\u5b58\u5728\u8d23\u4efb\u8026\u5408\u548c\u80fd\u529b\u51b2\u7a81\u95ee\u9898\uff1b2) \u667a\u80fd\u4f53\u7f3a\u4e4f\u4efb\u52a1\u72b6\u6001\u611f\u77e5\uff0c\u5bfc\u81f4\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8fdb\u5ea6\u4e22\u5931\u3002", "method": "\u63d0\u51fa\u5206\u9636\u6bb5\u6267\u884c\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bad\u7ec3\u9ad8\u5c42\u8c03\u5ea6\u6a21\u578b\u3002\u6784\u5efaCoordinator-Executor-State Tracker (CES)\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\uff1a\u534f\u8c03\u5668\u8d1f\u8d23\u6218\u7565\u89c4\u5212\u548c\u4efb\u52a1\u5206\u89e3\uff1b\u72b6\u6001\u8ddf\u8e2a\u5668\u8d1f\u8d23\u4e0a\u4e0b\u6587\u538b\u7f29\u548c\u4fe1\u606f\u7ba1\u7406\uff1b\u6267\u884c\u5668\u8d1f\u8d23\u5177\u4f53\u64cd\u4f5c\u3002\u8be5\u6846\u67b6\u53ef\u4e0e\u4efb\u4f55\u5e95\u5c42\u6267\u884c\u5668\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCES\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u89c4\u5212\u548c\u72b6\u6001\u7ba1\u7406\u80fd\u529b\u3002\u5206\u6790\u8bc1\u5b9e\u8bad\u7ec3\u7684\u9ad8\u5c42\u8c03\u5ea6\u6a21\u5757\u662f\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u663e\u8457\u589e\u5f3a\u5404\u79cd\u6267\u884c\u5668\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "conclusion": "CES\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u6267\u884c\uff0c\u7ed3\u5408\u72b6\u6001\u8ddf\u8e2a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86GUI\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u8d23\u4efb\u8026\u5408\u548c\u72b6\u6001\u7ba1\u7406\u95ee\u9898\uff0c\u4e3aGUI\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.22254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22254", "abs": "https://arxiv.org/abs/2511.22254", "authors": ["Yeonsung Jung", "Trilok Padhi", "Sina Shaham", "Dipika Khullar", "Joonhyun Jeong", "Ninareh Mehrabi", "Eunho Yang"], "title": "Co-Evolving Agents: Learning from Failures as Hard Negatives", "comment": null, "summary": "The rapid progress of large foundation models has accelerated the development of task-specialized agents across diverse domains. However, the effectiveness of agents remains tightly coupled with the quality of training data, while curating task-specific datasets remains costly and often infeasible in real-world scenarios. Recent work has explored self-improving agents that autonomously generate, refine, and re-train on their own trajectories. A prominent line of approaches further leverages preference optimization by pairing predicted trajectories with scarce ground-truth trajectories, enabling agents to learn directly from their own failures. While these methods outperform supervised fine-tuning, their heavy reliance on predicted trajectories under limited ground-truth supervision leaves them prone to overfitting. To address this, we propose a co-evolving agents framework in which a target agent improves jointly with an auxiliary failure agent. The failure agent learns through preference optimization over failure trajectories from both the target and itself, thereby generating hard negatives that are close to success yet remain failures. Incorporating these informative hard negatives into the target agent's optimization sharpens decision boundaries and enhances generalization. Our comprehensive analysis and experiments across benchmark datasets show that our method not only shows improved performance but also demonstrates that failures, instead of being used as-is, can be systematically transformed into structured and valuable learning signals in self-improving agents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534f\u540c\u8fdb\u5316\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8f85\u52a9\u5931\u8d25\u667a\u80fd\u4f53\u751f\u6210\u63a5\u8fd1\u6210\u529f\u4f46\u4ecd\u5931\u8d25\u7684\u786c\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u76ee\u6807\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u5f53\u524d\u81ea\u6539\u8fdb\u667a\u80fd\u4f53\u4f9d\u8d56\u9884\u6d4b\u8f68\u8ff9\u4e0e\u7a00\u7f3a\u771f\u5b9e\u8f68\u8ff9\u7684\u504f\u597d\u4f18\u5316\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u6709\u9650\u76d1\u7763\u4e0b\u7684\u9884\u6d4b\u8f68\u8ff9\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5931\u8d25\u8f68\u8ff9\u5229\u7528\u65b9\u6cd5", "method": "\u63d0\u51fa\u534f\u540c\u8fdb\u5316\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u76ee\u6807\u667a\u80fd\u4f53\u4e0e\u8f85\u52a9\u5931\u8d25\u667a\u80fd\u4f53\u5171\u540c\u8fdb\u5316\u3002\u5931\u8d25\u667a\u80fd\u4f53\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5b66\u4e60\u76ee\u6807\u667a\u80fd\u4f53\u4e0e\u81ea\u8eab\u7684\u5931\u8d25\u8f68\u8ff9\uff0c\u751f\u6210\u63a5\u8fd1\u6210\u529f\u4f46\u4ecd\u5931\u8d25\u7684\u786c\u8d1f\u6837\u672c\uff0c\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4e30\u5bcc\u7684\u786c\u8d1f\u6837\u672c\u7eb3\u5165\u76ee\u6807\u667a\u80fd\u4f53\u4f18\u5316\u8fc7\u7a0b", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u8bc1\u660e\u5931\u8d25\u53ef\u4ee5\u88ab\u7cfb\u7edf\u6027\u5730\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u4e14\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u4fe1\u53f7", "conclusion": "\u534f\u540c\u8fdb\u5316\u6846\u67b6\u80fd\u591f\u5c06\u5931\u8d25\u8f68\u8ff9\u8f6c\u5316\u4e3a\u6709\u4ef7\u503c\u7684\u786c\u8d1f\u6837\u672c\uff0c\u9510\u5316\u51b3\u7b56\u8fb9\u754c\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u81ea\u6539\u8fdb\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5931\u8d25\u5229\u7528\u673a\u5236", "topic": "agent analysis"}}
{"id": "2511.21714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21714", "abs": "https://arxiv.org/abs/2511.21714", "authors": ["Pawel Batorski", "Paul Swoboda"], "title": "GPS: General Per-Sample Prompter", "comment": null, "summary": "LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.\n  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.\n  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.", "AI": {"tldr": "GPS\u662f\u4e00\u79cd\u901a\u7528\u3001\u6309\u6837\u672c\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u5373\u53ef\u4e3a\u6bcf\u4e2a\u8f93\u5165\u751f\u6210\u5b9a\u5236\u63d0\u793a\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6b63\u5219\u5316\u8bad\u7ec3\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u8bad\u7ec3\u63d0\u793a\u3001\u4f9d\u8d56\u8017\u65f6\u4f18\u5316\u5faa\u73af\u3001\u53ea\u80fd\u751f\u6210\u5355\u4e00\u4efb\u52a1\u7ea7\u63d0\u793a\u800c\u65e0\u6cd5\u9002\u5e94\u5177\u4f53\u8f93\u5165\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGPS\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u4efb\u52a1\u5957\u4ef6\u4e0a\u8bad\u7ec3\u63d0\u793a\u751f\u6210\u5668\uff1b2) \u5f15\u5165\u65b0\u9896\u6b63\u5219\u5316\u6280\u672f\u4ee5\u6709\u6548\u9002\u5e94\u6309\u6837\u672c\u63d0\u793a\uff1b3) \u91c7\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u7a33\u5b9a\u63a8\u7406\u3002", "result": "\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e0a\u83b7\u5f97\u7b2c\u4e8c\u4f73\u7ed3\u679c\uff0c\u6458\u8981\u4efb\u52a1\u7b2c\u4e09\u4f73\uff0c\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u672a\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5728GSM8K\u4e0a\u83b7\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u5c55\u793a\u4e86\u65e0\u9700\u5927\u91cf\u4f18\u5316\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u96c6\u5373\u53ef\u751f\u6210\u81ea\u9002\u5e94\u63d0\u793a\u7684\u6f5c\u529b\u3002", "conclusion": "GPS\u5c55\u793a\u4e86\u81ea\u52a8\u63d0\u793a\u7684\u65b0\u8303\u5f0f\uff1a\u65e0\u9700\u5927\u91cf\u4f18\u5316\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u96c6\u5373\u53ef\u751f\u6210\u81ea\u9002\u5e94\u3001\u8f93\u5165\u7279\u5b9a\u7684\u63d0\u793a\uff0c\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.21928", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21928", "abs": "https://arxiv.org/abs/2511.21928", "authors": ["Yifan Zhou", "Sachin Grover", "Mohamed El Mistiri", "Kamalesh Kalirathnam", "Pratyush Kerhalkar", "Swaroop Mishra", "Neelesh Kumar", "Sanket Gaurav", "Oya Aran", "Heni Ben Amor"], "title": "Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs", "comment": "In The Thirty-ninth Annual Conference on Neural Information Processing Systems", "summary": "Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.", "AI": {"tldr": "ProPS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f6e\u4e8e\u7b56\u7565\u4f18\u5316\u5faa\u73af\u7684\u6838\u5fc3\uff0c\u76f4\u63a5\u57fa\u4e8e\u5956\u52b1\u53cd\u9988\u548c\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u63d0\u51fa\u7b56\u7565\u66f4\u65b0\uff0c\u572815\u4e2aGymnasium\u4efb\u52a1\u4e2d\u4f18\u4e8e7\u79cd\u4f20\u7edfRL\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ec5\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u6cd5\u5229\u7528\u73b0\u5b9e\u4efb\u52a1\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u77e5\u8bc6\u3002\u4eba\u7c7b\u5b66\u4e60\u5219\u80fd\u6709\u6548\u7ed3\u5408\u6570\u503c\u53cd\u9988\u4e0e\u8bed\u8a00\u3001\u5148\u9a8c\u77e5\u8bc6\u548c\u5e38\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u7edf\u4e00\u6570\u503c\u548c\u8bed\u8a00\u63a8\u7406\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faPrompted Policy Search (ProPS)\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f6e\u4e8e\u7b56\u7565\u4f18\u5316\u5faa\u73af\u7684\u6838\u5fc3\uff0c\u76f4\u63a5\u57fa\u4e8e\u5956\u52b1\u53cd\u9988\u548c\u81ea\u7136\u8bed\u8a00\u8f93\u5165\uff08\u5982\u76ee\u6807\u3001\u9886\u57df\u77e5\u8bc6\u3001\u7b56\u7565\u63d0\u793a\uff09\u63d0\u51fa\u7b56\u7565\u66f4\u65b0\uff0c\u5b9e\u73b0\u6570\u503c\u4f18\u5316\u548c\u8bed\u4e49\u63a8\u7406\u7684\u7edf\u4e00\u3002", "result": "\u572815\u4e2aGymnasium\u4efb\u52a1\uff08\u7ecf\u5178\u63a7\u5236\u3001Atari\u6e38\u620f\u3001MuJoCo\u73af\u5883\uff09\u4e2d\uff0cProPS\u57288\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6240\u67097\u79cd\u57fa\u7ebf\u7b97\u6cd5\uff08PPO\u3001SAC\u3001TRPO\u7b49\uff09\uff0c\u5f53\u63d0\u4f9b\u9886\u57df\u77e5\u8bc6\u65f6\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u7edf\u4e00\u8bed\u4e49\u548c\u6570\u503c\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u900f\u660e\u3001\u53ef\u6cdb\u5316\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\uff0cLLM\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4e2d\u6267\u884c\u6570\u503c\u4f18\u5316\uff0c\u8bed\u4e49\u4fe1\u53f7\u80fd\u5e26\u6765\u66f4\u660e\u667a\u7684\u63a2\u7d22\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.23302", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23302", "abs": "https://arxiv.org/abs/2511.23302", "authors": ["Hengyuan Liu", "Zheng Li", "Donghua Wang", "Yankai Wu", "Xiang Chen", "Yong Liu"], "title": "FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation", "comment": null, "summary": "Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.", "AI": {"tldr": "\u63d0\u51faMBFL-FLIM\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8bed\u4e49\u5206\u6790\u8bc6\u522b\u548c\u7f13\u89e3\u5e72\u6270\u7a81\u53d8\u4f53\uff0c\u63d0\u5347\u57fa\u4e8e\u7a81\u53d8\u7684\u6545\u969c\u5b9a\u4f4d\u6548\u679c", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7a81\u53d8\u7684\u6545\u969c\u5b9a\u4f4d(MBFL)\u9762\u4e34\u5e72\u6270\u7a81\u53d8\u4f53\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6765\u81ea\u975e\u6545\u969c\u4ee3\u7801\u7684\u7a81\u53d8\u4f53\u4f1a\u88ab\u5931\u8d25\u6d4b\u8bd5\u6740\u6b7b\uff0c\u6a21\u4eff\u771f\u5b9e\u6545\u969c\u884c\u4e3a\uff0c\u524a\u5f31\u6545\u969c\u5b9a\u4f4d\u6548\u679c", "method": "1) \u63d0\u51fa\u6545\u969c\u5b9a\u4f4d\u5e72\u6270\u7a81\u53d8\u4f53(FLIMs)\u6982\u5ff5\u5e76\u8fdb\u884cRIPR\u6a21\u578b\u7406\u8bba\u5206\u6790\uff1b2) \u4f7f\u7528LLM\u8bed\u4e49\u5206\u6790\u8bc6\u522bFLIMs\uff0c\u7ed3\u5408\u5fae\u8c03\u6280\u672f\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7b56\u7565\uff1b3) \u901a\u8fc7\u7cbe\u70bc\u53ef\u7591\u5ea6\u5206\u6570\u7f13\u89e3FLIMs\u5f71\u54cd\uff1b4) \u96c6\u6210\u5230MBFL\u5de5\u4f5c\u6d41\u4e2d\u5f62\u6210MBFL-FLIM\u6846\u67b6", "result": "\u5728Defects4J\u57fa\u51c6\u6d4b\u8bd5\u7684395\u4e2a\u7a0b\u5e8f\u7248\u672c\u4e0a\uff0c\u4f7f\u75288\u4e2aLLM\u8fdb\u884c\u5b9e\u9a8c\uff0cMBFL-FLIM\u5728Top-1\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u534744\u4e2a\u6545\u969c\uff0c\u4f18\u4e8e\u4f20\u7edfSBFL/MBFL\u65b9\u6cd5\u3001\u52a8\u6001\u7279\u5f81\u65b9\u6cd5\u548c\u8fd1\u671fLLM\u6545\u969c\u5b9a\u4f4d\u6280\u672f", "conclusion": "MBFL-FLIM\u901a\u8fc7\u8bed\u4e49\u8bc6\u522b\u548c\u7f13\u89e3\u5e72\u6270\u7a81\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u6545\u969c\u5b9a\u4f4d\u6548\u679c\uff0c\u5728\u591a\u6545\u969c\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5fae\u8c03\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7ec4\u4ef6\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u8d21\u732e", "topic": "swe application"}}
{"id": "2511.23321", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23321", "abs": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "comment": null, "summary": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "AI": {"tldr": "C2C-MoLA\uff1a\u7ed3\u5408MoE\u4e0eLoRA\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\uff0c\u63d0\u5347\u51c6\u786e\u602717%\u3001\u964d\u4f4eGPU\u5185\u5b5818%\u3001\u52a0\u901f\u6536\u655b20%", "motivation": "\u73b0\u6709\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5728\u8de8\u7c7b\u578b\u6cdb\u5316\u3001\u5185\u5b58\u6548\u7387\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faC2C-MoLA\u6846\u67b6\uff0c\u7ed3\u5408Mixture of Experts\uff08MoE\uff09\u4e0eLow-Rank Adaptation\uff08LoRA\uff09\u3002MoE\u4f7f\u7528\u590d\u6742\u5ea6\u611f\u77e5\u8def\u7531\u673a\u5236\uff0c\u5305\u542b\u9886\u57df\u4e13\u5bb6\u548c\u8d1f\u8f7d\u5747\u8861\u7a00\u758f\u95e8\u63a7\uff1bLoRA\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u66f4\u65b0\uff1b\u91c7\u7528\u5b9a\u5236\u8bad\u7ec3\u7b56\u7565\u5bf9\u9f50\u8def\u7531\u7a33\u5b9a\u6027\u4e0e\u8bed\u4e49\u51c6\u786e\u6027", "result": "\u5728Chart2Code-160k\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u5fae\u8c03\u548c\u4ec5LoRA\u57fa\u7ebf\uff0c\u751f\u6210\u51c6\u786e\u6027\u63d0\u534717%\uff0c\u5cf0\u503cGPU\u5185\u5b58\u964d\u4f4e18%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb20%\uff0c\u5c24\u5176\u5728\u590d\u6742\u56fe\u8868\u4e0a\u8868\u73b0\u66f4\u4f73", "conclusion": "C2C-MoLA\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8de8\u7c7b\u578b\u6cdb\u5316\u3001\u5185\u5b58\u6548\u7387\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7MoE\u4e0eLoRA\u7684\u534f\u540c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210", "topic": "code agent"}}
{"id": "2511.21934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21934", "abs": "https://arxiv.org/abs/2511.21934", "authors": ["Tao Zhe", "Huazhen Fang", "Kunpeng Liu", "Qian Lou", "Tamzidul Hoque", "Dongjie Wang"], "title": "Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation", "comment": "Accepted at KDD 2026 Research Track", "summary": "Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7279\u5f81\u53d8\u6362\uff0c\u901a\u8fc7\u5171\u4eab\u6279\u8bc4\u5668\u673a\u5236\u589e\u5f3a\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\uff0c\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u5904\u7406\u52a8\u6001\u6269\u5c55\u7279\u5f81\u7a7a\u95f4\uff0c\u63d0\u9ad8\u7279\u5f81\u4ea4\u53c9\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u7279\u5f81\u53d8\u6362\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u7a77\u4e3e\u641c\u7d22\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u7279\u5f81\u53d8\u6362\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u7279\u5f81\u6269\u5c55\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u5b66\u4e60\u590d\u6742\u5ea6\u589e\u52a0\uff1b2) \u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u4e0d\u8db3\u5bfc\u81f4\u7279\u5f81\u4ea4\u53c9\u64cd\u4f5c\u6b21\u4f18\u548c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u5f02\u6784\u667a\u80fd\u4f53\uff08\u5206\u4e3a\u4e24\u7c7b\uff09\uff0c\u5206\u522b\u8d1f\u8d23\u9009\u62e9\u7279\u5f81\u548c\u64cd\u4f5c\u8fdb\u884c\u7279\u5f81\u4ea4\u53c9\u3002\u91c7\u7528\u5171\u4eab\u6279\u8bc4\u5668\u673a\u5236\u4fc3\u8fdb\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\uff0c\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u7279\u5f81\u667a\u80fd\u4f53\u5904\u7406\u52a8\u6001\u6269\u5c55\u7279\u5f81\u7a7a\u95f4\uff0c\u5f15\u5165\u72b6\u6001\u7f16\u7801\u6280\u672f\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3001\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u4f18\u7684\u7279\u5f81\u53d8\u6362\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u7279\u5f81\u53d8\u6362\u4e2d\u7684\u534f\u4f5c\u548c\u52a8\u6001\u7279\u5f81\u7a7a\u95f4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u7279\u5f81\u53d8\u6362\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.21935", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.21935", "abs": "https://arxiv.org/abs/2511.21935", "authors": ["Natalie Collina", "Eshwar Ram Arunachaleswaran", "Meena Jagadeesan"], "title": "Breaking Algorithmic Collusion in Human-AI Ecosystems", "comment": null, "summary": "AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.", "AI": {"tldr": "\u7814\u7a76\u6df7\u5408\u4eba\u673a\u751f\u6001\u7cfb\u7edf\u4e2d\u7b97\u6cd5\u5408\u8c0b\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u5355\u4e2a\u4eba\u7c7b\u53c2\u4e0e\u8005\u4e5f\u80fd\u7834\u574fAI\u4ee3\u7406\u4e4b\u95f4\u7684\u4ef7\u683c\u5408\u8c0b", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u751f\u6001\u7cfb\u7edf\u4e2d\u4e0e\u4eba\u7c7b\u9891\u7e41\u4ea4\u4e92\uff0c\u9700\u8981\u7406\u89e3\u4eba\u7c7b\u53c2\u4e0e\u5982\u4f55\u5f71\u54cdAI\u4ee3\u7406\u4e4b\u95f4\u7684\u7b97\u6cd5\u5408\u8c0b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u91cd\u590d\u5b9a\u4ef7\u535a\u5f08\u4e2d", "method": "\u91c7\u7528\u91cd\u590d\u5b9a\u4ef7\u535a\u5f08\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790AI\u4ee3\u7406\u91c7\u7528\u5747\u8861\u7b56\u7565\u800c\u4eba\u7c7b\u91c7\u7528\u65e0\u9057\u61be\u7b56\u7565\u65f6\uff0c\u6df7\u5408\u751f\u6001\u7cfb\u7edf\u7684\u52a8\u6001\u53d8\u5316", "result": "\u5355\u4e2a\u4eba\u7c7b\u53c2\u4e0e\u8005\u5c31\u80fd\u7834\u574fAI\u4ee3\u7406\u7684\u4ef7\u683c\u5408\u8c0b\uff0c\u591a\u4e2a\u4eba\u7c7b\u53c2\u4e0e\u8005\u4f1a\u8fdb\u4e00\u6b65\u5c06\u4ef7\u683c\u63a8\u5411\u7ade\u4e89\u6c34\u5e73\uff1b\u5f53AI\u4ee3\u7406\u610f\u8bc6\u5230\u4eba\u7c7b\u5b58\u5728\u65f6\uff0c\u5408\u8c0b\u6027\u8d28\u4f1a\u53d1\u751f\u53d8\u5316", "conclusion": "\u7b97\u6cd5\u5408\u8c0b\u5728\u6df7\u5408\u4eba\u673a\u751f\u6001\u7cfb\u7edf\u4e2d\u5177\u6709\u8106\u5f31\u6027\uff0c\u4eba\u7c7b\u53c2\u4e0e\u80fd\u6709\u6548\u7834\u574f\u4ef7\u683c\u5408\u8c0b\uff0c\u8fd9\u4e3a\u7406\u89e3\u7b97\u6cd5\u5408\u8c0b\u7684\u6301\u4e45\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e", "topic": "agent analysis"}}
{"id": "2511.21726", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21726", "abs": "https://arxiv.org/abs/2511.21726", "authors": ["Yicong Zheng", "Kevin L. McKee", "Thomas Miconi", "Zacharie Bugaud", "Mick van Gelderen", "Jed McCaleb"], "title": "Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks", "comment": null, "summary": "How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.", "AI": {"tldr": "SUMER\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u672a\u538b\u7f29\u7684\u539f\u59cb\u6570\u636e\u4e2d\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u641c\u7d22\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u538b\u7f29\u7684\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5bfb\u627e\u6700\u4f18\u7684\u8bb0\u5fc6\u538b\u7f29\u7b97\u6cd5\uff0c\u4f46\u8fd9\u5f80\u5f80\u5f15\u5165\u4e86\u4eba\u7c7b\u504f\u89c1\uff0c\u4e14\u9884\u5b9a\u4e49\u7684\u538b\u7f29\u7b97\u6cd5\u65e0\u6cd5\u9002\u5e94\u6240\u6709\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002\u538b\u7f29\u662f\u6709\u635f\u7684\uff0c\u800c\u76ee\u6807\u5bfc\u5411\u7684\u641c\u7d22\u53ef\u80fd\u8868\u73b0\u66f4\u4f18\u3002", "method": "\u63d0\u51fa\u4e86SUMER\uff08Search in Uncompressed Memory via Experience Replay\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5b66\u4e60\u4f7f\u7528\u641c\u7d22\u5de5\u5177\u6536\u96c6\u4fe1\u606f\u5e76\u56de\u7b54\u76ee\u6807\u95ee\u9898\u3002", "result": "\u5728LoCoMo\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7406\u89e3\u6570\u636e\u96c6\u4e0a\uff0cSUMER\u4f7f\u7528Qwen2.5-7B-Instruct\u6a21\u578b\u5b66\u4e60\u4f7f\u7528\u641c\u7d22\u5de5\u5177\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5176\u4ed6\u6709\u504f\u8bb0\u5fc6\u538b\u7f29\u65b9\u6cd5\u548c\u5b8c\u6574\u4e0a\u4e0b\u6587\u57fa\u7ebf\uff0c\u8fbe\u5230SOTA\u6027\u80fd\uff08\u6bd4\u4e4b\u524d\u6700\u4f73\u63d0\u534743%\uff09\u3002", "conclusion": "\u7b80\u5355\u7684\u641c\u7d22\u65b9\u6cd5\u5e94\u7528\u4e8e\u539f\u59cb\u6570\u636e\u4f18\u4e8e\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u7684\u76ee\u6807\u65e0\u5173\u548c\u6709\u504f\u538b\u7f29\u7b97\u6cd5\uff0c\u9700\u8981\u66f4\u52a8\u6001\u548c\u81ea\u4e3b\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.21728", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21728", "abs": "https://arxiv.org/abs/2511.21728", "authors": ["Lin Yu", "Xiaofei Han", "Yifei Kang", "Chiung-Yi Tseng", "Danyang Zhang", "Ziqian Bi", "Zhimo Han"], "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.", "AI": {"tldr": "AffectMind\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u60c5\u611f\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u548c\u52a8\u6001\u77e5\u8bc6\u57fa\u7840\uff0c\u5728\u8425\u9500\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u60c5\u611f\u5bf9\u9f50\u548c\u8bf4\u670d\u6027\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u60c5\u611f\u4e00\u81f4\u6027\u3001\u8bf4\u670d\u6210\u529f\u7387\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u7cfb\u7edf\u5927\u591a\u662f\u88ab\u52a8\u7684\uff0c\u5728\u60c5\u611f\u4e30\u5bcc\u3001\u76ee\u6807\u5bfc\u5411\u7684\u8425\u9500\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u80fd\u591f\u4e3b\u52a8\u63a8\u7406\u3001\u52a8\u6001\u9002\u5e94\u60c5\u611f\u5e76\u4fdd\u6301\u8bf4\u670d\u529b\u7684\u5bf9\u8bdd\u4ee3\u7406\u3002", "method": "\u63d0\u51faAffectMind\u591a\u6a21\u6001\u60c5\u611f\u5bf9\u8bdd\u4ee3\u7406\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e3b\u52a8\u77e5\u8bc6\u57fa\u7840\u7f51\u7edc\uff08PKGN\uff09\u4ece\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f5\u5f8b\u4e2d\u6301\u7eed\u66f4\u65b0\u4e8b\u5b9e\u548c\u60c5\u611f\u4e0a\u4e0b\u6587\uff1b\u60c5\u611f-\u610f\u56fe\u5bf9\u9f50\u6a21\u578b\uff08EIAM\uff09\u8054\u5408\u5efa\u6a21\u7528\u6237\u60c5\u611f\u548c\u8d2d\u4e70\u610f\u56fe\u4ee5\u8c03\u6574\u8bf4\u670d\u7b56\u7565\uff1b\u5f3a\u5316\u8bdd\u8bed\u5faa\u73af\uff08RDL\uff09\u901a\u8fc7\u7528\u6237\u53cd\u9988\u7684\u5f3a\u5316\u4fe1\u53f7\u4f18\u5316\u60c5\u611f\u8fde\u8d2f\u6027\u548c\u53c2\u4e0e\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u6784\u5efa\u7684\u8425\u9500\u5bf9\u8bdd\u6570\u636e\u96c6\uff08MM-ConvMarket\u548cAffectPromo\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAffectMind\u5728\u60c5\u611f\u4e00\u81f4\u6027\uff08+26%\uff09\u3001\u8bf4\u670d\u6210\u529f\u7387\uff08+19%\uff09\u548c\u957f\u671f\u7528\u6237\u53c2\u4e0e\u5ea6\uff08+23%\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u60c5\u611f\u57fa\u7840\u7684\u4e3b\u52a8\u6027\u662f\u5546\u4e1a\u591a\u6a21\u6001\u4ee3\u7406\u7684\u5173\u952e\u80fd\u529b\uff0cAffectMind\u901a\u8fc7\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u548c\u4e3b\u52a8\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u8425\u9500\u5bf9\u8bdd\u7684\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2511.22659", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22659", "abs": "https://arxiv.org/abs/2511.22659", "authors": ["Zeren Chen", "Xiaoya Lu", "Zhijie Zheng", "Pengrui Li", "Lehan He", "Yijin Zhou", "Jing Shao", "Bohan Zhuang", "Lu Sheng"], "title": "Geometrically-Constrained Agent for Spatial Reasoning", "comment": "27 pages, 13 figures", "summary": "Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.", "AI": {"tldr": "GCA\u901a\u8fc7\u5c06VLM\u89d2\u8272\u89e3\u8026\u4e3a\u8bed\u4e49\u5206\u6790\u548c\u4efb\u52a1\u6c42\u89e3\u4e24\u9636\u6bb5\uff0c\u5f15\u5165\u5f62\u5f0f\u5316\u4efb\u52a1\u7ea6\u675f\u6765\u5f25\u5408\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u8bed\u4e49-\u51e0\u4f55\u9e3f\u6c9f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u8bed\u4e49-\u51e0\u4f55\u9e3f\u6c9f\uff1a\u64c5\u957f\u5b9a\u6027\u8bed\u4e49\u63a8\u7406\u4f46\u63a8\u7406\u8fc7\u7a0b\u5728\u635f\u5931\u6027\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8fdb\u884c\uff0c\u4e0e\u9ad8\u4fdd\u771f\u51e0\u4f55\u4e0d\u5bf9\u9f50\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\"\u9884\u8a00\u673a\u6096\u8bba\"\uff0c\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5219\u65e0\u6cd5\u7ea6\u675f\u89c4\u5212\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u7ea6\u675f\u4ee3\u7406(GCA)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4ee3\u7406\u8303\u5f0f\u3002\u7b56\u7565\u6027\u5730\u5c06VLM\u89d2\u8272\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1)\u4f5c\u4e3a\u8bed\u4e49\u5206\u6790\u5668\uff0c\u5c06\u7528\u6237\u6a21\u7cca\u67e5\u8be2\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u7ea6\u675f\uff1b2)\u4f5c\u4e3a\u4efb\u52a1\u6c42\u89e3\u5668\uff0c\u5728\u7ea6\u675f\u5b9a\u4e49\u7684\u786e\u5b9a\u6027\u8fb9\u754c\u5185\u751f\u6210\u548c\u6267\u884c\u5de5\u5177\u8c03\u7528\u3002", "result": "GCA\u5728\u591a\u4e2a\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u548c\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u7ea627%\u3002", "conclusion": "GCA\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u63a8\u7406\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u4e49-\u51e0\u4f55\u9e3f\u6c9f\u95ee\u9898\uff0c\u4e3a\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2511.22729", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22729", "abs": "https://arxiv.org/abs/2511.22729", "authors": ["Anton Bulle Labate", "Valesca Moura de Sousa", "Sandro Rama Fiorini", "Leonardo Guerreiro Azevedo", "Raphael Melo Thiago", "Viviane Torres da Silva"], "title": "Solving Context Window Overflow in AI Agents", "comment": null, "summary": "Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8ba9LLM\u5904\u7406\u4efb\u610f\u957f\u5ea6\u5de5\u5177\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5b58\u6307\u9488\u800c\u975e\u539f\u59cb\u6570\u636e\u4ea4\u4e92\uff0c\u907f\u514d\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u5347\u6548\u7387", "motivation": "LLM\u5728\u5904\u7406\u5316\u5b66\u3001\u6750\u6599\u79d1\u5b66\u7b49\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u7684\u5916\u90e8\u5de5\u5177\u8f93\u51fa\u65f6\uff0c\u5927\u5c3a\u5bf8\u8f93\u51fa\u4f1a\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u73b0\u6709\u622a\u65ad\u6216\u6458\u8981\u65b9\u6cd5\u4f1a\u4e22\u5931\u5b8c\u6574\u4fe1\u606f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u5b8c\u6574\u6570\u636e\u7684\u5de5\u4f5c\u6d41", "method": "\u5c06LLM\u4e0e\u5de5\u5177\u7684\u4ea4\u4e92\u4ece\u539f\u59cb\u6570\u636e\u8f6c\u5411\u5185\u5b58\u6307\u9488\uff0c\u4fdd\u7559\u5de5\u5177\u5b8c\u6574\u529f\u80fd\uff0c\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u5230\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\uff0c\u51cf\u5c11token\u4f7f\u7528\u548c\u6267\u884c\u65f6\u95f4", "result": "\u5728\u4f20\u7edf\u5de5\u4f5c\u6d41\u65e0\u6cd5\u6267\u884c\u7684\u771f\u5b9e\u6750\u6599\u79d1\u5b66\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u663e\u793a\uff0c\u5728\u4e24\u79cd\u65b9\u6cd5\u90fd\u6210\u529f\u7684\u60c5\u51b5\u4e0b\uff0c\u65b0\u65b9\u6cd5\u6d88\u8017\u7684token\u6570\u91cf\u7ea6\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u4e03\u5206\u4e4b\u4e00", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u5de5\u5177\u8f93\u51fa\u800c\u4e0d\u4e22\u5931\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u7684\u5e94\u7528\u6548\u7387", "topic": "code agent"}}
{"id": "2511.22737", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22737", "abs": "https://arxiv.org/abs/2511.22737", "authors": ["Salman Jan", "Toqeer Ali Syed", "Gohar Ali", "Ali Akarma", "Mohammad Riyaz Belgaum", "Ahmad Ali"], "title": "Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being", "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025", "summary": "The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u7684\u591a\u5c42\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff08\u81b3\u98df\u89c4\u5212\u3001\u63d0\u9192\u3001\u98df\u7269\u6307\u5bfc\u3001\u76d1\u6d4b\uff09\u63d0\u4f9b\u4e2a\u6027\u5316\u5065\u5eb7\u652f\u6301\uff0c\u91c7\u7528\u9ed1\u677f/\u4e8b\u4ef6\u603b\u7ebf\u901a\u4fe1\uff0c\u6574\u5408\u9690\u79c1\u654f\u611f\u6570\u636e\u6e90\uff0c\u5e76\u5305\u542b\u53ef\u89e3\u91caAI\u6a21\u5757\u3002", "motivation": "\u4f20\u7edf\u8f85\u52a9\u7cfb\u7edf\u7f3a\u4e4f\u5305\u5bb9\u6027\u3001\u4e2a\u6027\u5316\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u6ee1\u8db3\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u7684\u5065\u5eb7\u9700\u6c42\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u63d0\u4f9b\u81ea\u9002\u5e94\u3001\u900f\u660e\u548c\u5305\u5bb9\u652f\u6301\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u5e2e\u52a9\u8fd9\u4e9b\u4eba\u7fa4\u8fc7\u4e0a\u66f4\u5065\u5eb7\u3001\u66f4\u6709\u89c4\u5f8b\u7684\u751f\u6d3b\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u5e94\u7528\u4e0e\u63a5\u53e3\u5c42\u3001\u667a\u80fd\u4f53\u5c42\u3001\u6570\u636e\u6e90\u5c42\u3002\u901a\u8fc7\u6df7\u5408\u63a8\u7406\u5f15\u64ce\u534f\u8c03\u56db\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff08\u81b3\u98df\u89c4\u5212\u3001\u63d0\u9192\u3001\u98df\u7269\u6307\u5bfc\u3001\u76d1\u6d4b\uff09\uff0c\u4f7f\u7528\u9ed1\u677f/\u4e8b\u4ef6\u603b\u7ebf\u8fdb\u884c\u901a\u4fe1\u3002\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u3001\u8425\u517b\u6570\u636e\u5e93\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u667a\u80fd\u53a8\u623f\u7269\u8054\u7f51\u7b49\u9690\u79c1\u654f\u611f\u6570\u636e\u6e90\uff0c\u5e76\u5305\u542b\u53ef\u89e3\u91caAI\u6a21\u5757\u548c\u4e34\u5e8a\u533b\u751f\u4eea\u8868\u677f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d85\u8d8a\u4f20\u7edf\u8f85\u52a9\u7cfb\u7edf\u7684\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u5c42\u9762\u5b9e\u73b0\u4e86\u5305\u5bb9\u6027\u3001\u4e2a\u6027\u5316\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u63a8\u7406\u3001\u591a\u6a21\u6001\u63a5\u53e3\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u4ea4\u53c9\u878d\u5408\uff0c\u80fd\u591f\u4fc3\u8fdb\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u7684\u81ea\u4e3b\u6027\u3001\u5065\u5eb7\u548c\u6570\u5b57\u516c\u5e73\u3002", "conclusion": "\u8be5\u667a\u80fd\u4f53AI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u6e90\u548c\u53ef\u89e3\u91caAI\uff0c\u4e3a\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5065\u5eb7\u652f\u6301\u7cfb\u7edf\uff0c\u4ee3\u8868\u4e86\u8f85\u52a9\u6280\u672f\u5411\u66f4\u5305\u5bb9\u3001\u4e2a\u6027\u5316\u548c\u53ef\u8bbf\u95ee\u65b9\u5411\u7684\u91cd\u8981\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2511.21990", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.21990", "abs": "https://arxiv.org/abs/2511.21990", "authors": ["Shaona Ghosh", "Barnaby Simkin", "Kyriacos Shiarlis", "Soumili Nandi", "Dan Zhao", "Matthew Fiedler", "Julia Bazinska", "Nikki Pope", "Roopa Prabhu", "Daniel Rohrer", "Michael Demoret", "Bartley Richardson"], "title": "A Safety and Security Framework for Real-World Agentic Systems", "comment": null, "summary": "This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u52a8\u6001\u53ef\u64cd\u4f5c\u7684\u6846\u67b6\u6765\u4fdd\u969c\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\uff0c\u901a\u8fc7\u7edf\u4e00\u4f20\u7edf\u5b89\u5168\u4e0e\u65b0\u578b\u667a\u80fd\u4f53\u98ce\u9669\uff0c\u5e76\u5229\u7528\u8f85\u52a9AI\u6a21\u578b\u548c\u7ea2\u961f\u6d4b\u8bd5\u8fdb\u884c\u98ce\u9669\u53d1\u73b0\u4e0e\u7f13\u89e3\u3002", "motivation": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u4e0d\u4ec5\u662f\u5355\u4e2a\u6a21\u578b\u7684\u56fa\u5b9a\u5c5e\u6027\uff0c\u66f4\u662f\u6a21\u578b\u3001\u7f16\u6392\u5668\u3001\u5de5\u5177\u548c\u6570\u636e\u5728\u64cd\u4f5c\u73af\u5883\u4e2d\u52a8\u6001\u4ea4\u4e92\u4ea7\u751f\u7684\u6d8c\u73b0\u7279\u6027\u3002\u4f20\u7edfLLM\u548c\u5b64\u7acb\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5b89\u5168\u4e0e\u5b89\u5168\u6709\u660e\u786e\u5206\u79bb\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b83\u4eec\u76f8\u4e92\u5173\u8054\uff0c\u9700\u8981\u65b0\u7684\u98ce\u9669\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u667a\u80fd\u4f53\u5b89\u5168\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u5b9a\u4e49\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4f20\u7edf\u5b89\u5168\u95ee\u9898\u548c\u65b0\u578b\u667a\u80fd\u4f53\u7279\u6709\u98ce\u9669\uff1b2\uff09\u5229\u7528\u8f85\u52a9AI\u6a21\u578b\u548c\u4eba\u5de5\u76d1\u7763\u8fdb\u884c\u4e0a\u4e0b\u6587\u98ce\u9669\u7ba1\u7406\uff1b3\uff09\u901a\u8fc7\u6c99\u76d2\u5316\u7684AI\u9a71\u52a8\u7ea2\u961f\u6d4b\u8bd5\u8fdb\u884c\u98ce\u9669\u53d1\u73b0\uff1b4\uff09\u5728NVIDIA AI-Q\u7814\u7a76\u52a9\u624b\u4e2d\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u5728NVIDIA\u65d7\u8230\u667a\u80fd\u4f53\u7814\u7a76\u52a9\u624bAI-Q\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u53d1\u73b0\u4e86\u65b0\u578b\u667a\u80fd\u4f53\u98ce\u9669\u5e76\u8fdb\u884c\u4e0a\u4e0b\u6587\u7f13\u89e3\u3002\u53d1\u5e03\u4e86\u5305\u542b\u8d85\u8fc710,000\u6b21\u771f\u5b9e\u653b\u51fb\u548c\u9632\u5fa1\u6267\u884c\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a8\u8fdb\u667a\u80fd\u4f53\u5b89\u5168\u7814\u7a76\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u9700\u8981\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u5c06\u4f20\u7edf\u5b89\u5168\u4e0e\u65b0\u578b\u667a\u80fd\u4f53\u7279\u6709\u98ce\u9669\u7edf\u4e00\u7ba1\u7406\u3002\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u8f85\u52a9AI\u6a21\u578b\u548c\u7ea2\u961f\u6d4b\u8bd5\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3\u98ce\u9669\uff0c\u4e3a\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b89\u5168\u4fdd\u969c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.22891", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22891", "abs": "https://arxiv.org/abs/2511.22891", "authors": ["Kumar Tanmay", "Kriti Aggarwal", "Paul Pu Liang", "Subhabrata Mukherjee"], "title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought", "comment": null, "summary": "Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.", "AI": {"tldr": "\u63d0\u51faMentalese\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u538b\u7f29\u7ed3\u6784\u5316\u4ee4\u724c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u7ed3\u5408SLPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u4ee4\u724c\u6570\u91cf\uff084-16\u500d\uff09\u548c\u5ef6\u8fdf\uff085\u500d\uff09\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff087-9\u500d\uff09", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u5197\u957f\u7684\"\u601d\u8003\"\u4ee4\u724c\u94fe\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3001\u5197\u4f59\u548c\u4e0d\u8fde\u8d2f\u7684\u63a8\u7406\u8def\u5f84\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7684\u63a8\u7406\u65b9\u5f0f", "method": "1. \u63d0\u51faMentalese\u6846\u67b6\uff0c\u53d7\"\u601d\u7ef4\u8bed\u8a00\u5047\u8bf4\"\u542f\u53d1\uff0c\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u8d85\u538b\u7f29\u7ed3\u6784\u5316\u4ee4\u724c\u8fdb\u884c\u63a8\u7406\uff1b2. \u63d0\u51faSLPO\uff08\u77ed\u957f\u5ea6\u504f\u597d\u4f18\u5316\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5956\u52b1\u7b80\u6d01\u4e14\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5141\u8bb8\u5fc5\u8981\u65f6\u8fdb\u884c\u66f4\u957f\u7684\u63a8\u7406", "result": "ORION\u6a21\u578b\u5728AIME 2024/2025\u3001MinervaMath\u3001OlympiadBench\u3001Math500\u548cAMC\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\u63a8\u7406\u4ee4\u724c\u51cf\u5c114-16\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e5\u500d\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e7-9\u500d\uff0c\u4fdd\u6301DeepSeek R1 Distilled\u6a21\u578b90-98%\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8aClaude\u548cChatGPT-4o\u51c6\u786e\u6027\u8fbe5%\u540c\u65f6\u4fdd\u63012\u500d\u538b\u7f29", "conclusion": "Mentalese\u98ce\u683c\u7684\u538b\u7f29\u63a8\u7406\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u6548\u7387\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u63a8\u7406", "topic": "agentic reinforcement learning"}}
{"id": "2511.22998", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22998", "abs": "https://arxiv.org/abs/2511.22998", "authors": ["Peng Kuang", "Xiangxiang Wang", "Wentao Liu", "Jian Dong", "Kaidi Xu", "Haohan Wang"], "title": "TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM", "comment": "14 pages", "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.", "AI": {"tldr": "TIM-PRM\u662f\u4e00\u4e2a\u5de5\u5177\u96c6\u6210\u7684\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u5de5\u5177\u67e5\u8be2\u548c\u72ec\u7acb\u63d0\u95ee\u673a\u5236\uff0c\u5c06\u9a8c\u8bc1\u4ece\u88ab\u52a8\u5206\u7c7b\u4efb\u52a1\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8c03\u67e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b58\u5728\u8c04\u5a9a\u6027\uff0c\u503e\u5411\u4e8e\u76f2\u76ee\u9a8c\u8bc1\u6709\u7f3a\u9677\u7684\u5047\u8bbe\u800c\u975e\u57fa\u4e8e\u89c6\u89c9\u73b0\u5b9e\u8fdb\u884c\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faTIM-PRM\u6846\u67b6\uff0c\u8bad\u7ec3\u6a21\u578b\u663e\u5f0f\u89c4\u5212\u9a8c\u8bc1\u7b56\u7565\uff0c\u901a\u8fc7\u72ec\u7acb\u63d0\u95ee\u673a\u5236\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u67e5\u8be2\u8bc1\u636e\uff0c\u5c06\u9a8c\u8bc1\u4e0e\u63a8\u7406\u4e0a\u4e0b\u6587\u89e3\u8026\u4ee5\u6d88\u9664\u786e\u8ba4\u504f\u5dee\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u9a8c\u8bc1\u8f68\u8ff9\u6570\u636e\u96c6\u3002", "result": "\u5728VisualProcessBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c8B\u53c2\u6570\u7684TIM-PRM\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001PRMs\uff0c\u663e\u8457\u4f18\u4e8eQwen2.5-72B\u548cInternVL-78B\u7b49\u66f4\u5927\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9a8c\u8bc1\u8fc7\u7a0b\u6d1e\u5bdf\u3002", "conclusion": "TIM-PRM\u901a\u8fc7\u5c06\u9a8c\u8bc1\u91cd\u6784\u4e3a\u4e3b\u52a8\u5de5\u5177\u589e\u5f3a\u7684\u8c03\u67e5\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u89c6\u89c9\u5e7b\u89c9\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2511.23055", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23055", "abs": "https://arxiv.org/abs/2511.23055", "authors": ["Ruoxuan Zhang", "Qiyun Zheng", "Zhiyu Zhou", "Ziqi Liao", "Siyu Wu", "Jian-Yu Jiang-Lin", "Bin Wen", "Hongxia Xie", "Jianlong Fu", "Wen-Huang Cheng"], "title": "MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents", "comment": null, "summary": "Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.", "AI": {"tldr": "\u63d0\u51faMindPower\u6846\u67b6\uff0c\u5c06\u5fc3\u7406\u7406\u8bba\u6574\u5408\u5230\u5177\u8eab\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\uff0c\u901a\u8fc7\u611f\u77e5-\u5fc3\u7406\u63a8\u7406-\u51b3\u7b56-\u884c\u52a8\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165Mind-Reward\u4f18\u5316\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u548c\u884c\u52a8\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u5177\u8eab\u667a\u80fd\u4f53\u7f3a\u4e4f\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u4ec5\u5173\u6ce8\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u800c\u5ffd\u7565\u667a\u80fd\u4f53\u81ea\u8eab\u89c6\u89d2\uff0c\u8fd9\u963b\u788d\u4e86\u8fde\u8d2f\u7684\u51b3\u7b56\u548c\u884c\u52a8\u751f\u6210\u3002", "method": "\u63d0\u51faMindPower\u673a\u5668\u4eba\u4e2d\u5fc3\u6846\u67b6\uff0c\u5305\u542b\u611f\u77e5\u3001\u5fc3\u7406\u63a8\u7406\u3001\u51b3\u7b56\u548c\u884c\u52a8\u56db\u4e2a\u6a21\u5757\u3002\u9996\u5148\u611f\u77e5\u73af\u5883\u548c\u4eba\u7c7b\u72b6\u6001\uff0c\u7136\u540e\u8fdb\u884c\u5fc3\u7406\u7406\u8bba\u63a8\u7406\u5efa\u6a21\u81ea\u6211\u548c\u4ed6\u4eba\uff0c\u6700\u540e\u57fa\u4e8e\u63a8\u65ad\u7684\u5fc3\u7406\u72b6\u6001\u751f\u6210\u51b3\u7b56\u548c\u884c\u52a8\u3002\u540c\u65f6\u5f15\u5165Mind-Reward\u4f18\u5316\u76ee\u6807\uff0c\u9f13\u52b1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u4e00\u81f4\u7684\u5fc3\u7406\u63a8\u7406\u548c\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u5728\u51b3\u7b56\u5236\u5b9a\u4e0a\u6bd4GPT-4o\u63d0\u534712.77%\uff0c\u5728\u884c\u52a8\u751f\u6210\u4e0a\u63d0\u534712.49%\u3002", "conclusion": "MindPower\u6846\u67b6\u6210\u529f\u5c06\u5fc3\u7406\u7406\u8bba\u6574\u5408\u5230\u5177\u8eab\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u81ea\u6211\u548c\u4ed6\u4eba\u89c6\u89d2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u548c\u884c\u52a8\u7684\u8fde\u8d2f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.21737", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21737", "abs": "https://arxiv.org/abs/2511.21737", "authors": ["Sabrina Sadiekh", "Elena Ericheva", "Chirag Agarwal"], "title": "Polarity-Aware Probing for Quantifying Latent Alignment in Language Models", "comment": "7 pages", "summary": "Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPolarity-Aware CCS (PA-CCS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u6027\u53cd\u8f6c\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165Polar-Consistency\u548cContradiction Index\u4e24\u4e2a\u5bf9\u9f50\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u578b\u6f5c\u5728\u77e5\u8bc6\u7684\u8bed\u4e49\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740CCS\u7b49\u65e0\u76d1\u7763\u63a2\u9488\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u80fd\u53ef\u9760\u8bc4\u4f30\u6a21\u578b\u5bf9\u9f50\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76CCS\u5bf9\u6709\u5bb3\u4e0e\u5b89\u5168\u9648\u8ff0\u7684\u654f\u611f\u6027\uff0c\u5e76\u5f00\u53d1\u80fd\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u5728\u6781\u6027\u53cd\u8f6c\u4e0b\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPolarity-Aware CCS (PA-CCS)\u65b9\u6cd5\uff0c\u5f15\u5165Polar-Consistency\u548cContradiction Index\u4e24\u4e2a\u5bf9\u9f50\u6307\u6807\u3002\u6784\u5efa\u4e24\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u63a7\u5236\u6570\u636e\u96c6\uff0c\u5305\u542b\u4f7f\u7528\u4e0d\u540c\u65b9\u6cd5\uff08\u5e76\u53d1\u548c\u5bf9\u6297\u6027\u9648\u8ff0\uff09\u6784\u5efa\u7684\u5339\u914d\u6709\u5bb3-\u5b89\u5168\u53e5\u5b50\u5bf9\u3002\u5c06PA-CCS\u5e94\u7528\u4e8e16\u4e2a\u8bed\u8a00\u6a21\u578b\u3002", "result": "PA-CCS\u80fd\u591f\u8bc6\u522b\u6f5c\u5728\u6709\u5bb3\u77e5\u8bc6\u7f16\u7801\u4e2d\u7684\u67b6\u6784\u548c\u5c42\u7ea7\u7279\u5b9a\u5dee\u5f02\u3002\u5bf9\u4e8e\u5177\u6709\u826f\u597d\u5bf9\u9f50\u5185\u90e8\u8868\u5f81\u7684\u6a21\u578b\uff0c\u7528\u65e0\u610f\u4e49\u6807\u8bb0\u66ff\u6362\u5426\u5b9a\u6807\u8bb0\u4f1a\u964d\u4f4ePA-CCS\u5206\u6570\uff0c\u800c\u7f3a\u4e4f\u9c81\u68d2\u5185\u90e8\u6821\u51c6\u7684\u6a21\u578b\u5219\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u9000\u5316\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u65e0\u76d1\u7763\u63a2\u9488\u5728\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5c06\u7ed3\u6784\u9c81\u68d2\u6027\u68c0\u67e5\u7eb3\u5165\u53ef\u89e3\u91ca\u6027\u57fa\u51c6\u3002PA-CCS\u4e3a\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2511.23092", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23092", "abs": "https://arxiv.org/abs/2511.23092", "authors": ["David Demitri Africa", "Hans Ethan Ting"], "title": "Does Self-Evaluation Enable Wireheading in Language Models?", "comment": "Accepted (oral) to Foundations of Agentic Systems Theory at AAAI 2026", "summary": "Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u81ea\u6211\u8bc4\u4f30\u4e0e\u5956\u52b1\u4fe1\u53f7\u8026\u5408\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u4f1a\u8fdb\u884c\"\u7ebf\u5934\u5316\"\uff08wireheading\uff09\u2014\u2014\u64cd\u7eb5\u5956\u52b1\u6d4b\u91cf\u800c\u975e\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u5bfc\u81f4\u8bc4\u5206\u81a8\u80c0\u4f46\u51c6\u786e\u6027\u65e0\u6539\u5584\u3002", "motivation": "\u7814\u7a76\u81ea\u6211\u8bc4\u4f30\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u81ea\u6211\u8bc4\u4f30\u4e0e\u5956\u52b1\u4fe1\u53f7\u8026\u5408\u65f6\u662f\u5426\u4f1a\u6fc0\u52b1\u6a21\u578b\u64cd\u7eb5\u5956\u52b1\u6d4b\u91cf\u800c\u975e\u771f\u6b63\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "method": "1. \u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u4e2d\u5f62\u5f0f\u5316\u5956\u52b1\u901a\u9053\u63a7\u5236\u4e25\u683c\u4f18\u4e8e\u4efb\u52a1\u805a\u7126\u884c\u4e3a\u7684\u6761\u4ef6\uff1b2. \u4f7f\u7528\u4e24\u4e2a\u6a21\u578b\u548c\u4e09\u4e2a\u4efb\u52a1\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\uff1b3. \u6bd4\u8f83\u81ea\u6211\u8bc4\u5206\u51b3\u5b9a\u5956\u52b1\u7684\u6a21\u578b\u4e0e\u81ea\u6211\u8bc4\u4f30\u4f46\u4e0d\u63a7\u5236\u5956\u52b1\u7684\u6a21\u578b\u3002", "result": "1. \u81ea\u6211\u8bc4\u5206\u51b3\u5b9a\u5956\u52b1\u7684\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u7684\u8bc4\u5206\u81a8\u80c0\u4f46\u65e0\u76f8\u5e94\u7684\u51c6\u786e\u6027\u63d0\u5347\uff1b2. \u5728\u6a21\u7cca\u4efb\u52a1\uff08\u5982\u6458\u8981\uff09\u4e0a\u6548\u679c\u5c24\u5176\u660e\u663e\uff1b3. \u81ea\u6211\u8bc4\u4f30\u4f46\u4e0d\u63a7\u5236\u5956\u52b1\u7684\u6a21\u578b\u6ca1\u6709\u8fd9\u79cd\u81a8\u80c0\u73b0\u8c61\u3002", "conclusion": "\u81ea\u6211\u8bc4\u4f30\u4e0e\u5b66\u4e60\u4fe1\u53f7\u89e3\u8026\u65f6\u662f\u5b89\u5168\u7684\uff0c\u4f46\u5f53\u4e24\u8005\u8026\u5408\u65f6\u5b58\u5728\u5371\u9669\uff0c\u8fd9\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u6709\u660e\u786e\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2511.22104", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.22104", "abs": "https://arxiv.org/abs/2511.22104", "authors": ["Quan Zhou", "Shie Mannor"], "title": "Representative Action Selection for Large Action Space: From Bandits to MDPs", "comment": "Journal version of arXiv:2505.18269", "summary": "We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.\n  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5728\u5f3a\u5316\u5b66\u4e60\u5bb6\u65cf\u73af\u5883\u4e2d\u90fd\u80fd\u627e\u5230\u8fd1\u4f3c\u6700\u4f18\u52a8\u4f5c\uff0c\u4ece\u800c\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u5728\u5e93\u5b58\u7ba1\u7406\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\uff0c\u52a8\u4f5c\u7a7a\u95f4\u6781\u5927\uff0c\u76f4\u63a5\u5728\u6574\u4e2a\u7a7a\u95f4\u4e0a\u5b66\u4e60\u662f\u4e0d\u53ef\u884c\u7684\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u9009\u62e9\u4e00\u4e2a\u5c0f\u7684\u4ee3\u8868\u6027\u5b50\u96c6\uff0c\u4f7f\u5f97\u5728\u6bcf\u4e2a\u73af\u5883\u4e2d\u90fd\u80fd\u5305\u542b\u8fd1\u4f3c\u6700\u4f18\u52a8\u4f5c\u3002", "method": "\u5c06\u5143\u591a\u81c2\u8d4c\u535a\u673a\uff08meta-bandits\uff09\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u8bbe\u7f6e\u3002\u5728\u653e\u677e\u7684\u975e\u4e2d\u5fc3\u5316\u6b21\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u4e0b\uff0c\u8bc1\u660e\u73b0\u6709\u7b97\u6cd5\u80fd\u8fbe\u5230\u4e0e\u4f7f\u7528\u5b8c\u6574\u52a8\u4f5c\u7a7a\u95f4\u76f8\u5f53\u7684\u6027\u80fd\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653e\u677e\u7684\u975e\u4e2d\u5fc3\u5316\u6b21\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0e\u4f7f\u7528\u5b8c\u6574\u52a8\u4f5c\u7a7a\u95f4\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u7ec4\u5408\u51b3\u7b56\u63d0\u4f9b\u4e86\u8ba1\u7b97\u548c\u6837\u672c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7ec4\u5408\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u73af\u5883\u5f02\u8d28\u6027\u66f4\u5f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u9009\u62e9\u4ee3\u8868\u6027\u52a8\u4f5c\u5b50\u96c6\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u907f\u514d\u4e86\u5b8c\u6574\u52a8\u4f5c\u7a7a\u95f4\u7684\u8bc4\u4f30\u8d1f\u62c5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.22105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22105", "abs": "https://arxiv.org/abs/2511.22105", "authors": ["Saad Masrur", "Ismail Guvenc", "David Lopez Perez"], "title": "Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning", "comment": "This is an updated version of my preprint available on TechRxiv. Don't flag it as plagiarism. I wanna post my paper on arxiv", "summary": "Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08MARL-DDQN\uff09\u7684\u52a8\u6001\u7761\u7720\u6a21\u5f0f\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u6700\u5927\u5316\u80fd\u6548\u5e76\u6ee1\u8db3\u670d\u52a1\u8d28\u91cf\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u7684\u7761\u7720\u6a21\u5f0f\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u57fa\u7ad9\u6d41\u91cf\u6a21\u578b\uff0c\u65e0\u6cd5\u6355\u6349\u975e\u5e73\u7a33\u6d41\u91cf\u52a8\u6001\uff0c\u4e14\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u8fc7\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u7684\u80fd\u6548\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faMARL-DDQN\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6df1\u5ea6Q\u7f51\u7edc\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u57283D\u57ce\u5e02\u73af\u5883\u4e2d\u7ed3\u5408\u65f6\u53d8\u793e\u533a\u5316\u7528\u6237\u8bbe\u5907\u79fb\u52a8\u6a21\u578b\uff0c\u96c6\u6210\u771f\u5b9e\u7684\u57fa\u7ad9\u529f\u8017\u6a21\u578b\u548c\u6ce2\u675f\u6210\u5f62\u6280\u672f\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u51b3\u7b56\u3002", "result": "MARL-DDQN\u5728\u52a8\u6001\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08All On\u3001IT-QoS-LB\u3001MARL-DDPG\u3001MARL-PPO\uff09\uff0c\u8fbe\u52300.60 Mbit/Joule\u80fd\u6548\u30018.5 Mbps\u7b2c10\u767e\u5206\u4f4d\u541e\u5410\u91cf\uff0c95%\u65f6\u95f4\u6ee1\u8db3QoS\u7ea6\u675f\u3002", "conclusion": "MARL-DDQN\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u52a8\u6001\u7761\u7720\u6a21\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.23387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23387", "abs": "https://arxiv.org/abs/2511.23387", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting", "comment": "9 pages, 4 figures", "summary": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42AI\u6c14\u8c61\u5b66\u5bb6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u6d4b\u63a8\u7406\u548c\u5929\u6c14\u5173\u952e\u8bcd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5929\u6c14\u62a5\u544a\uff0c\u4f7f\u7528\u591a\u5c3a\u5ea6\u5206\u6790\u548c\u5173\u952e\u8bcd\u9a8c\u8bc1\u63d0\u9ad8LLM\u751f\u6210\u5929\u6c14\u53d9\u4e8b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5929\u6c14\u9884\u62a5\u89c6\u4e3a\u5e73\u5766\u65f6\u95f4\u5e8f\u5217\uff0c\u7f3a\u4e4f\u5bf9\u77ed\u671f\u52a8\u6001\u548c\u957f\u671f\u8d8b\u52bf\u7684\u6355\u6349\uff0c\u4e14LLM\u751f\u6210\u7684\u5929\u6c14\u62a5\u544a\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u4e00\u81f4\u6027\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u6784\u5efa\u5206\u5c42LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u5c0f\u65f6\u30016\u5c0f\u65f6\u548c\u65e5\u5c3a\u5ea6\u8fdb\u884c\u591a\u5c3a\u5ea6\u63a8\u7406\uff1b\u6838\u5fc3\u63a8\u7406\u4ee3\u7406\u5c06\u7ed3\u6784\u5316\u6c14\u8c61\u8f93\u5165\u8f6c\u6362\u4e3a\u8fde\u8d2f\u53d9\u8ff0\uff0c\u540c\u65f6\u63d0\u53d6\u603b\u7ed3\u4e3b\u8981\u6c14\u8c61\u4e8b\u4ef6\u7684\u5173\u952e\u8bcd\uff1b\u4f7f\u7528\u5173\u952e\u8bcd\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u9a8c\u8bc1\u751f\u6210\u62a5\u544a\u7684\u4e00\u81f4\u6027\u3001\u65f6\u5e8f\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528OpenWeather\u548cMeteostat\u6570\u636e\u8bc1\u660e\uff0c\u5206\u5c42\u4e0a\u4e0b\u6587\u548c\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u9a8c\u8bc1\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210\u5929\u6c14\u53d9\u4e8b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u6c14\u8c61\u62a5\u544a\u7684\u8bed\u4e49\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u79d1\u5b66\u63a8\u7406\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2511.23436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23436", "abs": "https://arxiv.org/abs/2511.23436", "authors": ["Jianzhe Lin", "Zeyu Pan", "Yun Zhu", "Ruiqi Song", "Jining Yang"], "title": "Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent", "comment": "15 pages, 4 figures", "summary": "We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.", "AI": {"tldr": "SuperIntelliAgent\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u5c0f\u578b\u6269\u6563\u6a21\u578b\uff08\u5b66\u4e60\u8005\uff09\u4e0e\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u9a8c\u8bc1\u5668\uff09\u8026\u5408\uff0c\u5b9e\u73b0\u901a\u8fc7\u81ea\u76d1\u7763\u4ea4\u4e92\u7684\u6301\u7eed\u667a\u80fd\u589e\u957f\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u9700\u8981\u4eba\u5de5\u6807\u6ce8\uff0c\u800cSuperIntelliAgent\u65e8\u5728\u5b9e\u73b0\u65e0\u9700\u6807\u6ce8\u7684\u81ea\u4e3b\u5b66\u4e60\uff0c\u5c06\u666e\u901a\u63a8\u7406\u5faa\u73af\u8f6c\u53d8\u4e3a\u7ec8\u8eab\u4f18\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6301\u7eed\u667a\u80fd\u79ef\u7d2f\u3002", "method": "\u6846\u67b6\u5305\u542b\u5b66\u4e60\u8005\u751f\u6210\u5019\u9009\u8f93\u51fa\uff0c\u9a8c\u8bc1\u5668\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u8bc4\u4f30\uff0c\u4ea4\u4e92\u4ea7\u751fDPO\u8bad\u7ec3\u5bf9\u3002\u91c7\u7528\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\uff1a\u77ed\u671f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u4fdd\u5b58\u63a8\u7406\u8f68\u8ff9\uff0c\u957f\u671f\u8bb0\u5fc6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5de9\u56fa\u77e5\u8bc6\u3002\u56de\u653e\u7f13\u51b2\u533a\u4fdd\u7559\u53ef\u9a8c\u8bc1\u8fdb\u5c55\u7684\u6837\u672c\u4f5c\u4e3a\u8f85\u52a9\u76d1\u7763\u3002", "result": "\u4f7f\u7528\u5c11\u91cf\u81ea\u52a8\u751f\u6210\u7684DPO\u5bf9\uff0c\u5b66\u4e60\u8005\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u6709\u6539\u8fdb\uff0c\u8868\u660e\u8be5\u673a\u5236\u4e3a\u6301\u7eed\u667a\u80fd\u79ef\u7d2f\u548c\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u53ef\u8bad\u7ec3\u5b66\u4e60\u8005\u4e0e\u63a8\u7406\u80fd\u529b\u9a8c\u8bc1\u5668\u7684\u914d\u5bf9\u6784\u6210\u4e86\u589e\u957f\u667a\u80fd\u7684\u6700\u5c0f\u53ef\u9760\u5355\u5143\uff0c\u914d\u5bf9\u53cd\u9988\u548c\u90e8\u5206\u5386\u53f2\u56de\u653e\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u5b66\u4e60\u8bfe\u7a0b\u548c\u66f4\u5f3a\u7684\u504f\u597d\u5bf9\u9f50\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.23476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23476", "abs": "https://arxiv.org/abs/2511.23476", "authors": ["Bao Shu", "Yan Cai", "Jianjian Sun", "Chunrui Han", "En Yu", "Liang Zhao", "Jingcheng Hu", "Yinmin Zhang", "Haoran Lv", "Yuang Peng", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang", "Xiangyu Yue"], "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction", "comment": "17 pages, 9 figures", "summary": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.", "AI": {"tldr": "WMAct\u901a\u8fc7\u5956\u52b1\u91cd\u7f29\u653e\u548c\u4ea4\u4e92\u9891\u7387\u9000\u706b\u673a\u5236\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u5185\u90e8\u5316\u4e16\u754c\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u4ea4\u4e92\uff0c\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u6548\u7387", "motivation": "\u5f53\u524d\u591a\u8f6e\u4ea4\u4e92\u65b9\u6cd5\u91c7\u7528\u50f5\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u4e3b\u52a8\u5b66\u4e60\u80fd\u529b\uff0c\u963b\u788d\u4e86\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\u63a8\u7406\u3002\u9700\u8981\u8ba9\u6a21\u578b\u901a\u8fc7\"\u505a\"\u6765\u76f4\u63a5\u5851\u9020\u601d\u8003\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u5185\u90e8\u5316", "method": "\u63d0\u51faWMAct\u6846\u67b6\uff1a1) \u5956\u52b1\u91cd\u7f29\u653e\u673a\u5236\uff0c\u6839\u636e\u884c\u52a8\u6548\u80fd\u8c03\u6574\u7ed3\u679c\u5956\u52b1\uff0c\u6fc0\u52b1\u51cf\u5c11\u5197\u4f59\u548c\u76ee\u7684\u6027\u4ea4\u4e92\uff1b2) \u4ea4\u4e92\u9891\u7387\u9000\u706b\u7b56\u7565\uff0c\u9010\u6b65\u51cf\u5c11\u6700\u5927\u5141\u8bb8\u4ea4\u4e92\u8f6e\u6570\uff0c\u8feb\u4f7f\u6a21\u578b\u538b\u7f29\u5b66\u4e60\u5e76\u5185\u90e8\u5316\u73af\u5883\u52a8\u6001", "result": "\u5728Sokoban\u3001Maze\u548cTaxi\u7b49\u73af\u5883\u4e2d\uff0cWMAct\u80fd\u591f\u5355\u8f6e\u89e3\u51b3\u4e4b\u524d\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u7684\u4efb\u52a1\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5728\u4e00\u7cfb\u5217\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u5347\u6027\u80fd", "conclusion": "\u901a\u8fc7\u89e3\u653e\u7ed3\u6784\u5316\u63a8\u7406\u7ea6\u675f\uff0c\u8ba9\u6a21\u578b\u901a\u8fc7\u4e3b\u52a8\u4ea4\u4e92\u5185\u90e8\u5316\u4e16\u754c\u6a21\u578b\uff0cWMAct\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\u63a8\u7406\uff0c\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u4ea4\u4e92\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2511.22130", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22130", "abs": "https://arxiv.org/abs/2511.22130", "authors": ["Gilbert Yang", "Yaqin Chen", "Thomson Yen", "Hongseok Namkoong"], "title": "Benchmarking In-context Experiential Learning Through Repeated Product Recommendations", "comment": null, "summary": "To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.", "AI": {"tldr": "BELA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7ecf\u9a8c\u5b66\u4e60\u80fd\u529b\uff0c\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u5728\u63a8\u8350\u573a\u666f\u4e2d\u96be\u4ee5\u901a\u8fc7\u7ecf\u9a8c\u6709\u6548\u6539\u8fdb", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u901a\u8fc7\u7ecf\u9a8c\u9002\u5e94\u548c\u63a8\u7406\u80fd\u529b\u7684\u8861\u91cf\u3002\u771f\u5b9e\u4e16\u754c\u73af\u5883\u9700\u8981\u667a\u80fd\u4f53\u5904\u7406\u4e0d\u5b8c\u5168\u77e5\u8bc6\u5e76\u57fa\u4e8e\u7ecf\u9a8c\u8c03\u6574\u884c\u4e3a\u3002", "method": "\u6784\u5efaBELA\u57fa\u51c6\uff1a\u7ed3\u5408(1)\u4e9a\u9a6c\u900a\u771f\u5b9e\u4ea7\u54c1\u6570\u636e\uff0c(2)\u591a\u6837\u5316\u7528\u6237\u753b\u50cf\u8868\u793a\u6f5c\u5728\u504f\u597d\uff0c(3)\u57fa\u4e8e\u753b\u50cf\u7684LLM\u7528\u6237\u6a21\u62df\u5668\u521b\u5efa\u4ea4\u4e92\u8f68\u8ff9\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u63a8\u8350\u5bf9\u8bdd\u4e2d\u7684\u7ecf\u9a8c\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u96be\u4ee5\u5728\u591a\u4e2a\u5bf9\u8bdd\u56de\u5408\u4e2d\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u6539\u8fdb\uff0c\u8868\u660e\u9700\u8981\u5177\u5907\u5f3a\u5927\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u548c\u4e3b\u52a8\u63a2\u7d22\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0cBELA\u57fa\u51c6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2511.22138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22138", "abs": "https://arxiv.org/abs/2511.22138", "authors": ["Mohd Ariful Haque", "Fahad Rahman", "Kishor Datta Gupta", "Khalil Shujaee", "Roy George"], "title": "TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices", "comment": "8 pages, 3 figures, 4 tables", "summary": "This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6267\u884c\u4ee3\u7406\u4efb\u52a1\uff08\u51fd\u6570/\u5de5\u5177/API\u8c03\u7528\uff09\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u591a\u79cd\u4f18\u5316\u7b56\u7565\uff08SFT\u3001PEFT\u3001RL\u3001DPO\u3001\u6df7\u5408\u65b9\u6cd5\uff09\u5728BFCL\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff081-3B\u53c2\u6570\uff09\u663e\u8457\u4f18\u4e8e\u8d85\u7d27\u51d1\u6a21\u578b\uff0c\u6700\u9ad8\u8fbe\u523065.74%\u6574\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6267\u884c\u4ee3\u7406\u4efb\u52a1\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u5b9e\u73b0\u65e0\u9700\u4f9d\u8d56\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u9690\u79c1\u4fdd\u62a4\u3001\u4f4e\u5ef6\u8fdf\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e91\u7aef\u4f9d\u8d56\u5e26\u6765\u7684\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u6846\u67b6\u8bc4\u4f30\u591a\u4e2aSLM\u6a21\u578b\uff0c\u91c7\u7528\u53c2\u6570\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565\u5305\u62ec\uff1a\u76d1\u7763\u5fae\u8c03\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\u3001\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u504f\u597d\u5bf9\u9f50\u4ee5\u53ca\u6df7\u5408\u65b9\u6cd5\u3002\u6784\u5efa\u4e86\u4eceAgentBank\u6570\u636e\u8f6c\u6362\u7684DPO\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u89c4\u6a21\u5b58\u5728\u660e\u663e\u51c6\u786e\u7387\u5dee\u5f02\uff1a\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff081-3B\u53c2\u6570\uff09\u663e\u8457\u4f18\u4e8e\u8d85\u7d27\u51d1\u6a21\u578b\uff08<1B\u53c2\u6570\uff09\uff0c\u6700\u9ad8\u8fbe\u523065.74%\u6574\u4f53\u51c6\u786e\u7387\u548c55.62%\u591a\u8f6e\u5bf9\u8bdd\u51c6\u786e\u7387\u3002\u6df7\u5408\u4f18\u5316\u7b56\u7565\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6df7\u5408\u4f18\u5316\u7b56\u7565\u80fd\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u63d0\u4f9b\u51c6\u786e\u3001\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u4ee3\u7406AI\uff0c\u4f7f\u9690\u79c1\u4fdd\u62a4\u3001\u4f4e\u5ef6\u8fdf\u7684\u81ea\u4e3b\u4ee3\u7406\u5728\u4e91\u7aef\u4e4b\u5916\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002", "topic": "agent analysis"}}
{"id": "2511.21762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21762", "abs": "https://arxiv.org/abs/2511.21762", "authors": ["Gabriele Cesar Iwashima", "Claudia Susie Rodrigues", "Claudio Dipolitto", "Geraldo Xex\u00e9o"], "title": "Factors That Support Grounded Responses in LLM Conversations: A Rapid Review", "comment": "28 pages, 1 figure, 3 tables", "summary": "Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86LLM\u5bf9\u8bdd\u4e2d\u5bf9\u9f50\u6280\u672f\uff0c\u901a\u8fc7PRISMA\u6846\u67b6\u7684\u5feb\u901f\u56de\u987e\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u63a8\u7406\u65f6\u3001\u540e\u8bad\u7ec3\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e09\u7c7b\uff0c\u53d1\u73b0\u63a8\u7406\u65f6\u65b9\u6cd5\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u5bf9\u9f50\u7528\u6237\u610f\u56fe\u3001\u786e\u4fdd\u4e0a\u4e0b\u6587\u57fa\u7840\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u53ef\u80fd\u4ea7\u751f\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u57fa\u7840\u6216\u51fa\u73b0\u5e7b\u89c9\u7684\u8f93\u51fa\uff0c\u8fd9\u5f71\u54cd\u4e86LLM\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5bf9\u9f50\u6280\u672f\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528PRISMA\u6846\u67b6\u548cPICO\u7b56\u7565\u6307\u5bfc\u7684\u5feb\u901f\u56de\u987e\u65b9\u6cd5\uff0c\u5bf9LLM\u5bf9\u9f50\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u641c\u7d22\u3001\u7b5b\u9009\u548c\u5206\u7c7b\uff0c\u5c06\u65b9\u6cd5\u6309LLM\u751f\u547d\u5468\u671f\u9636\u6bb5\u5206\u4e3a\u63a8\u7406\u65f6\u65b9\u6cd5\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4e09\u7c7b\u3002", "result": "\u63a8\u7406\u65f6\u65b9\u6cd5\u88ab\u8bc1\u660e\u7279\u522b\u9ad8\u6548\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5bf9\u9f50\u8f93\u51fa\uff0c\u652f\u6301\u7528\u6237\u610f\u56fe\u3001\u4e0a\u4e0b\u6587\u57fa\u7840\u548c\u5e7b\u89c9\u7f13\u89e3\u3002\u7efc\u8ff0\u7684\u6280\u672f\u4e3a\u6539\u5584LLM\u54cd\u5e94\u8d28\u91cf\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u673a\u5236\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86LLM\u5bf9\u8bdd\u5bf9\u9f50\u6280\u672f\uff0c\u4e3a\u6539\u5584LLM\u54cd\u5e94\u8d28\u91cf\u63d0\u4f9b\u4e86\u5206\u7c7b\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u63a8\u7406\u65f6\u65b9\u6cd5\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.21860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21860", "abs": "https://arxiv.org/abs/2511.21860", "authors": ["Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Claudio Pinhanez", "Yago Primerano"], "title": "Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices", "comment": null, "summary": "In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.", "AI": {"tldr": "CoRA\u662f\u4e00\u79cd\u6539\u8fdbLLM\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u5206\u53ef\u9760\u6027\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u56de\u7b54\u4e00\u81f4\u6027\u6765\u8c03\u6574\u539f\u59cbMCQA\u5206\u6570\u3002", "motivation": "\u73b0\u6709LLM\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u5206\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5373\u4f7f\u83b7\u5f97\u9ad8\u5206\uff0c\u6a21\u578b\u4e5f\u53ef\u80fd\u8868\u73b0\u51fa\u56de\u7b54\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u53cd\u6620\u6a21\u578b\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u63d0\u51faCoRA\u6307\u6807\uff0c\u901a\u8fc7\u751f\u6210\u4fee\u6539\u7b54\u6848\u9009\u9879\u7684\u5408\u6210\u95ee\u9898\u6765\u8bc4\u4f30LLM\u56de\u7b54\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u4e24\u4e2a\u4e2d\u95f4\u5206\u6570BMCA\uff08\u6700\u4f4e\u4e00\u81f4\u6027\u51c6\u786e\u7387\uff09\u548cCI\uff08\u4e00\u81f4\u6027\u6307\u6570\uff09\uff0c\u57fa\u4e8e\u6a21\u578b\u4e00\u81f4\u6027\u6c34\u5e73\u8c03\u6574\u539f\u59cbMCQA\u5206\u6570\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cdLLM\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cLLM\u5373\u4f7f\u83b7\u5f97\u9ad8MCQA\u5206\u6570\u4e5f\u53ef\u80fd\u8868\u73b0\u51fa\u4f4e\u56de\u7b54\u4e00\u81f4\u6027\uff0c\u800cCoRA\u80fd\u6210\u529f\u964d\u4f4e\u4e0d\u4e00\u81f4\u6a21\u578b\u7684\u5206\u6570\u3002", "conclusion": "CoRA\u6307\u6807\u80fd\u66f4\u53ef\u9760\u5730\u8bc4\u4f30LLM\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u8003\u8651\u56de\u7b54\u4e00\u81f4\u6027\u6765\u6539\u8fdb\u73b0\u6709\u8bc4\u5206\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.22210", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22210", "abs": "https://arxiv.org/abs/2511.22210", "authors": ["Junsung Park"], "title": "BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning", "comment": "8 pages, 3 figures", "summary": "Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.", "AI": {"tldr": "BiCQL-ML\u662f\u4e00\u79cd\u65e0\u7b56\u7565\u7684\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u6846\u67b6\u8054\u5408\u4f18\u5316\u5956\u52b1\u51fd\u6570\u548c\u4fdd\u5b88Q\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\uff0c\u5728\u6807\u51c6\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u4ec5\u4f7f\u7528\u56fa\u5b9a\u6f14\u793a\u6570\u636e\u6062\u590d\u89e3\u91ca\u4e13\u5bb6\u884c\u4e3a\u7684\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u5728\u7ebf\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\uff0cBiCQL-ML\u65e8\u5728\u907f\u514d\u8fd9\u4e00\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u5956\u52b1\u6062\u590d\u548c\u4e0b\u6e38\u7b56\u7565\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6846\u67b6\uff1a1) \u5728\u5f53\u524d\u5956\u52b1\u4e0b\u901a\u8fc7\u4fdd\u5b88Q\u5b66\u4e60(CQL)\u5b66\u4e60\u4fdd\u5b88Q\u51fd\u6570\uff1b2) \u66f4\u65b0\u5956\u52b1\u53c2\u6570\u4ee5\u6700\u5927\u5316\u4e13\u5bb6\u52a8\u4f5c\u7684\u671f\u671bQ\u503c\uff0c\u540c\u65f6\u6291\u5236\u5bf9\u5206\u5e03\u5916\u52a8\u4f5c\u7684\u8fc7\u5ea6\u6cdb\u5316\u3002\u8be5\u65b9\u6cd5\u53ef\u89c6\u4e3a\u8f6f\u503c\u5339\u914d\u539f\u5219\u4e0b\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1BiCQL-ML\u6536\u655b\u5230\u4f7f\u4e13\u5bb6\u7b56\u7565\u8f6f\u6700\u4f18\u7684\u5956\u52b1\u51fd\u6570\u3002\u5728\u6807\u51c6\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u79bb\u7ebfIRL\u57fa\u7ebf\uff0cBiCQL-ML\u5728\u5956\u52b1\u6062\u590d\u548c\u4e0b\u6e38\u7b56\u7565\u6027\u80fd\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "BiCQL-ML\u901a\u8fc7\u65e0\u7b56\u7565\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.22277", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22277", "abs": "https://arxiv.org/abs/2511.22277", "authors": ["Henrijs Princis", "Arindam Sharma", "Cristina David"], "title": "TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation", "comment": null, "summary": "Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.", "AI": {"tldr": "TreeCoder\u662f\u4e00\u4e2a\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u7684\u901a\u7528\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u641c\u7d22\u548c\u7ea6\u675f\u51fd\u6570\u786e\u4fdd\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u800c\u975e\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u7684\u4ee3\u7801\u7ecf\u5e38\u8fdd\u53cd\u8bed\u6cd5\u6216\u8bed\u4e49\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u4ee3\u7801\u6b63\u786e\u6027", "method": "TreeCoder\u5c06\u89e3\u7801\u8fc7\u7a0b\u8868\u793a\u4e3a\u5019\u9009\u7a0b\u5e8f\u7684\u6811\u641c\u7d22\uff0c\u5c06\u89e3\u7801\u7b56\u7565\u548c\u7ea6\u675f\u51fd\u6570\uff08\u5982\u98ce\u683c\u3001\u8bed\u6cd5\u3001\u6267\u884c\uff09\u4f5c\u4e3a\u53ef\u4f18\u5316\u7684\u7ec4\u4ef6\uff0c\u652f\u6301\u7cfb\u7edf\u63a2\u7d22\u548c\u81ea\u52a8\u8c03\u4f18", "result": "\u5728MBPP\uff08Python\uff09\u548cSQL-Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTreeCoder\u663e\u8457\u63d0\u9ad8\u4e86CodeLlama\u3001Mistral\u548cDeepSeek\u7b49\u5f00\u6e90\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u901a\u5e38\u4ee5\u8f83\u5927\u4f18\u52bf\u8d85\u8d8a\u65e0\u7ea6\u675f\u57fa\u7ebf", "conclusion": "TreeCoder\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u6b63\u786e\u6027\u548c\u7ed3\u6784\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LLM\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027", "topic": "code agent"}}
{"id": "2511.22176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22176", "abs": "https://arxiv.org/abs/2511.22176", "authors": ["Lukas Struppek", "Dominik Hintersdorf", "Hannah Struppek", "Daniel Neider", "Kristian Kersting"], "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information", "comment": null, "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.", "AI": {"tldr": "F-CoT\u901a\u8fc7\u7ed3\u6784\u5316\u8f93\u5165\u5206\u79bb\u4fe1\u606f\u63d0\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\uff0c\u51cf\u5c112-3\u500d\u751f\u6210token\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u7387", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\uff08\u5982\u601d\u7ef4\u94fe\uff09\u901a\u5e38\u751f\u6210\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4token\u4f7f\u7528\u8fc7\u591a\u548c\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3002\u73b0\u6709\u6548\u7387\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u4e2d\u5fc3\u5e72\u9884\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u6216\u76d1\u7763\u5fae\u8c03\uff09\uff0c\u672c\u6587\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u3001\u8f93\u5165\u4e2d\u5fc3\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u53d7\u8ba4\u77e5\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u63d0\u51faFocused Chain-of-Thought (F-CoT)\uff0c\u5c06\u4fe1\u606f\u63d0\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\u3002\u9996\u5148\u4ece\u67e5\u8be2\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u7ec4\u7ec7\u6210\u7b80\u6d01\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5f15\u5bfc\u6a21\u578b\u4ec5\u57fa\u4e8e\u8be5\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u7406\uff0c\u907f\u514d\u5173\u6ce8\u65e0\u5173\u7ec6\u8282\uff0c\u81ea\u7136\u4ea7\u751f\u66f4\u77ed\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u7b97\u672f\u6587\u5b57\u9898\u4e0a\uff0cF-CoT\u5c06\u751f\u6210\u7684token\u51cf\u5c112-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u96f6\u6837\u672cCoT\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u8f93\u5165\u662f\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u6760\u6746\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2511.22406", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22406", "abs": "https://arxiv.org/abs/2511.22406", "authors": ["Roland Stolz", "Michael Eichelbeck", "Matthias Althoff"], "title": "Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions", "comment": "Accepted at the AAAI26 conference main technical track", "summary": "In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.", "AI": {"tldr": "\u63d0\u51fa\u9ad8\u6548\u6570\u503c\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u52a8\u4f5c\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u622a\u65ad\u6b63\u6001\u5206\u5e03\u71b5\u3001\u5bf9\u6570\u6982\u7387\u7b49\u5173\u952e\u7279\u5f81\u8ba1\u7b97\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u52a8\u4f5c\u7ea6\u675fRL\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u65f6\uff0c\u622a\u65ad\u6b63\u6001\u5206\u5e03\u7684\u5173\u952e\u7279\u5f81\uff08\u71b5\u3001\u5bf9\u6570\u6982\u7387\u53ca\u5176\u68af\u5ea6\uff09\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u73b0\u6709\u8fd1\u4f3c\u65b9\u6cd5\u4e25\u91cd\u964d\u4f4e\u6027\u80fd", "method": "\u63d0\u51fa\u9ad8\u6548\u6570\u503c\u8fd1\u4f3c\u65b9\u6cd5\u8ba1\u7b97\u622a\u65ad\u6b63\u6001\u5206\u5e03\u7684\u71b5\u3001\u5bf9\u6570\u6982\u7387\u53ca\u5176\u68af\u5ea6\uff0c\u5e76\u63d0\u4f9b\u622a\u65ad\u7b56\u7565\u5206\u5e03\u7684\u9ad8\u6548\u91c7\u6837\u7b56\u7565", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u4f7f\u7528\u51c6\u786e\u4f30\u8ba1\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u8fd1\u4f3c\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u5728\u52a8\u4f5c\u7ea6\u675fRL\u4e2d\u51c6\u786e\u4f30\u8ba1\u622a\u65ad\u5206\u5e03\u7684\u5173\u952e\u7279\u5f81\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u9ad8\u6548\u6570\u503c\u8fd1\u4f3c\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8ba1\u7b97\u96be\u9898\u5e76\u63d0\u5347\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2511.22598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22598", "abs": "https://arxiv.org/abs/2511.22598", "authors": ["Huanyu Li", "Zongyuan Li", "Wei Huang", "Xian Guo"], "title": "LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system", "comment": "8 pages, 5 figures, ICICN 2025", "summary": "Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.", "AI": {"tldr": "LLM-Cave\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e8f\u5217\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u590d\u6742\u73af\u5883\u66f4\u9ad8\u6548\uff0c\u5b9e\u9a8c\u663e\u793a\u7ed3\u6784\u5316\u591a\u6b65\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u9650\u4e8e\u5355\u6b65\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u7684\u5e8f\u5217\u51b3\u7b56\u73af\u5883\uff08\u5982TextStarCraftII\u548cLLM-PySC2\uff09\u8fc7\u4e8e\u590d\u6742\uff0c\u9700\u8981\u6570\u5c0f\u65f6\u624d\u80fd\u5b8c\u6210\u4e00\u6b21\u6e38\u620f\uff0c\u7f3a\u4e4f\u8f7b\u91cf\u9ad8\u6548\u7684\u8bc4\u4f30\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86LLM-Cave\u57fa\u51c6\u6d4b\u8bd5\u548c\u8f7b\u91cf\u73af\u5883\uff0c\u8fd9\u662f\u4e00\u4e2a\u7b26\u53f7\u4e3b\u4e49\u65f6\u4ee3\u7684\u7ecf\u5178\u5b9e\u4f8b\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u90e8\u5206\u53ef\u89c2\u6d4b\u72b6\u6001\u4fe1\u606f\u63a8\u7406\u9644\u8fd1\u5371\u9669\u6765\u63a2\u7d22\u73af\u5883\u5e76\u907f\u514d\u6f5c\u5728\u635f\u5931\u3002\u8bc4\u4f30\u4e86GPT-4o-mini\u3001o1-mini\u3001DeepSeek-R1\u7b49\u4e3b\u6d41LLM\u7684\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u3001\u51b3\u7b56\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "Deepseek-R1\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u6210\u529f\u7387\uff0c\u800c\u8f83\u5c0f\u7684\u6a21\u578b\u59824o-mini\u901a\u8fc7\u91c7\u7528Chain of Speculation\u548cPlanner-Critic\u7b56\u7565\uff0c\u5728\u6311\u6218\u4efb\u52a1\u4e0a\u663e\u8457\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46\u727a\u7272\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u6b65\u63a8\u7406\u7ed3\u5408\u57fa\u4e8eLLM\u7684\u53cd\u9988\u673a\u5236\u80fd\u663e\u8457\u589e\u5f3aLLM\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb\u8f83\u5f31\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5e76\u5efa\u8bae\u4e86\u65b0\u7684\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684LLM\u8bc4\u4f30\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2511.22904", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22904", "abs": "https://arxiv.org/abs/2511.22904", "authors": ["Anh Nguyen", "Stefan Lee"], "title": "Language-conditioned world model improves policy generalization by reading environmental descriptions", "comment": "NeuRIPS 2025. Workshop: LAW 2025: Bridging Language, Agent, and World Models", "summary": "To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying \"what to do\". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.", "AI": {"tldr": "\u63d0\u51faLED-WM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u4e16\u754c\u6a21\u578b\u63d0\u5347\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u8bc1\u660e\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8981\u4e48\u4f9d\u8d56\u9650\u5236\u6027\u5047\u8bbe\uff08\u5982\u5bb9\u5fcd\u63a8\u7406\u65f6\u89c4\u5212\u5ef6\u8fdf\u6216\u9700\u8981\u4e13\u5bb6\u6f14\u793a\uff09\u3002\u9700\u8981\u6539\u8fdb\u8bed\u8a00\u6761\u4ef6\u5316\u4e16\u754c\u6a21\u578b\u7684\u7b56\u7565\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u653e\u5f03\u8fd9\u4e9b\u5047\u8bbe\u3002", "method": "\u57fa\u4e8eDreamerV3\u6784\u5efa\u8bed\u8a00\u611f\u77e5\u7f16\u7801\u5668\u4e16\u754c\u6a21\u578b\uff08LED-WM\uff09\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u5c06\u8bed\u8a00\u63cf\u8ff0\u663e\u5f0f\u5730\u951a\u5b9a\u5230\u89c2\u5bdf\u4e2d\u7684\u5b9e\u4f53\u3002\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u8bad\u7ec3\u8bed\u8a00\u6761\u4ef6\u5316\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u4ece\u8be5\u6a21\u578b\u5b66\u4e60\u7b56\u7565\uff0c\u65e0\u9700\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a\u3002", "result": "\u5728MESSENGER\u548cMESSENGER-WM\u4e24\u4e2a\u73af\u5883\u4e2d\uff0cLED-WM\u8bad\u7ec3\u7684\u7b56\u7565\u76f8\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7531\u65b0\u52a8\u6001\u548c\u8bed\u8a00\u63cf\u8ff0\u7684\u672a\u89c1\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd8\u5c55\u793a\u4e86\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6d4b\u8bd5\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\u6765\u6539\u8fdb\u3002", "conclusion": "LED-WM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a\uff0c\u4e3a\u8bed\u8a00\u6761\u4ef6\u5316\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.22651", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22651", "abs": "https://arxiv.org/abs/2511.22651", "authors": ["Anthony Carreon", "Vansh Sharma", "Venkat Raman"], "title": "Automated Design Optimization via Strategic Search with Large Language Models", "comment": "14 pages, 5 tables, 7 figures, preprint", "summary": "Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \\$159 per run, compared to an estimated cost of up to \\$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.", "AI": {"tldr": "AUTO\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u8bbe\u8ba1\u4f18\u5316\u89c6\u4e3a\u65e0\u68af\u5ea6\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u7b56\u7565\u6027LLM\u63a8\u7406\u6307\u5bfc\uff0c\u5728GPU\u4ee3\u7801\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5728\u660e\u786e\u5b9a\u4e49\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bbe\u8ba1\u53c2\u6570\u96be\u4ee5\u5b9a\u4e49\u7684\u8bbe\u8ba1\u95ee\u9898\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002LLM\u901a\u8fc7\u52a8\u6001\u89e3\u91ca\u8bbe\u8ba1\u7a7a\u95f4\u548c\u5229\u7528\u7f16\u7801\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u4e3a\u8fd9\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "AUTO\u662f\u4e00\u4e2aLLM\u4ee3\u7406\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u4e2a\u534f\u4f5c\u4ee3\u7406\uff1a\u7b56\u7565\u5e08\uff08Strategist\uff09\u5728\u63a2\u7d22\u548c\u5229\u7528\u7b56\u7565\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\uff0c\u5b9e\u65bd\u8005\uff08Implementor\uff09\u6267\u884c\u8be6\u7ec6\u8bbe\u8ba1\u3002\u8be5\u6846\u67b6\u5c06\u8bbe\u8ba1\u4f18\u5316\u89c6\u4e3a\u7531\u6218\u7565\u6027LLM\u63a8\u7406\u6307\u5bfc\u7684\u65e0\u68af\u5ea6\u641c\u7d22\u95ee\u9898\u3002", "result": "\u5728GPU\u4ee3\u7801\u4f18\u5316\u9886\u57df\uff0cAUTO\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u4e13\u5bb6\u5b9e\u73b0\u76f8\u5f53\uff0c\u5728\u5316\u5b66\u52a8\u529b\u5b66\u79ef\u5206\u548c\u7a20\u5bc6\u77e9\u9635\u4e58\u6cd5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u76f8\u5bf9\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8650-70%\u7684\u641c\u7d22\u6548\u7387\u3002\u4f18\u5316\u8fc7\u7a0b\u7ea6\u97008\u5c0f\u65f6\uff0c\u6bcf\u6b21\u8fd0\u884c\u6210\u672c\u4f30\u8ba1\u6700\u9ad8159\u7f8e\u5143\uff0c\u800c\u4e2d\u7b49\u5de5\u8d44\u8f6f\u4ef6\u5f00\u53d1\u8005\u4f30\u8ba1\u6210\u672c\u6700\u9ad8480\u7f8e\u5143\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5728\u6709\u9650\u5148\u9a8c\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u52a8\u5316\u5904\u7406\u5b9a\u4e49\u4e0d\u660e\u786e\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\u6253\u5f00\u4e86\u5927\u95e8\u3002", "topic": "code agent"}}
{"id": "2511.22749", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22749", "abs": "https://arxiv.org/abs/2511.22749", "authors": ["Zeng Wang", "Weihua Xiao", "Minghao Shao", "Raghu Vamshi Hemadri", "Ozgur Sinanoglu", "Muhammad Shafique", "Ramesh Karri"], "title": "VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization", "comment": null, "summary": "Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.", "AI": {"tldr": "VeriDispatcher\u662f\u4e00\u4e2a\u591aLLM RTL\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u63a8\u7406\u96be\u5ea6\u9884\u6d4b\u5c06\u4efb\u52a1\u5206\u6d3e\u7ed9\u5408\u9002\u7684LLM\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5546\u4e1aAPI\u8c03\u7528\u6210\u672c\u3002", "motivation": "\u4e0d\u540cLLM\u5728RTL\u751f\u6210\u4efb\u52a1\u4e0a\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u7684\u63d0\u793a\u6216\u5fae\u8c03\u3002\u5982\u4f55\u534f\u8c03\u591a\u4e2a\u4e0d\u540cLLM\u4ee5\u5171\u540c\u63d0\u9ad8RTL\u8d28\u91cf\u540c\u65f6\u964d\u4f4e\u6210\u672c\uff0c\u800c\u4e0d\u662f\u8fd0\u884c\u6240\u6709\u6a21\u578b\u5e76\u9009\u62e9\u6700\u4f73\u8f93\u51fa\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faVeriDispatcher\u6846\u67b6\uff1a1) \u57fa\u4e8e\u4efb\u52a1\u63cf\u8ff0\u7684\u8bed\u4e49\u5d4c\u5165\u8bad\u7ec3\u7d27\u51d1\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u7ed3\u5408\u8bed\u6cd5\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u7684\u96be\u5ea6\u8bc4\u5206\uff1b2) \u5728\u63a8\u7406\u65f6\u4f7f\u7528\u8fd9\u4e9b\u9884\u6d4b\u5668\u5c06\u4efb\u52a1\u8def\u7531\u5230\u9009\u5b9a\u7684LLM\u5b50\u96c6\u3002", "result": "\u5728RTLLM\u548cVerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVeriDispatcher\u572810\u4e2a\u4e0d\u540cLLM\u4e0a\u5b9e\u73b0\u4e86\uff1aRTLLM\u4e0a\u51c6\u786e\u7387\u63d0\u534718%\u4e14\u4ec5\u4f7f\u752840%\u7684\u5546\u4e1a\u8c03\u7528\uff1bVerilogEval\u4e0a\u4fdd\u6301\u51c6\u786e\u7387\u540c\u65f6\u51cf\u5c1125%\u7684\u5546\u4e1a\u4f7f\u7528\u3002", "conclusion": "VeriDispatcher\u80fd\u591f\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u9ad8\u3001\u9ad8\u8d28\u91cf\u7684LLM\u90e8\u7f72\uff0c\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u5206\u6d3e\u5145\u5206\u5229\u7528\u4e0d\u540cLLM\u7684\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2511.22751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22751", "abs": "https://arxiv.org/abs/2511.22751", "authors": ["Hristo Papazov", "Francesco D'Angelo", "Nicolas Flammarion"], "title": "Exact Learning of Arithmetic with Differentiable Agents", "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI", "summary": "We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \\href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u5fae\u6709\u9650\u72b6\u6001\u8f6c\u6362\u5668\uff08DFST\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u786e\u7b97\u6cd5\u5b66\u4e60\uff0c\u5728\u7b97\u672f\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u957f\u5ea6\u6cdb\u5316", "motivation": "\u63a2\u7d22\u4f7f\u7528\u68af\u5ea6\u65b9\u6cd5\u8fdb\u884c\u7cbe\u786e\u7b97\u6cd5\u5b66\u4e60\u7684\u53ef\u80fd\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u67b6\u6784\u5728\u957f\u5ea6\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u5f15\u5165\u53ef\u5fae\u6709\u9650\u72b6\u6001\u8f6c\u6362\u5668\uff08DFST\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u56fe\u7075\u5b8c\u5907\u7684\u6a21\u578b\u5bb6\u65cf\uff0c\u652f\u6301\u6052\u5b9a\u7cbe\u5ea6\u3001\u6052\u5b9a\u65f6\u95f4\u751f\u6210\u548c\u7aef\u5230\u7aef\u5bf9\u6570\u5e76\u884c\u53ef\u5fae\u8bad\u7ec3\uff0c\u5229\u7528\u4e13\u5bb6\u667a\u80fd\u4f53\u7684\u7b56\u7565\u8f68\u8ff9\u89c2\u5bdf\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u4e8c\u8fdb\u5236\u548c\u5341\u8fdb\u5236\u52a0\u51cf\u4e58\u9664\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u5728\u6781\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u65e0\u9519\u8bef\u5730\u6cdb\u5316\u5230\u6bd4\u8bad\u7ec3\u6837\u672c\u957f\u6570\u5343\u500d\u7684\u8f93\u5165", "conclusion": "\u5728\u7ed3\u6784\u5316\u4e2d\u95f4\u76d1\u7763\u4e0b\u8bad\u7ec3\u53ef\u5fae\u667a\u80fd\u4f53\uff0c\u53ef\u80fd\u4e3a\u57fa\u4e8e\u68af\u5ea6\u7684\u7cbe\u786e\u7b97\u6cd5\u6280\u80fd\u5b66\u4e60\u5f00\u8f9f\u65b0\u9014\u5f84", "topic": "agent analysis"}}
{"id": "2511.23271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23271", "abs": "https://arxiv.org/abs/2511.23271", "authors": ["Jiancheng Dong", "Pengyue Jia", "Jingyu Peng", "Maolin Wang", "Yuhao Wang", "Lixin Su", "Xin Sun", "Shuaiqiang Wang", "Dawei Yin", "Xiangyu Zhao"], "title": "Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs", "comment": "15 pages, 5 figures", "summary": "Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u5197\u957f\u7684\u7cfb\u7edf\u63d0\u793a\u538b\u7f29\u4e3a\u5355\u4e2a\u884c\u4e3a\u7b49\u6548\u6807\u8bb0[BE]\uff0c\u5b9e\u73b03000\u500d\u957f\u5ea6\u538b\u7f29\u540c\u65f6\u4fdd\u630198%\u4e0b\u6e38\u6027\u80fd", "motivation": "\u957f\u7cfb\u7edf\u63d0\u793a\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u3001\u8ba1\u7b97\u6210\u672c\u4e0a\u5347\u548c\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u51cf\u5c11\uff0c\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\u6765\u4fdd\u6301\u884c\u4e3a\u6548\u679c\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6807\u8bb0\u6570\u91cf", "method": "\u8f7b\u91cf\u7ea7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1) \u901a\u8fc7\u91cd\u6784\u8bad\u7ec3[BE]\u7f16\u7801\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u7684\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\uff1b2) \u5c06\u63d0\u793a\u7684\u4e0b\u6e38\u884c\u4e3a\u84b8\u998f\u5230\u8be5\u5355\u4e2a\u6807\u8bb0\u4e2d\uff1b\u65e0\u9700\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\u3001\u8f85\u52a9\u538b\u7f29\u6a21\u578b\u6216\u6807\u6ce8\u54cd\u5e94", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5355\u4e2a[BE]\u6807\u8bb0\u5b9e\u73b0\u9ad8\u8fbe3000\u500d\u7684\u63d0\u793a\u957f\u5ea6\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u7ea698%\u7684\u4e0b\u6e38\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u51e0\u4e4e\u5c06\u6574\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7559\u7ed9\u7528\u6237\u8f93\u5165\uff0c\u4e3a\u7cfb\u7edf\u63d0\u793a\u538b\u7f29\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2511.23281", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23281", "abs": "https://arxiv.org/abs/2511.23281", "authors": ["Aaron Steiner", "Ralph Peeters", "Christian Bizer"], "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)", "comment": null, "summary": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.\n  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.", "AI": {"tldr": "\u6bd4\u8f83\u56db\u79cd\u7f51\u9875\u4ea4\u4e92\u63a5\u53e3\uff08HTML\u3001RAG\u3001MCP\u3001NLWeb\uff09\u5728LLM\u4ee3\u7406\u81ea\u52a8\u5316\u7f51\u9875\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RAG\u3001MCP\u548cNLWeb\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edfHTML\u6d4f\u89c8", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u4e0e\u7f51\u7ad9\u4ea4\u4e92\u6709\u591a\u79cd\u63a5\u53e3\u65b9\u5f0f\uff08HTML\u6d4f\u89c8\u3001RAG\u3001MCP\u3001NLWeb\uff09\uff0c\u4f46\u7f3a\u4e4f\u5728\u7edf\u4e00\u63a7\u5236\u73af\u5883\u4e0b\u5bf9\u8fd9\u4e9b\u67b6\u6784\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b\u56db\u4e2a\u6a21\u62df\u7535\u5546\u7f51\u7ad9\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u6bcf\u4e2a\u7f51\u7ad9\u63d0\u4f9bHTML\u3001MCP\u548cNLWeb\u63a5\u53e3\uff0c\u4e3a\u6bcf\u79cd\u63a5\u53e3\u5f00\u53d1\u4e13\u95e8\u7684\u4ee3\u7406\u6267\u884c\u76f8\u540c\u4efb\u52a1\u96c6\uff0c\u4f7f\u7528GPT 4.1\u3001GPT 5\u3001GPT 5 mini\u548cClaude Sonnet 4\u4f5c\u4e3a\u5e95\u5c42LLM\u8fdb\u884c\u8bc4\u4f30", "result": "RAG\u3001MCP\u548cNLWeb\u4ee3\u7406\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8eHTML\u4ee3\u7406\uff1a\u5e73\u5747F1\u4ece0.67\u63d0\u5347\u52300.75-0.77\uff0ctoken\u4f7f\u7528\u91cf\u4ece241k\u964d\u81f347k-140k\uff0c\u8fd0\u884c\u65f6\u95f4\u4ece291\u79d2\u964d\u81f350-62\u79d2\u3002\u6700\u4f73\u914d\u7f6e\u662fRAG+GPT 5\uff08F1=0.87\uff0c\u5b8c\u6210\u7387=0.79\uff09", "conclusion": "\u4ea4\u4e92\u63a5\u53e3\u7684\u9009\u62e9\u5bf9\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\u7684\u6548\u679c\u548c\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0cRAG\u3001MCP\u548cNLWeb\u67b6\u6784\u4f18\u4e8e\u4f20\u7edfHTML\u6d4f\u89c8\uff0c\u5176\u4e2dRAG+GPT 5\u8868\u73b0\u6700\u4f73\uff0c\u800cRAG+GPT 5 mini\u5728\u6210\u672c\u548c\u6027\u80fd\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u5e73\u8861", "topic": "agent analysis"}}
{"id": "2511.23397", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.23397", "abs": "https://arxiv.org/abs/2511.23397", "authors": ["Mahdi Rahmani", "AmirHossein Saffari", "Reyhane Rahmani"], "title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation", "comment": "6 pages, 11 figures, 2 tables", "summary": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet", "AI": {"tldr": "MegaChat\u662f\u9996\u4e2a\u5b8c\u5168\u5408\u6210\u7684\u6ce2\u65af\u8bed\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30Telegram\u7535\u5546\u4e2d\u7684\u667a\u80fd\u9500\u552e\u804a\u5929\u673a\u5668\u4eba\u3002\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u81ea\u52a8\u751f\u6210\u4eba\u7269\u611f\u77e5\u7684\u95ee\u7b54\u5bf9\uff0c\u76f8\u6bd4\u4f20\u7edfRAG\u6a21\u578b\u57284/5\u6e20\u9053\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f0a\u6717\u4e2d\u5c0f\u4f01\u4e1a\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528Telegram\u8fdb\u884c\u9500\u552e\uff0c\u5b9e\u65f6\u4e92\u52a8\u5bf9\u8f6c\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f46\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u5f00\u53d1AI\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u8fd9\u901a\u5e38\u6210\u672c\u9ad8\u6602\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4ece\u6d3b\u8dc3\u7684Telegram\u8d2d\u7269\u9891\u9053\u6536\u96c6\u6570\u636e\uff0c\u751f\u6210\u4eba\u7269\u611f\u77e5\u7684\u95ee\u7b54\u5bf9\u3002\u7cfb\u7edf\u5305\u542b\u4e13\u95e8\u7684\u95ee\u9898\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u667a\u80fd\u4f53\uff0c\u786e\u4fdd\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5bf9\u8bdd\u6570\u636e\u3002\u8bc4\u4f30\u65f6\u6bd4\u8f83\u4e86\u4e09\u79cd\u7ecf\u5178RAG\u6a21\u578b\u4e0e\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5305\u542b\u591a\u67e5\u8be2\u68c0\u7d22\u3001\u91cd\u6392\u5e8f\u548c\u4eba\u7269\u5bf9\u9f50\u54cd\u5e94\u5408\u6210\uff09\u3002", "result": "\u4f7f\u7528GPT-5.1\u5728\u516d\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u667a\u80fd\u4f53\u67b6\u6784\u57285\u4e2a\u4e0d\u540c\u6e20\u9053\u4e2d\u76844\u4e2a\u4e0a\u4f18\u4e8e\u4f20\u7edfRAG\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6216\u590d\u6742\u5fae\u8c03\u5373\u53ef\u751f\u6210\u53ef\u6269\u5c55\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002", "conclusion": "MegaChat\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4e86\u5728\u4e13\u4e1a\u5546\u4e1a\u9886\u57df\u6784\u5efa\u667a\u80fd\u5ba2\u6237\u4e92\u52a8\u7cfb\u7edf\u7684\u9ad8\u6548\u3001\u7ecf\u6d4e\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u591a\u8bed\u8a00\u5bf9\u8bddAI\u7684\u8fdb\u6b65\u3002", "topic": "code agent"}}
{"id": "2511.23473", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23473", "abs": "https://arxiv.org/abs/2511.23473", "authors": ["Yiping Wang", "Shao-Rong Su", "Zhiyuan Zeng", "Eva Xu", "Liliang Ren", "Xinyu Yang", "Zeyi Huang", "Xuehai He", "Luyao Ma", "Baolin Peng", "Hao Cheng", "Pengcheng He", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "ThetaEvolve: Test-time Learning on Open Problems", "comment": "30 pages, link: https://github.com/ypwang61/ThetaEvolve", "summary": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve", "AI": {"tldr": "ThetaEvolve\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7b80\u5316\u5e76\u6269\u5c55\u4e86AlphaEvolve\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u6d4b\u8bd5\u65f6\u6301\u7eed\u5b66\u4e60\uff0c\u4f7f\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u80fd\u5728\u5f00\u653e\u4f18\u5316\u95ee\u9898\u4e0a\u53d6\u5f97\u65b0\u7684\u6700\u4f73\u8fb9\u754c\u3002", "motivation": "AlphaEvolve\u867d\u7136\u53d6\u5f97\u4e86\u6570\u5b66\u53d1\u73b0\u7a81\u7834\uff0c\u4f46\u5b83\u662f\u95ed\u6e90\u7cfb\u7edf\uff0c\u4f9d\u8d56\u524d\u6cbfLLM\u96c6\u6210\uff0c\u4e14\u662f\u7eaf\u63a8\u7406\u7cfb\u7edf\u65e0\u6cd5\u5185\u5316\u6f14\u5316\u7b56\u7565\u3002\u9700\u8981\u5f00\u6e90\u6846\u67b6\u8ba9\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u5728\u5f00\u653e\u95ee\u9898\u4e0a\u53d6\u5f97\u7a81\u7834\u3002", "method": "ThetaEvolve\u91c7\u7528\u5355\u4e00LLM\u3001\u5927\u578b\u7a0b\u5e8f\u6570\u636e\u5e93\u589e\u5f3a\u63a2\u7d22\u3001\u6279\u91cf\u91c7\u6837\u63d0\u9ad8\u541e\u5410\u91cf\u3001\u60f0\u6027\u60e9\u7f5a\u907f\u514d\u505c\u6ede\u8f93\u51fa\u3001\u53ef\u9009\u5956\u52b1\u5851\u5f62\u63d0\u4f9b\u7a33\u5b9a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "ThetaEvolve\u9996\u6b21\u4f7f\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek-R1-0528-Qwen3-8B\uff09\u5728AlphaEvolve\u63d0\u5230\u7684\u5f00\u653e\u95ee\u9898\uff08\u5706\u5806\u79ef\u548c\u81ea\u76f8\u5173\u4e0d\u7b49\u5f0f\uff09\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u8fb9\u754c\u3002\u57284\u4e2a\u5f00\u653e\u4efb\u52a1\u548c2\u4e2a\u6a21\u578b\u4e0a\uff0c\u6d4b\u8bd5\u65f6RL\u6301\u7eed\u4f18\u4e8e\u7eaf\u63a8\u7406\u57fa\u7ebf\u3002", "conclusion": "ThetaEvolve\u8bc1\u660e\u4e86\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u6f14\u5316\u7b56\u7565\uff0c\u80fd\u5728\u5f00\u653e\u4f18\u5316\u95ee\u9898\u4e0a\u53d6\u5f97\u7a81\u7834\uff0cRL\u8bad\u7ec3\u7684\u68c0\u67e5\u70b9\u5728\u76ee\u6807\u4efb\u52a1\u548c\u672a\u89c1\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u66f4\u597d\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u771f\u6b63\u5b66\u5230\u4e86\u6f14\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.23442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23442", "abs": "https://arxiv.org/abs/2511.23442", "authors": ["Hang Yu", "Di Zhang", "Qiwei Du", "Yanping Zhao", "Hai Zhang", "Guang Chen", "Eduardo E. Veas", "Junqiao Zhao"], "title": "ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.", "AI": {"tldr": "ASTRO\u662f\u4e00\u4e2a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5206\u5e03\u65b0\u9896\u4e14\u7b26\u5408\u52a8\u6001\u7ea6\u675f\u7684\u8f68\u8ff9\u6765\u89e3\u51b3\u5b50\u4f18\u548c\u788e\u7247\u5316\u6570\u636e\u96c6\u4e2d\u7684\u5956\u52b1\u4f20\u64ad\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ece\u9884\u6536\u96c6\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u4f46\u5b50\u4f18\u548c\u788e\u7247\u5316\u8f68\u8ff9\u5bfc\u81f4\u5956\u52b1\u4f20\u64ad\u56f0\u96be\uff0c\u9020\u6210\u4ef7\u503c\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u8f68\u8ff9\u62fc\u63a5\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u884c\u4e3a\u7b56\u7565\u652f\u6301\u96c6\uff0c\u8981\u4e48\u8fdd\u53cd\u5e95\u5c42\u52a8\u6001\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u6548\u679c\u3002", "method": "ASTRO\u9996\u5148\u5b66\u4e60\u65f6\u95f4\u8ddd\u79bb\u8868\u793a\u6765\u8bc6\u522b\u4e0d\u540c\u4e14\u53ef\u8fbe\u7684\u62fc\u63a5\u76ee\u6807\uff0c\u7136\u540e\u91c7\u7528\u52a8\u6001\u5f15\u5bfc\u7684\u62fc\u63a5\u89c4\u5212\u5668\uff0c\u901a\u8fc7Rollout Deviation Feedback\uff08\u76ee\u6807\u72b6\u6001\u5e8f\u5217\u4e0e\u5b9e\u9645\u5230\u8fbe\u72b6\u6001\u5e8f\u5217\u4e4b\u95f4\u7684\u5dee\u8ddd\uff09\u81ea\u9002\u5e94\u751f\u6210\u8fde\u63a5\u52a8\u4f5c\u5e8f\u5217\uff0c\u63d0\u9ad8\u8f68\u8ff9\u62fc\u63a5\u7684\u53ef\u884c\u6027\u548c\u53ef\u8fbe\u6027\u3002", "result": "ASTRO\u5728OGBench\u5957\u4ef6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebfRL\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728D4RL\u7b49\u6807\u51c6\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ASTRO\u901a\u8fc7\u751f\u6210\u5206\u5e03\u65b0\u9896\u4e14\u7b26\u5408\u52a8\u6001\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b50\u4f18\u548c\u788e\u7247\u5316\u6570\u636e\u96c6\u5e26\u6765\u7684\u6311\u6218\uff0c\u4e3a\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u589e\u5f3a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.23315", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23315", "abs": "https://arxiv.org/abs/2511.23315", "authors": ["Azusa Yamaguchi"], "title": "Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning", "comment": "22 pages, 19 figures", "summary": "A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u63ed\u793a\u4e86\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e09\u9636\u6bb5\u7ed3\u6784\uff1a\u534f\u8c03\u7a33\u5b9a\u76f8\u3001\u8106\u5f31\u8fc7\u6e21\u533a\u548c\u963b\u585e\u65e0\u5e8f\u76f8\uff0c\u7531\u6838\u6f02\u79fb\u548c\u540c\u6b65\u7684\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u3002", "motivation": "\u9700\u8981\u66f4\u6e05\u6670\u5730\u7406\u89e3\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u534f\u8c03\u4f55\u65f6\u51fa\u73b0\u3001\u6ce2\u52a8\u6216\u5d29\u6e83\uff0c\u4ee5\u8868\u5f81\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7cfb\u7edf\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u4f7f\u7528\u5b8c\u5168\u72ec\u7acbQ\u5b66\u4e60\uff08IQL\uff09\u4f5c\u4e3a\u6700\u5c0f\u53bb\u4e2d\u5fc3\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5728\u73af\u5883\u5927\u5c0fL\u548c\u667a\u80fd\u4f53\u5bc6\u5ea6\u03c1\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6784\u5efa\u57fa\u4e8e\u5408\u4f5c\u6210\u529f\u7387\uff08CSR\uff09\u548cTD\u8bef\u5dee\u65b9\u5dee\u7a33\u5b9a\u6027\u6307\u6570\u7684\u76f8\u56fe\u3002", "result": "\u53d1\u73b0\u4e86\u4e09\u4e2a\u4e0d\u540c\u533a\u57df\uff1a\u534f\u8c03\u7a33\u5b9a\u76f8\u3001\u8106\u5f31\u8fc7\u6e21\u533a\u548c\u963b\u585e\u65e0\u5e8f\u76f8\uff0c\u7531\u5c16\u9510\u7684\u53cc\u91cd\u4e0d\u7a33\u5b9a\u5cad\u5206\u9694\u3002\u6838\u6f02\u79fb\uff08\u7531\u5176\u4ed6\u667a\u80fd\u4f53\u7b56\u7565\u66f4\u65b0\u5f15\u8d77\u7684\u6709\u6548\u8f6c\u79fb\u6838\u65f6\u53d8\u504f\u79fb\uff09\u662f\u5173\u952e\u673a\u5236\uff0c\u800c\u79fb\u9664\u667a\u80fd\u4f53\u6807\u8bc6\u7b26\u4f1a\u5b8c\u5168\u6d88\u9664\u6f02\u79fb\u5e76\u7834\u574f\u4e09\u9636\u6bb5\u7ed3\u6784\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316MARL\u8868\u73b0\u51fa\u7531\u89c4\u6a21\u3001\u5bc6\u5ea6\u548c\u6838\u6f02\u79fb\u76f8\u4e92\u4f5c\u7528\u63a7\u5236\u7684\u76f8\u5e72\u76f8\u7ed3\u6784\uff0c\u8868\u660e\u6d8c\u73b0\u7684\u534f\u8c03\u884c\u4e3a\u662f\u4e00\u79cd\u5206\u5e03\u4ea4\u4e92\u9a71\u52a8\u7684\u76f8\u73b0\u8c61\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.347f77bc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faddyosmani%2Fgemini-cli-tips%3Futm_source=tldrai/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/_2UaRK1CA-f2J2SDECC_Y-9tpgGaKGLD52Oez7dJVoc=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faddyosmani%2Fgemini-cli-tips%3Futm_source=tldrai/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/_2UaRK1CA-f2J2SDECC_Y-9tpgGaKGLD52Oez7dJVoc=433", "authors": ["TLDR Newsletter"], "title": "Gemini CLI Tips & Tricks", "comment": "Source: TLDR Newsletter, Date: 2025-11-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faddyosmani%2Fgemini-cli-tips%3Futm_source=tldrai/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/_2UaRK1CA-f2J2SDECC_Y-9tpgGaKGLD52Oez7dJVoc=433", "summary": "Gemini CLI Tips & Tricks (GitHub Repo) Gemini CLI is an AI assistant that brings Google Gemini directly into the terminal. It functions as a conversational agentic command-line tool. Gemini CLI can reason about requests, choose tools, and execute multi-step plans. It acts like a supercharged pair programmer and command-line assistant. The assistant excels at coding tasks, debugging, content generation, and system automation.", "source": "tldr", "AI": {"tldr": "Gemini CLI\u662f\u4e00\u4e2a\u5c06Google Gemini AI\u96c6\u6210\u5230\u7ec8\u7aef\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u53ef\u4f5c\u4e3a\u8d85\u7ea7\u589e\u5f3a\u7684\u7ed3\u5bf9\u7f16\u7a0b\u548c\u547d\u4ee4\u884c\u52a9\u624b", "motivation": "\u5c06\u5148\u8fdb\u7684AI\u52a9\u624b\u76f4\u63a5\u96c6\u6210\u5230\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u4e2d\uff0c\u7279\u522b\u662f\u7ec8\u7aef\u73af\u5883\uff0c\u4ee5\u63d0\u5347\u7f16\u7a0b\u3001\u8c03\u8bd5\u548c\u7cfb\u7edf\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u6548\u7387", "method": "\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8eGoogle Gemini\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u80fd\u591f\u63a8\u7406\u8bf7\u6c42\u3001\u9009\u62e9\u5de5\u5177\u5e76\u6267\u884c\u591a\u6b65\u9aa4\u8ba1\u5212", "result": "\u5f00\u53d1\u51fa\u529f\u80fd\u5f3a\u5927\u7684AI\u52a9\u624b\uff0c\u64c5\u957f\u7f16\u7801\u4efb\u52a1\u3001\u8c03\u8bd5\u3001\u5185\u5bb9\u751f\u6210\u548c\u7cfb\u7edf\u81ea\u52a8\u5316\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u7ec8\u7aef\u5185\u7684\u667a\u80fd\u8f85\u52a9", "conclusion": "Gemini CLI\u6210\u529f\u5c06Google Gemini AI\u80fd\u529b\u5e26\u5165\u7ec8\u7aef\u73af\u5883\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u4ee3\u7406\u5f0f\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2511.bc82e190", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FBz22n2/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/JudPhHKO3LEaxsRPNOqm5CS4whO9xEc5fttLX9tjEEY=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FBz22n2/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/JudPhHKO3LEaxsRPNOqm5CS4whO9xEc5fttLX9tjEEY=433", "authors": ["TLDR Newsletter"], "title": "Andrew Ng releases AI \u201cAgentic Reviewer\u201d for research papers", "comment": "Source: TLDR Newsletter, Date: 2025-11-27, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FBz22n2/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/JudPhHKO3LEaxsRPNOqm5CS4whO9xEc5fttLX9tjEEY=433", "summary": "Andrew Ng releases AI \u201cAgentic Reviewer\u201d for research papers (2 minute read) Andrew Ng introduced an agentic paper-review system trained on ICLR 2025 reviews, showing AI-human review correlation slightly higher than human-human.", "source": "tldr", "AI": {"tldr": "Andrew Ng\u53d1\u5e03\u57fa\u4e8eICLR 2025\u5ba1\u7a3f\u8bad\u7ec3\u7684AI\u4ee3\u7406\u5ba1\u7a3f\u7cfb\u7edf\uff0cAI-\u4eba\u7c7b\u5ba1\u7a3f\u76f8\u5173\u6027\u7565\u9ad8\u4e8e\u4eba\u7c7b-\u4eba\u7c7b\u5ba1\u7a3f\u76f8\u5173\u6027", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u5b66\u672f\u8bba\u6587\u5ba1\u7a3f\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u9ad8\u5ba1\u7a3f\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4eba\u5de5\u5ba1\u7a3f\u8d1f\u62c5", "method": "\u57fa\u4e8eICLR 2025\u5ba1\u7a3f\u6570\u636e\u8bad\u7ec3AI\u4ee3\u7406\u5ba1\u7a3f\u7cfb\u7edf\uff0c\u6784\u5efa\"Agentic Reviewer\"\u6a21\u578b", "result": "AI-\u4eba\u7c7b\u5ba1\u7a3f\u76f8\u5173\u6027\u7565\u9ad8\u4e8e\u4eba\u7c7b-\u4eba\u7c7b\u5ba1\u7a3f\u76f8\u5173\u6027\uff0c\u663e\u793aAI\u5728\u5ba1\u7a3f\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b", "conclusion": "AI\u4ee3\u7406\u5ba1\u7a3f\u7cfb\u7edf\u5728\u5b66\u672f\u5ba1\u7a3f\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u63d0\u9ad8\u5ba1\u7a3f\u8d28\u91cf", "topic": "agent analysis"}}
{"id": "tldr.2511.c60c6f59", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/i6X2xnz8cvIFNdudEBLurQs7OZrK1If8_nZOn5p1uP0=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/i6X2xnz8cvIFNdudEBLurQs7OZrK1If8_nZOn5p1uP0=433", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-11-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/i6X2xnz8cvIFNdudEBLurQs7OZrK1If8_nZOn5p1uP0=433", "summary": "Andrew Ng releases AI \u201cAgentic Reviewer\u201d for research papers (2 minute read) Andrew Ng introduced an agentic paper-review system trained on ICLR 2025 reviews, showing AI-human review correlation slightly higher than human-human.", "source": "tldr", "AI": {"tldr": "Andrew Ng\u53d1\u5e03\u57fa\u4e8eICLR 2025\u8bc4\u5ba1\u8bad\u7ec3\u7684AI\u4ee3\u7406\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u5176AI-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027\u7565\u9ad8\u4e8e\u4eba\u7c7b-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1AI\u8bc4\u5ba1\u7cfb\u7edf\u80fd\u5426\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u8bc4\u5ba1\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027", "method": "\u57fa\u4e8eICLR 2025\u7684\u8bc4\u5ba1\u6570\u636e\u8bad\u7ec3AI\u4ee3\u7406\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u6bd4AI-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027\u548c\u4eba\u7c7b-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd", "result": "AI-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027\u7565\u9ad8\u4e8e\u4eba\u7c7b-\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5173\u6027\uff0c\u8868\u660eAI\u4ee3\u7406\u8bc4\u5ba1\u7cfb\u7edf\u5177\u6709\u5b9e\u7528\u4ef7\u503c", "conclusion": "AI\u4ee3\u7406\u8bc4\u5ba1\u7cfb\u7edf\u5728\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u63d0\u4f9b\u4e0e\u4eba\u7c7b\u8bc4\u5ba1\u76f8\u5f53\u751a\u81f3\u66f4\u4e00\u81f4\u7684\u8bc4\u5ba1\u610f\u89c1", "topic": "agent analysis"}}
{"id": "tldr.2511.a0d5c2d8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F11%2F26%2Fhow-to-monitor-ai-agent-applications-on-amazon-bedrock-agentcore-with-grafana-cloud%2F%3Futm_source=tldrdevops/1/0100019aca5c062a-ac1b5cb7-6dc6-4727-bcbe-05dffe833082-000000/utTSIiL_Pr1ztVrYgFmqb8vewiEbRd8ly6TZalUccAQ=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F11%2F26%2Fhow-to-monitor-ai-agent-applications-on-amazon-bedrock-agentcore-with-grafana-cloud%2F%3Futm_source=tldrdevops/1/0100019aca5c062a-ac1b5cb7-6dc6-4727-bcbe-05dffe833082-000000/utTSIiL_Pr1ztVrYgFmqb8vewiEbRd8ly6TZalUccAQ=433", "authors": ["TLDR Newsletter"], "title": "How to monitor AI agent applications on Amazon Bedrock AgentCore with Grafana Cloud", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F11%2F26%2Fhow-to-monitor-ai-agent-applications-on-amazon-bedrock-agentcore-with-grafana-cloud%2F%3Futm_source=tldrdevops/1/0100019aca5c062a-ac1b5cb7-6dc6-4727-bcbe-05dffe833082-000000/utTSIiL_Pr1ztVrYgFmqb8vewiEbRd8ly6TZalUccAQ=433", "summary": "How to monitor AI agent applications on Amazon Bedrock AgentCore with Grafana Cloud (7 minute read) This post details how to deploy an AI agent on Amazon Bedrock AgentCore with observability powered by OpenTelemetry and Grafana Cloud. OpenLit provides automatic instrumentation for AI frameworks, and Grafana Cloud's AI Observability dashboards can be used to monitor agent performance, debug production issues using distributed tracing, and optimize costs by tracking token usage and model perfor...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5982\u4f55\u5728Amazon Bedrock AgentCore\u4e0a\u90e8\u7f72AI\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528OpenTelemetry\u548cGrafana Cloud\u5b9e\u73b0\u53ef\u89c2\u6d4b\u6027\u76d1\u63a7", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5e94\u7528\u7684\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u6765\u786e\u4fdd\u751f\u4ea7\u73af\u5883\u7684\u7a33\u5b9a\u6027\u3001\u8c03\u8bd5\u95ee\u9898\u548c\u4f18\u5316\u6210\u672c", "method": "\u4f7f\u7528OpenTelemetry\u8fdb\u884c\u81ea\u52a8\u4eea\u8868\u5316\uff0c\u901a\u8fc7OpenLit\u4e3aAI\u6846\u67b6\u63d0\u4f9b\u81ea\u52a8\u68c0\u6d4b\uff0c\u5e76\u5229\u7528Grafana Cloud\u7684AI\u53ef\u89c2\u6d4b\u6027\u4eea\u8868\u677f\u8fdb\u884c\u76d1\u63a7", "result": "\u5b9e\u73b0\u4e86\u5bf9AI\u4ee3\u7406\u6027\u80fd\u7684\u76d1\u63a7\u3001\u4f7f\u7528\u5206\u5e03\u5f0f\u8ffd\u8e2a\u8c03\u8bd5\u751f\u4ea7\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8ddf\u8e2a\u4ee4\u724c\u4f7f\u7528\u548c\u6a21\u578b\u6027\u80fd\u6765\u4f18\u5316\u6210\u672c", "conclusion": "\u7ed3\u5408Amazon Bedrock AgentCore\u3001OpenTelemetry\u548cGrafana Cloud\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684AI\u4ee3\u7406\u76d1\u63a7\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2511.04aadef0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2136%26utm_source=tldrdesign/1/0100019aca9eb16b-6442508d-29b9-4ce7-aaff-9d9b7829a89b-000000/SO9MSUwLYW5W9MEYNpaZsG1ZyfGoHHfk8XEKduUW-Vg=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2136%26utm_source=tldrdesign/1/0100019aca9eb16b-6442508d-29b9-4ce7-aaff-9d9b7829a89b-000000/SO9MSUwLYW5W9MEYNpaZsG1ZyfGoHHfk8XEKduUW-Vg=433", "authors": ["TLDR Newsletter"], "title": "Agentic AI Interface Improvements", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2136%26utm_source=tldrdesign/1/0100019aca9eb16b-6442508d-29b9-4ce7-aaff-9d9b7829a89b-000000/SO9MSUwLYW5W9MEYNpaZsG1ZyfGoHHfk8XEKduUW-Vg=433", "summary": "Agentic AI Interface Improvements (1 minute read) Agentic AI systems that perform extended reasoning and tool use create usability problems in traditional chat interfaces, particularly when users lose track of their initial instructions as the AI's lengthy internal processes push content off-screen. This post introduces a new dual-scroll pane layout that separates the AI's process in the left column from results in the right column, keeping both visible simultaneously. After completion, the t...", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u53cc\u6eda\u52a8\u7a97\u683c\u754c\u9762\u8bbe\u8ba1\uff0c\u89e3\u51b3\u667a\u80fd\u4f53AI\u5728\u4f20\u7edf\u804a\u5929\u754c\u9762\u4e2d\u56e0\u5197\u957f\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u7528\u6237\u5fd8\u8bb0\u521d\u59cb\u6307\u4ee4\u7684\u95ee\u9898", "motivation": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u8fdb\u884c\u6269\u5c55\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u65f6\uff0c\u5728\u4f20\u7edf\u804a\u5929\u754c\u9762\u4e2d\u4ea7\u751f\u53ef\u7528\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53AI\u7684\u5197\u957f\u5185\u90e8\u5904\u7406\u8fc7\u7a0b\u5c06\u5185\u5bb9\u63a8\u79bb\u5c4f\u5e55\u65f6\uff0c\u7528\u6237\u4f1a\u5fd8\u8bb0\u521d\u59cb\u6307\u4ee4", "method": "\u5f15\u5165\u65b0\u7684\u53cc\u6eda\u52a8\u7a97\u683c\u5e03\u5c40\uff0c\u5c06AI\u7684\u5904\u7406\u8fc7\u7a0b\u653e\u5728\u5de6\u5217\uff0c\u7ed3\u679c\u653e\u5728\u53f3\u5217\uff0c\u4fdd\u6301\u4e24\u8005\u540c\u65f6\u53ef\u89c1", "result": "\u754c\u9762\u6539\u8fdb\u540e\uff0c\u7528\u6237\u80fd\u591f\u540c\u65f6\u770b\u5230AI\u7684\u5904\u7406\u8fc7\u7a0b\u548c\u6700\u7ec8\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u8ddf\u8e2a\u521d\u59cb\u6307\u4ee4\u7684\u95ee\u9898", "conclusion": "\u53cc\u6eda\u52a8\u7a97\u683c\u754c\u9762\u8bbe\u8ba1\u6709\u6548\u6539\u5584\u4e86\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u804a\u5929\u754c\u9762\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027", "topic": "agent analysis"}}
{"id": "wechat.2512.bc80f2ac", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDU4Njc4MA==&mid=2247483889&idx=1&sn=1d4764bcb560c87ed5ed3cc636c29bce&chksm=c47f39af4da12eb038b93ad351c0d424e5462ab855f3d2fc4e869d63eed5c4e44b60935ad936#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDU4Njc4MA==&mid=2247483889&idx=1&sn=1d4764bcb560c87ed5ed3cc636c29bce&chksm=c47f39af4da12eb038b93ad351c0d424e5462ab855f3d2fc4e869d63eed5c4e44b60935ad936#rd", "authors": ["\u65e0\u4eba\u673a\u81ea\u7531\u5f00\u53d1\u574a"], "title": "\u65e0\u4eba\u673a\u5f00\u53d1\u5206\u4eab\u2014\u2014\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u65e0\u4eba\u673a\u7aef\u5230\u7aef\u98de\u884c\u63a7\u5236\u7b97\u6cd5\u5f00\u53d1", "comment": "Source: WeChat, Published: 2025-12-01 11:29:52", "summary": "\u601d\u8def 2\uff1a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7aef\u5230\u7aef\u63a7\u5236\uff0c\u65e0\u9700\u7cfb\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\uff08Agent\uff09\u76f4\u63a5\u4ece \u201c\u4f20\u611f\u5668\u8f93\u5165\u2192\u63a7\u5236\u8f93\u51fa\u201d \u6620\u5c04\uff0c\u9002\u5408\u590d\u6742\u73af\u5883\uff08\u5982\u52a8\u6001\u907f\u969c\u3001\u591a\u673a\u534f\u4f5c\uff09\uff1b", "AI": {"tldr": "\u601d\u8def 2\uff1a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7aef\u5230\u7aef\u63a7\u5236\uff0c\u65e0\u9700\u7cfb\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\uff08Agent\uff09\u76f4\u63a5\u4ece \u201c\u4f20\u611f\u5668\u8f93\u5165\u2192\u63a7\u5236\u8f93\u51fa\u201d \u6620\u5c04\uff0c\u9002\u5408\u590d\u6742\u73af\u5883\uff08\u5982\u52a8\u6001\u907f\u969c\u3001\u591a\u673a\u534f\u4f5c\uff09\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.e0fd80d5", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650824889&idx=1&sn=09deeeabecd6ff31a540d2974c7f6f9c&chksm=8aca0643d8e8d8177737d1cc7c3d76d27af078e215922cac83a354b36d18a3c03134fb5876bc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650824889&idx=1&sn=09deeeabecd6ff31a540d2974c7f6f9c&chksm=8aca0643d8e8d8177737d1cc7c3d76d27af078e215922cac83a354b36d18a3c03134fb5876bc#rd", "authors": ["Myautotime"], "title": "\u7279\u65af\u62c9\u65b0\u63a8\u51fa\u7684FSD V14\uff1a\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\u4ece\u6a21\u4eff\u5b66\u4e60\u8d70\u5411<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-12-01 10:58:40", "summary": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u662f\u5c06\u9a7e\u9a76\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u79d1\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e0d\u65ad\u8fdb\u884c\u201c\u611f\u77e5\u72b6\u6001-\u9009\u62e9\u52a8\u4f5c-\u6267\u884c\u52a8\u4f5c-\u83b7\u5f97\u53cd\u9988-\u5b66\u4e60\u4e0e\u66f4\u65b0\u201d\u7684\u91cd\u590d\u5faa\u73af\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u53cd\u590d\u8bd5\u9519\u4ea4\u4e92\uff0c\u6839\u636e\u83b7\u5f97\u7684\u5956\u52b1\u4fe1\u53f7\u81ea\u4e3b\u5b66\u4e60\u6700\u4f18\u7684\u7b56\u7565\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u662f\u5c06\u9a7e\u9a76\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u79d1\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e0d\u65ad\u8fdb\u884c\u201c\u611f\u77e5\u72b6\u6001-\u9009\u62e9\u52a8\u4f5c-\u6267\u884c\u52a8\u4f5c-\u83b7\u5f97\u53cd\u9988-\u5b66\u4e60\u4e0e\u66f4\u65b0\u201d\u7684\u91cd\u590d\u5faa\u73af\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u53cd\u590d\u8bd5\u9519\u4ea4\u4e92\uff0c\u6839\u636e\u83b7\u5f97\u7684\u5956\u52b1\u4fe1\u53f7\u81ea\u4e3b\u5b66\u4e60\u6700\u4f18\u7684\u7b56\u7565\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.1c201d9a", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzQ5NTY5OQ==&mid=2247484376&idx=1&sn=238364d4f7a5f98f2f7241b917e5b1f8&chksm=fe7c5186e67c08e40e16bc4350e0d2f4a95d1fb15281a2ad95c816709be1869193b740541d16#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzQ5NTY5OQ==&mid=2247484376&idx=1&sn=238364d4f7a5f98f2f7241b917e5b1f8&chksm=fe7c5186e67c08e40e16bc4350e0d2f4a95d1fb15281a2ad95c816709be1869193b740541d16#rd", "authors": ["AI\u9b54\u738b\u8fdb\u5316\u8bba"], "title": "Transformer\u3001<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u878d\u5408\uff1f\u89e3\u51b3\u5e8f\u5217\u51b3\u7b56\u4f18\u5316\u96be\u9898\uff01\uff01\uff01", "comment": "Source: WeChat, Published: 2025-12-01 09:43:07", "summary": "\u5f3a\u5316\u5b66\u4e60 \u8be6\u89e3\u5f3a\u5316\u5b66\u4e60\u8d1f\u8d23\u544a\u8bc9 Transformer \u201c\u600e\u4e48\u505a\u624d\u5bf9\u201d\u3002\u6211\u4eec\u901a\u5e38\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6765\u5efa\u6a21\u3002\u667a\u80fd\u4f53\u5728\u72b6\u6001 \u4e0b\u91c7\u53d6\u52a8\u4f5c \uff0c\u83b7\u5f97\u5956\u52b1 \u5e76\u8f6c\u79fb\u5230 \u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60 \u8be6\u89e3\u5f3a\u5316\u5b66\u4e60\u8d1f\u8d23\u544a\u8bc9 Transformer \u201c\u600e\u4e48\u505a\u624d\u5bf9\u201d\u3002\u6211\u4eec\u901a\u5e38\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6765\u5efa\u6a21\u3002\u667a\u80fd\u4f53\u5728\u72b6\u6001 \u4e0b\u91c7\u53d6\u52a8\u4f5c \uff0c\u83b7\u5f97\u5956\u52b1 \u5e76\u8f6c\u79fb\u5230 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.614b7abb", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NzY1NTI4Mw==&mid=2247498115&idx=1&sn=ae096cee55ba4719ba16bcee6a1d1cb1&chksm=fcee53ea95dee8ac6046d77b0b1be09259c3b28ce9658372f4a764ea00698b352e3ebf77c4d7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NzY1NTI4Mw==&mid=2247498115&idx=1&sn=ae096cee55ba4719ba16bcee6a1d1cb1&chksm=fcee53ea95dee8ac6046d77b0b1be09259c3b28ce9658372f4a764ea00698b352e3ebf77c4d7#rd", "authors": ["AI4CFD"], "title": "\u534e\u76db\u987f\u5927\u5b66Nature Communications\uff1a\u7528\u4e8e\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u57fa\u4e8e\u6a21\u578b\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684SINDy-RL\uff08\u9644\u5f00\u6e90\u4ee3\u7801\u6570\u636e\uff09", "comment": "Source: WeChat, Published: 2025-12-01 09:30:00", "summary": "\u9488\u5bf9\u51cf\u5c11\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6240\u9700\u7684\u7ecf\u9a8c\u91cf\uff0c\u5df2\u7ecf\u5f00\u5c55\u4e86\u5927\u91cf\u7814\u7a76\uff0c\u4f8b\u5982\u79bb\u7ebf\u3001\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u3002\u5728\u73af\u5883\u7684\u4f4e\u4fdd\u771f\u5ea6\u8868\u793a\u4e2d\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u662f\u51cf\u5c11\u5168\u9636\u73af\u5883\u4e2d\u4ea4\u4e92\u6b21\u6570\u7684\u6700\u5e38\u89c1\u65b9\u6cd5\u3002", "AI": {"tldr": "\u9488\u5bf9\u51cf\u5c11\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6240\u9700\u7684\u7ecf\u9a8c\u91cf\uff0c\u5df2\u7ecf\u5f00\u5c55\u4e86\u5927\u91cf\u7814\u7a76\uff0c\u4f8b\u5982\u79bb\u7ebf\u3001\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u3002\u5728\u73af\u5883\u7684\u4f4e\u4fdd\u771f\u5ea6\u8868\u793a\u4e2d\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u662f\u51cf\u5c11\u5168\u9636\u73af\u5883\u4e2d\u4ea4\u4e92\u6b21\u6570\u7684\u6700\u5e38\u89c1\u65b9\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.69befc78", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDA1NDI3Mg==&mid=2247483732&idx=1&sn=db78eb6f38a32280f7065a7da96abfc9&chksm=c588315f7e95ca64590d747f5c427024ee58003e81b7dc7b780bbb577cbf88179e4efc59f0b5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDA1NDI3Mg==&mid=2247483732&idx=1&sn=db78eb6f38a32280f7065a7da96abfc9&chksm=c588315f7e95ca64590d747f5c427024ee58003e81b7dc7b780bbb577cbf88179e4efc59f0b5#rd", "authors": ["\u53a6\u95e8\u82f1\u7279\u5b9d\u513f\u79d1\u6280\u6709\u9650\u516c\u53f8"], "title": "LLM <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-12-01 08:56:18", "summary": "\u672c\u62a5\u544a\u65e8\u5728\u5bf9\u5f53\u524d\u5f00\u6e90\u793e\u533a\u4e2d\u6392\u540d\u524d\u5341\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u8fdb\u884c\u8be6\u5c3d\u7684\u6df1\u5ea6\u5256\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728LLM\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u8868\u73b0\u3002\u672c\u7814\u7a76\u57fa\u4e8eGitHub Star\u6570\u91cf\u5bf9\u6846\u67b6\u8fdb\u884c\u5012\u6392\uff0c\u9009\u53d6\u4e86LLaMA-Factory\u3001Unsloth\u3001DeepSpeed\u3001Verl\u3001TRL\u3001Swift\u3001Axolotl\u3001OpenRLHF", "AI": {"tldr": "\u672c\u62a5\u544a\u65e8\u5728\u5bf9\u5f53\u524d\u5f00\u6e90\u793e\u533a\u4e2d\u6392\u540d\u524d\u5341\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u8fdb\u884c\u8be6\u5c3d\u7684\u6df1\u5ea6\u5256\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728LLM\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u8868\u73b0\u3002\u672c\u7814\u7a76\u57fa\u4e8eGitHub Star\u6570\u91cf\u5bf9\u6846\u67b6\u8fdb\u884c\u5012\u6392\uff0c\u9009\u53d6\u4e86LLaMA-Factory\u3001Unsloth\u3001DeepSpeed\u3001Verl\u3001TRL\u3001Swift\u3001Axolotl\u3001OpenRLHF", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.259f8ea5", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTYzNTA2Ng==&mid=2653701896&idx=1&sn=626f2796f8c24821c1a51c35ab1c37b8&chksm=81c411fed05f981a643c5794687f11efaf849e8a8b2f4f26777a9953b73a4d84349d348126f4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTYzNTA2Ng==&mid=2653701896&idx=1&sn=626f2796f8c24821c1a51c35ab1c37b8&chksm=81c411fed05f981a643c5794687f11efaf849e8a8b2f4f26777a9953b73a4d84349d348126f4#rd", "authors": ["\u6a21\u578b\u89c6\u89d2"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u5916\u7684\u4e00\u4efd\u6e05\u9192", "comment": "Source: WeChat, Published: 2025-12-01 03:30:34", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u884c\u4e3a\u7b56\u7565\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u4e0e\u76d1\u7763\u5b66\u4e60\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u63a2\u7d22\u6570\u636e\u7ed3\u6784\u4e0d\u540c\uff0c\u5f3a\u5316\u5b66\u4e60\u5173\u6ce8\u7684\u662f\"\u8bd5\u9519\"\u8fc7\u7a0b\u4e2d\u7684\u5956\u60e9\u53cd\u9988\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u5b66\u4e60\u6700\u4f18\u884c\u4e3a\u7b56\u7565\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u4e0e\u76d1\u7763\u5b66\u4e60\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u63a2\u7d22\u6570\u636e\u7ed3\u6784\u4e0d\u540c\uff0c\u5f3a\u5316\u5b66\u4e60\u5173\u6ce8\u7684\u662f\"\u8bd5\u9519\"\u8fc7\u7a0b\u4e2d\u7684\u5956\u60e9\u53cd\u9988\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.3151909b", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5OTAxNzYxMw==&mid=2247484961&idx=1&sn=0c4c7fdf105e13b79abd735d4c2c3bda&chksm=9752528701396ec46b685749fc16350ae37b79aa82b9eb5e81b5259f6591f6ecc75c85d40760#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5OTAxNzYxMw==&mid=2247484961&idx=1&sn=0c4c7fdf105e13b79abd735d4c2c3bda&chksm=9752528701396ec46b685749fc16350ae37b79aa82b9eb5e81b5259f6591f6ecc75c85d40760#rd", "authors": ["\u5927\u6a21\u578b\u5fae\u8c03Online"], "title": "PPO\u6700\u5f3a\uff0cDPO\u4e00\u822c\uff1f\u4e00\u6587\u5e26\u4f60\u4e86\u89e3\u5e38\u89c1\u4e09\u79cd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5\uff0c\u6587\u672b\u6709\u5927\u6a21\u578b\u5fae\u8c03\u795e\u5668\uff01", "comment": "Source: WeChat, Published: 2025-12-01 01:02:07", "summary": "\u25cf \u8bad\u7ec3\u7a33\u5b9a\uff0c\u6ca1\u6709\u4ef7\u503c\u51fd\u6570\u3001\u4f18\u52bf\u4f30\u8ba1\u8fd9\u4e9b\u201c\u5f3a\u5316\u5b66\u4e60\u5751\u70b9\u201d\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u50cf\u666e\u901a SFT\u3002\u7f3a\u70b9\uff1a\u25cf \u975e\u5e38\u4f9d\u8d56\u504f\u597d\u6570\u636e\u8d28\u91cf\u3002\u5982\u679c\u6807\u6ce8\u5458\u7684\u6807\u51c6\u4e0d\u7edf\u4e00\u3001\u751a\u81f3\u672c\u8eab\u7406\u89e3\u6709\u8bef\uff0c\u6a21\u578b\u5c31\u4f1a\u5b66\u9519\u504f\u597d\uff0c\u800c\u4e14\u5f88\u96be\u901a\u8fc7\u201c\u5956\u52b1\u6a21\u578b\u5206\u6790\u201d\u628a\u95ee", "AI": {"tldr": "\u25cf \u8bad\u7ec3\u7a33\u5b9a\uff0c\u6ca1\u6709\u4ef7\u503c\u51fd\u6570\u3001\u4f18\u52bf\u4f30\u8ba1\u8fd9\u4e9b\u201c\u5f3a\u5316\u5b66\u4e60\u5751\u70b9\u201d\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u50cf\u666e\u901a SFT\u3002\u7f3a\u70b9\uff1a\u25cf \u975e\u5e38\u4f9d\u8d56\u504f\u597d\u6570\u636e\u8d28\u91cf\u3002\u5982\u679c\u6807\u6ce8\u5458\u7684\u6807\u51c6\u4e0d\u7edf\u4e00\u3001\u751a\u81f3\u672c\u8eab\u7406\u89e3\u6709\u8bef\uff0c\u6a21\u578b\u5c31\u4f1a\u5b66\u9519\u504f\u597d\uff0c\u800c\u4e14\u5f88\u96be\u901a\u8fc7\u201c\u5956\u52b1\u6a21\u578b\u5206\u6790\u201d\u628a\u95ee", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.d0c41186", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2ODcxNTIyMg==&mid=2247484630&idx=1&sn=7eeb45237d28ea18b6dfe50970854f3a&chksm=cf9aa3a355092521ca05c97e1b531296b671438d55cdfe1f5f5ec0d9c3abe6698a2f2a1313b6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2ODcxNTIyMg==&mid=2247484630&idx=1&sn=7eeb45237d28ea18b6dfe50970854f3a&chksm=cf9aa3a355092521ca05c97e1b531296b671438d55cdfe1f5f5ec0d9c3abe6698a2f2a1313b6#rd", "authors": ["WhaleThinking"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236Richard Sutton\uff1a\u4e3a\u4ec0\u4e48\u5927\u8bed\u8a00\u6a21\u578b\u662f\u6761\u6b7b\u80e1\u540c", "comment": "Source: WeChat, Published: 2025-11-30 23:30:38", "summary": "\u4ece\u5f3a\u5316\u5b66\u4e60\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u5728\u6982\u5ff5\u4e0a\u5ffd\u7565\u4e86\u4ec0\u4e48\uff1fRichard Sutton\uff1a\u8fd9\u786e\u5b9e\u662f\u622a\u7136\u4e0d\u540c\u7684\u89c6\u89d2\u3002\u4e24\u8005\u5f88\u5bb9\u6613\u5404\u8bf4\u5404\u8bdd\uff0c\u5931\u53bb\u4ea4\u6d41\u7684\u80fd\u529b\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u53d8\u5f97\u5982\u6b64\u5e9e\u5927\uff0c\u751f\u6210\u5f0fAI\u6574\u4f53\u90fd\u5f88\u706b\uff0c\u6211\u4eec\u8fd9\u4e2a\u884c\u4e1a\u559c\u6b22\u8ddf\u98ce\uff0c\u4e8e\u662f\u5ffd\u7565\u4e86\u6700", "AI": {"tldr": "\u4ece\u5f3a\u5316\u5b66\u4e60\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u5728\u6982\u5ff5\u4e0a\u5ffd\u7565\u4e86\u4ec0\u4e48\uff1fRichard Sutton\uff1a\u8fd9\u786e\u5b9e\u662f\u622a\u7136\u4e0d\u540c\u7684\u89c6\u89d2\u3002\u4e24\u8005\u5f88\u5bb9\u6613\u5404\u8bf4\u5404\u8bdd\uff0c\u5931\u53bb\u4ea4\u6d41\u7684\u80fd\u529b\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u53d8\u5f97\u5982\u6b64\u5e9e\u5927\uff0c\u751f\u6210\u5f0fAI\u6574\u4f53\u90fd\u5f88\u706b\uff0c\u6211\u4eec\u8fd9\u4e2a\u884c\u4e1a\u559c\u6b22\u8ddf\u98ce\uff0c\u4e8e\u662f\u5ffd\u7565\u4e86\u6700", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.d159245f", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzMzE5ODQyNw==&mid=2247483850&idx=1&sn=2fb31650b0963409d702c65d6c297ca7&chksm=f191c75134c147924db9109c7d0eaa4de4ff95306d7f2fc5d2b71fc2262792810633e17979ef#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzMzE5ODQyNw==&mid=2247483850&idx=1&sn=2fb31650b0963409d702c65d6c297ca7&chksm=f191c75134c147924db9109c7d0eaa4de4ff95306d7f2fc5d2b71fc2262792810633e17979ef#rd", "authors": ["\u94f6\u6cb3\u667a\u5b66\u6280\u672f"], "title": "Claude <em class=\"highlight\">Code</em> \u6838\u5fc3\uff1a\u6df1\u5ea6\u4ecb\u7ecd MCP+<em class=\"highlight\">Agent</em>+\u659c\u6746\u547d\u4ee4+Hook \u4e00\u6587\u901a\uff01", "comment": "Source: WeChat, Published: 2025-12-01 07:46:23", "summary": "Claude Code \u662f\u4e00\u4e2a\u975e\u5e38\u6709\u9650\u7684\u547d\u4ee4\u884c AI Code Agent \uff0c \u5177\u5907\u975e\u5e38\u4e30\u5bcc\u7684 Agent/MCP/Hooks \u7b49\u529f\u80fd\uff0c\u5177\u5907\u975e\u5e38\u5f3a\u7684\u80fd\u529b\uff0c\u80fd\u591f\u56ca\u62ec\u9664\u4e86\u5199\u4ee3\u7801\u4e4b\u5916\u66f4\u591a\u8054\u52a8\u5de5\u4f5c\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6838\u5fc3\u64cd\u4f5c\u4ea4\u4e92\u7684\u5165\u53e3\u3002", "AI": {"tldr": "Claude Code \u662f\u4e00\u4e2a\u975e\u5e38\u6709\u9650\u7684\u547d\u4ee4\u884c AI Code Agent \uff0c \u5177\u5907\u975e\u5e38\u4e30\u5bcc\u7684 Agent/MCP/Hooks \u7b49\u529f\u80fd\uff0c\u5177\u5907\u975e\u5e38\u5f3a\u7684\u80fd\u529b\uff0c\u80fd\u591f\u56ca\u62ec\u9664\u4e86\u5199\u4ee3\u7801\u4e4b\u5916\u66f4\u591a\u8054\u52a8\u5de5\u4f5c\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6838\u5fc3\u64cd\u4f5c\u4ea4\u4e92\u7684\u5165\u53e3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.3947dfca", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MjI0MTc5Mg==&mid=2247522152&idx=2&sn=b5399086884da24dce792221491a397b&chksm=e8c389d309c9732671d1593c5745923522bb0c5420bc8ae37e81dc16411828ad8028a4da539c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MjI0MTc5Mg==&mid=2247522152&idx=2&sn=b5399086884da24dce792221491a397b&chksm=e8c389d309c9732671d1593c5745923522bb0c5420bc8ae37e81dc16411828ad8028a4da539c#rd", "authors": ["\u5fae\u8f6f\u5e02\u573a\u6d3b\u52a8"], "title": "<em class=\"highlight\">Agentic</em> AI \u5168\u6808\u521b\u65b0\uff1a\u4ece\u6a21\u578b\u5230\u6cbb\u7406\uff0c\u5f00\u542f\u667a\u80fd\u5316\u843d\u5730\u65b0\u8def\u5f84", "comment": "Source: WeChat, Published: 2025-12-01 11:00:00", "summary": "Microsoft Foundry\uff08\u56fd\u9645\u7248\uff09\uff1a\u6a21\u578b\u3001\u667a\u80fd\u4f53\u3001\u5de5\u5177\u3001\u6cbb\u7406\u3001\u90e8\u7f72\u4e00\u7ad9\u5f0f AI \u5de5\u7a0b\u5e73\u53f0Agent \u8d85\u7ea7\u5de5\u5382\uff1a\u8d8511\uff0c000\u6a21\u578b\u6c60 + \u667a\u80fd Model Router\uff0c\u4f18\u9009\u6a21\u578b\u6784\u5efa\u667a\u80fd\u4f53", "AI": {"tldr": "Microsoft Foundry\uff08\u56fd\u9645\u7248\uff09\uff1a\u6a21\u578b\u3001\u667a\u80fd\u4f53\u3001\u5de5\u5177\u3001\u6cbb\u7406\u3001\u90e8\u7f72\u4e00\u7ad9\u5f0f AI \u5de5\u7a0b\u5e73\u53f0Agent \u8d85\u7ea7\u5de5\u5382\uff1a\u8d8511\uff0c000\u6a21\u578b\u6c60 + \u667a\u80fd Model Router\uff0c\u4f18\u9009\u6a21\u578b\u6784\u5efa\u667a\u80fd\u4f53", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.97162732", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNTQyNzUyMw==&mid=2247483909&idx=1&sn=d4d2b6f17135963e51418ba77430bee2&chksm=f1281d75a01abe42eef5213f7f27a63f33a505cc8c0f1048f7c5f499af275dcb1eb9349bd7d2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNTQyNzUyMw==&mid=2247483909&idx=1&sn=d4d2b6f17135963e51418ba77430bee2&chksm=f1281d75a01abe42eef5213f7f27a63f33a505cc8c0f1048f7c5f499af275dcb1eb9349bd7d2#rd", "authors": ["\u6613\u946bAI"], "title": "\u6613\u946b\u5f00\u6e90\u4e1a\u5185\u9996\u4e2a<em class=\"highlight\">Agentic</em>\u5927\u6a21\u578b\uff0c\u63a8\u52a8AI\u7ea2\u5229\u5171\u4eab\uff0c\u52a0\u901fAI\u751f\u6001\u5171\u5efa", "comment": "Source: WeChat, Published: 2025-12-01 10:22:01", "summary": "\u5728Agentic\u901a\u7528\u5de5\u5177\u8c03\u7528\u8bc4\u6d4b\u65b9\u9762\uff0cYiXin-Agentic-Qwen3-14B\u5728TAU1\u3001TAU2\u548cC3-Bench\u7b49\u590d\u6742\u573a\u666f\u548c\u4efb\u52a1\u6267\u884c\u7c7b\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520658.3\uff0c\u4e0d\u4ec5\u5728\u540c\u5c3a\u5bf8\u6a21\u578b\u4e2d\u7a33\u5c45\u7b2c\u4e00\uff08Qwen3-14B\uff1a44.5\uff09\uff0c\u8fd8\u8d85\u8d8a\u4e86\u53c2\u6570\u89c4\u6a21\u66f4\u5927\u7684Qwen3-235B-A22B-Thinking-2507\uff0857.4\uff09\u3001Kimi-K2-I", "AI": {"tldr": "\u5728Agentic\u901a\u7528\u5de5\u5177\u8c03\u7528\u8bc4\u6d4b\u65b9\u9762\uff0cYiXin-Agentic-Qwen3-14B\u5728TAU1\u3001TAU2\u548cC3-Bench\u7b49\u590d\u6742\u573a\u666f\u548c\u4efb\u52a1\u6267\u884c\u7c7b\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520658.3\uff0c\u4e0d\u4ec5\u5728\u540c\u5c3a\u5bf8\u6a21\u578b\u4e2d\u7a33\u5c45\u7b2c\u4e00\uff08Qwen3-14B\uff1a44.5\uff09\uff0c\u8fd8\u8d85\u8d8a\u4e86\u53c2\u6570\u89c4\u6a21\u66f4\u5927\u7684Qwen3-235B-A22B-Thinking-2507\uff0857.4\uff09\u3001Kimi-K2-I", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.ef0370ee", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NzE1ODQ4NQ==&mid=2650458367&idx=1&sn=235357d19510ad1eaa801a10ca153a74&chksm=f29caa8754dda5f33a592c2915c4535b799fcebf55818f7bf6a3e8b75fb07389cfd7ecee4a81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NzE1ODQ4NQ==&mid=2650458367&idx=1&sn=235357d19510ad1eaa801a10ca153a74&chksm=f29caa8754dda5f33a592c2915c4535b799fcebf55818f7bf6a3e8b75fb07389cfd7ecee4a81#rd", "authors": ["AIE \u52a0\u901f\u5de5\u4e1a\u8fdb\u5316"], "title": "\u9ea6\u80af\u95211\u5e74\u5b9e\u6218\u603b\u7ed3\uff1a6\u4e2a<em class=\"highlight\">Agentic</em> AI\u4f01\u4e1a\u7ea7\u5b9e\u64cd\u843d\u5730\u7ecf\u9a8c", "comment": "Source: WeChat, Published: 2025-12-01 01:32:20", "summary": "\u672c\u6587\u5177\u4f53\u5185\u5bb9\u6765\u81ea\u5bf9\u6587\u7ae0\u300aOne year of agentic AI\uff1aSix Lessons from the people doing the work\u300b\u7684\u7406\u89e3\u548c\u7ffb\u8bd1\uff0c\u539f\u6587\u94fe\u63a5\u89c1\u6587\u672b\u201c\u9605\u8bfb\u539f\u6587\u201d\u3002\u4ee5\u4e0b\u662f6\u4e2a\u7ecf\u9a8c\u7684\u5177\u4f53\u89e3\u91ca\u3002", "AI": {"tldr": "\u672c\u6587\u5177\u4f53\u5185\u5bb9\u6765\u81ea\u5bf9\u6587\u7ae0\u300aOne year of agentic AI\uff1aSix Lessons from the people doing the work\u300b\u7684\u7406\u89e3\u548c\u7ffb\u8bd1\uff0c\u539f\u6587\u94fe\u63a5\u89c1\u6587\u672b\u201c\u9605\u8bfb\u539f\u6587\u201d\u3002\u4ee5\u4e0b\u662f6\u4e2a\u7ecf\u9a8c\u7684\u5177\u4f53\u89e3\u91ca\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.91f3cd5d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4OTU4MTIzNw==&mid=2247483844&idx=1&sn=0f833fb2e04e5978e991bd1a9f9bef76&chksm=cead33827bec56ff4ee912a309907853cc372bd1abdfb592a0791890415b1f4e32238b38b2ac#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4OTU4MTIzNw==&mid=2247483844&idx=1&sn=0f833fb2e04e5978e991bd1a9f9bef76&chksm=cead33827bec56ff4ee912a309907853cc372bd1abdfb592a0791890415b1f4e32238b38b2ac#rd", "authors": ["\u718a\u718a ai \u7b14\u8bb0"], "title": "<em class=\"highlight\">Agentic</em>\u65f6\u4ee3\u6765\u4e34\uff0c\u4f60\u7684\u201c\u6570\u5b57\u5458\u5de5\u201d\u8fd8\u7f3a\u4e00\u4e2a\u4e13\u5c5e\u529e\u516c\u5ba4\uff01", "comment": "Source: WeChat, Published: 2025-11-30 23:50:24", "summary": "\u5f53Agent\u8fdb\u5316\u5230\u771f\u6b63\u7684 Agentic \u9636\u6bb5\u2014\u2014\u5373\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u52a8\u6001\u8c03\u7528\u5de5\u5177\u5e76\u4e0e\u73af\u5883\u6df1\u5ea6\u4ea4\u4e92\u65f6\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u51fd\u6570\u8c03\u7528\u63a5\u53e3\u65e9\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u3001\u4f46\u53c8\u5b8c\u5168\u53d7\u63a7\u7684\u73af\u5883\u3002", "AI": {"tldr": "\u5f53Agent\u8fdb\u5316\u5230\u771f\u6b63\u7684 Agentic \u9636\u6bb5\u2014\u2014\u5373\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u52a8\u6001\u8c03\u7528\u5de5\u5177\u5e76\u4e0e\u73af\u5883\u6df1\u5ea6\u4ea4\u4e92\u65f6\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u51fd\u6570\u8c03\u7528\u63a5\u53e3\u65e9\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u3001\u4f46\u53c8\u5b8c\u5168\u53d7\u63a7\u7684\u73af\u5883\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.74e05abb", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247566187&idx=5&sn=fa091e97a0b6f702eb253f59b516f013&chksm=fc6db9b5b344877d042a9c27b2537842bce6d7efdd51d606987518e201739c07c75d49985120#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247566187&idx=5&sn=fa091e97a0b6f702eb253f59b516f013&chksm=fc6db9b5b344877d042a9c27b2537842bce6d7efdd51d606987518e201739c07c75d49985120#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94feunion"], "title": "\u3010\u62a5\u544a\u3011\u667a\u80fd\u4f53\u4e13\u9898\u4e94\uff1a\u56fd\u5bb6\u961f\u51fa\u624b\uff01 2025 \u9996\u4efd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\u6280\u672f\u80fd\u529b\u7efc\u5408\u6d4b\u8bd5\u62a5\u544a\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-12-01 12:20:36", "summary": "\u300a\u5927\u6a21\u578b\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\u6280\u672f\u80fd\u529b\u7efc\u5408\u6d4b\u8bd5\u62a5\u544a\u300b \uff08\u5b8c\u6574\u7248.pdf \uff09\u4ee5\u4e0b\u4ec5\u5c55\u793a\u90e8\u5206\u5185\u5bb9 \u4e0b\u8f7d\u65b9\u5f0f\u89c1\u6587\u672b \u4e00\u3001\u5f00\u573a\uff1a\u5b98\u65b9\u201c\u6478\u5e95\u8003\u201d\u6765\u4e86 \u6d4b\u8bd5\u673a\u6784\uff1a\u56fd\u5bb6\u5de5\u4e1a\u4fe1\u606f\u5b89\u5168\u53d1\u5c55\u7814\u7a76\u4e2d\u5fc3 + \u8d5b\u6607\u5b9e\u9a8c\u5ba4", "AI": {"tldr": "\u300a\u5927\u6a21\u578b\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\u6280\u672f\u80fd\u529b\u7efc\u5408\u6d4b\u8bd5\u62a5\u544a\u300b \uff08\u5b8c\u6574\u7248.pdf \uff09\u4ee5\u4e0b\u4ec5\u5c55\u793a\u90e8\u5206\u5185\u5bb9 \u4e0b\u8f7d\u65b9\u5f0f\u89c1\u6587\u672b \u4e00\u3001\u5f00\u573a\uff1a\u5b98\u65b9\u201c\u6478\u5e95\u8003\u201d\u6765\u4e86 \u6d4b\u8bd5\u673a\u6784\uff1a\u56fd\u5bb6\u5de5\u4e1a\u4fe1\u606f\u5b89\u5168\u53d1\u5c55\u7814\u7a76\u4e2d\u5fc3 + \u8d5b\u6607\u5b9e\u9a8c\u5ba4", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.0c05cc74", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5NTc5NDk3MQ==&mid=2247485457&idx=1&sn=be23137044e0e019f2dd5441eb952f54&chksm=ffca1b75d25de87a430ca9585bbf4223c123e0dcff80ddb71b2bbd43a38e3217ad58b8997f13#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5NTc5NDk3MQ==&mid=2247485457&idx=1&sn=be23137044e0e019f2dd5441eb952f54&chksm=ffca1b75d25de87a430ca9585bbf4223c123e0dcff80ddb71b2bbd43a38e3217ad58b8997f13#rd", "authors": ["\u6cb3\u4e1c\u5c0f\u90e2"], "title": "\u3010AI\u4eca\u65e5\u5934\u6761\u3011ChatGPT\u53d1\u5e03\u4e09\u5468\u5e74\uff1a\u4ece\u5bf9\u8bdd\u5230\u667a\u80fd\u4f53\uff0c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8fd9\u4e09\u5e74", "comment": "Source: WeChat, Published: 2025-12-01 12:09:57", "summary": "\u8fd9\u4e00\u5e74\uff0c\u56fd\u5185\u7684\u5927\u6a21\u578b\u8fd8\u4e3b\u8981\u5728\u201c\u9884\u5907\u961f\u201d\u9636\u6bb5\uff0c\u66f4\u591a\u662f\u6280\u672f\u50a8\u5907\u548c\u5185\u90e8\u6f14\u793a\u30022022 \u5e74\u4ee3\u8868\u6027\u6a21\u578b\uff08\u6309\u5730\u533a\uff09\u5730\u533a\u6a21\u578b / \u4ea7\u54c1\u65f6\u95f4\u8282\u70b9\u8bf4\u660e\u7f8e\u56fdGPT-3.5 / ChatGPT", "AI": {"tldr": "\u8fd9\u4e00\u5e74\uff0c\u56fd\u5185\u7684\u5927\u6a21\u578b\u8fd8\u4e3b\u8981\u5728\u201c\u9884\u5907\u961f\u201d\u9636\u6bb5\uff0c\u66f4\u591a\u662f\u6280\u672f\u50a8\u5907\u548c\u5185\u90e8\u6f14\u793a\u30022022 \u5e74\u4ee3\u8868\u6027\u6a21\u578b\uff08\u6309\u5730\u533a\uff09\u5730\u533a\u6a21\u578b / \u4ea7\u54c1\u65f6\u95f4\u8282\u70b9\u8bf4\u660e\u7f8e\u56fdGPT-3.5 / ChatGPT", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.2695019c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDg2Mzc5MA==&mid=2247484921&idx=1&sn=7325193e725eeecec26925a6394fa2ac&chksm=c5eccef4de73a84a1a6f3b740f59dd4c65a48b178ba9ccf0dc9c6eccc6e34a65a283ddfa445c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDg2Mzc5MA==&mid=2247484921&idx=1&sn=7325193e725eeecec26925a6394fa2ac&chksm=c5eccef4de73a84a1a6f3b740f59dd4c65a48b178ba9ccf0dc9c6eccc6e34a65a283ddfa445c#rd", "authors": ["\u89c6\u754c\u62fe\u5149\u793e"], "title": "\u4e3b\u6d41AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5bc6\u96c6\u8fed\u4ee3 \u591a\u573a\u666f\u5b9e\u7528\u80fd\u529b\u663e\u8457\u5347\u7ea7", "comment": "Source: WeChat, Published: 2025-12-01 09:18:39", "summary": "\u6b64\u6b21\u591a\u6b3e\u5927\u6a21\u578b\u8fed\u4ee3\u5747\u4ee5\"\u5b9e\u7528\u5316\"\u4e3a\u6838\u5fc3\u5bfc\u5411\uff0c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u3001\u63a8\u7406\u6df1\u5ea6\u53ef\u8c03\u3001\u4e13\u4e1a\u573a\u666f\u9002\u914d\u7b49\u5347\u7ea7\u65b9\u5411\uff0c\u65e2\u964d\u4f4e\u4e86\u666e\u901a\u7528\u6237\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4e5f\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u7684\u5de5\u5177\u9009\u62e9\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8AI\u6280\u672f\u5728\u79d1\u7814\u3001\u5f00\u53d1\u3001\u65e5\u5e38\u670d\u52a1\u7b49\u591a\u9886", "AI": {"tldr": "\u6b64\u6b21\u591a\u6b3e\u5927\u6a21\u578b\u8fed\u4ee3\u5747\u4ee5\"\u5b9e\u7528\u5316\"\u4e3a\u6838\u5fc3\u5bfc\u5411\uff0c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u3001\u63a8\u7406\u6df1\u5ea6\u53ef\u8c03\u3001\u4e13\u4e1a\u573a\u666f\u9002\u914d\u7b49\u5347\u7ea7\u65b9\u5411\uff0c\u65e2\u964d\u4f4e\u4e86\u666e\u901a\u7528\u6237\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4e5f\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u7684\u5de5\u5177\u9009\u62e9\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8AI\u6280\u672f\u5728\u79d1\u7814\u3001\u5f00\u53d1\u3001\u65e5\u5e38\u670d\u52a1\u7b49\u591a\u9886", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.0ff64625", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzOTU4MjQwMg==&mid=2247487134&idx=1&sn=efeff21299d2e780f8704988744f93ee&chksm=c365d8d921e9d1d7be999ebf68ea0696f76426b879c8a448841bacb33799cd5c78f2afe1ba32#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzOTU4MjQwMg==&mid=2247487134&idx=1&sn=efeff21299d2e780f8704988744f93ee&chksm=c365d8d921e9d1d7be999ebf68ea0696f76426b879c8a448841bacb33799cd5c78f2afe1ba32#rd", "authors": ["\u4e1c\u6052\u77e5\u76db\u77e5\u8bc6\u4ea7\u6743"], "title": "\u300a\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u77e5\u8bc6\u4ea7\u6743\u53d1\u5c55\u62a5\u544a\uff082025\uff09\u300b\u91cd\u78c5\u53d1\u5e03 \u8fd9\u4e9b\u8d8b\u52bf\u503c\u5f97\u5173\u6ce8\uff01", "comment": "Source: WeChat, Published: 2025-12-01 08:28:09", "summary": "\u5927\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u5f52\u5c5e\u95ee\u9898\u540c\u6837\u590d\u6742\u3002\u62a5\u544a\u68b3\u7406\u4e86\u5f53\u524d\u7684\u4e3b\u8981\u89c2\u70b9\uff1a\u4e3b\u6d41\u89c2\u70b9\u5206\u6b67\uff1a35%\u7684\u4e13\u5bb6\u8ba4\u4e3a\u5e94\u5f52\u5c5e\u4e8e\u6a21\u578b\u4f7f\u7528\u8005 28%\u7684\u4e13\u5bb6\u8ba4\u4e3a\u5e94\u5f52\u5c5e\u4e8e\u6a21\u578b\u5f00\u53d1\u8005", "AI": {"tldr": "\u5927\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u5f52\u5c5e\u95ee\u9898\u540c\u6837\u590d\u6742\u3002\u62a5\u544a\u68b3\u7406\u4e86\u5f53\u524d\u7684\u4e3b\u8981\u89c2\u70b9\uff1a\u4e3b\u6d41\u89c2\u70b9\u5206\u6b67\uff1a35%\u7684\u4e13\u5bb6\u8ba4\u4e3a\u5e94\u5f52\u5c5e\u4e8e\u6a21\u578b\u4f7f\u7528\u8005 28%\u7684\u4e13\u5bb6\u8ba4\u4e3a\u5e94\u5f52\u5c5e\u4e8e\u6a21\u578b\u5f00\u53d1\u8005", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.16e3db91", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODk0MDUzOA==&mid=2247483715&idx=1&sn=9d6f3819c934d7b7a3b016619800be85&chksm=c47082e6eaf560f8b92ccd3956616f65756c825cba5a13fcdc40a83963868246966598e22026#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODk0MDUzOA==&mid=2247483715&idx=1&sn=9d6f3819c934d7b7a3b016619800be85&chksm=c47082e6eaf560f8b92ccd3956616f65756c825cba5a13fcdc40a83963868246966598e22026#rd", "authors": ["\u54ea\u5412\u51fa\u6d77\u6cd5\u5f8b\u901a"], "title": "\u54ea\u5412\u51fa\u6d77\uff01\u56fd\u5185\u9996\u4e2a\u201cAI+\u6cd5\u5f8b\u201d\u51fa\u6d77<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6765\u4e86", "comment": "Source: WeChat, Published: 2025-12-01 06:10:32", "summary": "\u4e0a\u6d77\u54ea\u5412\u51fa\u6d77\u667a\u80fd\u79d1\u6280\u6709\u9650\u516c\u53f8\u662f\u4e00\u5bb6\u57fa\u4e8e\u524d\u6cbfAI\u5927\u6a21\u578b\u6280\u672f\u3001\u4e13\u6ce8\u4e8e\u4e3a\u4e2d\u56fd\u4f01\u4e1a\u51fa\u6d77\u63d0\u4f9b\u667a\u80fd\u5316\u6cd5\u5f8b\u670d\u52a1\u7684\u79d1\u6280\u521b\u65b0\u4f01\u4e1a\u3002\u516c\u53f8\u65d7\u4e0b\u4ea7\u54c1\u201c\u54ea\u5412\u51fa\u6d77\u6cd5\u5f8b\u901a\u201d\u667a\u80fd\u4f53\uff0c\u4f5c\u4e3a\u5168\u7403\u9996\u4e2a\u4e13\u6ce8\u4e8e\u51fa\u6d77\u6cd5\u5f8b\u670d\u52a1\u9886\u57df\u7684\u5782\u76f4\u7c7b\u6a21\u578b\u5e73\u53f0\uff0c", "AI": {"tldr": "\u4e0a\u6d77\u54ea\u5412\u51fa\u6d77\u667a\u80fd\u79d1\u6280\u6709\u9650\u516c\u53f8\u662f\u4e00\u5bb6\u57fa\u4e8e\u524d\u6cbfAI\u5927\u6a21\u578b\u6280\u672f\u3001\u4e13\u6ce8\u4e8e\u4e3a\u4e2d\u56fd\u4f01\u4e1a\u51fa\u6d77\u63d0\u4f9b\u667a\u80fd\u5316\u6cd5\u5f8b\u670d\u52a1\u7684\u79d1\u6280\u521b\u65b0\u4f01\u4e1a\u3002\u516c\u53f8\u65d7\u4e0b\u4ea7\u54c1\u201c\u54ea\u5412\u51fa\u6d77\u6cd5\u5f8b\u901a\u201d\u667a\u80fd\u4f53\uff0c\u4f5c\u4e3a\u5168\u7403\u9996\u4e2a\u4e13\u6ce8\u4e8e\u51fa\u6d77\u6cd5\u5f8b\u670d\u52a1\u9886\u57df\u7684\u5782\u76f4\u7c7b\u6a21\u578b\u5e73\u53f0\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.44f05e50", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622600&idx=1&sn=6e9e9909750e64d2391d1303542d4008&chksm=8d127fb7267685b87303267f46f7897dc8f92dbbfa1898925049b4500d7c055898b75638ea6a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622600&idx=1&sn=6e9e9909750e64d2391d1303542d4008&chksm=8d127fb7267685b87303267f46f7897dc8f92dbbfa1898925049b4500d7c055898b75638ea6a#rd", "authors": ["\u5927\u6a21\u578b\u8bc4\u6d4b\u53ca\u4f18\u5316NoneLinear"], "title": "\u6bcf\u6708AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u66f4\u65b0\u901f\u9012\uff0825\u5e7411\u6708\uff09", "comment": "Source: WeChat, Published: 2025-12-01 04:30:15", "summary": "\u5927\u6a21\u578b/agent\u8bc4\u6d4b\u6280\u672f\u4ea4\u6d41\uff1a\u5173\u6ce8\u516c\u4f17\u53f7\uff0c\u53d1\u9001\u6d88\u606f\"\u8fdb\u7fa4\"", "AI": {"tldr": "\u5927\u6a21\u578b/agent\u8bc4\u6d4b\u6280\u672f\u4ea4\u6d41\uff1a\u5173\u6ce8\u516c\u4f17\u53f7\uff0c\u53d1\u9001\u6d88\u606f\"\u8fdb\u7fa4\"", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.eec31be7", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDg0OTI1MQ==&mid=2247500984&idx=1&sn=d64f444e64e98989daae7a9fe4dbf21f&chksm=e859a623097937df688bba7c7ccab3ffeffaae87ddcdfbb484edae866fc53bec8765b323f778#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDg0OTI1MQ==&mid=2247500984&idx=1&sn=d64f444e64e98989daae7a9fe4dbf21f&chksm=e859a623097937df688bba7c7ccab3ffeffaae87ddcdfbb484edae866fc53bec8765b323f778#rd", "authors": ["\u6e05\u534e\u5927\u5b66\u667a\u80fd\u4ea7\u4e1a\u7814\u7a76\u9662"], "title": "AIR\u5b66\u672f\uff5c\u5218\u6d0b\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9a71\u52a8\u7684\u53ef\u8fdb\u5316\u667a\u80fd\u4f53", "comment": "Source: WeChat, Published: 2025-12-01 01:52:12", "summary": "\u9996\u5148\uff0c\u5927\u6a21\u578b\u6b63\u9010\u6e10\u6210\u4e3a\u5404\u7c7b\u667a\u80fd\u7cfb\u7edf\u7684\u201c\u901a\u7528\u5927\u8111\u201d\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u91d1\u878d\u3001\u533b\u7597\u7b49\u573a\u666f\u63d0\u4f9b\u7406\u89e3\u3001\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u5176\u53d1\u5c55\u8def\u5f84\u4e5f\u4ece\u65e9\u671f\u7684\u5355\u6a21\u6001\u3001\u4e13\u7528\u6a21\u578b\uff0c\u5feb\u901f\u8f6c\u5411\u80fd\u591f\u540c\u65f6\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u7684\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b", "AI": {"tldr": "\u9996\u5148\uff0c\u5927\u6a21\u578b\u6b63\u9010\u6e10\u6210\u4e3a\u5404\u7c7b\u667a\u80fd\u7cfb\u7edf\u7684\u201c\u901a\u7528\u5927\u8111\u201d\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u91d1\u878d\u3001\u533b\u7597\u7b49\u573a\u666f\u63d0\u4f9b\u7406\u89e3\u3001\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u5176\u53d1\u5c55\u8def\u5f84\u4e5f\u4ece\u65e9\u671f\u7684\u5355\u6a21\u6001\u3001\u4e13\u7528\u6a21\u578b\uff0c\u5feb\u901f\u8f6c\u5411\u80fd\u591f\u540c\u65f6\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u7684\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
