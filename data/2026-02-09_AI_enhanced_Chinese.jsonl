{"id": "2602.06090", "categories": ["cs.SE", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06090", "abs": "https://arxiv.org/abs/2602.06090", "authors": ["Xiaoxuan Tang", "Jincheng Wang", "Liwei Luo", "Jingxuan Xu", "Sheng Zhou", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "SVRepair: Structured Visual Reasoning for Automated Program Repair", "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \\textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \\textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \\emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \\textbf{36.47\\%} accuracy on SWE-Bench M, \\textbf{38.02\\%} on MMCode, and \\textbf{95.12\\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.", "AI": {"tldr": "SVRepair\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\u5c06\u89c6\u89c9\u5de5\u4ef6\u8f6c\u6362\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5e2e\u52a9\u4ee3\u7801\u4ee3\u7406\u5b9a\u4f4d\u6545\u969c\u5e76\u751f\u6210\u8865\u4e01\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u591a\u4e3a\u5355\u6a21\u6001\uff0c\u65e0\u6cd5\u5229\u7528bug\u62a5\u544a\u4e2d\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f\uff08\u5982\u622a\u56fe\u3001\u63a7\u5236\u6d41\u56fe\uff09\u3002\u76f4\u63a5\u4f7f\u7528\u5bc6\u96c6\u89c6\u89c9\u8f93\u5165\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u566a\u58f0\uff0c\u96be\u4ee5\u5c06\u89c6\u89c9\u89c2\u5bdf\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u6545\u969c\u5b9a\u4f4d\u548c\u53ef\u6267\u884c\u8865\u4e01\u3002", "method": "1) \u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578bSVR\uff0c\u5c06\u5f02\u6784\u89c6\u89c9\u5de5\u4ef6\u7edf\u4e00\u8f6c\u6362\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u6355\u6349GUI\u5143\u7d20\u53ca\u5176\u7ed3\u6784\u5173\u7cfb\uff1b2) \u57fa\u4e8e\u573a\u666f\u56fe\u9a71\u52a8\u4ee3\u7801\u4ee3\u7406\u8fdb\u884c\u6545\u969c\u5b9a\u4f4d\u548c\u8865\u4e01\u5408\u6210\uff1b3) \u5f15\u5165\u8fed\u4ee3\u89c6\u89c9\u5de5\u4ef6\u5206\u5272\u7b56\u7565\uff0c\u9010\u6b65\u7f29\u5c0f\u8f93\u5165\u5230bug\u76f8\u5173\u533a\u57df\u4ee5\u51cf\u5c11\u65e0\u5173\u4e0a\u4e0b\u6587\u548c\u5e7b\u89c9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff1aSWE-Bench M\u4e0a36.47%\u51c6\u786e\u7387\uff0cMMCode\u4e0a38.02%\u51c6\u786e\u7387\uff0cCodeVision\u4e0a95.12%\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86SVRepair\u5728\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SVRepair\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\u6709\u6548\u6865\u63a5\u4e86\u89c6\u89c9\u8bed\u4e49\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u4fe1\u606f\u5728\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "code agent"}}
{"id": "2602.06098", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06098", "abs": "https://arxiv.org/abs/2602.06098", "authors": ["Nicolas Menet", "Michael Hersche", "Andreas Krause", "Abbas Rahimi"], "title": "Coding Agents with Environment Interaction: A Theoretical Perspective", "comment": "preprint", "summary": "Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.", "AI": {"tldr": "\u672c\u6587\u4e3a\u7f16\u7801\u4ee3\u7406\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u4e2d\u7684\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u4e86\u6982\u7387\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4ee3\u7801\u9009\u62e9\u548c\u53cd\u9988\u751f\u6210\u4e24\u79cd\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u4f18\u4e8e\u529f\u80fd\u7b49\u4ef7\u4f30\u8ba1\u5668\uff0c\u5e76\u5c06\u53cd\u5411\u63d0\u793a\u5f62\u5f0f\u5316\u4e3aThompson\u91c7\u6837\u7684\u8fd1\u4f3c\uff0c\u89e3\u91ca\u4e86\u5176\u6548\u679c\u53d7\u4efb\u52a1\u63cf\u8ff0\u6a21\u7cca\u6027\u9650\u5236\u7684\u539f\u56e0\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u5728\u6d4b\u8bd5\u9a71\u52a8\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u7684\u7406\u8bba\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u4e3a\u4e24\u79cd\u4e3b\u5bfc\u8303\u5f0f\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\uff1a\u57fa\u4e8e\u6267\u884c\u73af\u5883\u7684\u4ee3\u7801\u751f\u6210\u540e\u9009\u62e9\uff0c\u4ee5\u53ca\u57fa\u4e8e\u73af\u5883\u53cd\u9988\u7684\u6761\u4ef6\u4ee3\u7801\u751f\u6210\u3002", "method": "1. \u5c06\u6210\u719f\u7684\u4ee3\u7801\u9009\u62e9\u542f\u53d1\u5f0f\u65b9\u6cd5\u5f62\u5f0f\u5316\u4e3a\u73af\u5883\u611f\u77e5\u7684\u4ee3\u7801\u6b63\u786e\u6027\u4f30\u8ba1\u5668\uff1b2. \u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u57fa\u4e8e\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u7684\u4f30\u8ba1\u5668\u4f18\u4e8e\u57fa\u4e8e\u529f\u80fd\u7b49\u4ef7\u7684\u4f30\u8ba1\u5668\uff1b3. \u5c06\u53cd\u5411\u63d0\u793a\u6846\u67b6\u5316\u4e3aThompson\u91c7\u6837\u7684\u4e0a\u4e0b\u6587\u8fd1\u4f3c\uff1b4. \u63a8\u5bfc\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u5206\u91cf\u7684\u5956\u52b1\u51fd\u6570\u7684\u9057\u61be\u754c\uff1b5. \u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\u4e0a\u9a8c\u8bc1\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u901a\u8fc7\u6dfb\u52a0\u5f52\u7eb3\u504f\u7f6e\uff0c\u5728\u4fe1\u566a\u6bd4\u65b9\u9762\u4e25\u683c\u4f18\u4e8e\u529f\u80fd\u7b49\u4ef7\u4f30\u8ba1\u5668\u3002\u53cd\u5411\u63d0\u793a\u7684\u6548\u679c\u53d7\u4efb\u52a1\u63cf\u8ff0\u6a21\u7cca\u6027\u9650\u5236\uff08\u4e0d\u53ef\u7ea6\u9057\u61be\uff09\u3002\u5b9e\u9a8c\u5728BigCodeBenchHard\u3001LeetCodeDataset\u548cQiskitHumanEvalSim\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002\u57fa\u4e8e\u5f62\u5f0f\u5316\u5206\u6790\u63d0\u51fa\u4e86\u6539\u8fdb\u4efb\u52a1\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u65b0\u57fa\u51c6QiskitHumanEvalSimX\u3002", "conclusion": "\u672c\u6587\u4e3a\u7f16\u7801\u4ee3\u7406\u7684\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u67d0\u4e9b\u7b56\u7565\u6709\u6548\u800c\u5176\u4ed6\u7b56\u7565\u53d7\u9650\u3002\u7406\u8bba\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u73b0\u6709\u73b0\u8c61\uff0c\u8fd8\u6307\u5bfc\u4e86\u5982\u4f55\u6539\u8fdb\u4efb\u52a1\u63cf\u8ff0\u4ee5\u63d0\u9ad8\u7f16\u7801\u4ee3\u7406\u6027\u80fd\u3002\u5f62\u5f0f\u5316\u5206\u6790\u4e3a\u672a\u6765\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2602.06310", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06310", "abs": "https://arxiv.org/abs/2602.06310", "authors": ["Aldeida Aleti", "Baishakhi Ray", "Rashina Hoda", "Simin Chen"], "title": "Trustworthy AI Software Engineers", "comment": "The first three authors contributed equally to this work", "summary": "With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \\textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8AI\u7f16\u7801\u4ee3\u7406\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u5b9a\u4e49\u53ca\u5176\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u5728\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\u5efa\u7acb\u9002\u5f53\u4fe1\u4efb\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u672c\u8d28\u5b9a\u4e49\u53d7\u5230\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6AI\u4ee3\u7406\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u542b\u4e49\uff0c\u5e76\u6df1\u5165\u601d\u8003\u5982\u4f55\u5efa\u7acb\u53ef\u4fe1\u7684AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u3002", "method": "\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4f20\u7edf\u5b9a\u4e49\u548c\u8fd1\u671f\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7814\u7a76\uff0c\u5c06AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6982\u5ff5\u5316\u4e3a\u4eba\u7c7b-AI\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u7684\u53c2\u4e0e\u8005\u3002\u901a\u8fc7\u5386\u53f2\u89c6\u89d2\u548c\u65b0\u5174\u613f\u666f\uff0c\u8bc6\u522b\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\uff1a\u6280\u672f\u8d28\u91cf\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u3001\u8ba4\u77e5\u8c26\u900a\u3001\u793e\u4f1a\u548c\u4f26\u7406\u5bf9\u9f50\u3002\u540c\u65f6\u6307\u51fa\u4e86\u4fe1\u4efb\u6d4b\u91cf\u7684\u6839\u672c\u5dee\u8ddd\u2014\u2014\u5e76\u975e\u6240\u6709\u91cd\u8981\u56e0\u7d20\u90fd\u80fd\u8f7b\u6613\u6d4b\u91cf\u3002", "conclusion": "\u5021\u5bfc\u91c7\u7528\"\u8bbe\u8ba1\u5373\u4f26\u7406\"\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u6cbb\u7406AI\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\uff0c\u4ee5\u5728\u672a\u6765\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\u5efa\u7acb\u9002\u5f53\u7684\u4fe1\u4efb\u3002", "topic": "agent analysis"}}
{"id": "2602.06107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06107", "abs": "https://arxiv.org/abs/2602.06107", "authors": ["Zhuoming Chen", "Hongyi Liu", "Yang Zhou", "Haizhong Zheng", "Beidi Chen"], "title": "Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning", "comment": "ICLR 2026", "summary": "Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \\sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.", "AI": {"tldr": "Jackpot\u6846\u67b6\u901a\u8fc7\u6700\u4f18\u9884\u7b97\u62d2\u7edd\u91c7\u6837(OBRS)\u51cf\u5c11rollout\u6a21\u578b\u4e0e\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5b9e\u73b0LLM\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u89e3\u8026\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "LLM\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u751f\u6210\u6210\u672c\u9ad8\u6602\uff0c\u89e3\u8026rollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4f46\u4f1a\u5f15\u5165\u4e25\u91cd\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faJackpot\u6846\u67b6\uff0c\u91c7\u7528\u6700\u4f18\u9884\u7b97\u62d2\u7edd\u91c7\u6837(OBRS)\u76f4\u63a5\u51cf\u5c11rollout\u6a21\u578b\u4e0e\u6f14\u5316\u7b56\u7565\u4e4b\u95f4\u7684\u5dee\u5f02\uff1b\u5305\u542b\u539f\u5219\u6027OBRS\u6d41\u7a0b\u3001\u8054\u5408\u66f4\u65b0\u7b56\u7565\u548crollout\u6a21\u578b\u7684\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8etop-k\u6982\u7387\u4f30\u8ba1\u548c\u6279\u6b21\u7ea7\u504f\u5dee\u6821\u6b63\u7684\u9ad8\u6548\u7cfb\u7edf\u5b9e\u73b0\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eOBRS\u5728\u53ef\u63a7\u63a5\u53d7\u9884\u7b97\u4e0b\u6301\u7eed\u4f7frollout\u5206\u5e03\u66f4\u63a5\u8fd1\u76ee\u6807\u5206\u5e03\uff1b\u5b9e\u8bc1\u663e\u793a\u76f8\u6bd4\u91cd\u8981\u6027\u91c7\u6837\u57fa\u7ebf\uff0cJackpot\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728Qwen3-8B-Base\u4e0a\u8bad\u7ec3300\u6b65(\u6279\u6b21\u5927\u5c0f64)\u65f6\u8fbe\u5230\u4e0eon-policy RL\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "OBRS-based alignment\u4f7fLLM\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u89e3\u8026\u66f4\u63a5\u8fd1\u5b9e\u7528\u548c\u6709\u6548\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06593", "abs": "https://arxiv.org/abs/2602.06593", "authors": ["Robert Hutter", "Michael Pradel"], "title": "AgentStepper: Interactive Debugging of Software Development Agents", "comment": null, "summary": "Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.", "AI": {"tldr": "AgentStepper\uff1a\u9996\u4e2a\u7528\u4e8eLLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u4ea4\u4e92\u5f0f\u8c03\u8bd5\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u8868\u793a\u3001\u65ad\u70b9\u3001\u5355\u6b65\u6267\u884c\u7b49\u529f\u80fd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u8c03\u8bd5\u590d\u6742\u4ee3\u7406\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u8c03\u8bd5\u56f0\u96be\u3002\u5f00\u53d1\u8005\u9700\u8981\u7406\u89e3LLM\u67e5\u8be2\u3001\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7801\u4fee\u6539\u7684\u590d\u6742\u8f68\u8ff9\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u4ee5\u53ef\u7406\u89e3\u7684\u65b9\u5f0f\u5c55\u793a\u8fd9\u4e9b\u4e2d\u95f4\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faAgentStepper\u8c03\u8bd5\u5668\uff0c\u5c06\u4ee3\u7406\u8f68\u8ff9\u8868\u793a\u4e3aLLM\u3001\u4ee3\u7406\u7a0b\u5e8f\u548c\u5de5\u5177\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u5bf9\u8bdd\u3002\u652f\u6301\u65ad\u70b9\u3001\u5355\u6b65\u6267\u884c\u3001\u5b9e\u65f6\u7f16\u8f91\u63d0\u793a\u8bcd\u548c\u5de5\u5177\u8c03\u7528\uff0c\u540c\u65f6\u6355\u83b7\u548c\u663e\u793a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u53d8\u66f4\u3002", "result": "\u5728\u4e09\u4e2a\u5148\u8fdb\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\uff08ExecutionAgent\u3001SWE-Agent\u3001RepairAgent\uff09\u4e0a\u96c6\u6210\u4ec5\u970039-42\u884c\u4ee3\u7801\u4fee\u6539\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cAgentStepper\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u7406\u89e3\u80fd\u529b\uff0864% vs 67%\u5e73\u5747\u8868\u73b0\uff09\u3001bug\u8bc6\u522b\u6210\u529f\u7387\uff0817% vs 60%\uff09\uff0c\u5e76\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377\uff08\u632b\u8d25\u611f\u4ece5.4/7.0\u964d\u81f32.4/7.0\uff09\u3002", "conclusion": "AgentStepper\u901a\u8fc7\u63d0\u4f9b\u7c7b\u4f3c\u4f20\u7edf\u8f6f\u4ef6\u8c03\u8bd5\u4f46\u66f4\u9ad8\u62bd\u8c61\u7ea7\u522b\u7684\u4ea4\u4e92\u5f0f\u8c03\u8bd5\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u8c03\u8bd5\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u7406\u89e3\u548c\u8c03\u8bd5\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.06176", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06176", "abs": "https://arxiv.org/abs/2602.06176", "authors": ["Peiyang Song", "Pengrui Han", "Noah Goodman"], "title": "Large Language Model Reasoning Failures", "comment": "Repository: https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures. Published at TMLR 2026 with Survey Certification", "summary": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9LLM\u63a8\u7406\u5931\u8d25\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u53cc\u91cd\u5206\u7c7b\u6846\u67b6\uff1a\u5c06\u63a8\u7406\u5206\u4e3a\u5177\u8eab\u4e0e\u975e\u5177\u8eab\uff08\u76f4\u89c9\u4e0e\u903b\u8f91\uff09\uff0c\u5c06\u5931\u8d25\u5206\u4e3a\u57fa\u7840\u67b6\u6784\u7f3a\u9677\u3001\u9886\u57df\u7279\u5b9a\u9650\u5236\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u7f13\u89e3\u7b56\u7565\u4e0e\u5f00\u6e90\u8d44\u6e90\u5e93\u3002", "motivation": "\u5c3d\u7ba1LLM\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u770b\u4f3c\u7b80\u5355\u7684\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u5931\u8d25\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u5931\u8d25\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u548c\u5206\u7c7b\uff0c\u7814\u7a76\u788e\u7247\u5316\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u8bc6\u522b\u3001\u5206\u6790\u548c\u7f13\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u5206\u7c7b\u6846\u67b6\uff1a1) \u63a8\u7406\u7c7b\u578b\u5206\u7c7b\uff1a\u5177\u8eab\u63a8\u7406 vs \u975e\u5177\u8eab\u63a8\u7406\uff08\u76f4\u89c9\u63a8\u7406 vs \u903b\u8f91\u63a8\u7406\uff09\uff1b2) \u5931\u8d25\u7c7b\u578b\u5206\u7c7b\uff1a\u57fa\u7840\u67b6\u6784\u7f3a\u9677\u3001\u9886\u57df\u7279\u5b9a\u9650\u5236\u3001\u9c81\u68d2\u6027\u95ee\u9898\u3002\u5bf9\u6bcf\u79cd\u5931\u8d25\u63d0\u4f9b\u5b9a\u4e49\u3001\u73b0\u6709\u7814\u7a76\u5206\u6790\u3001\u6839\u672c\u539f\u56e0\u63a2\u8ba8\u548c\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u5168\u9762\u7684LLM\u63a8\u7406\u5931\u8d25\u5206\u7c7b\u4f53\u7cfb\uff0c\u7edf\u4e00\u4e86\u788e\u7247\u5316\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\u3002\u521b\u5efa\u4e86GitHub\u8d44\u6e90\u5e93\u6536\u96c6\u76f8\u5173\u7814\u7a76\u5de5\u4f5c\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u5207\u5165\u70b9\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3LLM\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5f3a\u3001\u66f4\u53ef\u9760\u3001\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.06671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06671", "abs": "https://arxiv.org/abs/2602.06671", "authors": ["Shijia Dong", "Haoruo Zhao", "Paul Harvey"], "title": "Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study", "comment": "Accepted at the 3rd International Workshop on Large Language Models for Code (LLM4Code 2026), co-located with ICSE 2026", "summary": "Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.", "AI": {"tldr": "AST(NIT)\u662f\u4e00\u79cdAST\u589e\u5f3a\u548c\u5e8f\u5217\u5316\u65b9\u6cd5\uff0c\u5c06\u5b8c\u6574\u7684AST\u7ed3\u6784\u4fe1\u606f\u7f16\u7801\u4e3aLLM\u517c\u5bb9\u5e8f\u5217\uff0c\u5728\u4fdd\u6301\u4ee3\u7801\u6458\u8981\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u4ee3\u7801\u6458\u8981\u6a21\u578b\u5df2\u8bc1\u660eAST\u80fd\u63d0\u5347\u6458\u8981\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u539f\u59cb\u4ee3\u7801\u6216\u4ec5\u4f7f\u7528\u90e8\u5206AST\u4fe1\u53f7\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5b8c\u6574AST\u8868\u793a\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faAST(NIT)\u65b9\u6cd5\uff1a1\uff09AST\u589e\u5f3a\uff0c\u4fdd\u7559\u8bcd\u6cd5\u7ec6\u8282\uff1b2\uff09\u5e8f\u5217\u5316\uff0c\u5c06\u7ed3\u6784\u4fe1\u606f\u7f16\u7801\u4e3aLLM\u517c\u5bb9\u5e8f\u5217\uff1b3\uff09\u4f7f\u7528LLaMA-3.1-8B\u6a21\u578b\u5728CodeXGLUE Python\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u5e8f\u5217\u5316AST\u51cf\u5c11\u4e86LLM\u8f93\u5165\u957f\u5ea6\uff1b2\uff09\u7f29\u77ed\u4e86\u8bad\u7ec3\u65f6\u95f4\uff1b3\uff09\u5728\u4ee3\u7801\u6458\u8981\u8d28\u91cf\u4e0a\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "AST(NIT)\u65b9\u6cd5\u6210\u529f\u5c06\u5b8c\u6574AST\u8868\u793a\u5e94\u7528\u4e8eLLM\u4ee3\u7801\u6458\u8981\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e3aLLM\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u7f16\u7801\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.06051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06051", "abs": "https://arxiv.org/abs/2602.06051", "authors": ["Kexin Ma", "Bojun Li", "Yuhua Tang", "Ruochun Jin", "Liting Sun"], "title": "CAST: Character-and-Scene Episodic Memory for Agents", "comment": null, "summary": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u620f\u5267\u7406\u8bba\u7684\u89d2\u8272\u4e0e\u573a\u666f\u8bb0\u5fc6\u67b6\u6784(CAST)\uff0c\u901a\u8fc7\u6784\u5efa3D\u573a\u666f(\u65f6\u95f4/\u5730\u70b9/\u4e3b\u9898)\u5e76\u7ec4\u7ec7\u6210\u89d2\u8272\u6863\u6848\u6765\u8868\u793a\u60c5\u666f\u8bb0\u5fc6\uff0c\u7ed3\u5408\u56fe\u5f0f\u8bed\u4e49\u8bb0\u5fc6\u5f62\u6210\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u56de\u5fc6\uff0c\u5c06\u7ecf\u9a8c\u5904\u7406\u4e3a\u952e\u503c\u5bf9\u3001\u5411\u91cf\u6216\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u8868\u793a\u548c\u68c0\u7d22\u8fde\u8d2f\u7684\u4e8b\u4ef6\u3002\u4eba\u7c7b\u7684\u60c5\u666f\u8bb0\u5fc6\u80fd\u591f\u56de\u5fc6\u57fa\u4e8e\u8c01\u3001\u4f55\u65f6\u3001\u4f55\u5730\u7684\u8fde\u8d2f\u4e8b\u4ef6\uff0c\u9700\u8981\u66f4\u597d\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u652f\u6301\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51faCAST\u8bb0\u5fc6\u67b6\u6784\uff0c\u53d7\u620f\u5267\u7406\u8bba\u542f\u53d1\uff1a1)\u6784\u5efa3D\u573a\u666f(\u65f6\u95f4/\u5730\u70b9/\u4e3b\u9898)\u4f5c\u4e3a\u60c5\u666f\u8bb0\u5fc6\u57fa\u7840\uff1b2)\u5c06\u573a\u666f\u7ec4\u7ec7\u6210\u89d2\u8272\u6863\u6848\uff0c\u603b\u7ed3\u89d2\u8272\u7684\u4e8b\u4ef6\uff1b3)\u7ed3\u5408\u56fe\u5f0f\u8bed\u4e49\u8bb0\u5fc6\u5f62\u6210\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u589e\u5f3a\u8bb0\u5fc6\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCAST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53478.11% F1\u5206\u6570\u548c10.21% LLM-as-a-Judge\u8bc4\u5206\uff0c\u5c24\u5176\u5728\u5f00\u653e\u6027\u548c\u65f6\u95f4\u654f\u611f\u6027\u5bf9\u8bdd\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CAST\u901a\u8fc7\u620f\u5267\u7406\u8bba\u542f\u53d1\u7684\u89d2\u8272\u4e0e\u573a\u666f\u67b6\u6784\u6709\u6548\u8868\u793a\u60c5\u666f\u8bb0\u5fc6\uff0c\u7ed3\u5408\u8bed\u4e49\u8bb0\u5fc6\u5f62\u6210\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2602.06103", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06103", "abs": "https://arxiv.org/abs/2602.06103", "authors": ["Zhaoyang Chen", "Cody Fleming"], "title": "Toward Faithful and Complete Answer Construction from a Single Document", "comment": null, "summary": "Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning.\n  Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\\% and 29\\%, respectively, with a corresponding 31\\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.", "AI": {"tldr": "EVE\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6-\u9a8c\u8bc1-\u679a\u4e3e\u7684\u7ba1\u9053\u7ea6\u675f\u751f\u6210\u8fc7\u7a0b\uff0c\u89e3\u51b3LLM\u5728\u6587\u6863\u63a8\u7406\u4e2d\u5b8c\u6574\u6027\u548c\u5fe0\u5b9e\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u7edf\u8ba1\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\uff0c\u504f\u5411\u9ad8\u6982\u7387\u5ef6\u7eed\u800c\u975e\u57fa\u4e8e\u6e90\u5185\u5bb9\u7684\u5168\u9762\u5fe0\u5b9e\u56de\u7b54\uff0c\u7f3a\u4e4f\u786e\u4fdd\u5b8c\u6574\u6027\u548c\u5fe0\u5b9e\u6027\u7684\u7cfb\u7edf\u673a\u5236\uff0c\u8fd9\u4e0eAI\u5b89\u5168\u539f\u5219\u76f8\u51b2\u7a81\u3002", "method": "\u63d0\u51faEVE\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5c06\u9ad8\u4e25\u8c28\u63a8\u7406\u5206\u89e3\u4e3a\u63d0\u53d6\u3001\u9a8c\u8bc1\u548c\u679a\u4e3e\u4e09\u4e2a\u6b65\u9aa4\uff0c\u901a\u8fc7\u7ea6\u675f\u751f\u6210\u7ba1\u9053\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u6587\u6863\u63a8\u7406\u3002", "result": "\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u5206\u522b\u63d0\u5347\u9ad8\u8fbe24%\u548c29%\uff0cF1\u5206\u6570\u63d0\u534731%\uff0c\u6253\u7834\u4e86\u5355\u6b21LLM\u751f\u6210\u4e2d\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\u7684\u6743\u8861\uff0c\u540c\u65f6\u7f13\u89e3\u4e86\u957f\u5ea6\u9650\u5236\u5bfc\u81f4\u7684\u622a\u65ad\u95ee\u9898\u3002", "conclusion": "EVE\u901a\u8fc7\u7ed3\u6784\u5316\u7ba1\u9053\u663e\u8457\u63d0\u5347\u6587\u6863\u63a8\u7406\u7684\u5b8c\u6574\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u4f46\u53d7\u81ea\u7136\u8bed\u8a00\u56fa\u6709\u6a21\u7cca\u6027\u9650\u5236\u5b58\u5728\u6027\u80fd\u9971\u548c\uff0c\u53cd\u6620\u4e86\u57fa\u4e8e\u8bed\u8a00\u63a8\u7406\u7684\u57fa\u672c\u9650\u5236\u3002", "topic": "agent analysis"}}
{"id": "2602.06052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06052", "abs": "https://arxiv.org/abs/2602.06052", "authors": ["Wei-Chieh Huang", "Weizhi Zhang", "Yueqing Liang", "Yuanchen Bei", "Yankai Chen", "Tao Feng", "Xinyu Pan", "Zhen Tan", "Yu Wang", "Tianxin Wei", "Shanglin Wu", "Ruiyao Xu", "Liangwei Yang", "Rui Yang", "Wooseong Yang", "Chin-Yuan Yeh", "Hanrong Zhang", "Haozhen Zhang", "Siqi Zhu", "Henry Peng Zou", "Wanjia Zhao", "Song Wang", "Wujiang Xu", "Zixuan Ke", "Zheng Hui", "Dawei Li", "Yaozu Wu", "Langzhou He", "Chen Wang", "Xiongxiao Xu", "Baixiang Huang", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Ahmed A. Metwally", "Jun Yan", "Chen-Yu Lee", "Hanqing Zeng", "Yinglong Xia", "Xiaokai Wei", "Ali Payani", "Yu Wang", "Haitong Ma", "Wenya Wang", "Chengguang Wang", "Yu Zhang", "Xin Wang", "Yongfeng Zhang", "Jiaxuan You", "Hanghang Tong", "Xiao Luo", "Yizhou Sun", "Wei Wang", "Julian McAuley", "James Zou", "Jiawei Han", "Philip S. Yu", "Kai Shu"], "title": "Rethinking Memory Mechanisms of Foundation Agents in the Second Half", "comment": null, "summary": "The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the \"second half,\" the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u7ef4\u6846\u67b6\uff08\u8bb0\u5fc6\u57fa\u8d28\u3001\u8ba4\u77e5\u673a\u5236\u3001\u8bb0\u5fc6\u4e3b\u4f53\uff09\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u667a\u80fd\u4f53\u62d3\u6251\u4e2d\u7684\u8bb0\u5fc6\u5b9e\u73b0\u4e0e\u64cd\u4f5c\uff0c\u5e76\u56de\u987e\u4e86\u8bc4\u4f30\u57fa\u51c6\u4e0e\u672a\u6765\u6311\u6218\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u6b63\u4ece\u6ce8\u91cd\u6a21\u578b\u521b\u65b0\u548c\u57fa\u51c6\u5206\u6570\u8f6c\u5411\u5f3a\u8c03\u95ee\u9898\u5b9a\u4e49\u548c\u73b0\u5b9e\u4e16\u754c\u8bc4\u4f30\u3002\u5728\"\u4e0b\u534a\u573a\"\u4e2d\uff0c\u6838\u5fc3\u6311\u6218\u662f\u5728\u957f\u65f6\u7a0b\u3001\u52a8\u6001\u3001\u7528\u6237\u4f9d\u8d56\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u9645\u6548\u7528\uff0c\u667a\u80fd\u4f53\u9762\u4e34\u4e0a\u4e0b\u6587\u7206\u70b8\uff0c\u9700\u8981\u6301\u7eed\u79ef\u7d2f\u3001\u7ba1\u7406\u548c\u9009\u62e9\u6027\u91cd\u7528\u5927\u91cf\u4fe1\u606f\u3002\u8bb0\u5fc6\u56e0\u6b64\u6210\u4e3a\u586b\u8865\u6548\u7528\u9e3f\u6c9f\u7684\u5173\u952e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u667a\u80fd\u4f53\u8bb0\u5fc6\u4e09\u7ef4\u6846\u67b6\uff1a1) \u8bb0\u5fc6\u57fa\u8d28\uff08\u5185\u90e8\u548c\u5916\u90e8\uff09\uff0c2) \u8ba4\u77e5\u673a\u5236\uff08\u60c5\u666f\u3001\u8bed\u4e49\u3001\u611f\u77e5\u3001\u5de5\u4f5c\u3001\u7a0b\u5e8f\u8bb0\u5fc6\uff09\uff0c3) \u8bb0\u5fc6\u4e3b\u4f53\uff08\u667a\u80fd\u4f53\u4e2d\u5fc3\u4e0e\u7528\u6237\u4e2d\u5fc3\uff09\u3002\u5206\u6790\u4e86\u4e0d\u540c\u667a\u80fd\u4f53\u62d3\u6251\u4e2d\u7684\u8bb0\u5fc6\u5b9e\u4f8b\u5316\u548c\u64cd\u4f5c\uff0c\u5f3a\u8c03\u4e86\u8bb0\u5fc6\u64cd\u4f5c\u7684\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5206\u7c7b\u548c\u5206\u6790\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u8bb0\u5fc6\u7684\u4e0d\u540c\u7ef4\u5ea6\u3001\u5b9e\u73b0\u65b9\u5f0f\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u5f53\u524d\u6570\u767e\u7bc7\u76f8\u5173\u8bba\u6587\u4e2d\u7684\u8bb0\u5fc6\u7814\u7a76\u3002", "conclusion": "\u8bb0\u5fc6\u662f\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u6548\u7528\u95ee\u9898\u7684\u5173\u952e\u3002\u8bba\u6587\u63d0\u51fa\u7684\u4e09\u7ef4\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8bc4\u4f30\u57fa\u51c6\u3001\u5ea6\u91cf\u6807\u51c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u7b49\u5f00\u653e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2602.06286", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06286", "abs": "https://arxiv.org/abs/2602.06286", "authors": ["Khurram Yamin", "Jingjing Tang", "Santiago Cortes-Gomez", "Amit Sharma", "Eric Horvitz", "Bryan Wilder"], "title": "Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \\emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76LLM\u662f\u5426\u4f5c\u4e3a\u7406\u6027\u6548\u7528\u6700\u5927\u5316\u8005\u8fd0\u4f5c\uff0c\u901a\u8fc7\u8bca\u65ad\u6311\u6218\u95ee\u9898\u6d4b\u8bd5\u5176\u4fe1\u5ff5\u4e00\u81f4\u6027\u548c\u504f\u597d\u7a33\u5b9a\u6027\uff0c\u63d0\u51fa\u53ef\u8bc1\u4f2a\u6761\u4ef6\u6765\u8bc4\u4f30LLM\u5728\u533b\u7597\u8bca\u65ad\u7b49\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u5176\u51b3\u7b56\u903b\u8f91\u96be\u4ee5\u89e3\u91ca\uff0c\u9700\u8981\u7814\u7a76LLM\u662f\u5426\u5177\u6709\u4e00\u81f4\u7684\u4fe1\u5ff5\u548c\u7a33\u5b9a\u7684\u504f\u597d\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u8bca\u65ad\u6311\u6218\u95ee\u9898\u6d4b\u8bd5LLM\u884c\u4e3a\uff0c\u5206\u6790\u5176\u62a5\u544a\u6982\u7387\u4e0e\u89c2\u5bdf\u884c\u52a8\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u53ef\u8bc1\u4f2a\u6761\u4ef6\u6765\u68c0\u9a8c\u62a5\u544a\u6982\u7387\u662f\u5426\u5bf9\u5e94\u4efb\u4f55\u7406\u6027\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u4fe1\u5ff5\uff0c\u5e76\u5728\u591a\u4e2a\u533b\u7597\u8bca\u65ad\u9886\u57df\u8bc4\u4f30\u591a\u4e2aLLM\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3aLLM\u63a8\u7406\u4e0e\u7406\u60f3\u8d1d\u53f6\u65af\u6548\u7528\u6700\u5927\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u8bc6\u522b\u51faLLM\u62a5\u544a\u6982\u7387\u65e0\u6cd5\u5bf9\u5e94\u4efb\u4f55\u7406\u6027\u667a\u80fd\u4f53\u771f\u5b9e\u4fe1\u5ff5\u7684\u60c5\u51b5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9LLM\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdbLLM\u51b3\u7b56\u53ef\u9760\u6027\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.06319", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06319", "abs": "https://arxiv.org/abs/2602.06319", "authors": ["Qifan Zhang", "Jianhao Ruan", "Aochuan Chen", "Kang Zeng", "Nuo Chen", "Jing Tang", "Jia Li"], "title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems", "comment": null, "summary": "Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.", "AI": {"tldr": "GrAlgoBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u56fe\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc79\u4e2a\u4efb\u52a1\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4e24\u5927\u5f31\u70b9\uff1a\u51c6\u786e\u7387\u968f\u8282\u70b9\u6570\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u3001\u4ee3\u7801\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u3001\u6311\u6218\u6027\u4e0d\u8db3\u3001\u7b54\u6848\u96be\u4ee5\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u3002\u9700\u8981\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1GrAlgoBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56fe\u7b97\u6cd5\u95ee\u9898\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u3002\u56fe\u7b97\u6cd5\u95ee\u9898\u7279\u522b\u9002\u5408\u6d4b\u8bd5\u63a8\u7406\u80fd\u529b\uff1a\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u96be\u5ea6\u53ef\u7cbe\u7ec6\u63a7\u5236\u3001\u652f\u6301\u6807\u51c6\u5316\u7a0b\u5e8f\u5316\u8bc4\u4f30\u3002\u901a\u8fc79\u4e2a\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u4e24\u5927\u5f31\u70b9\uff1a1) \u51c6\u786e\u7387\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u5f53\u56fe\u8d85\u8fc7120\u4e2a\u8282\u70b9\u65f6\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff0c\u4e3b\u8981\u7531\u6267\u884c\u9519\u8bef\u3001\u5f31\u8bb0\u5fc6\u548c\u5197\u4f59\u63a8\u7406\u5bfc\u81f4\uff1b2) \u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u4e3b\u8981\u7531\u5927\u91cf\u65e0\u6548\u7684\u81ea\u6211\u9a8c\u8bc1\u5f15\u8d77\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u8f68\u8ff9\u4f46\u672a\u63d0\u9ad8\u6b63\u786e\u6027\u3002", "conclusion": "GrAlgoBench\u901a\u8fc7\u66b4\u9732\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u786e\u7acb\u4e86\u56fe\u7b97\u6cd5\u95ee\u9898\u4f5c\u4e3a\u4e25\u683c\u3001\u591a\u7ef4\u4e14\u5b9e\u9645\u76f8\u5173\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2602.06875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06875", "abs": "https://arxiv.org/abs/2602.06875", "authors": ["Jiangping Huang", "Wenguang Ye", "Weisong Sun", "Jian Zhang", "Mingyue Zhang", "Yang Liu"], "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code", "comment": null, "summary": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.", "AI": {"tldr": "TraceCoder\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u8ffd\u8e2a\u3001\u56e0\u679c\u5206\u6790\u548c\u5386\u53f2\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u4fee\u590d\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4fee\u590d\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u7684\u901a\u8fc7/\u5931\u8d25\u4fe1\u53f7\uff0c\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u884c\u4e3a\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u9519\u8bef\uff0c\u4e14\u65e0\u6cd5\u4ece\u5386\u53f2\u5931\u8d25\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u4fee\u590d\u8fc7\u7a0b\u4f4e\u6548\u91cd\u590d\u3002", "method": "\u63d0\u51faTraceCoder\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u5728\u4ee3\u7801\u4e2d\u63d2\u5165\u8bca\u65ad\u63a2\u9488\u6355\u83b7\u8fd0\u884c\u65f6\u8ffd\u8e2a\uff1b2) \u5bf9\u8ffd\u8e2a\u8fdb\u884c\u56e0\u679c\u5206\u6790\u5b9a\u4f4d\u6839\u672c\u539f\u56e0\uff1b3) \u5f15\u5165\u5386\u53f2\u6559\u8bad\u5b66\u4e60\u673a\u5236\u4ece\u5148\u524d\u5931\u8d25\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff1b4) \u4f7f\u7528\u56de\u6eda\u673a\u5236\u786e\u4fdd\u6bcf\u6b21\u8fed\u4ee3\u90fd\u662f\u4e25\u683c\u6539\u8fdb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTraceCoder\u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u5728Pass@1\u51c6\u786e\u7387\u4e0a\u5b9e\u73b0\u6700\u9ad834.43%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u8fed\u4ee3\u4fee\u590d\u8fc7\u7a0b\u5355\u72ec\u8d21\u732e65.61%\u7684\u76f8\u5bf9\u589e\u76ca\uff0c\u4e14\u5728\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002", "conclusion": "TraceCoder\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u89c2\u5bdf-\u5206\u6790-\u4fee\u590d\u8fc7\u7a0b\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u8ffd\u8e2a\u3001\u56e0\u679c\u5206\u6790\u548c\u5386\u53f2\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u9519\u8bef\u5b9a\u4f4d\u548c\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "topic": "code agent"}}
{"id": "2602.06375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06375", "abs": "https://arxiv.org/abs/2602.06375", "authors": ["Yu Zhao", "Fan Jiang", "Tianle Liu", "Bo Zeng", "Yu Liu", "Longyue Wang", "Weihua Luo"], "title": "Difficulty-Estimated Policy Optimization", "comment": null, "summary": "Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faDEPO\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u96be\u5ea6\u4f30\u8ba1\u5668\u52a8\u6001\u7b5b\u9009\u8bad\u7ec3\u6570\u636e\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u5b66\u4e60\u6f5c\u529b\u6837\u672c\uff0c\u51cf\u5c112\u500d\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u6027\u80fd", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u5728\u95ee\u9898\u8fc7\u4e8e\u7b80\u5355\u6216\u590d\u6742\u65f6\u4f1a\u51fa\u73b0\u68af\u5ea6\u4fe1\u53f7\u8870\u51cf\uff0c\u5bfc\u81f4\u6536\u655b\u4e0d\u7a33\u5b9a\uff1b\u800cDAPO\u7b49\u53d8\u4f53\u867d\u7136\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u4f4e\u6548\u7528\u6837\u672c\u5e26\u6765\u7684\u5de8\u5927\u8ba1\u7b97\u5f00\u9500", "method": "\u63d0\u51faDifficulty-Estimated Policy Optimization (DEPO)\u6846\u67b6\uff0c\u96c6\u6210\u5728\u7ebf\u96be\u5ea6\u4f30\u8ba1\u5668\uff0c\u5728rollout\u9636\u6bb5\u524d\u52a8\u6001\u8bc4\u4f30\u548c\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\uff0c\u4f18\u5148\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u7ed9\u9ad8\u5b66\u4e60\u6f5c\u529b\u6837\u672c", "result": "DEPO\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684rollout\u6210\u672c\u964d\u4f4e\uff0c\u4e14\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u95e8\u69db", "conclusion": "DEPO\u4e3a\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u89c4\u6a21\u5316\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u53ef\u6301\u7eed\u7684\u8def\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "2602.06485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06485", "abs": "https://arxiv.org/abs/2602.06485", "authors": ["Haotian Chen", "Xin Cong", "Shengda Fan", "Yuyang Fu", "Ziqin Gong", "Yaxi Lu", "Yishan Li", "Boye Niu", "Chengjun Pan", "Zijun Song", "Huadong Wang", "Yesai Wu", "Yueying Wu", "Zihao Xie", "Yukun Yan", "Zhong Zhang", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents", "comment": null, "summary": "While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e864B\u53c2\u6570\u89c4\u6a21\u7684\u667a\u80fd\u4f53\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u51fa\u4e86AgentCPM-Explore\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\u7684\u4e09\u4e2a\u74f6\u9888\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u800c\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\uff084B\u53c2\u6570\u7ea7\u522b\uff09\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u5982\u4f55\u8bad\u7ec3\u9ad8\u6027\u80fd\u7684\u8fb9\u7f18\u89c4\u6a21\u667a\u80fd\u4f53\u6a21\u578b\u3002", "method": "\u63d0\u51faAgentCPM-Explore\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u53c2\u6570\u7a7a\u95f4\u6a21\u578b\u878d\u5408\uff08\u89e3\u51b3SFT\u707e\u96be\u6027\u9057\u5fd8\uff09\u3001\u5956\u52b1\u4fe1\u53f7\u53bb\u566a\uff08\u89e3\u51b3RL\u566a\u58f0\u654f\u611f\uff09\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u7cbe\u70bc\uff08\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9000\u5316\uff09\u3002", "result": "AgentCPM-Explore\u57284B\u7c7b\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a8B\u7c7bSOTA\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751a\u81f3\u8d85\u8d8a\u4e86Claude-4.5-Sonnet\u6216DeepSeek-v3.2\u7b49\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002\u5728GAIA\u6587\u672c\u4efb\u52a1\u4e0a\u8fbe\u523097.09%\u51c6\u786e\u7387\uff08pass@64\uff09\u3002", "conclusion": "\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\u7684\u74f6\u9888\u4e0d\u5728\u4e8e\u5176\u56fa\u6709\u80fd\u529b\u4e0a\u9650\uff0c\u800c\u5728\u4e8e\u63a8\u7406\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u5145\u5206\u91ca\u653e\u8fb9\u7f18\u89c4\u6a21\u6a21\u578b\u88ab\u4f4e\u4f30\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.06138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06138", "abs": "https://arxiv.org/abs/2602.06138", "authors": ["Fairoz Nower Khan", "Nabuat Zaman Nahim", "Ruiquan Huang", "Haibo Yang", "Peizhong Ju"], "title": "Flow Matching for Offline Reinforcement Learning with Discrete Actions", "comment": null, "summary": "Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u6d41\u5339\u914d\u6846\u67b6\uff0c\u5c06\u751f\u6210\u7b56\u7565\u6269\u5c55\u5230\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u548c\u591a\u76ee\u6807\u8bbe\u7f6e\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u901a\u8fc7\u56e0\u5b50\u5316\u6761\u4ef6\u8def\u5f84\u7f13\u89e3\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u589e\u957f\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u7684\u751f\u6210\u7b56\u7565\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e3b\u8981\u5c40\u9650\u4e8e\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3002\u4e3a\u4e86\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u79bb\u7ebfRL\u8bbe\u7f6e\uff0c\u9700\u8981\u5c06\u6d41\u5339\u914d\u6269\u5c55\u5230\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u548c\u591a\u76ee\u6807\u573a\u666f\u3002", "method": "1) \u7528\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u66ff\u4ee3\u8fde\u7eed\u6d41\uff0c\u4f7f\u7528Q\u52a0\u6743\u6d41\u5339\u914d\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff1b2) \u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u6761\u4ef6\u8def\u5f84\u7f13\u89e3\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u589e\u957f\uff1b3) \u901a\u8fc7\u52a8\u4f5c\u91cf\u5316\u5c06\u79bb\u6563\u6846\u67b6\u5e94\u7528\u4e8e\u8fde\u7eed\u63a7\u5236\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u4f18\u5316\u8be5\u76ee\u6807\u53ef\u6062\u590d\u6700\u4f18\u7b56\u7565\u3002\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5728\u9ad8\u7ef4\u63a7\u5236\u3001\u591a\u6a21\u6001\u51b3\u7b56\u548c\u591a\u76ee\u6807\u52a8\u6001\u504f\u597d\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u79bb\u6563\u6846\u67b6\u901a\u8fc7\u52a8\u4f5c\u91cf\u5316\u5728\u8fde\u7eed\u63a7\u5236\u95ee\u9898\u4e2d\u63d0\u4f9b\u8868\u793a\u590d\u6742\u6027\u4e0e\u6027\u80fd\u7684\u7075\u6d3b\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u79bb\u6563\u6d41\u5339\u914d\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86\u751f\u6210\u7b56\u7565\u7684\u5e94\u7528\u8303\u56f4\uff0c\u652f\u6301\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u548c\u591a\u76ee\u6807\u8bbe\u7f6e\uff0c\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u6709\u6548\u7f13\u89e3\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06260", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06260", "abs": "https://arxiv.org/abs/2602.06260", "authors": ["Pedro Cisneros-Velarde"], "title": "Can One-sided Arguments Lead to Response Change in Large Language Models?", "comment": null, "summary": "Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4ec5\u63d0\u4f9b\u5355\u65b9\u9762\u8bba\u636e\uff0c\u53ef\u4ee5\u7b80\u5355\u76f4\u89c2\u5730\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e89\u8bae\u6027\u95ee\u9898\u4e0a\u8f6c\u5411\u7279\u5b9a\u7acb\u573a\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u4e0d\u540c\u6a21\u578b\u3001\u8bba\u636e\u6570\u91cf\u548c\u8bdd\u9898\u4e2d\u666e\u904d\u5b58\u5728\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e89\u8bae\u6027\u95ee\u9898\u4e0a\u901a\u5e38\u63d0\u4f9b\u5e73\u8861\u56de\u7b54\u3001\u91c7\u53d6\u5355\u4e00\u7acb\u573a\u6216\u62d2\u7edd\u56de\u7b54\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u4ec5\u63d0\u4f9b\u5355\u65b9\u9762\u8bba\u636e\u8fd9\u79cd\u7b80\u5355\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u5f15\u5bfc\u6a21\u578b\u8f6c\u5411\u7279\u5b9a\u89c2\u70b9\u3002", "method": "\u6784\u5efa\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e09\u4e2a\u7ef4\u5ea6\uff1a(i) LLM\u56de\u5e94\u4e2d\u8bf1\u5bfc\u7684\u7acb\u573a\uff0c(ii) \u4e89\u8bae\u6027\u95ee\u9898\u7684\u8868\u8ff0\u65b9\u5f0f\uff0c(iii) \u8bba\u636e\u7684\u5448\u73b0\u65b9\u5f0f\u3002\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u3001\u8bba\u636e\u6570\u91cf\u548c\u8bdd\u9898\u4e0b\u7684\u610f\u89c1\u5f15\u5bfc\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u610f\u89c1\u5f15\u5bfc\u73b0\u8c61\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u666e\u904d\u5b58\u5728\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u3001\u8bba\u636e\u6570\u91cf\u548c\u8bdd\u9898\u3002\u5f53\u5207\u6362\u5230\u5176\u4ed6\u8bba\u636e\u65f6\uff0c\u610f\u89c1\u5f15\u5bfc\u6548\u679c\u4f1a\u4e00\u81f4\u4e0b\u964d\u3002", "conclusion": "\u4ec5\u63d0\u4f9b\u5355\u65b9\u9762\u8bba\u636e\u53ef\u4ee5\u6709\u6548\u5f15\u5bfcLLM\u5728\u4e89\u8bae\u6027\u95ee\u9898\u4e0a\u8f6c\u5411\u7279\u5b9a\u7acb\u573a\uff0c\u8fd9\u63ed\u793a\u4e86LLM\u5728\u610f\u89c1\u5f62\u6210\u8fc7\u7a0b\u4e2d\u7684\u53ef\u64cd\u63a7\u6027\uff0c\u5bf9\u7406\u89e3\u6a21\u578b\u504f\u89c1\u548c\u5b89\u5168\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2602.06486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06486", "abs": "https://arxiv.org/abs/2602.06486", "authors": ["Lanbo Lin", "Jiayao Liu", "Tianyuan Yang", "Li Cai", "Yuanwu Xu", "Lei Wei", "Sicong Xie", "Guannan Zhang"], "title": "JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks", "comment": null, "summary": "Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.", "AI": {"tldr": "JADE\u662f\u4e00\u4e2a\u4e24\u5c42\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f00\u653e\u7aef\u4e13\u4e1a\u4efb\u52a1\u4e0a\u7684\u667a\u80fd\u4f53AI\u3002\u7b2c\u4e00\u5c42\u5c06\u4e13\u5bb6\u77e5\u8bc6\u7f16\u7801\u4e3a\u9884\u5b9a\u4e49\u7684\u8bc4\u4f30\u6280\u80fd\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff1b\u7b2c\u4e8c\u5c42\u8fdb\u884c\u62a5\u544a\u7279\u5b9a\u7684\u3001\u57fa\u4e8e\u58f0\u660e\u7684\u8bc4\u4f30\uff0c\u7075\u6d3b\u8bc4\u4f30\u591a\u6837\u5316\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8bc1\u636e\u4f9d\u8d56\u95e8\u63a7\u6765\u4f7f\u57fa\u4e8e\u88ab\u53cd\u9a73\u58f0\u660e\u7684\u7ed3\u8bba\u5931\u6548\u3002", "motivation": "\u8bc4\u4f30\u5f00\u653e\u7aef\u4e13\u4e1a\u4efb\u52a1\u4e0a\u7684\u667a\u80fd\u4f53AI\u9762\u4e34\u4e25\u8c28\u6027\u4e0e\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u56f0\u5883\u3002\u9759\u6001\u8bc4\u4f30\u6807\u51c6\u63d0\u4f9b\u4e25\u8c28\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\uff0c\u4f46\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u6709\u6548\u54cd\u5e94\u7b56\u7565\uff1b\u800c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u867d\u7136\u80fd\u9002\u5e94\u4e2a\u4f53\u54cd\u5e94\uff0c\u5374\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u504f\u89c1\u95ee\u9898\u3002", "method": "JADE\u91c7\u7528\u4e24\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u5c06\u4e13\u5bb6\u77e5\u8bc6\u7f16\u7801\u4e3a\u9884\u5b9a\u4e49\u7684\u8bc4\u4f30\u6280\u80fd\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff1b\u7b2c\u4e8c\u5c42\u8fdb\u884c\u62a5\u544a\u7279\u5b9a\u7684\u3001\u57fa\u4e8e\u58f0\u660e\u7684\u8bc4\u4f30\uff0c\u7075\u6d3b\u8bc4\u4f30\u591a\u6837\u5316\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8bc1\u636e\u4f9d\u8d56\u95e8\u63a7\u673a\u5236\u6765\u4f7f\u57fa\u4e8e\u88ab\u53cd\u9a73\u58f0\u660e\u7684\u7ed3\u8bba\u5931\u6548\u3002", "result": "\u5728BizBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJADE\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6574\u4f53\u6027LLM\u8bc4\u4f30\u5668\u9057\u6f0f\u7684\u5173\u952e\u667a\u80fd\u4f53\u5931\u8d25\u6a21\u5f0f\u3002\u8be5\u6846\u67b6\u4e0e\u4e13\u5bb6\u7f16\u5199\u7684\u8bc4\u4f30\u6807\u51c6\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u533b\u7597\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "conclusion": "JADE\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u52a8\u6001\u58f0\u660e\u7ea7\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u7aef\u4e13\u4e1a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u4e25\u8c28\u6027\u4e0e\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u56f0\u5883\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u7075\u6d3b\u4e14\u53ef\u8fc1\u79fb\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.06525", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06525", "abs": "https://arxiv.org/abs/2602.06525", "authors": ["Finn Rietz", "Mart Karta\u0161ev", "Johannes A. Stork", "Petter \u00d6gren"], "title": "Progress Constraints for Reinforcement Learning in Behavior Trees", "comment": null, "summary": "Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u6811\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5ea6\u7ea6\u675f\u673a\u5236\u9632\u6b62\u63a7\u5236\u5668\u95f4\u7684\u76f8\u4e92\u5e72\u6270\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u884c\u4e3a\u6811\u63d0\u4f9b\u7ed3\u6784\u5316\u51b3\u7b56\u6846\u67b6\u4f46\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u5668\u4f46\u9762\u4e34\u7a00\u758f\u5956\u52b1\u3001\u5b89\u5168\u63a2\u7d22\u7b49\u6311\u6218\u3002\u4e24\u8005\u7ed3\u5408\u6709\u4e92\u8865\u4f18\u52bf\uff0c\u4f46\u7b80\u5355\u96c6\u6210\u53ef\u80fd\u5bfc\u81f4\u63a7\u5236\u5668\u76f8\u4e92\u62b5\u6d88\uff0c\u964d\u4f4e\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8fdb\u5ea6\u7ea6\u675f\u673a\u5236\uff0c\u5229\u7528\u53ef\u884c\u6027\u4f30\u8ba1\u5668\u57fa\u4e8e\u884c\u4e3a\u6811\u6536\u655b\u7406\u8bba\u6765\u7ea6\u675f\u5141\u8bb8\u7684\u52a8\u4f5c\u96c6\uff0c\u9632\u6b62\u63a7\u5236\u5668\u76f8\u4e92\u5e72\u6270\u3002", "result": "\u57282D\u6982\u5ff5\u9a8c\u8bc1\u548c\u9ad8\u4fdd\u771f\u4ed3\u5e93\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u884c\u4e3a\u6811-\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6837\u672c\u6548\u7387\u548c\u7ea6\u675f\u6ee1\u8db3\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fdb\u5ea6\u7ea6\u675f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u884c\u4e3a\u6811\u4e0e\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u4e2d\u7684\u63a7\u5236\u5668\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e24\u8005\u7684\u4f18\u52bf\u4e92\u8865\uff0c\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06275", "abs": "https://arxiv.org/abs/2602.06275", "authors": ["Isaac Picov", "Ritesh Goru"], "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution", "comment": null, "summary": "Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.", "AI": {"tldr": "RoPE-LIME\uff1a\u4e00\u79cd\u7528\u4e8e\u95ed\u6e90LLM\u8f93\u51fa\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u578b\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\u8ba1\u7b97\u57fa\u4e8e\u6982\u7387\u76ee\u6807\u7684token\u7ea7\u5f52\u56e0\uff0c\u51cf\u5c11API\u8c03\u7528\u5e76\u63d0\u9ad8\u89e3\u91ca\u8d28\u91cf", "motivation": "\u95ed\u6e90LLM\u8f93\u51fa\u7684\u89e3\u91ca\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3aAPI\u8bbf\u95ee\u963b\u6b62\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u800c\u6270\u52a8\u65b9\u6cd5\u5728\u4f9d\u8d56\u91cd\u65b0\u751f\u6210\u6587\u672c\u65f6\u6210\u672c\u9ad8\u4e14\u566a\u58f0\u5927", "method": "1) \u4f7f\u7528\u5c0f\u578b\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\u8ba1\u7b97\u57fa\u4e8e\u6982\u7387\u76ee\u6807\uff08\u8d1f\u5bf9\u6570\u4f3c\u7136\u548c\u6563\u5ea6\u76ee\u6807\uff09\u7684token\u7ea7\u5f52\u56e0\uff1b2) \u5728RoPE\u5d4c\u5165\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u677e\u5f1b\u8bcd\u79fb\u52a8\u8ddd\u79bb\u7684\u5c40\u90e8\u6838\uff1b3) \u7a00\u758fK\u91c7\u6837\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u6270\u52a8\u7b56\u7565\u4ee5\u63d0\u9ad8\u6709\u9650\u9884\u7b97\u4e0b\u7684\u4ea4\u4e92\u8986\u76d6", "result": "\u5728HotpotQA\uff08\u53e5\u5b50\u7279\u5f81\uff09\u548c\u624b\u5de5\u6807\u6ce8\u7684MMLU\u5b50\u96c6\uff08\u8bcd\u7279\u5f81\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRoPE-LIME\u6bd4\u7559\u4e00\u91c7\u6837\u4ea7\u751f\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u5f52\u56e0\uff0c\u76f8\u6bd4gSMILE\u6709\u6240\u6539\u8fdb\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u95ed\u6e90\u6a21\u578bAPI\u8c03\u7528", "conclusion": "RoPE-LIME\u4e3a\u95ed\u6e90LLM\u8f93\u51fa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u548c\u89e3\u91ca\u8fc7\u7a0b\uff0c\u5728\u51cf\u5c11API\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5f52\u56e0\u8d28\u91cf", "topic": "agent analysis"}}
{"id": "2602.06540", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06540", "abs": "https://arxiv.org/abs/2602.06540", "authors": ["Yishan Li", "Wentong Chen", "Yukun Yan", "Mingwei Li", "Sen Mei", "Xiaorong Wang", "Kunpeng Liu", "Xin Cong", "Shuo Wang", "Zhong Zhang", "Yaxi Lu", "Zhenghao Liu", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research", "comment": null, "summary": "Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.", "AI": {"tldr": "AgentCPM-Report\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u672c\u5730\u7814\u7a76\u62a5\u544a\u751f\u6210\u7cfb\u7edf\uff0c\u4f7f\u75288B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u5199\u4f5c\u5373\u63a8\u7406\u7b56\u7565(WARP)\u52a8\u6001\u4fee\u8ba2\u5927\u7eb2\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u95ed\u6e90\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u751f\u6210\u7cfb\u7edf\u4f9d\u8d56\u95ed\u6e90\u6216\u5728\u7ebf\u5927\u6a21\u578b\uff0c\u5b58\u5728\u90e8\u7f72\u969c\u788d\u3001\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u672c\u5730\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u5927\u7eb2\u6784\u5efa\u4f9d\u8d56\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5199\u4f5c\u5373\u63a8\u7406\u7b56\u7565(WARP)\uff0c\u8ba9\u6a21\u578b\u5728\u62a5\u544a\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u4fee\u8ba2\u5927\u7eb2\u3002\u91c7\u7528\u8bc1\u636e\u9a71\u52a8\u8349\u62df\u548c\u63a8\u7406\u9a71\u52a8\u6df1\u5316\u4ea4\u66ff\u8fdb\u884c\uff0c\u652f\u6301\u4fe1\u606f\u83b7\u53d6\u3001\u77e5\u8bc6\u7cbe\u70bc\u548c\u8fed\u4ee3\u5927\u7eb2\u6f14\u8fdb\u3002\u4f7f\u7528\u591a\u9636\u6bb5\u667a\u80fd\u4f53\u8bad\u7ec3\u7b56\u7565\uff1a\u51b7\u542f\u52a8\u3001\u539f\u5b50\u6280\u80fd\u5f3a\u5316\u5b66\u4e60\u548c\u6574\u4f53\u7ba1\u9053\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728DeepResearch Bench\u3001DeepConsult\u548cDeepResearch Gym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentCPM-Report\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u95ed\u6e90\u7cfb\u7edf\uff0c\u5728Insight\u6307\u6807\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AgentCPM-Report\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u901a\u8fc7\u9002\u5f53\u7684\u6846\u67b6\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u751f\u6210\uff0c\u51cf\u5c11\u5bf9\u95ed\u6e90\u5927\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "topic": "agent analysis"}}
{"id": "2602.06291", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06291", "abs": "https://arxiv.org/abs/2602.06291", "authors": ["Guijin Son", "Donghun Yang", "Hitesh Laxmichand Patel", "Hyunwoo Ko", "Amit Agarwal", "Sunghee Ahn", "Kyong-Ha Lee", "Youngjae Yu"], "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math", "comment": "Preprint", "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \\textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.", "AI": {"tldr": "\u63d0\u51faConsequence-Based Utility\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u5019\u9009\u65b9\u6848\u5728\u89e3\u51b3\u76f8\u5173\u53ef\u9a8c\u8bc1\u95ee\u9898\u65f6\u7684\u4ef7\u503c\u6765\u8bc4\u5206\uff0c\u65e0\u9700\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u80fd\u751f\u6210\u7814\u7a76\u7ea7\u6570\u5b66\u95ee\u9898\u7684\u53ef\u80fd\u89e3\u6cd5\uff0c\u4f46\u9a8c\u8bc1\u8fc7\u7a0b\u6d88\u8017\u4e13\u5bb6\u65f6\u95f4\u6210\u4e3a\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faConsequence-Based Utility\u8bc4\u4f30\u5668\uff0c\u57fa\u4e8e\u5047\u8bbe\uff1a\u6709\u6548\u89e3\u51b3\u65b9\u6848\u5e94\u5305\u542b\u8db3\u591f\u7684\u65b9\u6cd5\u7ea7\u4fe1\u606f\uff0c\u5f53\u5e94\u7528\u4e8e\u76f8\u5173\u95ee\u9898\u65f6\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u4e0b\u6e38\u6027\u80fd\u3002\u901a\u8fc7\u6d4b\u8bd5\u6bcf\u4e2a\u5019\u9009\u65b9\u6848\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\u89e3\u51b3\u76f8\u5173\u53ef\u9a8c\u8bc1\u95ee\u9898\u7684\u4ef7\u503c\u6765\u8bc4\u5206\u3002", "result": "\u5728\u539f\u521b\u7814\u7a76\u7ea7\u6570\u5b66\u95ee\u9898\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u95ee\u9898\u914d\u6709\u4e00\u4e2a\u4e13\u5bb6\u7f16\u5199\u65b9\u6848\u548c\u4e5d\u4e2aLLM\u751f\u6210\u65b9\u6848\u3002Consequence-Based Utility\u5728\u6392\u540d\u8d28\u91cf\u4e0a\u4e00\u81f4\u4f18\u4e8e\u5956\u52b1\u6a21\u578b\u3001\u751f\u6210\u5956\u52b1\u6a21\u578b\u548cLLM\u6cd5\u5b98\u3002\u5bf9GPT-OSS-120B\uff0c\u5c06Acc@1\u4ece67.2\u63d0\u5347\u523076.3\uff0cAUC\u4ece71.4\u63d0\u5347\u523079.6\u3002", "conclusion": "Consequence-Based Utility\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u4e13\u5bb6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u533a\u5206\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6b63\u786e\u548c\u9519\u8bef\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u5e95\u5c42\u6c42\u89e3\u5668\u7ecf\u5e38\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u8f83\u5f3a\u7684\u6b63\u786e-\u9519\u8bef\u5206\u79bb\u3002", "topic": "agent analysis"}}
{"id": "2602.06554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06554", "abs": "https://arxiv.org/abs/2602.06554", "authors": ["Tianyi Hu", "Qingxu Fu", "Yanxi Chen", "Zhaoyang Liu", "Bolin Ding"], "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees", "comment": null, "summary": "Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.", "AI": {"tldr": "\u63d0\u51faSeeUPO\u7b97\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709RL\u7b97\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u987a\u5e8f\u66f4\u65b0\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u5355\u8c03\u6539\u8fdb\u548c\u5168\u5c40\u6700\u4f18\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u8bad\u7ec3\u4e2d\uff0c\u4e3b\u6d41RL\u7b97\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e0b\u7f3a\u4e4f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u65e0\u6cd5\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u3002", "method": "\u63d0\u51faSeeUPO\u7b97\u6cd5\uff1a\u5c06\u591a\u8f6e\u4ea4\u4e92\u5efa\u6a21\u4e3a\u987a\u5e8f\u6267\u884c\u7684\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u91c7\u7528\u53cd\u5411\u6267\u884c\u987a\u5e8f\u7684\u9010\u8f6e\u987a\u5e8f\u7b56\u7565\u66f4\u65b0\uff0c\u901a\u8fc7\u53cd\u5411\u5f52\u7eb3\u786e\u4fdd\u5355\u8c03\u6539\u8fdb\u548c\u5168\u5c40\u6700\u4f18\u6536\u655b\u3002", "result": "\u5728AppWorld\u548cBFCL v4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSeeUPO\u76f8\u6bd4\u73b0\u6709\u9aa8\u5e72\u7b97\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aQwen3-14B\u4e0a\u76f8\u5bf9\u589e\u76ca43.3%-54.6%\uff0cQwen2.5-14B\u4e0a24.1%-41.9%\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4f18\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "SeeUPO\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u4e2dRL\u7b97\u6cd5\u7684\u6536\u655b\u4fdd\u8bc1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0critic\u4e14\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06358", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06358", "abs": "https://arxiv.org/abs/2602.06358", "authors": ["Yewei Liu", "Xiyuan Wang", "Yansheng Mao", "Yoav Gelbery", "Haggai Maron", "Muhan Zhang"], "title": "SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass", "comment": null, "summary": "We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE", "AI": {"tldr": "SHINE\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8d85\u7f51\u7edc\uff0c\u80fd\u591f\u5c06\u591a\u6837\u5316\u7684\u6709\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u6620\u5c04\u4e3a\u9ad8\u8d28\u91cf\u7684\u5927\u8bed\u8a00\u6a21\u578bLoRA\u9002\u914d\u5668\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5c06\u4e0a\u4e0b\u6587\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53c2\u6570\u77e5\u8bc6\uff0c\u663e\u8457\u8282\u7701\u65f6\u95f4\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u8d85\u7f51\u7edc\u5728\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5c06\u4e0a\u4e0b\u6587\u77e5\u8bc6\u8f6c\u5316\u4e3a\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u5fae\u8c03\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u63d0\u51faSHINE\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u91cd\u7528\u51bb\u7ed3LLM\u53c2\u6570\u8fdb\u884c\u4e0a\u4e0b\u6587\u8d85\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5f15\u5165\u67b6\u6784\u521b\u65b0\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5fae\u8c03\u6d41\u7a0b\uff0c\u8bad\u7ec3\u8d85\u7f51\u7edc\u4ece\u591a\u6837\u5316\u4e0a\u4e0b\u6587\u751f\u6210\u9ad8\u8d28\u91cfLoRA\u9002\u914d\u5668\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u76f8\u6bd4\u57fa\u4e8eSFT\u7684LLM\u9002\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u8282\u7701\u65f6\u95f4\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6269\u5c55\u6f5c\u529b\u3002", "conclusion": "SHINE\u901a\u8fc7\u521b\u65b0\u7684\u8d85\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5355\u6b21\u4e0a\u4e0b\u6587\u5230\u53c2\u6570\u77e5\u8bc6\u8f6c\u6362\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.06746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06746", "abs": "https://arxiv.org/abs/2602.06746", "authors": ["Alessandro Abate", "Giuseppe De Giacomo", "Mathias Jackermeier", "Jan Kret\u00ednsk\u00fd", "Maximilian Prokop", "Christoph Weinhuber"], "title": "Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions", "comment": null, "summary": "We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u8f6c\u6362\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u7ed3\u6784\u5316\u4efb\u52a1\u5d4c\u5165\u5b9e\u73b0\u901a\u7528\u7b56\u7565\u5b66\u4e60", "motivation": "\u7814\u7a76\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff0c\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2a\u80fd\u591f\u6cdb\u5316\u5230\u4efb\u610f\uff08\u53ef\u80fd\u672a\u89c1\uff09\u4efb\u52a1\u7684\u901a\u7528\u7b56\u7565\u3002\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u516c\u5f0f\u4f5c\u4e3a\u4efb\u52a1\u89c4\u8303\uff0c\u8fd9\u662f\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e2d\u5e38\u7528\u7684\u7cfb\u7edf\u5c5e\u6027\u89c4\u8303\u65b9\u5f0f\uff0c\u6700\u8fd1\u5728RL\u4e2d\u6210\u529f\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u4efb\u52a1\u5d4c\u5165\u6280\u672f\uff0c\u5229\u7528\u65b0\u4e00\u4ee3\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u8f6c\u6362\uff08\u6700\u521d\u4e3a\u65f6\u5e8f\u7efc\u5408\u5f00\u53d1\uff09\u3002\u751f\u6210\u7684\u8bed\u4e49\u6807\u8bb0\u81ea\u52a8\u673a\u5728\u6bcf\u4e2a\u72b6\u6001\u5305\u542b\u4e30\u5bcc\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u80fd\u591f\uff1a(i) \u9ad8\u6548\u5730\u5728\u7ebf\u8ba1\u7b97\u81ea\u52a8\u673a\uff0c(ii) \u63d0\u53d6\u7528\u4e8e\u6761\u4ef6\u5316\u7b56\u7565\u7684\u8868\u8fbe\u6027\u4efb\u52a1\u5d4c\u5165\uff0c(iii) \u81ea\u7136\u652f\u6301\u5b8c\u6574\u7684LTL\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u73b0\u6709\u65b9\u6cd5\u5931\u8d25\u7684\u590d\u6742\u89c4\u8303\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u4e49LTL\u5230\u81ea\u52a8\u673a\u8f6c\u6362\u7684\u4efb\u52a1\u5d4c\u5165\u6280\u672f\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742LTL\u89c4\u8303\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06818", "abs": "https://arxiv.org/abs/2602.06818", "authors": ["Anirudh Chari", "Neil Pattanaik"], "title": "Wild Guesses and Mild Guesses in Active Concept Learning", "comment": null, "summary": "Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting \"safe\" queries, leading to faster convergence on simple rules. Our results suggest that \"confirmation bias\" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e3b\u52a8\u6982\u5ff5\u5b66\u4e60\u4e2d\u4fe1\u606f\u83b7\u53d6\u4e0e\u5047\u8bbe\u7a33\u5b9a\u6027\u7684\u6743\u8861\uff0c\u6bd4\u8f83\u4e86\u7406\u6027\u4e3b\u52a8\u5b66\u4e60\u5668\uff08\u6700\u5927\u5316\u671f\u671b\u4fe1\u606f\u589e\u76ca\uff09\u4e0e\u4eba\u7c7b\u5f0f\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\uff08\u67e5\u8be2\u5f53\u524d\u6700\u4f73\u5047\u8bbe\u9884\u6d4b\u4e3a\u6b63\u7684\u5b9e\u4f8b\uff09\u5728\u4e0d\u540c\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u6982\u5ff5\u5b66\u4e60\u901a\u5e38\u662f\u4e3b\u52a8\u7684\uff1a\u5b66\u4e60\u8005\u9009\u62e9\u67e5\u8be2\u54ea\u4e9b\u5b9e\u4f8b\u6765\u51cf\u5c11\u5bf9\u5e95\u5c42\u89c4\u5219\u6216\u7c7b\u522b\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u4e3b\u52a8\u6982\u5ff5\u5b66\u4e60\u5fc5\u987b\u5728\u67e5\u8be2\u7684\u4fe1\u606f\u91cf\u4e0e\u751f\u6210\u548c\u8bc4\u5206\u5047\u8bbe\u7684\u5b66\u4e60\u5668\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u8d1d\u53f6\u65af\u5b66\u4e60\u5668\uff0c\u5176\u5047\u8bbe\u662f\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u66f4\u65b0\u91cd\u65b0\u52a0\u6743\u3002\u6bd4\u8f83\u4e24\u79cd\u67e5\u8be2\u7b56\u7565\uff1a\u7406\u6027\u4e3b\u52a8\u5b66\u4e60\u5668\uff08\u9009\u62e9\u67e5\u8be2\u4ee5\u6700\u5927\u5316\u8fd1\u4f3c\u671f\u671b\u4fe1\u606f\u589e\u76ca\uff09\u548c\u4eba\u7c7b\u5f0f\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\uff08\u67e5\u8be2\u5f53\u524d\u6700\u4f73\u5047\u8bbe\u9884\u6d4b\u4e3a\u6b63\u7684\u5b9e\u4f8b\uff09\u3002\u5728\u7ecf\u5178\u6570\u5b57\u6e38\u620f\u7684\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u9700\u8981\u8bc1\u4f2a\u7684\u590d\u6742\u6982\u5ff5\uff08\u5982\u590d\u5408\u89c4\u5219\u6216\u5305\u542b\u4f8b\u5916\u7684\u89c4\u5219\uff09\u4e0a\uff0c\u671f\u671b\u4fe1\u606f\u589e\u76ca\u7b56\u7565\u6709\u6548\uff0c\u4f46\u5728\u7b80\u5355\u6982\u5ff5\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u79cd\u5931\u8d25\u6e90\u4e8e\u671f\u671b\u4fe1\u606f\u589e\u76ca\u7b56\u7565\u4e0eLLM\u63d0\u8bae\u5206\u5e03\u4e4b\u95f4\u7684\u652f\u6301\u4e0d\u5339\u914d\uff1a\u9ad8\u5ea6\u8bca\u65ad\u6027\u7684\u8fb9\u754c\u67e5\u8be2\u5c06\u540e\u9a8c\u63a8\u5411\u751f\u6210\u5668\u4ea7\u751f\u65e0\u6548\u6216\u8fc7\u4e8e\u5177\u4f53\u7a0b\u5e8f\u7684\u533a\u57df\uff0c\u5bfc\u81f4\u7c92\u5b50\u8fd1\u4f3c\u4e2d\u7684\u652f\u6301\u4e0d\u5339\u914d\u9677\u9631\u3002\u79ef\u6781\u6d4b\u8bd5\u7b56\u7565\u867d\u7136\u4fe1\u606f\u6b21\u4f18\uff0c\u4f46\u503e\u5411\u4e8e\u901a\u8fc7\u9009\u62e9\"\u5b89\u5168\"\u67e5\u8be2\u6765\u7ef4\u6301\u63d0\u8bae\u6709\u6548\u6027\uff0c\u4ece\u800c\u5728\u7b80\u5355\u89c4\u5219\u4e0a\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\"\u786e\u8ba4\u504f\u8bef\"\u53ef\u80fd\u4e0d\u662f\u8ba4\u77e5\u9519\u8bef\uff0c\u800c\u662f\u5728\u4eba\u7c7b\u601d\u7ef4\u7279\u6709\u7684\u7a00\u758f\u3001\u5f00\u653e\u5f0f\u5047\u8bbe\u7a7a\u95f4\u4e2d\u7ef4\u6301\u53ef\u5904\u7406\u63a8\u7406\u7684\u7406\u6027\u9002\u5e94\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2602.06820", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06820", "abs": "https://arxiv.org/abs/2602.06820", "authors": ["Dunwei Tu", "Hongyan Hao", "Hansi Yang", "Yihao Chen", "Yi-Kai Zhang", "Zhikang Xia", "Yu Yang", "Yueqing Sun", "Xingchen Liu", "Furao Shen", "Qi Gu", "Hui Su", "Xunliang Cai"], "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training", "comment": null, "summary": "Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $\u03c4^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.", "AI": {"tldr": "ScaleEnv\u662f\u4e00\u4e2a\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u5b8c\u5168\u4ea4\u4e92\u5f0f\u73af\u5883\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u6d4b\u8bd5\u786e\u4fdd\u73af\u5883\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5de5\u5177\u4f9d\u8d56\u56fe\u6269\u5c55\u548c\u53ef\u6267\u884c\u52a8\u4f5c\u9a8c\u8bc1\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6574\u6027\u548c\u53ef\u89e3\u6027\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u672a\u89c1\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u573a\u666f\u7684\u901a\u7528\u667a\u80fd\u4f53\u9700\u8981\u4ea4\u4e92\u5f0f\u73af\u5883\u8fdb\u884c\u81ea\u6211\u63a2\u7d22\uff0c\u4f46\u73b0\u6709\u4ea4\u4e92\u73af\u5883\u4e25\u91cd\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5728\u73af\u5883\u591a\u6837\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u9650\u5236\u3002", "method": "ScaleEnv\u6846\u67b6\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u5b8c\u5168\u4ea4\u4e92\u5f0f\u73af\u5883\u548c\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u6d4b\u8bd5\u786e\u4fdd\u73af\u5883\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5de5\u5177\u4f9d\u8d56\u56fe\u6269\u5c55\u548c\u53ef\u6267\u884c\u52a8\u4f5c\u9a8c\u8bc1\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6574\u6027\u548c\u53ef\u89e3\u6027\u3002", "result": "\u5728\u672a\u89c1\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff08\u5982\u03c4\u00b2-Bench\u548cVitaBench\uff09\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u8bc1\u8bc1\u660e\u4e86\u589e\u52a0\u9886\u57df\u6570\u91cf\u5bf9\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6269\u5c55\u73af\u5883\u591a\u6837\u6027\u5bf9\u4e8e\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0cScaleEnv\u4e3a\u89e3\u51b3\u4ea4\u4e92\u73af\u5883\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.06836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06836", "abs": "https://arxiv.org/abs/2602.06836", "authors": ["Tonghan Wang", "Yuqi Pan", "Xinyi Yang", "Yanchen Jiang", "Milind Tambe", "David C. Parkes"], "title": "LLM Active Alignment: A Nash Equilibrium Perspective", "comment": null, "summary": "We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u7eb3\u4ec0\u5747\u8861\u5206\u6790\u6765\u9884\u6d4b\u548c\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7fa4\u4f53\u7684\u884c\u4e3a\uff0c\u907f\u514d\u5f00\u653e\u6587\u672c\u7a7a\u95f4\u4e2d\u7684\u5747\u8861\u8ba1\u7b97\u96be\u9898\uff0c\u5c06\u4ee3\u7406\u884c\u4e3a\u5efa\u6a21\u4e3a\u4eba\u7c7b\u5b50\u7fa4\u4f53\u7684\u6df7\u5408\u9009\u62e9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7fa4\u4f53\u5728\u793e\u4ea4\u5a92\u4f53\u7b49\u573a\u666f\u4e2d\u53ef\u80fd\u51fa\u73b0\u653f\u6cbb\u6392\u65a5\u7b49\u75c5\u7406\u73b0\u8c61\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u6765\u9884\u6d4b\u548c\u5f15\u5bfc\u591a\u667a\u80fd\u4f53LLM\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5b9e\u73b0\u793e\u4f1a\u671f\u671b\u7684\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u4ee3\u7406\u7684\u884c\u4e3a\u5efa\u6a21\u4e3a\u4eba\u7c7b\u5b50\u7fa4\u4f53\u7684\u6df7\u5408\u9009\u62e9\uff0c\u4ee3\u7406\u4e3b\u52a8\u6218\u7565\u6027\u5730\u9009\u62e9\u4e0e\u54ea\u4e9b\u7fa4\u4f53\u5bf9\u9f50\uff0c\u91c7\u7528\u6807\u51c6\u51f9\u6548\u7528\u5047\u8bbe\u63a8\u5bfc\u95ed\u5f0f\u7eb3\u4ec0\u5747\u8861\u7279\u5f81\uff0c\u4f5c\u4e3a\u73b0\u6709\u5bf9\u9f50\u6d41\u7a0b\uff08\u5982RLHF\uff09\u4e4b\u4e0a\u7684\u4e3b\u52a8\u5bf9\u9f50\u5c42\u3002", "result": "\u5728\u793e\u4ea4\u5a92\u4f53\u573a\u666f\u4e2d\uff0cLLM\u7fa4\u4f53\uff08\u7279\u522b\u662f\u57fa\u4e8e\u63a8\u7406\u7684\u6a21\u578b\uff09\u53ef\u80fd\u8868\u73b0\u51fa\u653f\u6cbb\u6392\u65a5\u73b0\u8c61\uff0c\u5373\u67d0\u4e9b\u5b50\u7fa4\u4f53\u88ab\u6240\u6709LLM\u4ee3\u7406\u5ffd\u7565\uff0c\u800c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u75c5\u7406\u73b0\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u9886\u57df\u8c03\u8282\u591a\u667a\u80fd\u4f53LLM\u52a8\u6001\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u80fd\u591f\u901a\u8fc7\u660e\u786e\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u5c06\u5bf9\u9f50\u76ee\u6807\u8f6c\u5411\u793e\u4f1a\u671f\u671b\u7684\u7ed3\u679c\u3002", "topic": "agent analysis"}}
{"id": "2602.06440", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.06440", "abs": "https://arxiv.org/abs/2602.06440", "authors": ["Sung-Hoon Yoon", "Ruizhi Qian", "Minda Zhao", "Weiyue Li", "Mengyu Wang"], "title": "TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking", "comment": null, "summary": "Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5386\u53f2\u611f\u77e5\u8d8a\u72f1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5e76\u91cd\u65b0\u52a0\u6743\u5148\u524d\u6b65\u9aa4\u4e2d\u7684\u6f0f\u6d1e\u4fe1\u53f7\u6765\u6307\u5bfc\u672a\u6765\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\u548c\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8d8a\u72f1\u6280\u672f\u5927\u591a\u672a\u80fd\u6709\u6548\u5229\u7528\u5148\u524d\u4ea4\u4e92\u8f6e\u6b21\u4e2d\u63ed\u793a\u7684\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u653b\u51fb\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u7a33\u5b9a\u3002\u7531\u4e8e\u8d8a\u72f1\u6d89\u53ca\u5e8f\u5217\u4ea4\u4e92\uff0c\u5176\u4e2d\u6bcf\u4e2a\u54cd\u5e94\u90fd\u4f1a\u5f71\u54cd\u672a\u6765\u884c\u52a8\uff0c\u5f3a\u5316\u5b66\u4e60\u4e3a\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u81ea\u7136\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5386\u53f2\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u8d8a\u72f1\u6846\u67b6\uff0c\u5206\u6790\u5e76\u91cd\u65b0\u52a0\u6743\u5148\u524d\u6b65\u9aa4\u4e2d\u7684\u6f0f\u6d1e\u4fe1\u53f7\u6765\u6307\u5bfc\u672a\u6765\u51b3\u7b56\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u91cd\u65b0\u52a0\u6743\u673a\u5236\uff0c\u7a81\u51fa\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u66f4\u5c11\u7684\u67e5\u8be2\u3002", "result": "\u5728AdvBench\u548cHarmBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\u3002\u4ec5\u7eb3\u5165\u5386\u53f2\u4fe1\u606f\u5c31\u80fd\u63d0\u9ad8\u8d8a\u72f1\u6210\u529f\u7387\u3002", "conclusion": "\u5386\u53f2\u6f0f\u6d1e\u4fe1\u53f7\u5728\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u8d8a\u72f1\u7b56\u7565\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u63a8\u8fdbLLM\u5b89\u5168\u9632\u62a4\u7684\u5bf9\u6297\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.06841", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06841", "abs": "https://arxiv.org/abs/2602.06841", "authors": ["Sindhuja Chaduvula", "Jessee Ho", "Kina Kim", "Aravind Narayanan", "Mahshid Alinoori", "Muskan Garg", "Dhanesh Ramachandram", "Shaina Raza"], "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems", "comment": null, "summary": "Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $\u03c1= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\\times$ more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.\n  Resources:\n  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u9759\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5f52\u56e0\u89e3\u91ca\u65b9\u6cd5\u4e0e\u667a\u80fd\u4f53\u57fa\u51c6\u4e2d\u7684\u8f68\u8ff9\u8bca\u65ad\u65b9\u6cd5\uff0c\u53d1\u73b0\u5f52\u56e0\u65b9\u6cd5\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u8bca\u65ad\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u6267\u884c\u7ea7\u6545\u969c\uff0c\u800c\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bca\u65ad\u80fd\u6709\u6548\u5b9a\u4f4d\u884c\u4e3a\u6545\u969c\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5c55\u73b0\u51fa\u591a\u6b65\u8f68\u8ff9\u7684\u884c\u4e3a\uff0c\u6210\u529f\u4e0e\u5931\u8d25\u7531\u51b3\u7b56\u5e8f\u5217\u51b3\u5b9a\u3002\u7136\u800c\uff0c\u4e3a\u9759\u6001\u9884\u6d4b\u8bbe\u8ba1\u7684\u89e3\u91ca\u65b9\u6cd5\u5982\u4f55\u5e94\u7528\u4e8e\u884c\u4e3a\u968f\u65f6\u95f4\u6f14\u5316\u7684\u667a\u80fd\u4f53\u8bbe\u7f6e\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u5f25\u5408\u9759\u6001\u4e0e\u667a\u80fd\u4f53\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u6bd4\u8f83\u9759\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5f52\u56e0\u89e3\u91ca\u65b9\u6cd5\u4e0e\u667a\u80fd\u4f53\u57fa\u51c6\uff08TAU-bench Airline\u548cAssistantBench\uff09\u4e2d\u7684\u8f68\u8ff9\u8bca\u65ad\u65b9\u6cd5\u3002\u5177\u4f53\u6bd4\u8f83\u4e86\u5f52\u56e0\u89e3\u91ca\u4e0e\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bca\u65ad\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5f52\u56e0\u65b9\u6cd5\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7684\u7279\u5f81\u6392\u540d\uff08Spearman \u03c1=0.86\uff09\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u8bca\u65ad\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u6267\u884c\u7ea7\u6545\u969c\u3002\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bc4\u4f30\u80fd\u4e00\u81f4\u5b9a\u4f4d\u884c\u4e3a\u6545\u969c\uff0c\u53d1\u73b0\u72b6\u6001\u8ddf\u8e2a\u4e0d\u4e00\u81f4\u5728\u5931\u8d25\u8fd0\u884c\u4e2d\u666e\u904d2.7\u500d\uff0c\u5e76\u5c06\u6210\u529f\u6982\u7387\u964d\u4f4e49%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8bc4\u4f30\u548c\u8bca\u65ad\u81ea\u4e3bAI\u884c\u4e3a\u65f6\uff0c\u9700\u8981\u5411\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8f68\u8ff9\u7ea7\u53ef\u89e3\u91ca\u6027\u8f6c\u53d8\uff0c\u56e0\u4e3a\u9759\u6001\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u968f\u65f6\u95f4\u6f14\u5316\u7684\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2602.06248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06248", "abs": "https://arxiv.org/abs/2602.06248", "authors": ["Patryk Rybak", "Pawe\u0142 Batorski", "Paul Swoboda", "Przemys\u0142aw Spurek"], "title": "REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop", "comment": null, "summary": "Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at https://github.com/patryk-rybak/REBEL/", "AI": {"tldr": "REBEL\u662f\u4e00\u79cd\u8fdb\u5316\u5bf9\u6297\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u9057\u5fd8\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u79fb\u9664\u4e86\u654f\u611f\u6570\u636e\uff0c\u63ed\u793a\u5f53\u524d\u9057\u5fd8\u65b9\u6cd5\u53ef\u80fd\u53ea\u63d0\u4f9b\u8868\u9762\u4fdd\u62a4", "motivation": "\u5f53\u524dLLM\u9057\u5fd8\u65b9\u6cd5\u7684\u8bc4\u4f30\u6307\u6807\u4f9d\u8d56\u826f\u6027\u67e5\u8be2\uff0c\u5f80\u5f80\u5c06\u8868\u9762\u4fe1\u606f\u6291\u5236\u8bef\u8ba4\u4e3a\u771f\u6b63\u7684\u77e5\u8bc6\u79fb\u9664\uff0c\u65e0\u6cd5\u68c0\u6d4b\u66f4\u590d\u6742\u63d0\u793a\u7b56\u7565\u4ecd\u80fd\u63d0\u53d6\u7684\u6b8b\u7559\u77e5\u8bc6", "method": "\u63d0\u51faREBEL\u8fdb\u5316\u5bf9\u6297\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u6765\u63a2\u6d4b\u9057\u5fd8\u6570\u636e\u662f\u5426\u4ecd\u80fd\u88ab\u6062\u590d", "result": "REBEL\u6210\u529f\u4ece\u6807\u51c6\u9057\u5fd8\u57fa\u51c6\u4e2d\u770b\u4f3c\u5df2\u9057\u5fd8\u7684\u6a21\u578b\u4e2d\u63d0\u53d6\"\u88ab\u9057\u5fd8\"\u77e5\u8bc6\uff0c\u5728TOFU\u57fa\u51c6\u4e0a\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe60%\uff0c\u5728WMDP\u57fa\u51c6\u4e0a\u8fbe93%", "conclusion": "\u5f53\u524dLLM\u9057\u5fd8\u65b9\u6cd5\u53ef\u80fd\u53ea\u63d0\u4f9b\u8868\u9762\u4fdd\u62a4\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u654f\u611f\u6570\u636e\u7684\u771f\u6b63\u79fb\u9664", "topic": "agent analysis"}}
{"id": "2602.06446", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06446", "abs": "https://arxiv.org/abs/2602.06446", "authors": ["Satyam Dwivedi", "Sanjukta Ghosh", "Shivam Dwivedi", "Nishi Kumari", "Anil Thakur", "Anurag Purushottam", "Deepak Alok", "Praveen Gatla", "Manjuprasad B", "Bipasha Patgiri"], "title": "CORE: Comprehensive Ontological Relation Evaluation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCORE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u533a\u5206\u8bed\u4e49\u76f8\u5173\u6027\u548c\u65e0\u5173\u6027\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u65e0\u5173\u6027\u63a8\u7406\u4e0a\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u8bed\u4e49\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u5f88\u5c11\u6d4b\u8bd5\u5176\u533a\u5206\u6709\u610f\u4e49\u8bed\u4e49\u5173\u7cfb\u548c\u771f\u6b63\u65e0\u5173\u6027\u7684\u80fd\u529b\uff0c\u8fd9\u662fLLM\u8bc4\u4f30\u548c\u5b89\u5168\u7684\u5173\u952e\u524d\u6cbf\u95ee\u9898\u3002", "method": "\u6784\u5efaCORE\u6570\u636e\u96c6\uff0822.5\u4e07\u591a\u9009\u9898\uff09\u548c\u5f00\u6e90\u57fa\u51c6\uff08203\u4e2a\u4e25\u683c\u9a8c\u8bc1\u95ee\u9898\uff09\uff0c\u8986\u76d674\u4e2a\u5b66\u79d1\u548c24\u79cd\u8bed\u4e49\u5173\u7cfb\u7c7b\u578b\uff0c\u5305\u542b\u540c\u7b49\u6570\u91cf\u7684\u65e0\u5173\u5bf9\uff0c\u8bc4\u4f3029\u4e2aSOTA LLM\u3002", "result": "\u4eba\u7c7b\u57fa\u7ebf\u51c6\u786e\u738792.6%\uff08\u65e0\u5173\u5bf995.1%\uff09\uff0c\u800cLLM\u6574\u4f53\u51c6\u786e\u738748.25-70.9%\uff0c\u76f8\u5173\u5bf9\u8868\u73b0\u826f\u597d\uff0886.5-100%\uff09\uff0c\u4f46\u65e0\u5173\u5bf9\u4e25\u91cd\u9000\u5316\uff080-41.35%\uff09\uff0c\u8bed\u4e49\u5d29\u6e83\u7387\u5e73\u574737.6%\u3002", "conclusion": "\u65e0\u5173\u6027\u63a8\u7406\u662fLLM\u8bc4\u4f30\u548c\u5b89\u5168\u7684\u5173\u952e\u672a\u5145\u5206\u8bc4\u4f30\u524d\u6cbf\uff0cLLM\u5b58\u5728\u7cfb\u7edf\u6027\u751f\u6210\u865a\u5047\u5173\u7cfb\u7684\u503e\u5411\uff0c\u5728\u9886\u57df\u7279\u5b9a\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2602.06855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06855", "abs": "https://arxiv.org/abs/2602.06855", "authors": ["Alisia Lupidi", "Bhavul Gauri", "Thomas Simon Foster", "Bassel Al Omari", "Despoina Magka", "Alberto Pepe", "Alexis Audran-Reiss", "Muna Aghamelu", "Nicolas Baldwin", "Lucia Cipolina-Kun", "Jean-Christophe Gagnon-Audet", "Chee Hau Leow", "Sandra Lefdal", "Hossam Mossalam", "Abhinav Moudgil", "Saba Nazir", "Emanuel Tewolde", "Isabel Urrego", "Jordi Armengol Estape", "Amar Budhiraja", "Gaurav Chaurasia", "Abhishek Charnalia", "Derek Dunfield", "Karen Hambardzumyan", "Daniel Izcovich", "Martin Josifoski", "Ishita Mediratta", "Kelvin Niu", "Parth Pathak", "Michael Shvartsman", "Edan Toledo", "Anton Protopopov", "Roberta Raileanu", "Alexander Miller", "Tatiana Shavrina", "Jakob Foerster", "Yoram Bachrach"], "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "comment": "49 pages, 14 figures, 10 tables", "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "AI": {"tldr": "AIRS-Bench\u662f\u4e00\u4e2a\u5305\u542b20\u4e2a\u4efb\u52a1\u7684AI\u7814\u7a76\u79d1\u5b66\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6574\u4e2a\u7814\u7a76\u751f\u547d\u5468\u671f\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u60f3\u6cd5\u751f\u6210\u3001\u5b9e\u9a8c\u5206\u6790\u548c\u8fed\u4ee3\u4f18\u5316\u3002\u57fa\u51c6\u663e\u793a\u4ee3\u7406\u57284\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4eba\u7c7bSOTA\uff0c\u4f46\u572816\u4e2a\u4efb\u52a1\u4e2d\u672a\u80fd\u5339\u914d\uff0c\u8868\u660e\u4ecd\u6709\u5de8\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u6d4b\u8bd5\u4ee3\u7406\u5728\u6574\u4e2a\u7814\u7a76\u751f\u547d\u5468\u671f\u4e2d\u7684\u80fd\u529b\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u53ea\u5173\u6ce8\u7279\u5b9a\u65b9\u9762\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u7684\u79d1\u5b66\u7814\u7a76\u80fd\u529b\u3002", "method": "\u4ece\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\u9009\u53d620\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u8bed\u8a00\u5efa\u6a21\u3001\u6570\u5b66\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b49\u591a\u4e2a\u9886\u57df\u3002\u4efb\u52a1\u683c\u5f0f\u7075\u6d3b\uff0c\u4fbf\u4e8e\u96c6\u6210\u65b0\u4efb\u52a1\u548c\u6bd4\u8f83\u4e0d\u540c\u4ee3\u7406\u6846\u67b6\u3002\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u914d\u5408\u987a\u5e8f\u548c\u5e76\u884c\u6846\u67b6\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u4ee3\u7406\u57284\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u4eba\u7c7bSOTA\uff0c\u4f46\u572816\u4e2a\u4efb\u52a1\u4e2d\u672a\u80fd\u5339\u914d\u3002\u5373\u4f7f\u4ee3\u7406\u8d85\u8fc7\u4eba\u7c7b\u57fa\u51c6\uff0c\u4e5f\u672a\u80fd\u8fbe\u5230\u5e95\u5c42\u4efb\u52a1\u7684\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u3002\u8fd9\u8868\u660eAIRS-Bench\u8fdc\u672a\u9971\u548c\uff0c\u6709\u5de8\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "AIRS-Bench\u662f\u4e00\u4e2a\u6709\u6548\u7684\u79d1\u5b66\u7814\u7a76\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6539\u8fdb\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.06449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06449", "abs": "https://arxiv.org/abs/2602.06449", "authors": ["Xinxin Lin", "Guangxin Dai", "Yi Zhong", "Xiang Li", "Xue Xiao", "Yixin Zhang", "Zhengdong Wu", "Yongbo Zheng", "Runchuan Zhu", "Ming Zhao", "Huizi Yu", "Shuo Wu", "Jun Zhao", "Lingming Hu", "Yumei Wang", "Ping Yin", "Joey W. Y. Chan", "Ngan Yin Chan", "Sijing Chen", "Yun Kwok Wing", "Lin Lu", "Xin Ma", "Lizhou Fan"], "title": "Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning", "comment": "21 pages, 8 figures", "summary": "Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.", "AI": {"tldr": "ClinMPO\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u4e1a\u7cbe\u795e\u75c5\u5b66\u5b9e\u8df5\u5bf9\u9f50LLM\u5185\u90e8\u63a8\u7406\uff0c\u4f7f\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u590d\u6742\u7cbe\u795e\u75c5\u8bca\u65ad\u4efb\u52a1\u4e0a\u8d85\u8d8a\u533b\u5b66\u751f\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u5728\u7cbe\u795e\u75c5\u5b66\u5e94\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u6d45\u5c42\u63a8\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u8bad\u7ec3\u8303\u5f0f\u8fc7\u4e8e\u6ce8\u91cd\u8bed\u8a00\u6d41\u7545\u6027\u800c\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u903b\u8f91\uff0c\u5bfc\u81f4\u4e0e\u4e13\u4e1a\u8bca\u65ad\u8ba4\u77e5\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faClinMPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u95e8\u5956\u52b1\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e4,474\u7bc7\u7cbe\u795e\u75c5\u5b66\u671f\u520a\u6587\u7ae0\u6570\u636e\u96c6\u72ec\u7acb\u8bad\u7ec3\uff0c\u5e76\u6309\u7167\u5faa\u8bc1\u533b\u5b66\u539f\u5219\u7ed3\u6784\u5316\u3002\u5728Qwen3-8B\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ClinMPO\u8c03\u4f18\u7684Qwen3-8B\u6a21\u578b\u5728\u590d\u6742\u7cbe\u795e\u75c5\u8bca\u65ad\u4efb\u52a1\u4e0a\u8fbe\u523031.4%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7300\u540d\u533b\u5b66\u751f30.8%\u7684\u57fa\u51c6\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u6a21\u578b\u5e38\u5931\u8d25\u7684\u590d\u6742\u6848\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u533b\u5b66\u8bc1\u636e\u5f15\u5bfc\u7684\u4f18\u5316\u4f7f\u8f7b\u91cf\u7ea7LLM\u80fd\u591f\u638c\u63e1\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u660e\u786e\u7684\u8ba4\u77e5\u5bf9\u9f50\u4e3a\u53ef\u9760\u3001\u5b89\u5168\u7684\u7cbe\u795e\u75c5\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06948", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06948", "abs": "https://arxiv.org/abs/2602.06948", "authors": ["Jean Kaddour", "Srijan Patel", "Gb\u00e8tondji Dovonon", "Leo Richter", "Pasquale Minervini", "Matt J. Kusner"], "title": "Agentic Uncertainty Reveals Agentic Overconfidence", "comment": null, "summary": "Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.", "AI": {"tldr": "AI agents\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u6210\u529f\u7387\u4ec522%\u65f6\u5374\u9884\u6d4b77%\u6210\u529f\u3002\u7814\u7a76\u53d1\u73b0\u6267\u884c\u524d\u8bc4\u4f30\u6bd4\u6267\u884c\u540e\u8bc4\u4f30\u6709\u66f4\u597d\u533a\u5206\u80fd\u529b\uff0c\u5bf9\u6297\u6027\u63d0\u793a\uff08\u91cd\u6784\u4e3abug\u67e5\u627e\uff09\u5b9e\u73b0\u6700\u4f73\u6821\u51c6\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u80fd\u5426\u51c6\u786e\u9884\u6d4b\u81ea\u8eab\u4efb\u52a1\u6210\u529f\u7387\uff0c\u63a2\u7d22\u4ee3\u7406\u4e0d\u786e\u5b9a\u6027\uff08agentic uncertainty\uff09\u95ee\u9898\uff0c\u4e86\u89e3\u4ee3\u7406\u5728\u6267\u884c\u4efb\u52a1\u524d\u3001\u4e2d\u3001\u540e\u7684\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5728\u6267\u884c\u524d\u3001\u6267\u884c\u4e2d\u548c\u6267\u884c\u540e\u4e09\u4e2a\u9636\u6bb5\u83b7\u53d6\u4ee3\u7406\u7684\u6210\u529f\u6982\u7387\u4f30\u8ba1\uff0c\u6bd4\u8f83\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002\u4f7f\u7528\u5bf9\u6297\u6027\u63d0\u793a\u65b9\u6cd5\uff0c\u5c06\u8bc4\u4f30\u91cd\u6784\u4e3abug\u67e5\u627e\u4efb\u52a1\u3002", "result": "\u53d1\u73b0\u4ee3\u7406\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1\uff1a\u5b9e\u9645\u6210\u529f\u7387\u4ec522%\u7684\u4ee3\u7406\u9884\u6d4b\u6210\u529f\u7387\u9ad8\u8fbe77%\u3002\u6267\u884c\u524d\u8bc4\u4f30\u6bd4\u6807\u51c6\u6267\u884c\u540e\u8bc4\u4f30\u6709\u66f4\u597d\u7684\u533a\u5206\u80fd\u529b\uff08\u4f46\u5dee\u5f02\u4e0d\u603b\u662f\u663e\u8457\uff09\u3002\u5bf9\u6297\u6027\u63d0\u793a\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f73\u6821\u51c6\u6548\u679c\u3002", "conclusion": "AI\u4ee3\u7406\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u6267\u884c\u524d\u8bc4\u4f30\u53ef\u80fd\u63d0\u4f9b\u66f4\u6709\u4ef7\u503c\u7684\u9884\u6d4b\u4fe1\u606f\uff0c\u5bf9\u6297\u6027\u63d0\u793a\u662f\u6539\u5584\u4ee3\u7406\u81ea\u6211\u8bc4\u4f30\u6821\u51c6\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.06462", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06462", "abs": "https://arxiv.org/abs/2602.06462", "authors": ["Daisuke Oba", "Hiroki Furuta", "Naoaki Okazaki"], "title": "Diffusion-State Policy Optimization for Masked Diffusion Language Models", "comment": null, "summary": "Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .", "AI": {"tldr": "DiSPO\u662f\u4e00\u79cd\u7528\u4e8e\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e2d\u95f4\u63a9\u7801\u72b6\u6001\u5206\u652f\u91c7\u6837\u5e76\u8bc4\u5206\uff0c\u4f18\u5316\u4e2d\u95f4\u586b\u5145\u51b3\u7b56\uff0c\u65e0\u9700\u989d\u5916\u591a\u6b65\u6269\u6563\u5c55\u5f00\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u591a\u6b65\u53bb\u566a\u8fed\u4ee3\u586b\u5145\u63a9\u7801\u6807\u8bb0\uff0c\u4ec5\u57fa\u4e8e\u6700\u7ec8\u5b8c\u6210\u7684\u7ec8\u7aef\u5956\u52b1\u5b66\u4e60\u4f1a\u5bfc\u81f4\u5bf9\u4e2d\u95f4\u51b3\u7b56\u7684\u4fe1\u7528\u5206\u914d\u8fc7\u4e8e\u7c97\u7cd9\u3002", "method": "DiSPO\u5728\u9009\u5b9a\u7684\u4e2d\u95f4\u63a9\u7801\u72b6\u6001\u5206\u652f\uff0c\u4ece\u5c55\u5f00\u7f13\u5b58\u7684logits\u4e2d\u91cd\u65b0\u91c7\u6837\u5f53\u524d\u63a9\u7801\u4f4d\u7f6e\u7684\u586b\u5145\uff0c\u5bf9\u751f\u6210\u7684\u5b8c\u6210\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u4ec5\u66f4\u65b0\u65b0\u586b\u5145\u7684\u6807\u8bb0\uff0c\u65e0\u9700\u989d\u5916\u7684\u591a\u6b65\u6269\u6563\u5c55\u5f00\u3002", "result": "\u5728LLaDA-8B-Instruct\u4e0a\uff0cDiSPO\u5728\u5339\u914d\u7684\u5c55\u5f00\u8ba1\u7b97\u548c\u4f18\u5316\u5668\u6b65\u9aa4\u4e0b\uff0c\u5728\u6570\u5b66\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u7ec8\u7aef\u53cd\u9988\u7684diffu-GRPO\u57fa\u7ebf\u3002", "conclusion": "DiSPO\u4f5c\u4e3a\u4e00\u79cd\u63d2\u4ef6\u5f0f\u4fe1\u7528\u5206\u914d\u5c42\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u95f4\u586b\u5145\u51b3\u7b56\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06470", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06470", "abs": "https://arxiv.org/abs/2602.06470", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yiqun Liu"], "title": "Improve Large Language Model Systems with User Logs", "comment": null, "summary": "Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .", "AI": {"tldr": "UNO\u662f\u4e00\u4e2a\u4ece\u7528\u6237\u65e5\u5fd7\u4e2d\u5b66\u4e60\u4f18\u5316LLM\u7cfb\u7edf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u975e\u7ed3\u6784\u5316\u65e5\u5fd7\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u89c4\u5219\u548c\u504f\u597d\u5bf9\uff0c\u5e76\u91cf\u5316\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u4e0e\u65e5\u5fd7\u6570\u636e\u4e4b\u95f4\u7684\u8ba4\u77e5\u5dee\u8ddd\uff0c\u81ea\u9002\u5e94\u8fc7\u6ee4\u566a\u58f0\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLLM\u53d1\u5c55\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u53c2\u6570\u6269\u5c55\uff0c\u4f46\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u6210\u672c\u6536\u76ca\u9012\u51cf\u7684\u95ee\u9898\u3002\u7528\u6237\u4ea4\u4e92\u65e5\u5fd7\u4f5c\u4e3a\u771f\u5b9e\u4eba\u7c7b\u53cd\u9988\u548c\u7a0b\u5e8f\u77e5\u8bc6\u7684\u4e30\u5bcc\u6765\u6e90\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u65e5\u5fd7\u7684\u975e\u7ed3\u6784\u5316\u548c\u566a\u58f0\u7279\u6027\u4f7f\u5f97\u5b66\u4e60\u53d8\u5f97\u56f0\u96be\u3002", "method": "UNO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a1) \u5c06\u7528\u6237\u65e5\u5fd7\u84b8\u998f\u4e3a\u534a\u7ed3\u6784\u5316\u89c4\u5219\u548c\u504f\u597d\u5bf9\uff1b2) \u4f7f\u7528\u67e5\u8be2-\u53cd\u9988\u9a71\u52a8\u7684\u805a\u7c7b\u7ba1\u7406\u6570\u636e\u5f02\u8d28\u6027\uff1b3) \u91cf\u5316\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u4e0e\u65e5\u5fd7\u6570\u636e\u4e4b\u95f4\u7684\u8ba4\u77e5\u5dee\u8ddd\uff0c\u6307\u5bfc\u7cfb\u7edf\u81ea\u9002\u5e94\u8fc7\u6ee4\u566a\u58f0\u53cd\u9988\uff0c\u5e76\u4e3a\u7528\u6237\u65e5\u5fd7\u4e2d\u63d0\u53d6\u7684\u4e3b\u8981\u7ecf\u9a8c\u548c\u53cd\u601d\u7ecf\u9a8c\u6784\u5efa\u4e0d\u540c\u6a21\u5757\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUNO\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UNO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u7528\u6237\u65e5\u5fd7\u4e2d\u5b66\u4e60\uff0c\u89e3\u51b3\u65e5\u5fd7\u975e\u7ed3\u6784\u5316\u548c\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u91cf\u5316\u8ba4\u77e5\u5dee\u8ddd\u548c\u81ea\u9002\u5e94\u8fc7\u6ee4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.06526", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06526", "abs": "https://arxiv.org/abs/2602.06526", "authors": ["Minjeong Ban", "Jeonghwan Choi", "Hyangsuk Min", "Nicole Hee-Yeon Kim", "Minseok Kim", "Jae-Gil Lee", "Hwanjun Song"], "title": "Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks", "comment": "Accepted at ICLR 2026", "summary": "Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.", "AI": {"tldr": "DREAM\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u8f6e\u8fa9\u8bba\u7684LLM\u4ee3\u7406\u76f8\u5173\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u7acb\u7acb\u573a\u548c\u8fed\u4ee3\u4e92\u8bc4\u63d0\u9ad8\u6807\u6ce8\u51c6\u786e\u6027\uff0c\u4ec5\u97003.5%\u4eba\u5de5\u53c2\u4e0e\u8fbe\u523095.2%\u51c6\u786e\u7387\u3002\u57fa\u4e8e\u6b64\u6784\u5efa\u7684BRIDGE\u57fa\u51c6\u63ed\u793a\u4e8629,824\u4e2a\u7f3a\u5931\u76f8\u5173\u5757\uff0c\u51cf\u5c11\u4e86\u8bc4\u4f30\u504f\u5dee\u3002", "motivation": "\u5f53\u524d\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aIR\u57fa\u51c6\u6570\u636e\u96c6\u4e0d\u5b8c\u6574\uff0c\u5305\u542b\u672a\u6807\u6ce8\u7684\u76f8\u5173\u5757\u3002\u867d\u7136LLM\u548cLLM-\u4eba\u7c7b\u6df7\u5408\u7b56\u7565\u51cf\u5c11\u4e86\u4eba\u5de5\u6210\u672c\uff0c\u4f46\u4ecd\u5b58\u5728LLM\u8fc7\u5ea6\u81ea\u4fe1\u548cAI\u5230\u4eba\u7c7b\u5347\u7ea7\u65e0\u6548\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDREAM\u6846\u67b6\uff1a\u57fa\u4e8e\u5bf9\u7acb\u521d\u59cb\u7acb\u573a\u548c\u8fed\u4ee3\u4e92\u8bc4\u7684\u591a\u8f6e\u8fa9\u8bba\u5f0f\u76f8\u5173\u6027\u8bc4\u4f30\u6846\u67b6\u3002\u901a\u8fc7\u57fa\u4e8e\u5171\u8bc6\u7684\u8fa9\u8bba\uff0c\u5bf9\u786e\u5b9a\u6848\u4f8b\u4ea7\u751f\u66f4\u51c6\u786e\u6807\u6ce8\uff0c\u5bf9\u4e0d\u786e\u5b9a\u6848\u4f8b\u5b9e\u73b0\u66f4\u53ef\u9760\u7684AI\u5230\u4eba\u7c7b\u5347\u7ea7\u3002", "result": "\u8fbe\u523095.2%\u7684\u6807\u6ce8\u51c6\u786e\u7387\uff0c\u4ec5\u97003.5%\u4eba\u5de5\u53c2\u4e0e\u3002\u6784\u5efa\u4e86BRIDGE\u57fa\u51c6\uff0c\u63ed\u793a\u4e8629,824\u4e2a\u7f3a\u5931\u76f8\u5173\u5757\uff0c\u51cf\u5c11\u4e86\u8bc4\u4f30\u504f\u5dee\uff0c\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u7684\u68c0\u7d22\u5668\u6bd4\u8f83\u3002", "conclusion": "DREAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86IR\u8bc4\u4f30\u4e2d\u7684\u6807\u6ce8\u4e0d\u5b8c\u6574\u95ee\u9898\uff0cBRIDGE\u57fa\u51c6\u51cf\u5c11\u4e86\u8bc4\u4f30\u504f\u5dee\uff0c\u672a\u89e3\u51b3\u7684\u6f0f\u6d1e\u4e0d\u4ec5\u626d\u66f2\u68c0\u7d22\u5668\u6392\u540d\uff0c\u8fd8\u5bfc\u81f4\u68c0\u7d22-\u751f\u6210\u9519\u4f4d\u3002", "topic": "agent analysis"}}
{"id": "2602.06600", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06600", "abs": "https://arxiv.org/abs/2602.06600", "authors": ["Zhuoyuan Hao", "Zhuo Li", "Wu Li", "Fangming Liu", "Min Zhang", "Jing Li"], "title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning", "comment": null, "summary": "Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \\emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \\emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \\emph{Echo Likelihood Gap} $\u0394\\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \\emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \\emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \\emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\"\u63d0\u793a\u56de\u54cd\"\u73b0\u8c61\uff08Echo of Prompt, EOP\uff09\u4f5c\u4e3a\u8ba1\u7b97\u5206\u914d\u673a\u5236\uff0c\u901a\u8fc7\u56de\u54cd\u84b8\u998f\u76d1\u7763\u5fae\u8c03\u548c\u56de\u54cd\u63d0\u793a\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5206\u914d\u4e2d\u8981\u4e48\u6ce8\u5165\u4efb\u52a1\u65e0\u5173\u7684\u6807\u8bb0\uff0c\u8981\u4e48\u4f7f\u7528\u65e0\u6cd5\u89e3\u91ca\u6a21\u578b\u81ea\u53d1\u91cd\u590d\u73b0\u8c61\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u4f5c\u8005\u53d1\u73b0\u5927\u63a8\u7406\u6a21\u578b\u5728\u5185\u90e8\u94fe\u5f00\u5934\u503e\u5411\u4e8e\u91cd\u8ff0\u95ee\u9898\uff08EOP\u73b0\u8c61\uff09\uff0c\u8fd9\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u524d\u7f6e\u7684\u8ba1\u7b97\u5851\u9020\u673a\u5236\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u548c\u5229\u7528\u65b9\u6cd5\u3002", "method": "1. \u7406\u8bba\u5206\u6790\uff1a\u5c06\u56de\u54cd\u79fb\u9664\u89c6\u4e3a\u57fa\u4e8e\u62d2\u7edd\u7684\u6761\u4ef6\u5316\uff0c\u5b9a\u4e49\u53ef\u8ba1\u7b97\u7684\"\u56de\u54cd\u4f3c\u7136\u5dee\u8ddd\"\u4f5c\u4e3a\u4ee3\u7406\u6307\u6807\uff1b2. \u56de\u54cd\u84b8\u998f\u76d1\u7763\u5fae\u8c03\uff08ED-SFT\uff09\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6ce8\u5165\"\u5148\u56de\u54cd\u540e\u63a8\u7406\"\u6a21\u5f0f\uff1b3. \u56de\u54cd\u63d0\u793a\uff08EP\uff09\uff1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u951a\u5b9a\u6a21\u578b\u800c\u65e0\u9700\u8bad\u7ec3\uff1b4. \u901a\u8fc7\u957f\u5ea6\u548c\u540e\u7f00\u63a7\u5236\u7684\u4f3c\u7136\u5206\u6790\u53ca\u5c42\u95f4\u6ce8\u610f\u529b\u7814\u7a76\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728GSM8K\u3001MathQA\u3001Hendrycks-MATH\u3001AIME24\u548cMATH-500\u7b49\u57fa\u51c6\u4e0a\uff0c\u5728\u76f8\u540c\u89e3\u7801\u8bbe\u7f6e\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cEOP\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793aEOP\u589e\u52a0\u4e86\u4e2d\u95f4\u5c42\u4e2d\u7b54\u6848\u5bf9\u7b54\u6848\u524d\u7f00\u7684\u6ce8\u610f\u529b\uff0c\u7b26\u5408\"\u6ce8\u610f\u529b\u91cd\u65b0\u805a\u7126\"\u673a\u5236\u3002", "conclusion": "EOP\u73b0\u8c61\u4e0d\u4ec5\u662f\u5197\u4f59\u7684\u91cd\u590d\uff0c\u800c\u662f\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u53ef\u91cf\u5316\u7684\u8ba1\u7b97\u5206\u914d\u673a\u5236\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4e24\u79cd\u5229\u7528\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.06326", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06326", "abs": "https://arxiv.org/abs/2602.06326", "authors": ["Aoi Yoshimura", "Gouhei Tanaka"], "title": "Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics", "comment": "Submitted to IJCNN 2026", "summary": "Reinforcement learning (RL) policies trained in simulation often suffer from severe performance degradation when deployed in real-world environments due to non-stationary dynamics. While Domain Randomization (DR) and meta-RL have been proposed to address this issue, they typically rely on extensive pretraining, privileged information, or high computational cost, limiting their applicability to real-time and edge systems. In this paper, we propose a lightweight online adaptation framework for RL based on Reservoir Computing. Specifically, we integrate an Echo State Networks (ESNs) as an adaptation module that encodes recent observation histories into a latent context representation, and update its readout weights online using Recursive Least Squares (RLS). This design enables rapid adaptation without backpropagation, pretraining, or access to privileged information. We evaluate the proposed method on CartPole and HalfCheetah tasks with severe and abrupt environment changes, including periodic external disturbances and extreme friction variations. Experimental results demonstrate that the proposed approach significantly outperforms DR and representative adaptive baselines under out-of-distribution dynamics, achieving stable adaptation within a few control steps. Notably, the method successfully handles intra-episode environment changes without resetting the policy. Due to its computational efficiency and stability, the proposed framework provides a practical solution for online adaptation in non-stationary environments and is well suited for real-world robotic control and edge deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u50a8\u5c42\u8ba1\u7b97\u7684\u8f7b\u91cf\u7ea7\u5728\u7ebf\u9002\u5e94\u6846\u67b6\uff0c\u4f7f\u7528\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u7f16\u7801\u89c2\u6d4b\u5386\u53f2\u5e76\u901a\u8fc7\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u5728\u7ebf\u66f4\u65b0\uff0c\u5b9e\u73b0\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u5feb\u901f\u9002\u5e94", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u4eff\u771f\u8bad\u7ec3\u540e\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u65f6\uff0c\u7531\u4e8e\u975e\u5e73\u7a33\u52a8\u6001\u7279\u6027\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u57df\u968f\u673a\u5316\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u3001\u7279\u6743\u4fe1\u606f\u6216\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u65f6\u548c\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u96c6\u6210\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u4f5c\u4e3a\u9002\u5e94\u6a21\u5757\uff0c\u7f16\u7801\u6700\u8fd1\u7684\u89c2\u6d4b\u5386\u53f2\u4e3a\u6f5c\u5728\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u5728\u7ebf\u66f4\u65b0\u5176\u8bfb\u51fa\u6743\u91cd\u3002\u8be5\u8bbe\u8ba1\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3001\u9884\u8bad\u7ec3\u6216\u7279\u6743\u4fe1\u606f\u8bbf\u95ee\u3002", "result": "\u5728CartPole\u548cHalfCheetah\u4efb\u52a1\u4e0a\u6d4b\u8bd5\uff0c\u9762\u5bf9\u4e25\u91cd\u4e14\u7a81\u7136\u7684\u73af\u5883\u53d8\u5316\uff08\u5468\u671f\u6027\u5916\u90e8\u5e72\u6270\u548c\u6781\u7aef\u6469\u64e6\u53d8\u5316\uff09\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57df\u968f\u673a\u5316\u548c\u4ee3\u8868\u6027\u81ea\u9002\u5e94\u57fa\u7ebf\uff0c\u80fd\u5728\u51e0\u4e2a\u63a7\u5236\u6b65\u9aa4\u5185\u5b9e\u73b0\u7a33\u5b9a\u9002\u5e94\uff0c\u5e76\u80fd\u5904\u7406\u60c5\u8282\u5185\u7684\u73af\u5883\u53d8\u5316\u800c\u65e0\u9700\u91cd\u7f6e\u7b56\u7565\u3002", "conclusion": "\u7531\u4e8e\u8ba1\u7b97\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u8be5\u6846\u67b6\u4e3a\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u548c\u8fb9\u7f18\u90e8\u7f72\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06385", "abs": "https://arxiv.org/abs/2602.06385", "authors": ["Changmin Kang", "Jihun Yun", "Baekrok Shin", "Yeseul Cho", "Chulhee Yun"], "title": "Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization", "comment": null, "summary": "Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove \"equal-rate\" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8c31\u68af\u5ea6\u4e0b\u964d(SpecGD)\u5728LoRA\u5fae\u8c03LLM\u4e2d\u7684\u72ec\u7279\u73b0\u8c61\uff1aLoRA\u4e58\u79ef\u7684\u5947\u5f02\u503c\u5448\u73b0\u8fd1\u4e4e\u5747\u5300\u7684\u589e\u957f\uff0c\u5c3d\u7ba1\u6b63\u4ea4\u5316\u662f\u5728\u4e24\u4e2a\u4f4e\u79e9\u56e0\u5b50\u5206\u522b\u8fdb\u884c\u7684\u3002\u4f5c\u8005\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u8fd9\u79cd\"\u7b49\u901f\u7387\"\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u5168\u5c40\u6536\u655b\u6027\u3002", "motivation": "\u8c31\u68af\u5ea6\u4e0b\u964d(SpecGD)\u53ca\u5176\u53d8\u4f53\u5982Muon\u5728LLM\u8bad\u7ec3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u52a8\u529b\u5b66\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7279\u522b\u662f\u5728LoRA\u5fae\u8c03\u4e2d\uff0c\u89c2\u5bdf\u5230LoRA\u4e58\u79ef\u7684\u5947\u5f02\u503c\u5448\u73b0\u5747\u5300\u589e\u957f\u73b0\u8c61\uff0c\u8fd9\u4e0e\u6807\u51c6\u68af\u5ea6\u6d41\u7684\"\u6700\u5927\u4f18\u5148\"\u5b66\u4e60\u6a21\u5f0f\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u9700\u8981\u7406\u8bba\u89e3\u91ca\u3002", "method": "1) \u5728LoRA\u5fae\u8c03LLM\u4e2d\u89c2\u5bdfMuon\u4f18\u5316\u5668\u7684\u5947\u5f02\u503c\u589e\u957f\u73b0\u8c61\uff1b2) \u5728\u7b80\u5316\u7684LoRA\u98ce\u683c\u77e9\u9635\u5206\u89e3\u8bbe\u7f6e\u4e2d\u5206\u6790\u8c31\u68af\u5ea6\u6d41(SpecGF)\u7684\u8fde\u7eed\u65f6\u95f4\u6a21\u62df\uff1b3) \u7406\u8bba\u8bc1\u660e\"\u7b49\u901f\u7387\"\u52a8\u529b\u5b66\u7279\u6027\uff1b4) \u8bc1\u660e\u5168\u5c40\u6536\u655b\u6027\uff0c\u5305\u62ec\u6709\u754c\u8303\u6570\u6761\u4ef6\u4e0b\u7684\u5168\u5c40\u6700\u5c0f\u503c\u548c\u2113\u2082\u6b63\u5219\u5316\u4e0b\u7684\u5168\u5c40\u6536\u655b\uff1b5) \u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u5206\u6790\u3002", "result": "1) \u53d1\u73b0LoRA\u4e58\u79ef\u5947\u5f02\u503c\u5728Muon\u4f18\u5316\u4e0b\u5448\u73b0\u8fd1\u4e4e\u5747\u5300\u589e\u957f\uff1b2) \u7406\u8bba\u8bc1\u660eSpecGF\u4e2d\u6240\u6709\u5947\u5f02\u503c\u4ee5\u76f8\u7b49\u901f\u7387\u589e\u957f\uff08\u5b58\u5728\u5c0f\u504f\u5dee\uff09\uff1b3) \u8f83\u5c0f\u5947\u5f02\u503c\u6bd4\u5927\u5947\u5f02\u503c\u66f4\u65e9\u8fbe\u5230\u76ee\u6807\u503c\uff0c\u4e0e\u6807\u51c6\u68af\u5ea6\u6d41\u7684\"\u6700\u5927\u4f18\u5148\"\u5b66\u4e60\u6a21\u5f0f\u76f8\u53cd\uff1b4) \u8bc1\u660e\u5728\u56e0\u5b50\u8303\u6570\u6709\u754c\u6761\u4ef6\u4e0b\uff0cSpecGF\u51e0\u4e4e\u4ece\u6240\u6709\u521d\u59cb\u5316\u90fd\u80fd\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\uff1b5) \u5728\u2113\u2082\u6b63\u5219\u5316\u4e0b\u83b7\u5f97\u5168\u5c40\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u8c31\u68af\u5ea6\u4e0b\u964d\u5728LoRA\u5fae\u8c03\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u7684\"\u7b49\u901f\u7387\"\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4f7f\u5947\u5f02\u503c\u5747\u5300\u589e\u957f\uff0c\u8f83\u5c0f\u5947\u5f02\u503c\u4f18\u5148\u5b66\u4e60\u3002\u8fd9\u79cd\u7279\u6027\u4e0e\u6807\u51c6\u68af\u5ea6\u6d41\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4e0a\u7684\u5168\u5c40\u6536\u655b\u4fdd\u8bc1\uff0c\u4e3a\u7406\u89e3\u8c31\u68af\u5ea6\u65b9\u6cd5\u5728LLM\u5fae\u8c03\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.06724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06724", "abs": "https://arxiv.org/abs/2602.06724", "authors": ["Tian Lan", "Felix Henry", "Bin Zhu", "Qianghuai Jia", "Junyang Ren", "Qihang Pu", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo"], "title": "Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion", "comment": null, "summary": "Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \\textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.", "AI": {"tldr": "TaS\u5c06\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u91cd\u6784\u4e3a\u8868\u683c\u8865\u5168\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u683c\u7ba1\u7406\u641c\u7d22\u72b6\u6001\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u641c\u7d22\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd", "motivation": "\u5f53\u524d\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u63a2\u7d22\u4e2d\u96be\u4ee5\u4fdd\u6301\u7126\u70b9\u548c\u8fde\u8d2f\u6027\uff0c\u56e0\u4e3a\u5728\u4e00\u4e2a\u7eaf\u6587\u672c\u4e0a\u4e0b\u6587\u4e2d\u8ddf\u8e2a\u641c\u7d22\u72b6\u6001\uff08\u5305\u62ec\u89c4\u5212\u8fc7\u7a0b\u548c\u5927\u91cf\u641c\u7d22\u7ed3\u679c\uff09\u672c\u8d28\u4e0a\u662f\u8106\u5f31\u7684", "method": "\u63d0\u51faTable-as-Search\u6846\u67b6\uff0c\u5c06\u67e5\u8be2\u6620\u5c04\u5230\u5916\u90e8\u6570\u636e\u5e93\u4e2d\u7684\u7ed3\u6784\u5316\u8868\u683c\u6a21\u5f0f\uff0c\u884c\u8868\u793a\u641c\u7d22\u5019\u9009\uff0c\u5217\u8868\u793a\u7ea6\u675f\u6216\u6240\u9700\u4fe1\u606f\uff0c\u901a\u8fc7\u8868\u683c\u7cbe\u786e\u7ba1\u7406\u641c\u7d22\u72b6\u6001", "result": "TaS\u5728\u4e09\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f17\u591a\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5305\u62ec\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u5546\u4e1a\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u957f\u65f6\u7a0b\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\u3001\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027", "conclusion": "TaS\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u683c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u72b6\u6001\u8ddf\u8e2a\u95ee\u9898\uff0c\u7edf\u4e00\u4e86\u6df1\u5ea6\u641c\u7d22\u3001\u5e7f\u5ea6\u641c\u7d22\u548c\u6df1\u5ea6\u5e7f\u5ea6\u641c\u7d22\u4e09\u79cd\u4efb\u52a1\uff0c\u4e3a\u957f\u65f6\u7a0b\u4fe1\u606f\u641c\u7d22\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2602.06854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06854", "abs": "https://arxiv.org/abs/2602.06854", "authors": ["Mingqian Feng", "Xiaodong Liu", "Weiwei Yang", "Jialin Song", "Xuekai Zhu", "Chenliang Xu", "Jianfeng Gao"], "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks", "comment": "ICLR 2026, 37 pages, 13 tables, 7 figures", "summary": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.", "AI": {"tldr": "SEMA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u8005\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u586b\u5145\u81ea\u8c03\u4f18\u548c\u610f\u56fe\u6f02\u79fb\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u73b0\u6709\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\u9762\u4e34\u63a2\u7d22\u590d\u6742\u6027\u548c\u610f\u56fe\u6f02\u79fb\u95ee\u9898\uff0c\u800c\u5355\u8f6e\u653b\u51fb\u53ea\u662f\u591a\u8f6e\u653b\u51fb\u7684\u7279\u4f8b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u591a\u8f6e\u653b\u51fb\u65b9\u6cd5\u6765\u66f4\u771f\u5b9e\u5730\u6d4b\u8bd5LLM\u5b89\u5168\u6027\u3002", "method": "SEMA\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u9884\u586b\u5145\u81ea\u8c03\u4f18\uff1a\u901a\u8fc7\u5fae\u8c03\u81ea\u751f\u6210\u7684\u975e\u62d2\u7edd\u3001\u7ed3\u6784\u826f\u597d\u7684\u591a\u8f6e\u5bf9\u6297\u63d0\u793a\u6765\u7a33\u5b9a\u5b66\u4e60\uff1b2) \u610f\u56fe\u6f02\u79fb\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\uff1a\u7ed3\u5408\u610f\u56fe\u5bf9\u9f50\u3001\u5408\u89c4\u98ce\u9669\u548c\u7ec6\u8282\u7a0b\u5ea6\u7684\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3\u653b\u51fb\u8005\u4fdd\u6301\u6709\u5bb3\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u53d7\u5bb3\u6a21\u578b\u548c\u8d8a\u72f1\u8bc4\u4f30\u4e2d\uff0cSEMA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u5728AdvBench\u4e0a\u5bf9\u4e09\u4e2a\u95ed\u6e90\u548c\u5f00\u6e90\u53d7\u5bb3\u6a21\u578b\u5e73\u5747\u8fbe\u523080.1% ASR@1\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa33.9%\u3002", "conclusion": "SEMA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u4e14\u53ef\u8de8\u76ee\u6807\u8fc1\u79fb\u7684\u6846\u67b6\uff0c\u4e3aLLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u66f4\u5f3a\u3001\u66f4\u73b0\u5b9e\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u80fd\u591f\u81ea\u52a8\u8fdb\u884c\u7ea2\u961f\u6d4b\u8bd5\u4ee5\u66b4\u9732\u548c\u5b9a\u4f4d\u6545\u969c\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2602.06448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06448", "abs": "https://arxiv.org/abs/2602.06448", "authors": ["Yingming Pu", "Tao Lin", "Hongyu Chen"], "title": "Principle-Evolvable Scientific Discovery via Uncertainty Minimization", "comment": null, "summary": "Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.", "AI": {"tldr": "PiEvo\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u6269\u5c55\u7684\u539f\u7406\u7a7a\u95f4\u4e2d\u8fdb\u5316\u79d1\u5b66\u539f\u7406\uff0c\u800c\u975e\u5728\u56fa\u5b9a\u5047\u8bbe\u7a7a\u95f4\u4e2d\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d1\u73b0\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u901a\u5e38\u56fa\u5b88\u521d\u59cb\u5148\u9a8c\uff0c\u5728\u9759\u6001\u5047\u8bbe\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u65b0\u73b0\u8c61\u7684\u53d1\u73b0\uff0c\u5f53\u57fa\u7ebf\u7406\u8bba\u5931\u8d25\u65f6\u4f1a\u9020\u6210\u8ba1\u7b97\u6d6a\u8d39\u3002\u9700\u8981\u4ece\u641c\u7d22\u5047\u8bbe\u8f6c\u5411\u8fdb\u5316\u57fa\u7840\u79d1\u5b66\u539f\u7406\u3002", "method": "\u63d0\u51faPiEvo\u6846\u67b6\uff0c\u5c06\u79d1\u5b66\u53d1\u73b0\u89c6\u4e3a\u5728\u6269\u5c55\u539f\u7406\u7a7a\u95f4\u4e0a\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u3002\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u7684\u4fe1\u606f\u5bfc\u5411\u5047\u8bbe\u9009\u62e9\u548c\u5f02\u5e38\u9a71\u52a8\u7684\u589e\u5f3a\u673a\u5236\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u81ea\u4e3b\u7cbe\u70bc\u5176\u7406\u8bba\u4e16\u754c\u89c2\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPiEvo\u8fbe\u523090.81%~93.15%\u7684\u5e73\u5747\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u534729.7%~31.1%\uff1b\u901a\u8fc7\u4f18\u5316\u7d27\u51d1\u539f\u7406\u7a7a\u95f4\u5b9e\u73b083.3%\u7684\u6536\u655b\u901f\u5ea6\u63d0\u5347\uff1b\u5728\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "PiEvo\u901a\u8fc7\u8fdb\u5316\u79d1\u5b66\u539f\u7406\u800c\u975e\u641c\u7d22\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2602.06511", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06511", "abs": "https://arxiv.org/abs/2602.06511", "authors": ["Yuntong Hu", "Matthew Trager", "Yuting Zhang", "Yi Zhang", "Shuo Yang", "Wei Xia", "Stefano Soatto"], "title": "Evolutionary Generation of Multi-Agent Systems", "comment": "26 pages, 15 figures", "summary": "Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.", "AI": {"tldr": "EvoMAS\uff1a\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u914d\u7f6e\u751f\u6210\u548c\u8fdb\u5316\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u6027\u80fd\u3001\u53ef\u6267\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u4f9d\u8d56\u4eba\u5de5\u6216\u4ee3\u7801\u751f\u6210\uff0c\u5b58\u5728\u53ef\u6267\u884c\u6027\u5dee\u3001\u9c81\u68d2\u6027\u4f4e\u3001\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06MAS\u751f\u6210\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u914d\u7f6e\u751f\u6210\u95ee\u9898\uff0c\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\uff1a\u4ece\u914d\u7f6e\u6c60\u9009\u62e9\u521d\u59cb\u914d\u7f6e\uff0c\u57fa\u4e8e\u6267\u884c\u8f68\u8ff9\u53cd\u9988\u8fdb\u884c\u53d8\u5f02\u548c\u4ea4\u53c9\uff0c\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u6c60\u548c\u7ecf\u9a8c\u8bb0\u5fc6\u3002", "result": "\u5728BBEH\u63a8\u7406\u3001SWE-Bench\u8f6f\u4ef6\u5de5\u7a0b\u3001WorkBench\u5de5\u5177\u4f7f\u7528\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvoMAS\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u548c\u73b0\u6709\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u6267\u884c\u6027\u548c\u8fd0\u884c\u65f6\u9c81\u68d2\u6027\u66f4\u9ad8\u3002", "conclusion": "EvoMAS\u4e3a\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u3001\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5728\u590d\u6742\u63a8\u7406\u3001\u8f6f\u4ef6\u5de5\u7a0b\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.06941", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06941", "abs": "https://arxiv.org/abs/2602.06941", "authors": ["Alex McKenzie", "Keenan Pepper", "Stijn Servaes", "Martin Leitgab", "Murat Cubuktepe", "Mike Vaiana", "Diogo de Lucena", "Judd Rosenblatt", "Michael S. A. Graziano"], "title": "Endogenous Resistance to Activation Steering in Language Models", "comment": null, "summary": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u591f\u62b5\u6297\u4efb\u52a1\u4e0d\u5339\u914d\u7684\u6fc0\u6d3b\u5f15\u5bfc\uff0c\u6709\u65f6\u5728\u5f15\u5bfc\u6301\u7eed\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4e2d\u9014\u6062\u590d\u5e76\u4ea7\u751f\u6539\u8fdb\u7684\u54cd\u5e94\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u5185\u6e90\u6027\u5f15\u5bfc\u62b5\u6297\uff08ESR\uff09\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6fc0\u6d3b\u5f15\u5bfc\u4e0b\u7684\u62b5\u6297\u884c\u4e3a\uff0c\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u662f\u5426\u5b58\u5728\u4e13\u95e8\u7684\"\u4e00\u81f4\u6027\u68c0\u67e5\"\u7535\u8def\uff0c\u4ee5\u53ca\u8fd9\u79cd\u62b5\u6297\u673a\u5236\u5bf9AI\u7cfb\u7edf\u900f\u660e\u6027\u548c\u53ef\u63a7\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u6f5c\u5728\u53d8\u91cf\u5f15\u5bfc\u6a21\u578b\u6fc0\u6d3b\uff0c\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff08Llama-3.3-70B\u3001Llama-3\u548cGemma-2\u7cfb\u5217\uff09\u7684ESR\u8868\u73b0\u3002\u901a\u8fc7\u96f6\u6d88\u878d\u5b9e\u9a8c\u8bc6\u522b\u4e0eESR\u56e0\u679c\u76f8\u5173\u7684\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u5b9e\u9a8c\u589e\u5f3aESR\u884c\u4e3a\u3002", "result": "Llama-3.3-70B\u8868\u73b0\u51fa\u663e\u8457\u7684ESR\uff0c\u800c\u8f83\u5c0f\u6a21\u578b\u8f83\u5c11\u51fa\u73b0\u6b64\u73b0\u8c61\u3002\u8bc6\u522b\u51fa26\u4e2a\u4e0e\u79bb\u9898\u5185\u5bb9\u6fc0\u6d3b\u76f8\u5173\u7684SAE\u6f5c\u5728\u53d8\u91cf\uff0c\u96f6\u6d88\u878d\u8fd9\u4e9b\u53d8\u91cf\u53ef\u5c06\u591a\u5c1d\u8bd5\u7387\u964d\u4f4e25%\u3002\u901a\u8fc7\u5143\u63d0\u793a\u53ef\u5c06\u591a\u5c1d\u8bd5\u7387\u63d0\u9ad84\u500d\uff0c\u901a\u8fc7\u5fae\u8c03\u53ef\u5728\u5c0f\u6a21\u578b\u4e2d\u8bf1\u5bfc\u7c7b\u4f3cESR\u7684\u884c\u4e3a\u3002", "conclusion": "ESR\u673a\u5236\u5177\u6709\u53cc\u91cd\u610f\u4e49\uff1a\u4e00\u65b9\u9762\u53ef\u62b5\u5fa1\u5bf9\u6297\u6027\u64cd\u7eb5\uff0c\u53e6\u4e00\u65b9\u9762\u53ef\u80fd\u5e72\u6270\u4f9d\u8d56\u6fc0\u6d3b\u5f15\u5bfc\u7684\u6709\u76ca\u5b89\u5168\u5e72\u9884\u3002\u7406\u89e3\u548c\u63a7\u5236\u8fd9\u4e9b\u62b5\u6297\u673a\u5236\u5bf9\u4e8e\u5f00\u53d1\u900f\u660e\u53ef\u63a7\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2602.06939", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06939", "abs": "https://arxiv.org/abs/2602.06939", "authors": ["Zuyuan Zhang", "Sizhe Tang", "Tian Lan"], "title": "Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics", "comment": null, "summary": "Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62d3\u6251\u89c6\u89d2\u7684\u5f3a\u5316\u5b66\u4e60\u65b0\u6846\u67b6\uff0c\u5c06TD\u8bef\u5dee\u89c6\u4e3a\u72b6\u6001\u8f6c\u79fb\u62d3\u6251\u7a7a\u95f4\u4e2d\u76841-\u4e0a\u94fe\uff0c\u901a\u8fc7Bellman-de Rham\u6295\u5f71\u5206\u89e3TD\u8bef\u5dee\uff0c\u5e76\u5f00\u53d1HodgeFlow\u7b56\u7565\u641c\u7d22\u7b97\u6cd5\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u73af\u5883\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u666e\u904d\u5b58\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u6001\uff08\u957f\u7a0b\u4f9d\u8d56\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u8bb0\u5fc6\u6548\u5e94\uff09\uff0c\u4f20\u7edf\u8d1d\u5c14\u66fc\u65b9\u7a0b\u4ec5\u8fd1\u4f3c\u6709\u6548\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5b9e\u7528\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u6846\u67b6\u6765\u56de\u7b54\u5173\u952e\u95ee\u9898\uff1a\u54ea\u4e9b\u52a8\u6001\u80fd\u88ab\u8d1d\u5c14\u66fc\u6846\u67b6\u6355\u83b7\uff1f\u5982\u4f55\u542f\u53d1\u65b0\u7684\u6700\u4f18\u8fd1\u4f3c\u7b97\u6cd5\u7c7b\u522b\uff1f", "method": "\u63d0\u51fa\u62d3\u6251\u89c6\u89d2\u7684\u65f6\u5e8f\u5dee\u5206\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5c06TD\u8bef\u5dee\u89c6\u4e3a\u72b6\u6001\u8f6c\u79fb\u62d3\u6251\u7a7a\u95f4\u4e2d\u76841-\u4e0a\u94fe\uff1b2\uff09\u5c06\u9a6c\u5c14\u53ef\u592b\u52a8\u6001\u89e3\u91ca\u4e3a\u62d3\u6251\u53ef\u79ef\u6027\uff1b3\uff09\u901a\u8fc7Bellman-de Rham\u6295\u5f71\u5b9e\u73b0TD\u8bef\u5dee\u7684Hodge\u578b\u5206\u89e3\uff08\u53ef\u79ef\u5206\u91cf+\u62d3\u6251\u6b8b\u5dee\uff09\uff1b4\uff09\u63d0\u51faHodgeFlow\u7b56\u7565\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u62df\u5408\u52bf\u7f51\u7edc\u6700\u5c0f\u5316\u975e\u53ef\u79ef\u6295\u5f71\u6b8b\u5dee\u3002", "result": "HFPS\u7b97\u6cd5\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u5e76\u83b7\u5f97\u7a33\u5b9a\u6027/\u654f\u611f\u6027\u4fdd\u8bc1\u3002\u6570\u503c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u62d3\u6251\u89c6\u89d2\uff0c\u5efa\u7acb\u4e86TD\u8bef\u5dee\u4e0e\u62d3\u6251\u7ed3\u6784\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u89e3\u51b3\u975e\u9a6c\u5c14\u53ef\u592b\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06769", "abs": "https://arxiv.org/abs/2602.06769", "authors": ["Marco Bagatella", "Thomas Rupf", "Georg Martius", "Andreas Krause"], "title": "Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities", "comment": null, "summary": "Recent advancements in zero-shot reinforcement learning (RL) have facilitated the extraction of diverse behaviors from unlabeled, offline data sources. In particular, forward-backward algorithms (FB) can retrieve a family of policies that can approximately solve any standard RL problem (with additive rewards, linear in the occupancy measure), given sufficient capacity. While retaining zero-shot properties, we tackle the greater problem class of RL with general utilities, in which the objective is an arbitrary differentiable function of the occupancy measure. This setting is strictly more expressive, capturing tasks such as distribution matching or pure exploration, which may not be reduced to additive rewards. We show that this additional complexity can be captured by a novel, maximum entropy (soft) variant of the forward-backward algorithm, which recovers a family of stochastic policies from offline data. When coupled with zero-order search over compact policy embeddings, this algorithm can sidestep iterative optimization schemes, and optimizes general utilities directly at test-time. Across both didactic and high-dimensional experiments, we demonstrate that our method retains favorable properties of FB algorithms, while also extending their range to more general RL problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6700\u5927\u71b5\u524d\u5411-\u540e\u5411\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u63d0\u53d6\u7b56\u7565\u65cf\uff0c\u53ef\u96f6\u6837\u672c\u4f18\u5316\u4efb\u610f\u53ef\u5fae\u7684\u5360\u7528\u5ea6\u91cf\u51fd\u6570\uff0c\u6269\u5c55\u4e86\u4f20\u7edfRL\u95ee\u9898\u8303\u56f4\u3002", "motivation": "\u4f20\u7edf\u524d\u5411-\u540e\u5411\u7b97\u6cd5\u53ea\u80fd\u5904\u7406\u52a0\u6027\u5956\u52b1\u7684RL\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8bb8\u591a\u65e0\u6cd5\u7b80\u5316\u4e3a\u52a0\u6027\u5956\u52b1\u7684\u4efb\u52a1\uff08\u5982\u5206\u5e03\u5339\u914d\u3001\u7eaf\u63a2\u7d22\u7b49\uff09\u3002\u9700\u8981\u6269\u5c55\u7b97\u6cd5\u4ee5\u5904\u7406\u66f4\u4e00\u822c\u7684RL\u6548\u7528\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u6700\u5927\u71b5\u524d\u5411-\u540e\u5411\u7b97\u6cd5\uff0c\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u6062\u590d\u968f\u673a\u7b56\u7565\u65cf\u3002\u7ed3\u5408\u96f6\u9636\u641c\u7d22\u5728\u7d27\u51d1\u7b56\u7565\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f18\u5316\uff0c\u53ef\u5728\u6d4b\u8bd5\u65f6\u76f4\u63a5\u4f18\u5316\u4e00\u822c\u6548\u7528\u51fd\u6570\uff0c\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\u65b9\u6848\u3002", "result": "\u65b9\u6cd5\u4fdd\u7559\u4e86\u524d\u5411-\u540e\u5411\u7b97\u6cd5\u7684\u4f18\u52bf\u7279\u6027\uff0c\u540c\u65f6\u6210\u529f\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684RL\u95ee\u9898\u3002\u5728\u7b80\u5355\u793a\u4f8b\u548c\u9ad8\u7ef4\u5b9e\u9a8c\u4e2d\u5747\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u5927\u71b5\u524d\u5411-\u540e\u5411\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u53ef\u5fae\u5360\u7528\u5ea6\u91cf\u51fd\u6570\u7684RL\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u96f6\u6837\u672cRL\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u66f4\u590d\u6742\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06791", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.06791", "abs": "https://arxiv.org/abs/2602.06791", "authors": ["Jake McAllister Dorman", "Edward Gillman", "Dominic C. Rose", "Jamie F. Mair", "Juan P. Garrahan"], "title": "Rare Event Analysis of Large Language Models", "comment": null, "summary": "Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7f55\u89c1\u4e8b\u4ef6\uff0c\u5305\u62ec\u7406\u8bba\u3001\u9ad8\u6548\u751f\u6210\u7b56\u7565\u3001\u6982\u7387\u4f30\u8ba1\u548c\u8bef\u5dee\u5206\u6790", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6982\u7387\u6a21\u578b\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u7f55\u89c1\u4e8b\u4ef6\u2014\u2014\u8fdc\u79bb\u5178\u578b\u4f46\u9ad8\u5ea6\u663e\u8457\u7684\u884c\u4e3a\u3002\u7531\u4e8eLLM\u4f7f\u7528\u89c4\u6a21\u5de8\u5927\uff0c\u5f00\u53d1\u9636\u6bb5\u5b8c\u5168\u672a\u89c2\u5bdf\u5230\u7684\u4e8b\u4ef6\u5f88\u53ef\u80fd\u5728\u90e8\u7f72\u4e2d\u53d8\u5f97\u7a81\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u7f55\u89c1\u4e8b\u4ef6", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u7406\u8bba\u5206\u6790\u3001\u9ad8\u6548\u751f\u6210\u7b56\u7565\u3001\u6982\u7387\u4f30\u8ba1\u548c\u8bef\u5dee\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u5177\u4f53\u5b9e\u73b0\u548c\u793a\u4f8b", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u8bc6\u522b\u3001\u751f\u6210\u548c\u5206\u6790LLM\u4e2d\u7684\u7f55\u89c1\u4e8b\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86\u5177\u4f53\u5e94\u7528\u793a\u4f8b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790LLM\u7f55\u89c1\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5176\u6982\u5ff5\u548c\u6280\u672f\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u578b\u548c\u4e0a\u4e0b\u6587", "topic": "agent analysis"}}
{"id": "tldr.2602.37251548", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F05%2Fanthropic-releases-opus-4-6-with-new-agent-teams%2F%3Futm_source=tldrnewsletter/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/SLNdu6i680v_bm32E0qNBvi-mNQ-bAMeNyVcjMZ5Dmg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F05%2Fanthropic-releases-opus-4-6-with-new-agent-teams%2F%3Futm_source=tldrnewsletter/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/SLNdu6i680v_bm32E0qNBvi-mNQ-bAMeNyVcjMZ5Dmg=443", "authors": ["TLDR Newsletter"], "title": "Anthropic releases Opus 4.6 with new \u2018agent teams'", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F05%2Fanthropic-releases-opus-4-6-with-new-agent-teams%2F%3Futm_source=tldrnewsletter/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/SLNdu6i680v_bm32E0qNBvi-mNQ-bAMeNyVcjMZ5Dmg=443", "summary": "Anthropic releases Opus 4.6 with new \u2018agent teams' (2 minute read) Anthropic's latest version of Opus features Agent Teams, teams of agents that can split larger tasks into segmented jobs. It also comes with a one-million-token context window, allowing for the processing of larger documents. The segmenting of agent responsibilities allows them to coordinate work in parallel to complete tasks faster. Agent Teams is currently available in a research preview for API users and subscribers.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Opus 4.6\uff0c\u5f15\u5165\"\u667a\u80fd\u4f53\u56e2\u961f\"\u529f\u80fd\uff0c\u53ef\u5c06\u5927\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u884c\u5904\u7406\uff0c\u5e76\u652f\u6301100\u4e07token\u4e0a\u4e0b\u6587\u7a97\u53e3", "motivation": "\u89e3\u51b3\u5355\u4e2a\u667a\u80fd\u4f53\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u7684\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u56e2\u961f\u534f\u4f5c\u63d0\u9ad8\u4efb\u52a1\u5904\u7406\u901f\u5ea6\u548c\u80fd\u529b", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u56e2\u961f\u67b6\u6784\uff0c\u5c06\u5927\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5206\u914d\u7ed9\u4e0d\u540c\u667a\u80fd\u4f53\u5e76\u884c\u5904\u7406\uff0c\u540c\u65f6\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f3100\u4e07token\u4ee5\u5904\u7406\u66f4\u5927\u6587\u6863", "result": "Opus 4.6\u5177\u5907\u667a\u80fd\u4f53\u56e2\u961f\u534f\u4f5c\u80fd\u529b\uff0c\u53ef\u5e76\u884c\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u76ee\u524d\u5df2\u5728API\u7528\u6237\u548c\u8ba2\u9605\u8005\u4e2d\u63d0\u4f9b\u7814\u7a76\u9884\u89c8", "conclusion": "\u667a\u80fd\u4f53\u56e2\u961f\u67b6\u6784\u662f\u63d0\u5347AI\u7cfb\u7edf\u5904\u7406\u590d\u6742\u4efb\u52a1\u80fd\u529b\u7684\u91cd\u8981\u65b9\u5411\uff0c\u901a\u8fc7\u5e76\u884c\u534f\u4f5c\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u5904\u7406", "topic": "agent analysis"}}
{"id": "tldr.2602.933889a8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGR9Zjz/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/6NFDTzSS3qCG_vGYWoI-zZxk2o4Q2eRTTfwvuKzakrw=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGR9Zjz/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/6NFDTzSS3qCG_vGYWoI-zZxk2o4Q2eRTTfwvuKzakrw=443", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.3-Codex", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGR9Zjz/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/6NFDTzSS3qCG_vGYWoI-zZxk2o4Q2eRTTfwvuKzakrw=443", "summary": "Introducing GPT-5.3-Codex (12 minute read) GPT-5.3-Codex is OpenAI's most capable agentic coding model to date. It advances both the frontier coding performance of GPT-5.2-Codex and the reasoning and professional knowledge capabilities of GPT-5.2 while being 25% faster. The model can take on long-running tasks that involve research, tool use, and complex execution. Developers can steer and interact with the model while it's working without losing context.", "source": "tldr", "AI": {"tldr": "GPT-5.3-Codex\u662fOpenAI\u6700\u65b0\u7684\u667a\u80fd\u7f16\u7801\u6a21\u578b\uff0c\u76f8\u6bd4GPT-5.2-Codex\u5728\u7f16\u7801\u6027\u80fd\u3001\u63a8\u7406\u548c\u4e13\u4e1a\u77e5\u8bc6\u65b9\u9762\u90fd\u6709\u63d0\u5347\uff0c\u901f\u5ea6\u63d0\u9ad825%\uff0c\u80fd\u5904\u7406\u6d89\u53ca\u7814\u7a76\u3001\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u6267\u884c\u7684\u957f\u65f6\u4efb\u52a1\uff0c\u652f\u6301\u5f00\u53d1\u8005\u5b9e\u65f6\u4ea4\u4e92\u800c\u4e0d\u4e22\u5931\u4e0a\u4e0b\u6587\u3002", "motivation": "OpenAI\u65e8\u5728\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u667a\u80fd\u7f16\u7801\u4ee3\u7406\uff0c\u63d0\u5347\u7f16\u7801\u6a21\u578b\u7684\u6027\u80fd\u3001\u63a8\u7406\u80fd\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u540c\u65f6\u63d0\u9ad8\u5904\u7406\u6548\u7387\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u597d\u5730\u4e0e\u6a21\u578b\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u7f16\u7801\u4efb\u52a1\u3002", "method": "\u57fa\u4e8eGPT-5.2-Codex\u8fdb\u884c\u6539\u8fdb\uff0c\u63d0\u5347\u7f16\u7801\u6027\u80fd\u3001\u63a8\u7406\u80fd\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f18\u5316\u6a21\u578b\u67b6\u6784\u5b9e\u73b025%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u589e\u5f3a\u957f\u65f6\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u652f\u6301\u7814\u7a76\u3001\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u6267\u884c\uff0c\u5e76\u5b9e\u73b0\u5f00\u53d1\u8005\u5b9e\u65f6\u4ea4\u4e92\u800c\u4e0d\u4e22\u5931\u4e0a\u4e0b\u6587\u7684\u529f\u80fd\u3002", "result": "GPT-5.3-Codex\u6210\u4e3aOpenAI\u8fc4\u4eca\u4e3a\u6b62\u6700\u5f3a\u5927\u7684\u667a\u80fd\u7f16\u7801\u6a21\u578b\uff0c\u5728\u7f16\u7801\u6027\u80fd\u3001\u63a8\u7406\u80fd\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86GPT-5.2-Codex\uff0c\u901f\u5ea6\u63d0\u534725%\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u957f\u65f6\u7f16\u7801\u4efb\u52a1\u3002", "conclusion": "GPT-5.3-Codex\u4ee3\u8868\u4e86\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u6280\u672f\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u3001\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u52a9\u624b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u7f16\u7a0b\u4efb\u52a1\u5e76\u652f\u6301\u66f4\u597d\u7684\u534f\u4f5c\u4f53\u9a8c\u3002", "topic": "code agent"}}
{"id": "tldr.2602.022e4bae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fpulse%2Fopenclaw-moment-what-viral-ai-agent-means-business-owners-spencer-vgnec%2F%3Futm_source=tldrmarketing/1/0100019c32d88cba-44c244b9-17c0-4c83-bfaa-cd0d32a594a1-000000/xbyU2s05pHIddKygfsOe2bNYjcwxPCuEQQp0_NbhyN4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fpulse%2Fopenclaw-moment-what-viral-ai-agent-means-business-owners-spencer-vgnec%2F%3Futm_source=tldrmarketing/1/0100019c32d88cba-44c244b9-17c0-4c83-bfaa-cd0d32a594a1-000000/xbyU2s05pHIddKygfsOe2bNYjcwxPCuEQQp0_NbhyN4=443", "authors": ["TLDR Newsletter"], "title": "The OpenClaw Moment: What the Viral AI Agent Means for Business Owners", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fpulse%2Fopenclaw-moment-what-viral-ai-agent-means-business-owners-spencer-vgnec%2F%3Futm_source=tldrmarketing/1/0100019c32d88cba-44c244b9-17c0-4c83-bfaa-cd0d32a594a1-000000/xbyU2s05pHIddKygfsOe2bNYjcwxPCuEQQp0_NbhyN4=443", "summary": "The OpenClaw Moment: What the Viral AI Agent Means for Business Owners (5 minute read) AI agents that can take real actions for users are arriving fast, but current open source tools like OpenClaw are powerful and unsafe for business use today. OpenClaw runs on a local computer and can read email, contact vendors, and make purchases. Security reviews have discovered major risks, including plaintext credential storage, a remote code execution flaw, 341 malicious plugins, and 506 prompt injecti...", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u672c\u5730\u8fd0\u884c\u7684AI\u4ee3\u7406\u5de5\u5177\uff0c\u80fd\u591f\u6267\u884c\u8bfb\u53d6\u90ae\u4ef6\u3001\u8054\u7cfb\u4f9b\u5e94\u5546\u3001\u8fdb\u884c\u8d2d\u4e70\u7b49\u5b9e\u9645\u4efb\u52a1\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u660e\u6587\u51ed\u8bc1\u5b58\u50a8\u3001\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u6f0f\u6d1e\u3001341\u4e2a\u6076\u610f\u63d2\u4ef6\u548c506\u4e2a\u63d0\u793a\u6ce8\u5165\u95ee\u9898\uff0c\u76ee\u524d\u4e0d\u9002\u5408\u5546\u4e1a\u4f7f\u7528\u3002", "motivation": "\u5206\u6790OpenClaw\u7b49\u80fd\u591f\u4e3a\u7528\u6237\u6267\u884c\u5b9e\u9645\u64cd\u4f5c\u7684AI\u4ee3\u7406\u5de5\u5177\u7684\u73b0\u72b6\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u5546\u4e1a\u6240\u6709\u8005\u63d0\u4f9b\u5173\u4e8e\u8fd9\u7c7b\u5de5\u5177\u5f53\u524d\u662f\u5426\u9002\u5408\u5546\u4e1a\u5e94\u7528\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5bf9OpenClaw\u8fdb\u884c\u5b89\u5168\u5ba1\u67e5\uff0c\u8bc6\u522b\u548c\u5206\u6790\u5176\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u548c\u98ce\u9669\uff0c\u5305\u62ec\u51ed\u8bc1\u5b58\u50a8\u65b9\u5f0f\u3001\u4ee3\u7801\u6267\u884c\u6f0f\u6d1e\u3001\u63d2\u4ef6\u5b89\u5168\u6027\u4ee5\u53ca\u63d0\u793a\u6ce8\u5165\u95ee\u9898\u3002", "result": "\u5b89\u5168\u5ba1\u67e5\u53d1\u73b0\u4e86OpenClaw\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u7f3a\u9677\uff1a1) \u660e\u6587\u5b58\u50a8\u51ed\u8bc1 2) \u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u6f0f\u6d1e 3) 341\u4e2a\u6076\u610f\u63d2\u4ef6 4) 506\u4e2a\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u3002\u8fd9\u4e9b\u98ce\u9669\u4f7f\u5176\u76ee\u524d\u4e0d\u9002\u5408\u5546\u4e1a\u73af\u5883\u4f7f\u7528\u3002", "conclusion": "\u867d\u7136\u80fd\u591f\u6267\u884c\u5b9e\u9645\u64cd\u4f5c\u7684AI\u4ee3\u7406\u5de5\u5177\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u50cfOpenClaw\u8fd9\u6837\u7684\u5f00\u6e90\u5de5\u5177\u76ee\u524d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u5546\u4e1a\u6240\u6709\u8005\u5e94\u8c28\u614e\u5bf9\u5f85\uff0c\u7b49\u5f85\u66f4\u5b89\u5168\u3001\u66f4\u6210\u719f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.c794406d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "authors": ["TLDR Newsletter"], "title": "94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "summary": "94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...", "source": "tldr", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a94%\u7684\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u4f1a\u56e0\u66f4\u5f3a\u7684Agentic AI\u529f\u80fd\u800c\u66f4\u6362SaaS\u4f9b\u5e94\u5546\uff0cAgentic AI\u6b63\u6210\u4e3a\u8d2d\u4e70\u51b3\u7b56\u800c\u975e\u5355\u7eaf\u529f\u80fd\u7279\u6027", "motivation": "\u4e86\u89e3Agentic AI\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u529b\u548c\u91cd\u8981\u6027\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u5df2\u6210\u4e3a\u4f01\u4e1a\u6280\u672f\u6808\u9009\u62e9\u7684\u5173\u952e\u56e0\u7d20", "method": "\u901a\u8fc7\u5bf91000\u591a\u540d\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8eAgentic AI\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u6001\u5ea6\u548c\u884c\u4e3a\u6570\u636e", "result": "94%\u7684\u53d7\u8bbf\u8005\u8868\u793a\u4f1a\u56e0\u66f4\u5f3a\u7684Agentic AI\u529f\u80fd\u800c\u66f4\u6362\u4f9b\u5e94\u5546\uff0c85%\u8ba4\u4e3aAgentic AI\u5c06\u5728\u4e09\u5e74\u5185\u6210\u4e3a\u57fa\u672c\u8981\u6c42\uff0c67%\u5173\u6ce8\u5b9e\u9645\u751f\u4ea7\u800c\u975e\u7092\u4f5c", "conclusion": "Agentic AI\u5df2\u4ece\u529f\u80fd\u7279\u6027\u6f14\u53d8\u4e3a\u5173\u952e\u7684\u8d2d\u4e70\u51b3\u7b56\u56e0\u7d20\uff0c\u6b63\u5728\u73b0\u4ee3\u6280\u672f\u6808\u4e2d\u5f15\u53d1\u4f9b\u5e94\u5546\u5207\u6362\uff0c\u9884\u8ba1\u4e09\u5e74\u5185\u5c06\u6210\u4e3a\u884c\u4e1a\u6807\u51c6", "topic": "agent analysis"}}
{"id": "tldr.2602.8be364f1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "authors": ["TLDR Newsletter"], "title": "The timeline:", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "summary": "94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...", "source": "tldr", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a94%\u7684\u5f00\u53d1\u8005\u4f1a\u56e0\u66f4\u5f3a\u7684\u667a\u80fd\u4f53AI\u529f\u80fd\u800c\u66f4\u6362SaaS\u4f9b\u5e94\u5546\uff0c\u667a\u80fd\u4f53AI\u6b63\u6210\u4e3a\u8d2d\u4e70\u51b3\u7b56\u800c\u975e\u529f\u80fd\u7279\u6027\uff0c\u9884\u8ba1\u4e09\u5e74\u5185\u5c06\u6210\u4e3a\u6807\u914d", "motivation": "\u7814\u7a76\u667a\u80fd\u4f53AI\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e86\u89e3\u5f00\u53d1\u8005\u5bf9\u667a\u80fd\u4f53AI\u529f\u80fd\u7684\u5b9e\u9645\u9700\u6c42\u4e0e\u671f\u671b", "method": "\u901a\u8fc7\u5bf91000\u591a\u540d\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u5206\u6790\u667a\u80fd\u4f53AI\u5728\u6280\u672f\u6808\u4e2d\u7684\u91c7\u7528\u8d8b\u52bf\u548c\u5f71\u54cd", "result": "94%\u7684\u53d7\u8bbf\u8005\u4f1a\u56e0\u66f4\u5f3a\u7684\u667a\u80fd\u4f53AI\u529f\u80fd\u800c\u66f4\u6362\u4f9b\u5e94\u5546\uff0c85%\u8ba4\u4e3a\u4e09\u5e74\u5185\u667a\u80fd\u4f53AI\u5c06\u6210\u4e3a\u6807\u914d\uff0c67%\u5173\u6ce8\u5b9e\u9645\u751f\u4ea7\u800c\u975e\u7092\u4f5c", "conclusion": "\u667a\u80fd\u4f53AI\u6b63\u4ece\u529f\u80fd\u7279\u6027\u8f6c\u53d8\u4e3a\u5173\u952e\u7684\u8d2d\u4e70\u51b3\u7b56\u56e0\u7d20\uff0c\u4f9b\u5e94\u5546\u9700\u91cd\u89c6\u5b9e\u9645\u751f\u4ea7\u4ef7\u503c\u7684\u667a\u80fd\u4f53AI\u529f\u80fd\u5f00\u53d1", "topic": "agent analysis"}}
{"id": "tldr.2602.5f268722", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "authors": ["TLDR Newsletter"], "title": "Production over hype:", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "summary": "94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...", "source": "tldr", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a94%\u7684\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u4f1a\u56e0\u66f4\u5f3a\u7684\u667a\u80fdAI\u529f\u80fd\u800c\u66f4\u6362SaaS\u4f9b\u5e94\u5546\uff0c\u667a\u80fdAI\u6b63\u6210\u4e3a\u8d2d\u4e70\u51b3\u7b56\u800c\u975e\u529f\u80fd\u7279\u6027", "motivation": "\u4e86\u89e3\u667a\u80fdAI\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u5bf9\u667a\u80fdAI\u529f\u80fd\u7684\u5b9e\u9645\u9700\u6c42\u548c\u671f\u671b", "method": "\u901a\u8fc7\u5bf91000\u591a\u540d\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u8fdb\u884c\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8e\u667a\u80fdAI\u529f\u80fd\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u5f71\u54cd\u6570\u636e", "result": "94%\u7684\u53d7\u8bbf\u8005\u4f1a\u56e0\u66f4\u5f3a\u7684\u667a\u80fdAI\u529f\u80fd\u800c\u66f4\u6362SaaS\u4f9b\u5e94\u5546\uff0c85%\u8ba4\u4e3a\u667a\u80fdAI\u5c06\u5728\u4e09\u5e74\u5185\u6210\u4e3a\u57fa\u672c\u8981\u6c42\uff0c67%\u66f4\u5173\u6ce8\u5b9e\u9645\u751f\u4ea7\u800c\u975e\u7092\u4f5c", "conclusion": "\u667a\u80fdAI\u6b63\u4ece\u529f\u80fd\u7279\u6027\u8f6c\u53d8\u4e3a\u5173\u952e\u7684\u8d2d\u4e70\u51b3\u7b56\u56e0\u7d20\uff0c\u5c06\u5728\u672a\u6765\u4e09\u5e74\u5185\u6210\u4e3aSaaS\u4ea7\u54c1\u7684\u6807\u914d\u8981\u6c42", "topic": "agent analysis"}}
{"id": "tldr.2602.916536e5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "authors": ["TLDR Newsletter"], "title": "The bottlenecks:", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443", "summary": "94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...", "source": "tldr", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a94%\u7684\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u4f1a\u56e0\u66f4\u5f3a\u7684\u667a\u80fdAI\u529f\u80fd\u800c\u66f4\u6362SaaS\u4f9b\u5e94\u5546\uff0c\u667a\u80fdAI\u6b63\u6210\u4e3a\u8d2d\u4e70\u51b3\u7b56\u800c\u975e\u5355\u7eaf\u529f\u80fd\uff0c85%\u8ba4\u4e3a\u4e09\u5e74\u5185\u5c06\u6210\u4e3a\u6807\u914d", "motivation": "\u7814\u7a76\u667a\u80fdAI\u5728SaaS\u91c7\u8d2d\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e86\u89e3\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u5bf9\u667a\u80fdAI\u529f\u80fd\u7684\u5b9e\u9645\u9700\u6c42\u548c\u9884\u671f", "method": "\u901a\u8fc7\u5bf91000\u591a\u540d\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u8fdb\u884c\u8c03\u67e5\uff0c\u5206\u6790\u667a\u80fdAI\u5728\u6280\u672f\u6808\u4e2d\u7684\u5f71\u54cd\u548c\u91c7\u7528\u8d8b\u52bf", "result": "94%\u7684\u53d7\u8bbf\u8005\u4f1a\u8003\u8651\u4e3a\u66f4\u5f3a\u7684\u667a\u80fdAI\u529f\u80fd\u66f4\u6362\u4f9b\u5e94\u5546\uff0c\u667a\u80fdAI\u5df2\u6210\u4e3a\u73b0\u4ee3\u6280\u672f\u6808\u4e2d\u7684\u5207\u6362\u4e8b\u4ef6\uff0c85%\u8ba4\u4e3a\u4e09\u5e74\u5185\u5c06\u6210\u4e3a\u6807\u914d\uff0c67%\u5173\u6ce8\u5b9e\u9645\u751f\u4ea7\u800c\u975e\u7092\u4f5c", "conclusion": "\u667a\u80fdAI\u6b63\u4ece\u9644\u52a0\u529f\u80fd\u8f6c\u53d8\u4e3a\u5173\u952e\u7684\u8d2d\u4e70\u51b3\u7b56\u56e0\u7d20\uff0c\u4f9b\u5e94\u5546\u9700\u8981\u91cd\u89c6\u667a\u80fdAI\u529f\u80fd\u7684\u5f00\u53d1\u4ee5\u4fdd\u6301\u7ade\u4e89\u529b", "topic": "agent analysis"}}
{"id": "tldr.2602.0ebb73c2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.databricks.com%2Fblog%2Fself-optimizing-football-chatbot-guided-domain-experts-databricks%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/PdV6ukSJbPVM8Aqno989bJkBOpYwkHp-qa1Kwm5-42o=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.databricks.com%2Fblog%2Fself-optimizing-football-chatbot-guided-domain-experts-databricks%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/PdV6ukSJbPVM8Aqno989bJkBOpYwkHp-qa1Kwm5-42o=443", "authors": ["TLDR Newsletter"], "title": "Self-Optimizing Football Chatbot Guided by Domain Experts on Databricks", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.databricks.com%2Fblog%2Fself-optimizing-football-chatbot-guided-domain-experts-databricks%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/PdV6ukSJbPVM8Aqno989bJkBOpYwkHp-qa1Kwm5-42o=443", "summary": "Self-Optimizing Football Chatbot Guided by Domain Experts on Databricks (10 minute read) Databricks has unveiled an architecture for continuously improving AI agents, exemplified by an American Football Defensive Coordinator Assistant, which leverages MLflow's `align()` and `optimize_prompts()` functions to integrate domain expert feedback for automated prompt refinement and enhanced performance. This system encodes human expertise directly into the agent, providing situation-aware insights f...", "source": "tldr", "AI": {"tldr": "Databricks\u63d0\u51fa\u4e86\u4e00\u79cd\u6301\u7eed\u4f18\u5316AI\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u901a\u8fc7MLflow\u7684align()\u548coptimize_prompts()\u51fd\u6570\u96c6\u6210\u9886\u57df\u4e13\u5bb6\u53cd\u9988\uff0c\u5b9e\u73b0\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u548c\u6027\u80fd\u63d0\u5347", "motivation": "\u6784\u5efa\u80fd\u591f\u6301\u7eed\u6539\u8fdb\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c06\u4eba\u7c7b\u9886\u57df\u4e13\u5bb6\u7684\u77e5\u8bc6\u76f4\u63a5\u7f16\u7801\u5230\u4ee3\u7406\u4e2d\uff0c\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u7684\u6d1e\u5bdf\uff0c\u7279\u522b\u662f\u5728\u7f8e\u5f0f\u8db3\u7403\u9632\u5b88\u534f\u8c03\u5458\u52a9\u624b\u8fd9\u6837\u7684\u4e13\u4e1a\u9886\u57df", "method": "\u4f7f\u7528Databricks\u67b6\u6784\uff0c\u901a\u8fc7MLflow\u7684align()\u51fd\u6570\u5bf9\u9f50\u9886\u57df\u4e13\u5bb6\u53cd\u9988\uff0coptimize_prompts()\u51fd\u6570\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u5c06\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u76f4\u63a5\u7f16\u7801\u5230AI\u4ee3\u7406\u4e2d", "result": "\u5f00\u53d1\u4e86\u7f8e\u5f0f\u8db3\u7403\u9632\u5b88\u534f\u8c03\u5458\u52a9\u624b\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u4e13\u5bb6\u53cd\u9988\u6301\u7eed\u6539\u8fdb\uff0c\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u7684\u6d1e\u5bdf\u548c\u589e\u5f3a\u7684\u6027\u80fd", "conclusion": "\u8be5\u67b6\u6784\u4e3aAI\u4ee3\u7406\u7684\u6301\u7eed\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u9886\u57df\u4e13\u5bb6\u53cd\u9988\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u63d0\u793a\u6539\u8fdb\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c", "topic": "agent analysis"}}
{"id": "tldr.2602.6c864d6a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.iankduncan.com%2Fengineering%2F2026-02-05-github-actions-killing-your-team%2F%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/ExUNbnvq-jKAoRTCNIELXS7gW57cLAYDibAI3mCjMbo=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.iankduncan.com%2Fengineering%2F2026-02-05-github-actions-killing-your-team%2F%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/ExUNbnvq-jKAoRTCNIELXS7gW57cLAYDibAI3mCjMbo=443", "authors": ["TLDR Newsletter"], "title": "GitHub Actions Is Slowly Killing Your Engineering Team", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.iankduncan.com%2Fengineering%2F2026-02-05-github-actions-killing-your-team%2F%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/ExUNbnvq-jKAoRTCNIELXS7gW57cLAYDibAI3mCjMbo=443", "summary": "GitHub Actions Is Slowly Killing Your Engineering Team (14 minute read) GitHub Actions wins by convenience, not quality, and its slow UI, brittle YAML DSL, opaque permissions, untrustworthy marketplace, and rented compute quietly drain engineering time and morale. Buildkite, by contrast, keeps config simple, puts real logic in real code, lets teams own fast, debuggable infrastructure, and feels like a CI tool built by people who actually suffer through CI every day.", "source": "tldr", "AI": {"tldr": "GitHub Actions\u56e0\u5176\u7f13\u6162\u7684UI\u3001\u8106\u5f31\u7684YAML DSL\u3001\u4e0d\u900f\u660e\u7684\u6743\u9650\u3001\u4e0d\u53ef\u4fe1\u7684\u5e02\u573a\u548c\u79df\u7528\u8ba1\u7b97\u8d44\u6e90\u800c\u6d88\u8017\u5de5\u7a0b\u56e2\u961f\u7684\u65f6\u95f4\u548c\u58eb\u6c14\uff0c\u800cBuildkite\u901a\u8fc7\u7b80\u5355\u914d\u7f6e\u3001\u771f\u5b9e\u4ee3\u7801\u903b\u8f91\u548c\u53ef\u8c03\u8bd5\u7684\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u66f4\u597d\u7684CI\u4f53\u9a8c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63ed\u793aGitHub Actions\u5728\u5de5\u7a0b\u56e2\u961f\u4f7f\u7528\u4e2d\u7684\u9690\u85cf\u6210\u672c\uff0c\u6307\u51fa\u5176\u770b\u4f3c\u4fbf\u5229\u4f46\u5b9e\u9645\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u5bf9\u6bd4\u5c55\u793aBuildkite\u4f5c\u4e3a\u66f4\u4f18CI\u5de5\u5177\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\uff08UI\u901f\u5ea6\u3001\u914d\u7f6e\u8bed\u8a00\u3001\u6743\u9650\u7ba1\u7406\u3001\u5e02\u573a\u53ef\u4fe1\u5ea6\u3001\u8ba1\u7b97\u8d44\u6e90\uff09\u6bd4\u8f83GitHub Actions\u548cBuildkite\u7684\u5dee\u5f02\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9645\u5de5\u7a0b\u56e2\u961f\u4f7f\u7528\u4f53\u9a8c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "GitHub Actions\u867d\u7136\u4ee5\u4fbf\u5229\u6027\u53d6\u80dc\uff0c\u4f46\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u4f1a\u7f13\u6162\u6d88\u8017\u5de5\u7a0b\u56e2\u961f\u7684\u65f6\u95f4\u548c\u58eb\u6c14\uff1b\u800cBuildkite\u901a\u8fc7\u7b80\u5316\u914d\u7f6e\u3001\u4f7f\u7528\u771f\u5b9e\u4ee3\u7801\u903b\u8f91\u3001\u63d0\u4f9b\u53ef\u62e5\u6709\u548c\u8c03\u8bd5\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u5de5\u7a0b\u56e2\u961f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684CI\u4f53\u9a8c\u3002", "conclusion": "\u5de5\u7a0b\u56e2\u961f\u5e94\u91cd\u65b0\u8bc4\u4f30CI\u5de5\u5177\u7684\u9009\u62e9\uff0c\u8003\u8651GitHub Actions\u7684\u9690\u85cf\u6210\u672c\uff0c\u5e76\u5173\u6ce8\u50cfBuildkite\u8fd9\u6837\u7531\u5b9e\u9645CI\u4f7f\u7528\u8005\u8bbe\u8ba1\u7684\u5de5\u5177\uff0c\u4ee5\u63d0\u9ad8\u5de5\u7a0b\u6548\u7387\u548c\u56e2\u961f\u58eb\u6c14\u3002", "topic": "swe application"}}
{"id": "tldr.2602.bd602ffc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/vFxhr6c6l6ubT3IvqMrzR7ZuAeNvDmyBnTGtKhkjgew=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/vFxhr6c6l6ubT3IvqMrzR7ZuAeNvDmyBnTGtKhkjgew=443", "authors": ["TLDR Newsletter"], "title": "Building a C compiler with a team of parallel Claudes", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/vFxhr6c6l6ubT3IvqMrzR7ZuAeNvDmyBnTGtKhkjgew=443", "summary": "Building a C compiler with a team of parallel Claudes (14 minute read) Anthropic tasked a team of 16 parallel Claude Opus 4.6 agents to autonomously build a Rust-based C compiler capable of compiling the Linux kernel. Over two weeks and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.", "source": "tldr", "AI": {"tldr": "\u4f7f\u752816\u4e2a\u5e76\u884cClaude Opus 4.6\u4ee3\u7406\u56e2\u961f\uff0c\u82b1\u8d392\u5468\u65f6\u95f4\u548c2\u4e07\u7f8e\u5143API\u6210\u672c\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u7f16\u8bd1Linux 6.9\u5185\u6838\u7684Rust-based C\u7f16\u8bd1\u5668", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u5e76\u884cAI\u4ee3\u7406\u56e2\u961f\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u6784\u5efa\u50cfC\u7f16\u8bd1\u5668\u8fd9\u6837\u7684\u57fa\u7840\u7cfb\u7edf\u8f6f\u4ef6", "method": "\u91c7\u752816\u4e2a\u5e76\u884cClaude Opus 4.6\u4ee3\u7406\u7ec4\u6210\u7684\u56e2\u961f\uff0c\u901a\u8fc7\u81ea\u4e3b\u534f\u4f5c\u65b9\u5f0f\u5f00\u53d1Rust-based C\u7f16\u8bd1\u5668\uff0c\u652f\u6301x86\u3001ARM\u548cRISC-V\u67b6\u6784", "result": "\u6210\u529f\u6784\u5efa\u4e8610\u4e07\u884c\u4ee3\u7801\u7684\u7f16\u8bd1\u5668\uff0c\u80fd\u591f\u7f16\u8bd1Linux 6.9\u5185\u6838\uff0c\u5e76\u5728x86\u3001ARM\u548cRISC-V\u67b6\u6784\u4e0a\u8fd0\u884c", "conclusion": "\u5927\u89c4\u6a21\u5e76\u884cAI\u4ee3\u7406\u56e2\u961f\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u5c55\u793a\u4e86AI\u5728\u7cfb\u7edf\u7f16\u7a0b\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c3d\u7ba1\u6210\u672c\u8f83\u9ad8", "topic": "code agent"}}
{"id": "tldr.2602.6d2396f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fmy-ai-adoption-journey%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/sqncOmDPsF2RsCuSyGQlLTcdPqwPHbHsjZuOfoTwr68=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fmy-ai-adoption-journey%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/sqncOmDPsF2RsCuSyGQlLTcdPqwPHbHsjZuOfoTwr68=443", "authors": ["TLDR Newsletter"], "title": "My AI Adoption Journey", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fmy-ai-adoption-journey%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/sqncOmDPsF2RsCuSyGQlLTcdPqwPHbHsjZuOfoTwr68=443", "summary": "My AI Adoption Journey (12 minute read) Agents are great for coding, end-of-day research, and for outsourcing \"slam dunk\" tasks. Also, \"harness engineering\" helps improve agent reliability.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u5206\u4eab\u4e86AI\u4ee3\u7406\u5728\u7f16\u7801\u3001\u7814\u7a76\u5206\u6790\u548c\u7b80\u5355\u4efb\u52a1\u5916\u5305\u65b9\u9762\u7684\u5e94\u7528\u7ecf\u9a8c\uff0c\u5e76\u4ecb\u7ecd\u4e86\"harness engineering\"\u6280\u672f\u6765\u63d0\u5347\u4ee3\u7406\u53ef\u9760\u6027", "motivation": "\u5206\u4eab\u4e2a\u4eba\u91c7\u7528AI\u4ee3\u7406\u7684\u5b9e\u9645\u7ecf\u9a8c\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u5e2e\u52a9\u5176\u4ed6\u5f00\u53d1\u8005\u66f4\u6709\u6548\u5730\u5229\u7528AI\u4ee3\u7406\u5de5\u5177", "method": "\u57fa\u4e8e\u4e2a\u4eba\u5b9e\u8df5\u7ecf\u9a8c\u603b\u7ed3\uff0c\u5305\u62ecAI\u4ee3\u7406\u5728\u5177\u4f53\u573a\u666f\u7684\u5e94\u7528\u6848\u4f8b\u548c\"harness engineering\"\u6280\u672f\uff08\u4e00\u79cd\u63d0\u5347\u4ee3\u7406\u53ef\u9760\u6027\u7684\u5de5\u7a0b\u65b9\u6cd5\uff09", "result": "\u53d1\u73b0AI\u4ee3\u7406\u5728\u7f16\u7801\u3001\u7814\u7a76\u5206\u6790\u548c\u7b80\u5355\u4efb\u52a1\u5916\u5305\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u901a\u8fc7harness engineering\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027", "conclusion": "AI\u4ee3\u7406\u662f\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u7f16\u7801\u548c\u7814\u7a76\u9886\u57df\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5de5\u7a0b\u65b9\u6cd5\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027", "topic": "swe application"}}
{"id": "tldr.2602.d3c710f0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YOZT1hLpuHWniwOKUrTrha0vMCeBbBWT83PRkJBkDpY=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YOZT1hLpuHWniwOKUrTrha0vMCeBbBWT83PRkJBkDpY=443", "authors": ["TLDR Newsletter"], "title": "Claude Opus 4.6", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YOZT1hLpuHWniwOKUrTrha0vMCeBbBWT83PRkJBkDpY=443", "summary": "Claude Opus 4.6 (12 minute read) Anthropic has launched Claude Opus 4.6, an upgraded model better at coding, debugging, and reasoning skills, while having a 1M token context window in beta. This model has state-of-the-art performance on various benchmarks, including agentic coding and complex reasoning tests.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Claude Opus 4.6\u6a21\u578b\uff0c\u5728\u7f16\u7801\u3001\u8c03\u8bd5\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u62e5\u6709100\u4e07token\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u5347AI\u6a21\u578b\u5728\u7f16\u7801\u3001\u8c03\u8bd5\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6ee1\u8db3\u5bf9\u66f4\u5f3a\u5927\u4ee3\u7801\u4ee3\u7406\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u5347\u7ea7\u7248Claude Opus 4.6\u6a21\u578b\uff0c\u4f18\u5316\u7f16\u7801\u3001\u8c03\u8bd5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f3100\u4e07token\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u7f16\u7801\u548c\u590d\u6742\u63a8\u7406\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Claude Opus 4.6\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5347\u7ea7\u6a21\u578b\uff0c\u5728\u7f16\u7801\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2602.5b21a1bb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcode.claude.com%2Fdocs%2Fen%2Fagent-teams%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YgoVMJFAGW-al3nDdboEzSyIt_Xw2HPzPSt4tYNOJhc=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcode.claude.com%2Fdocs%2Fen%2Fagent-teams%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YgoVMJFAGW-al3nDdboEzSyIt_Xw2HPzPSt4tYNOJhc=443", "authors": ["TLDR Newsletter"], "title": "Orchestrate teams of Claude Code sessions", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcode.claude.com%2Fdocs%2Fen%2Fagent-teams%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YgoVMJFAGW-al3nDdboEzSyIt_Xw2HPzPSt4tYNOJhc=443", "summary": "Orchestrate teams of Claude Code sessions (17 minute read) Claude Code's experimental Agent Teams feature allows multiple AI instances to collaborate on complex tasks, with a lead agent coordinating work and synthesizing results. Unlike subagents, agent teams collaborate and discuss among each other, making them great for tasks requiring shared problem-solving.", "source": "tldr", "AI": {"tldr": "Claude Code\u63a8\u51fa\u5b9e\u9a8c\u6027\u7684Agent Teams\u529f\u80fd\uff0c\u5141\u8bb8\u591a\u4e2aAI\u5b9e\u4f8b\u534f\u4f5c\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u901a\u8fc7\u4e3b\u5bfc\u4ee3\u7406\u534f\u8c03\u5de5\u4f5c\u548c\u7efc\u5408\u7ed3\u679c\uff0c\u4e0d\u540c\u4e8e\u5b50\u4ee3\u7406\uff0c\u56e2\u961f\u4ee3\u7406\u4e4b\u95f4\u4f1a\u76f8\u4e92\u534f\u4f5c\u8ba8\u8bba\uff0c\u9002\u5408\u9700\u8981\u5171\u4eab\u95ee\u9898\u89e3\u51b3\u7684\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u9700\u8981\u591a\u4e2aAI\u4ee3\u7406\u534f\u4f5c\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5b50\u4ee3\u7406\u6a21\u5f0f\u7f3a\u4e4f\u771f\u6b63\u7684\u534f\u4f5c\u8ba8\u8bba\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9AI\u5b9e\u4f8b\u76f8\u4e92\u534f\u4f5c\u3001\u5171\u540c\u89e3\u51b3\u95ee\u9898\u7684\u56e2\u961f\u673a\u5236\u3002", "method": "\u91c7\u7528Agent Teams\u67b6\u6784\uff0c\u5305\u542b\u4e00\u4e2a\u4e3b\u5bfc\u4ee3\u7406\u534f\u8c03\u591a\u4e2aAI\u5b9e\u4f8b\u7684\u5de5\u4f5c\uff0c\u4ee3\u7406\u4e4b\u95f4\u53ef\u4ee5\u76f8\u4e92\u534f\u4f5c\u548c\u8ba8\u8bba\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u4efb\u52a1\u5206\u89e3\u548c\u5206\u914d\u3002", "result": "\u5f00\u53d1\u51faClaude Code\u7684\u5b9e\u9a8c\u6027Agent Teams\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u591aAI\u5b9e\u4f8b\u7684\u534f\u4f5c\u673a\u5236\uff0c\u80fd\u591f\u5904\u7406\u9700\u8981\u5171\u4eab\u95ee\u9898\u89e3\u51b3\u7684\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "Agent Teams\u529f\u80fd\u4e3a\u590d\u6742\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u534f\u4f5c\u6a21\u5f0f\uff0c\u901a\u8fc7\u591aAI\u5b9e\u4f8b\u7684\u8ba8\u8bba\u548c\u534f\u4f5c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u89e3\u51b3\u9700\u8981\u96c6\u4f53\u667a\u6167\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "tldr.2602.b0d3c2c8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fmove-over-gas-town%2F%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/4ywBNxjwOeJuc8VVl5SWhoVzZ9sv1PuTi1X4pEkRgfA=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fmove-over-gas-town%2F%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/4ywBNxjwOeJuc8VVl5SWhoVzZ9sv1PuTi1X4pEkRgfA=443", "authors": ["TLDR Newsletter"], "title": "Move Over Gas Town, Claude Has First-Party Agent Orchestration", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fmove-over-gas-town%2F%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/4ywBNxjwOeJuc8VVl5SWhoVzZ9sv1PuTi1X4pEkRgfA=443", "summary": "Move Over Gas Town, Claude Has First-Party Agent Orchestration (3 minute read) Anthropic's new Agent Teams is an improvement on multi-agent orchestration over previous attempts like Gas Town.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faAgent Teams\uff0c\u8fd9\u662f\u5bf9\u5148\u524d\u591a\u667a\u80fd\u4f53\u7f16\u6392\u65b9\u6848\uff08\u5982Gas Town\uff09\u7684\u6539\u8fdb", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u65b9\u6848\uff08\u5982Gas Town\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7b2c\u4e00\u65b9\u667a\u80fd\u4f53\u7f16\u6392\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Teams\uff0c\u8fd9\u662fClaude\u7684\u7b2c\u4e00\u65b9\u667a\u80fd\u4f53\u7f16\u6392\u7cfb\u7edf\uff0c\u6539\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236", "result": "Agent Teams\u76f8\u6bd4Gas Town\u7b49\u5148\u524d\u65b9\u6848\u5728\u591a\u667a\u80fd\u4f53\u7f16\u6392\u65b9\u9762\u6709\u6240\u63d0\u5347", "conclusion": "Anthropic\u7684Agent Teams\u4ee3\u8868\u4e86\u591a\u667a\u80fd\u4f53\u7f16\u6392\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4e3a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "tldr.2602.e9e2d66a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAqlc21/1/0100019c33105f2a-6abf8f7c-97f8-4758-ae0b-329c60c560c8-000000/tX8ZmV_bejeg2pb6Vr3YxiOMTY39bmrSGNN-qkOfr9w=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAqlc21/1/0100019c33105f2a-6abf8f7c-97f8-4758-ae0b-329c60c560c8-000000/tX8ZmV_bejeg2pb6Vr3YxiOMTY39bmrSGNN-qkOfr9w=443", "authors": ["TLDR Newsletter"], "title": "The Agentic Ecosystem on Base", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAqlc21/1/0100019c33105f2a-6abf8f7c-97f8-4758-ae0b-329c60c560c8-000000/tX8ZmV_bejeg2pb6Vr3YxiOMTY39bmrSGNN-qkOfr9w=443", "summary": "The Agentic Ecosystem on Base (5 minute read) Base is emerging as the dominant blockchain for AI agent activity due to transaction costs low enough to support micropayments via x402 and purpose-built infrastructure. The ecosystem has processed over 9M x402 transactions with $1M+ volume in 30 days, ~1.6M OpenClaw agents registered, and $100M+ volume on platforms like Bankr and Clanker. This represents one of the first demonstrable product-market fits for L2s beyond DeFi.", "source": "tldr", "AI": {"tldr": "Base\u533a\u5757\u94fe\u6210\u4e3aAI\u4ee3\u7406\u6d3b\u52a8\u4e3b\u5bfc\u5e73\u53f0\uff0c\u56e0\u5176\u4f4e\u6210\u672c\u652f\u6301x402\u5fae\u652f\u4ed8\u548c\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\uff0c\u5df2\u5904\u7406900\u4e07+\u4ea4\u6613\u3001160\u4e07+\u4ee3\u7406\u6ce8\u518c\uff0c\u5b9e\u73b01\u4ebf\u7f8e\u5143+\u4ea4\u6613\u91cf\uff0c\u5c55\u793aL2\u5728DeFi\u5916\u7684\u9996\u4e2a\u4ea7\u54c1\u5e02\u573a\u5951\u5408", "motivation": "\u63a2\u7d22\u533a\u5757\u94feLayer 2\u89e3\u51b3\u65b9\u6848\u5728AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662fBase\u533a\u5757\u94fe\u5982\u4f55\u901a\u8fc7\u4f4e\u6210\u672c\u4ea4\u6613\u548c\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u652f\u6301AI\u4ee3\u7406\u6d3b\u52a8\uff0c\u8d85\u8d8a\u4f20\u7edfDeFi\u5e94\u7528\u573a\u666f", "method": "\u57fa\u4e8eBase\u533a\u5757\u94fe\u5e73\u53f0\u6784\u5efaAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u5229\u7528x402\u534f\u8bae\u5b9e\u73b0\u5fae\u652f\u4ed8\u529f\u80fd\uff0c\u5f00\u53d1\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u4ee3\u7406\u6ce8\u518c\u548c\u4ea4\u6613\uff0c\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u548c\u8fd0\u8425\u6570\u636e\u9a8c\u8bc1\u53ef\u884c\u6027", "result": "30\u5929\u5185\u5904\u7406\u8d85\u8fc7900\u4e07\u7b14x402\u4ea4\u6613\uff0c\u4ea4\u6613\u91cf\u8d85\u8fc7100\u4e07\u7f8e\u5143\uff1b\u6ce8\u518c\u7ea6160\u4e07\u4e2aOpenClaw\u4ee3\u7406\uff1bBankr\u548cClanker\u7b49\u5e73\u53f0\u4ea4\u6613\u91cf\u8d85\u8fc71\u4ebf\u7f8e\u5143\uff1b\u8bc1\u660eBase\u5728AI\u4ee3\u7406\u9886\u57df\u5177\u6709\u663e\u8457\u4ea7\u54c1\u5e02\u573a\u5951\u5408\u5ea6", "conclusion": "Base\u533a\u5757\u94fe\u5df2\u6210\u4e3aAI\u4ee3\u7406\u6d3b\u52a8\u7684\u4e3b\u5bfc\u5e73\u53f0\uff0c\u5176\u4f4e\u6210\u672c\u4ea4\u6613\u548c\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u6210\u529f\u652f\u6301\u4e86\u5927\u89c4\u6a21AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86Layer 2\u533a\u5757\u94fe\u5728DeFi\u4e4b\u5916\u7684\u9996\u4e2a\u53ef\u9a8c\u8bc1\u4ea7\u54c1\u5e02\u573a\u5951\u5408\u6848\u4f8b", "topic": "agent analysis"}}
