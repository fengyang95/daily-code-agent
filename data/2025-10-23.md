<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.AI](#cs.AI) [Total: 7]
- [wechat.article](#wechat.article) [Total: 30]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.SE](#cs.SE) [Total: 6]
- [tldr.article](#tldr.article) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs](https://arxiv.org/abs/2510.18892)
*Richard J. Young,Brandon Gillins,Alice M. Matthews*

Main category: cs.CL

TL;DR: 提出了一个简化的评估框架，使用20个精心设计的提示来评估LLM在多样化任务类别中的指令遵循能力，通过大规模实证研究测试了256个模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型广泛部署，但系统评估其指令遵循能力仍然具有挑战性。现有基准可能被新模型训练过，需要新的评估方法来评估真实能力而非记忆性能。

Method: 开发了一个紧凑的测试套件，包含20个针对不同指令遵循方面的提示，包括格式合规、内容约束、逻辑排序和多步骤任务执行。在256个经过验证的模型上进行大规模实证研究。

Result: 研究揭示了持续存在的失败模式，并识别出特定指令类型带来的挑战。提供了主要提供商和新兴实现的比较性能分析。

Conclusion: 这项工作既贡献了一个实用的评估工具，也提供了当代LLM指令遵循能力最全面的实证分析之一。

Abstract: Despite widespread deployment of Large Language Models, systematic evaluation
of instruction-following capabilities remains challenging. While comprehensive
benchmarks exist, focused assessments that quickly diagnose specific
instruction adherence patterns are valuable. As newer models may be trained on
existing benchmarks, novel evaluation approaches are needed to assess genuine
capabilities rather than memorized performance. This paper presents a
streamlined evaluation framework using twenty carefully designed prompts to
assess LLM instruction-following across diverse task categories. We demonstrate
this framework through a large-scale empirical study conducted on October 14,
2025, testing 256 verified working models from 331 available via OpenRouter. To
ensure methodological rigor and prevent selection bias, we first verified each
model's basic functionality before inclusion. Unlike large-scale benchmarks
requiring extensive computational resources, our approach offers a practical
diagnostic tool researchers and practitioners can readily apply. Our
methodology builds upon verifiable instructions while introducing a compact
test suite balancing comprehensiveness with efficiency. Each prompt targets
distinct aspects of instruction following, including format compliance, content
constraints, logical sequencing, and multi-step task execution. We evaluate
models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and
emerging implementations (Qwen, DeepSeek, community models), providing
comparative performance analysis. Our findings reveal consistent failure modes
and identify specific instruction types posing particular challenges. This work
contributes both a practical evaluation tool and one of the most comprehensive
empirical analyses of instruction-following capabilities across the
contemporary LLM landscape.

</details>


### [2] [Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search](https://arxiv.org/abs/2510.18939)
*Howard Yen,Ashwin Paranjape,Mengzhou Xia,Thejas Venkatesh,Jack Hessel,Danqi Chen,Yuhao Zhang*

Main category: cs.CL

TL;DR: SLIM是一个轻量级信息管理框架，通过分离搜索和浏览工具并定期总结轨迹，解决了长轨迹智能搜索中的上下文限制问题，以更低的成本和更少的工具调用实现可比性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能搜索框架在长轨迹任务中面临上下文限制问题，包括积累冗长嘈杂内容、触及上下文窗口和工具预算限制、或过早停止，难以扩展到长轨迹搜索。

Method: 引入SLIM框架，将检索分离为独立的搜索和浏览工具，定期总结轨迹以保持上下文简洁，同时支持更长、更专注的搜索。

Result: 在长轨迹任务上，SLIM以显著更低的成本和更少的工具调用实现可比性能。使用o3作为基础模型时，在BrowseComp上达到56%，在HLE上达到31%，分别优于所有开源框架8和4个绝对百分点，同时工具调用减少4-6倍。

Conclusion: SLIM框架通过简单工具设计和信息管理策略有效解决了长轨迹智能搜索的挑战，减少了幻觉现象，为未来长轨迹智能体提供了参考。

Abstract: Long-horizon agentic search requires iteratively exploring the web over long
trajectories and synthesizing information across many sources, and is the
foundation for enabling powerful applications like deep research systems. In
this work, we show that popular agentic search frameworks struggle to scale to
long trajectories primarily due to context limitations-they accumulate long,
noisy content, hit context window and tool budgets, or stop early. Then, we
introduce SLIM (Simple Lightweight Information Management), a simple framework
that separates retrieval into distinct search and browse tools, and
periodically summarizes the trajectory, keeping context concise while enabling
longer, more focused searches. On long-horizon tasks, SLIM achieves comparable
performance at substantially lower cost and with far fewer tool calls than
strong open-source baselines across multiple base models. Specifically, with o3
as the base model, SLIM achieves 56% on BrowseComp and 31% on HLE,
outperforming all open-source frameworks by 8 and 4 absolute points,
respectively, while incurring 4-6x fewer tool calls. Finally, we release an
automated fine-grained trajectory analysis pipeline and error taxonomy for
characterizing long-horizon agentic search frameworks; SLIM exhibits fewer
hallucinations than prior systems. We hope our analysis framework and simple
tool design inform future long-horizon agents.

</details>


### [3] [That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation](https://arxiv.org/abs/2510.19116)
*Jaesung Bae,Cameron Churchwell,Mitchell Hermon,Tsun-An Hsieh,Jocelyn Xu,Yekaterina Yegorova,Mark Hasegawa-Johnson,Heng Ji*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在参数知识与提示中冲突信息不一致时的行为，提出了一个领域无关的框架来构建和解释代码生成中的知识冲突，并开发了新的评估方法和数据集。


<details>
  <summary>Details</summary>
Motivation: 扩展知识冲突研究到代码生成领域，探索LLMs如何处理参数知识与提示信息之间的冲突。

Method: 提出了领域无关的框架来构建和解释知识冲突，开发了针对代码冲突场景的评估方法和数据集，使用激活层引导技术进行实验。

Result: 大型LLMs在参数中编码了知识冲突的概念，检测准确率达80.65%；激活层引导比随机基线提升了12.6%的成功率。

Conclusion: 模型规模、任务领域和引导方向的平衡对知识冲突处理效果至关重要。

Abstract: This paper investigates how large language models (LLMs) behave when faced
with discrepancies between their parametric knowledge and conflicting
information contained in a prompt. Building on prior question-answering (QA)
research, we extend the investigation of knowledge conflicts to the realm of
code generation. We propose a domain-agnostic framework for constructing and
interpreting such conflicts, along with a novel evaluation method and dataset
tailored to code conflict scenarios. Our experiments indicate that sufficiently
large LLMs encode the notion of a knowledge conflict in their parameters,
enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy.
Building on these insights, we show that activation-level steering can achieve
up to a \textbf{12.6\%} improvement in steering success over a random baseline.
However, effectiveness depends critically on balancing model size, task domain,
and steering direction. The experiment code and data will be made publicly
available after acceptance.

</details>


### [4] [When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA](https://arxiv.org/abs/2510.19172)
*Nishanth Sridhar Nakshatri,Shamik Roy,Manoj Ghuhan Arivazhagan,Hanhan Zhou,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出了evolveQA基准测试，专门评估LLM处理时间演化知识的能力，基于AWS更新、Azure变更和WHO疾病报告三个真实时间戳语料库构建。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过基于结构化知识库的基准测试评估时间知识冲突，但关注流行实体且缺乏动态结构来公平评估不同知识截止日期的LLM。

Method: 从三个真实时间戳语料库识别自然发生的知识演化，生成针对不同LLM知识截止日期的问题和黄金答案。

Result: 对12个开源和闭源LLM在3种知识探测格式上的评估显示，与静态知识问题相比，evolveQA上性能下降高达31%。

Conclusion: evolveQA基准测试有效揭示了LLM在处理时间演化知识时的显著性能下降，为评估LLM的时间知识处理能力提供了更好的工具。

Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions
arising when facts evolve over time within their training data. Existing
studies evaluate this phenomenon through benchmarks built on structured
knowledge bases like Wikidata, but they focus on widely-covered,
easily-memorized popular entities and lack the dynamic structure needed to
fairly evaluate LLMs with different knowledge cut-off dates. We introduce
evolveQA, a benchmark specifically designed to evaluate LLMs on temporally
evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS
updates, Azure changes, and WHO disease outbreak reports. Our framework
identifies naturally occurring knowledge evolution and generates questions with
gold answers tailored to different LLM knowledge cut-off dates. Through
extensive evaluation of 12 open and closed-source LLMs across 3 knowledge
probing formats, we demonstrate significant performance drops of up to 31% on
evolveQA compared to static knowledge questions.

</details>


### [5] [Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems](https://arxiv.org/abs/2510.19186)
*Zhaoyi Joey Hou,Tanya Shourya,Yingfan Wang,Shamik Roy,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出了TRACE基准和SCOPE评估框架，用于评估使用外部工具的多轮对话AI系统中的复杂错误模式，特别是在用户满意度信号误导的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法捕捉多轮工具增强对话中的关键错误，特别是当代理误解工具结果但对用户仍显得满意的情况。

Method: 引入TRACE基准（系统合成的工具增强对话）和SCOPE评估框架（自动发现错误模式和评估标准）。

Result: 实验显示SCOPE显著优于基线方法，特别是在用户满意度信号具有误导性的挑战性案例上。

Conclusion: TRACE和SCOPE能够有效评估工具增强对话系统中的复杂错误交互，弥补现有评估方法的不足。

Abstract: Evaluating conversational AI systems that use external tools is challenging,
as errors can arise from complex interactions among user, agent, and tools.
While existing evaluation methods assess either user satisfaction or agents'
tool-calling capabilities, they fail to capture critical errors in multi-turn
tool-augmented dialogues-such as when agents misinterpret tool results yet
appear satisfactory to users. We introduce TRACE, a benchmark of systematically
synthesized tool-augmented conversations covering diverse error cases, and
SCOPE, an evaluation framework that automatically discovers diverse error
patterns and evaluation rubrics in tool-augmented dialogues. Experiments show
SCOPE significantly outperforms the baseline, particularly on challenging cases
where user satisfaction signals are misleading.

</details>


### [6] [DiSRouter: Distributed Self-Routing for LLM Selections](https://arxiv.org/abs/2510.19208)
*Hang Zheng,Hongshen Xu,Yongkai Lin,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 提出了DiSRouter（分布式自路由）范式，从集中式路由转向分布式路由，让LLM代理基于自我认知能力自主决定是否回答问题或路由给其他代理。


<details>
  <summary>Details</summary>
Motivation: 现有路由系统依赖固定的集中式外部路由器，无法充分理解不同LLM的知识边界，导致性能不佳且缺乏灵活性。

Method: 采用两阶段自我认知训练流程增强LLM的自我认知能力，在DiSRouter中查询在网络中遍历，每个LLM代理基于自身能力判断独立决定是否回答或路由。

Result: 实验表明DiSRouter在各种场景下显著优于现有路由方法，能有效区分简单和困难查询，并在领域外任务上表现出强泛化能力。

Conclusion: 利用LLM内在自我认知比外部评估更有效，为更模块化和高效的多智能体系统铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) has created a diverse
ecosystem of models with highly varying performance and costs, necessitating
effective query routing to balance performance and expense. Current routing
systems often rely on a centralized external router trained on a fixed set of
LLMs, making them inflexible and prone to poor performance since the small
router can not fully understand the knowledge boundaries of different LLMs. We
introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts
from centralized control to distributed routing. In DiSRouter, a query
traverses a network of LLM agents, each independently deciding whether to
answer or route to other agents based on its own self-awareness, its ability to
judge its competence. This distributed design offers superior flexibility,
scalability, and generalizability. To enable this, we propose a two-stage
Self-Awareness Training pipeline that enhances each LLM's self-awareness.
Extensive experiments demonstrate that DiSRouter significantly outperforms
existing routing methods in utility across various scenarios, effectively
distinguishes between easy and hard queries, and shows strong generalization to
out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic
self-awareness is more effective than external assessment, paving the way for
more modular and efficient multi-agent systems.

</details>


### [7] [SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets](https://arxiv.org/abs/2510.19247)
*Ziwei Wang,Jiayuan Su,Mengyu Zhou,Huaxing Zeng,Mengni Jia,Xiao Lv,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: SheetBrain是一个神经符号双工作流代理框架，用于在表格数据上进行准确推理，支持电子表格问答和操作任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在理解和推理复杂电子表格方面存在困难，难以准确捕捉表格的复杂结构并确保推理正确性。

Method: SheetBrain包含三个核心模块：理解模块生成电子表格的全面概览；执行模块集成Python沙箱和Excel工具包进行多轮推理；验证模块验证推理和答案的正确性。

Result: 在多个公开表格问答和操作基准测试中，SheetBrain显著提高了准确性，特别是在更具挑战性的SheetBench场景中。

Conclusion: SheetBrain通过神经符号双工作流方法有效解决了LLMs在复杂电子表格推理中的局限性。

Abstract: Understanding and reasoning over complex spreadsheets remain fundamental
challenges for large language models (LLMs), which often struggle with
accurately capturing the complex structure of tables and ensuring reasoning
correctness. In this work, we propose SheetBrain, a neuro-symbolic dual
workflow agent framework designed for accurate reasoning over tabular data,
supporting both spreadsheet question answering and manipulation tasks.
SheetBrain comprises three core modules: an understanding module, which
produces a comprehensive overview of the spreadsheet - including sheet summary
and query-based problem insight to guide reasoning; an execution module, which
integrates a Python sandbox with preloaded table-processing libraries and an
Excel helper toolkit for effective multi-turn reasoning; and a validation
module, which verifies the correctness of reasoning and answers, triggering
re-execution when necessary. We evaluate SheetBrain on multiple public tabular
QA and manipulation benchmarks, and introduce SheetBench, a new benchmark
targeting large, multi-table, and structurally complex spreadsheets.
Experimental results show that SheetBrain significantly improves accuracy on
both existing benchmarks and the more challenging scenarios presented in
SheetBench. Our code is publicly available at
https://github.com/microsoft/SheetBrain.

</details>


### [8] [TheMCPCompany: Creating General-purpose Agents with Task-specific Tools](https://arxiv.org/abs/2510.19286)
*Reza Esfandiarpoor,Vishwas Suryanarayanan,Stephen H. Bach,Vishal Chowdhary,Anthony Aue*

Main category: cs.CL

TL;DR: 提出了TheMCPCompany基准，用于评估工具调用代理在真实世界服务交互任务中的表现，包含18,000多个工具，实验显示先进模型能有效发现工具但难以处理复杂企业环境。


<details>
  <summary>Details</summary>
Motivation: 当前通用代理主要依赖网页浏览器与环境交互，而MCP协议带来了大量特定任务工具，需要评估工具调用代理在实际服务交互中的表现。

Method: 使用各种真实世界服务的REST API创建MCP服务器，提供手动标注的真实工具，通过工具检索和真实工具两种方式评估代理性能。

Result: 使用真实工具时性能提升且成本降低；通过工具检索时，所有模型表现相似或优于浏览器代理，但小模型无法充分利用可用工具，GPT-5接近真实工具性能。

Conclusion: 最先进的推理模型在简单环境中能有效发现工具，但在复杂企业环境中严重受挫，导航数万工具并以非平凡方式组合解决复杂问题仍是挑战。

Abstract: Since the introduction of the Model Context Protocol (MCP), the number of
available tools for Large Language Models (LLMs) has increased significantly.
These task-specific tool sets offer an alternative to general-purpose tools
such as web browsers, while being easier to develop and maintain than GUIs.
However, current general-purpose agents predominantly rely on web browsers for
interacting with the environment. Here, we introduce TheMCPCompany, a benchmark
for evaluating tool-calling agents on tasks that involve interacting with
various real-world services. We use the REST APIs of these services to create
MCP servers, which include over 18,000 tools. We also provide manually
annotated ground-truth tools for each task. In our experiments, we use the
ground truth tools to show the potential of tool-calling agents for both
improving performance and reducing costs assuming perfect tool retrieval. Next,
we explore agent performance using tool retrieval to study the real-world
practicality of tool-based agents. While all models with tool retrieval perform
similarly or better than browser-based agents, smaller models cannot take full
advantage of the available tools through retrieval. On the other hand, GPT-5's
performance with tool retrieval is very close to its performance with
ground-truth tools. Overall, our work shows that the most advanced reasoning
models are effective at discovering tools in simpler environments, but
seriously struggle with navigating complex enterprise environments.
TheMCPCompany reveals that navigating tens of thousands of tools and combining
them in non-trivial ways to solve complex problems is still a challenging task
for current models and requires both better reasoning and better retrieval
models.

</details>


### [9] [AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation](https://arxiv.org/abs/2510.19361)
*Xianyang Liu,Yilin Liu,Shuai Wang,Hao Cheng,Andrew Estornell,Yuzhi Zhao,Jiaheng Wei*

Main category: cs.CL

TL;DR: AgenticMath是一个用于生成高质量数学问答对的智能代理管道，通过四阶段流程提升LLM的数学推理能力，仅需3-6万样本即可在数学推理基准上达到或超越使用更大数据集的基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法生成的数据集质量低、答案错误多、信息丰富度有限，需要一种能自动生成高质量数学问答对的方法来提升LLM的监督微调效果。

Method: 四阶段管道：1)种子问题筛选（高信息量、复杂性、清晰度）；2)多代理问题重述（生成多样化逻辑一致的重述）；3)答案增强（使用思维链推理重写答案，提高数值和逻辑正确性）；4)问答对评估（保留最优对）。

Result: 在3B-8B参数的LLM上，仅使用3-6万数学样本进行微调，就能在多样化的领域内和领域外数学推理基准上达到或超越使用40万或230万样本的基线模型性能。

Conclusion: 有针对性的高质量数据生成比大规模低质量数据更能有效提升LLM的数学推理能力。

Abstract: The creation of high-quality datasets to improve Large Language Model (LLM)
reasoning remains a significant challenge, as current methods often suffer from
generating low-quality/incorrect answers and limited information richness from
available data sources. To address this, we propose AgenticMath, a novel
agentic pipeline for generating high-quality mathematical question-answer pairs
to enhance the supervised fine-tuning of LLMs. Our method operates through four
stages: (1) Seed Question Filter that selects questions with high information
richness, complexity, and clarity; (2) an Agentic Question Rephrase step that
employs a multi-agent system to generate diverse, logically consistent
paraphrases; (3) an Answer Augment step where rewrite answers using
chain-of-thought reasoning to enhance numerical and logical correctness,
without reliance on human-provided labels; and (4) a final Question and Answer
Evaluation that retains only the most superior pairs. Extensive experiments
demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated
datasets (comprising only 30-60K math samples) achieves competitive or superior
performance on diverse in domain and out-of-domain mathematical reasoning
benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M
samples). Our work demonstrates that targeted, high-quality data generation is
a more efficient path to improving mathematical reasoning in LLMs than
large-scale, low-quality alternatives.

</details>


### [10] [LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts](https://arxiv.org/abs/2510.19363)
*Siyuan Wang,Gaokai Zhang,Li Lyna Zhang,Ning Shang,Fan Yang,Dongyao Chen,Mao Yang*

Main category: cs.CL

TL;DR: LoongRL是一种数据驱动的强化学习方法，通过KeyChain技术将短多跳问答转换为高难度长上下文任务，训练模型在16K长度下有效解决128K任务，显著提升长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理对大语言模型至关重要，但现有强化学习方法主要针对短上下文，缺乏针对长上下文推理的高级思维模式训练数据和方法。

Method: 提出KeyChain合成方法，通过插入UUID链将短多跳问答转换为高难度长上下文任务，要求模型逐步追踪正确链、识别真实问题、检索相关事实并进行推理。

Result: 在Qwen2.5-7B和14B上，LoongRL将长上下文多跳问答准确率分别提升23.5%和21.1%，LoongRL-14B达到74.2分，媲美更大前沿模型。

Conclusion: LoongRL诱导出计划-检索-推理-复查的推理模式，能泛化到远超训练长度的任务，同时保持短上下文推理能力。

Abstract: Reasoning over long contexts is essential for large language models. While
reinforcement learning (RL) enhances short-context reasoning by inducing "Aha"
moments in chain-of-thought, the advanced thinking patterns required for
long-context reasoning remain largely unexplored, and high-difficulty RL data
are scarce. In this paper, we introduce LoongRL, a data-driven RL method for
advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis
approach that transforms short multi-hop QA into high-difficulty long-context
tasks by inserting UUID chains that hide the true question among large
collections of distracting documents. Solving these tasks requires the model to
trace the correct chain step-by-step, identify the true question, retrieve
relevant facts and reason over them to answer correctly. RL training on
KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning
pattern that generalizes far beyond training length. Models trained at 16K
effectively solve 128K tasks without prohibitive full-length RL rollout costs.
On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA
accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches
a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)
and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all
128K needle-in-a-haystack stress tests, and preserves short-context reasoning
capabilities.

</details>


### [11] [LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation](https://arxiv.org/abs/2510.19644)
*Daria Cherniuk,Nikita Sukhorukov,Nikita Sushko,Daniil Gusak,Danil Sivtsov,Elena Tutubalina,Evgeny Frolov*

Main category: cs.CL

TL;DR: LlavaCode框架通过将代码压缩成紧凑的语义表示来提升代码补全效率，减少检索上下文至几个压缩的单标记向量，在保持生成质量的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成在代码补全中很有效，但引入上下文会显著增加序列长度，导致推理变慢，这在IDE等交互式环境中是严重限制。

Method: 使用小型投影器模块将代码压缩成紧凑的语义表示，使代码LLM能够理解，将检索上下文减少到仅几个压缩的单标记向量。

Result: 实验表明，压缩上下文在线补全任务中相比完整RAG流水线实现了20-38%的首标记时间减少，同时提高了EM和ES指标。

Conclusion: LlavaCode框架通过上下文压缩在保持代码生成质量的同时显著降低了延迟，为交互式代码补全提供了有效解决方案。

Abstract: Retrieval-augmented generation has emerged as one of the most effective
approaches for code completion, particularly when context from a surrounding
repository is essential. However, incorporating context significantly extends
sequence length, leading to slower inference - a critical limitation for
interactive settings such as IDEs. In this work, we introduce LlavaCode, a
framework that compresses code into compact, semantically rich representations
interpretable by code LLM, enhancing generation quality while reducing the
retrieved context to only a few compressed single-token vectors. Using a small
projector module we can significantly increase the EM and ES metrics of coding
model with negligible latency increase. Our experiments demonstrate that
compressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on
line completion tasks compared to full-RAG pipelines.

</details>


### [12] [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)
*Xiang Liu,Xuming Hu,Xiaowen Chu,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出DiffAdapt框架，通过分析推理轨迹的熵模式来识别问题难度，为不同难度的问题选择不同的推理策略，在保持准确性的同时减少22.4%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时经常产生冗长的思考轨迹，但其效用不明确。研究旨在提高推理效率，使模型无需过度思考就能达到高性能。

Method: 分析推理轨迹中token概率的熵模式，发现U形熵分布。基于此提出DiffAdapt框架，根据问题难度和推理轨迹熵选择Easy/Normal/Hard推理策略，每个策略包含固定的提示、温度和最大token长度。

Result: 在5个模型和8个基准测试上的评估显示，该方法在保持或提高准确性的同时，将token使用量减少高达22.4%。

Conclusion: DiffAdapt为计算高效的推理提供了一条实用路径，无需微调基础LLM，仅需训练一个小的分类器来适应不同难度的问题。

Abstract: Recent reasoning Large Language Models (LLMs) demonstrate remarkable
problem-solving abilities but often generate long thinking traces whose utility
is unclear. Our work aims to improve their efficiency, enabling them to reach
high performance without overthinking. First, we analyze the entropy of token
probabilities in reasoning traces. Across three models, we observe a consistent
U-shaped entropy pattern: high entropy on easy problems despite high accuracy,
low entropy on problems with medium difficulty, and high entropy on hard
problems reflecting uncertainty. Specifically, we notice 22--25\% entropy
reduction from easy to medium difficulty regions, suggesting an {overthinking}
phenomenon on easy instances. Building on these insights, we introduce
\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard
inference strategies per question based on their difficulty and reasoning trace
entropy. Each inference strategy consists of a fixed prompt, temperature and
maximum token length. In contrast to existing efficiency optimization methods,
our approach does not fine-tune base LLM but a small probe that classifies
LLM's final hidden state, allowing inexpensive adaptation. We comprehensively
evaluate our method on five models and eight benchmarks. Our method achieves
comparable or improved accuracy while reducing token usage by up to 22.4\%,
establishing a practical path toward compute-efficient reasoning.

</details>


### [13] [Are Large Language Models Sensitive to the Motives Behind Communication?](https://arxiv.org/abs/2510.19687)
*Addison J. Wu,Ryan Liu,Kerem Oktar,Theodore R. Sumers,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 该论文研究了LLMs是否具备动机警觉能力，即能否像人类一样考虑信息源的动机来评估内容可信度。研究发现LLMs在受控实验中表现良好，但在真实广告场景中表现不佳，通过简单的引导干预可显著提升其表现。


<details>
  <summary>Details</summary>
Motivation: 人类沟通具有动机性，而LLMs处理的信息也受到人类意图和动机的影响。为了让LLMs在现实世界中有效运作，它们必须能够批判性地评估内容，考虑信息源的动机。

Method: 首先使用认知科学中的受控实验验证LLMs行为是否与理性模型一致，然后扩展到更自然的在线广告场景，并测试简单的引导干预效果。

Result: LLMs在受控实验中能成功像人类一样折扣有偏见来源的信息，但在真实广告场景中表现不佳。简单的引导干预显著提高了LLMs与理性模型的一致性。

Conclusion: LLMs具备对他人动机的基本敏感性，但要泛化到新的现实世界场景需要进一步改进这些模型。

Abstract: Human communication is motivated: people speak, write, and create content
with a particular communicative intent in mind. As a result, information that
large language models (LLMs) and AI agents process is inherently framed by
humans' intentions and incentives. People are adept at navigating such nuanced
information: we routinely identify benevolent or self-serving motives in order
to decide what statements to trust. For LLMs to be effective in the real world,
they too must critically evaluate content by factoring in the motivations of
the source -- for instance, weighing the credibility of claims made in a sales
pitch. In this paper, we undertake a comprehensive study of whether LLMs have
this capacity for motivational vigilance. We first employ controlled
experiments from cognitive science to verify that LLMs' behavior is consistent
with rational models of learning from motivated testimony, and find they
successfully discount information from biased sources in a human-like manner.
We then extend our evaluation to sponsored online adverts, a more naturalistic
reflection of LLM agents' information ecosystems. In these settings, we find
that LLMs' inferences do not track the rational models' predictions nearly as
closely -- partly due to additional information that distracts them from
vigilance-relevant considerations. However, a simple steering intervention that
boosts the salience of intentions and incentives substantially increases the
correspondence between LLMs and the rational model. These results suggest that
LLMs possess a basic sensitivity to the motivations of others, but generalizing
to novel real-world settings will require further improvements to these models.

</details>


### [14] [ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers](https://arxiv.org/abs/2510.19791)
*Saptarshi Sengupta,Zhengyu Zhou,Jun Araki,Xingbo Wang,Bingqing Wang,Suhang Wang,Zhe Feng*

Main category: cs.CL

TL;DR: ToolDreamer框架通过生成假设性工具描述来改进工具检索，使检索器能更好地理解用户查询与工具描述之间的语义对齐，从而提升大型工具集的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型基于用户查询与工具描述的相似度进行工具排名，但用户请求往往与工具描述语言不匹配，导致检索效果不佳。需要解决大型工具集超出LLM上下文窗口限制的问题。

Method: 提出ToolDreamer框架，使用LLM生成假设性（合成）工具描述，让检索器基于这些描述来获取工具，实现查询与工具在描述语言空间中的自然对齐。

Result: 在ToolRet数据集上应用ToolDreamer，提高了稀疏和密集检索器的性能，无论是否经过训练都表现出改进，展示了框架的灵活性。

Conclusion: ToolDreamer能够将部分推理负担转移到检索器上，使LLM能够有效处理大型工具集而不会超出上下文窗口限制。

Abstract: Tool calling has become increasingly popular for Large Language Models
(LLMs). However, for large tool sets, the resulting tokens would exceed the
LLM's context window limit, making it impossible to include every tool. Hence,
an external retriever is used to provide LLMs with the most relevant tools for
a query. Existing retrieval models rank tools based on the similarity between a
user query and a tool description (TD). This leads to suboptimal retrieval as
user requests are often poorly aligned with the language of TD. To remedy the
issue, we propose ToolDreamer, a framework to condition retriever models to
fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,
description of tools that the LLM feels will be potentially useful for the
query. The framework enables a more natural alignment between queries and tools
within the language space of TD's. We apply ToolDreamer on the ToolRet dataset
and show that our method improves the performance of sparse and dense
retrievers with and without training, thus showcasing its flexibility. Through
our proposed framework, our aim is to offload a portion of the reasoning burden
to the retriever so that the LLM may effectively handle a large collection of
tools without inundating its context window.

</details>


### [15] [Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning](https://arxiv.org/abs/2510.19807)
*Xichen Zhang,Sitong Wu,Yinghao Zhu,Haoru Tan,Shaozuo Yu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: Scaf-GRPO通过提供渐进式脚手架提示来解决LLM在强化学习中的"学习悬崖"问题，当模型学习停滞时注入分层提示，显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法面临"学习悬崖"问题：当遇到远超模型当前能力的问题时，模型持续失败产生零奖励信号，导致优势计算崩溃和学习停滞。

Method: 提出Scaf-GRPO框架，首先诊断学习停滞，然后注入分层提示（从抽象概念到具体步骤），让模型自行构建有效解决方案，仅在独立学习停滞时提供最小化指导。

Result: 在挑战性数学基准测试中，Scaf-GRPO将Qwen2.5-Math-7B模型在AIME24基准上的pass@1分数相对GRPO基线提升了44.3%。

Conclusion: 该框架为解锁LLM解决超出其能力范围问题的能力提供了稳健有效的方法，是推进自主推理前沿的关键步骤。

Abstract: Reinforcement learning from verifiable rewards has emerged as a powerful
technique for enhancing the complex reasoning abilities of Large Language
Models (LLMs). However, these methods are fundamentally constrained by the
''learning cliff'' phenomenon: when faced with problems far beyond their
current capabilities, models consistently fail, yielding a persistent
zero-reward signal. In policy optimization algorithms like GRPO, this collapses
the advantage calculation to zero, rendering these difficult problems invisible
to the learning gradient and stalling progress. To overcome this, we introduce
Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive
training framework that strategically provides minimal guidance only when a
model's independent learning has plateaued. The framework first diagnoses
learning stagnation and then intervenes by injecting tiered in-prompt hints,
ranging from abstract concepts to concrete steps, enabling the model to
construct a valid solution by itself. Extensive experiments on challenging
mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the
pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative
44.3% over a vanilla GRPO baseline. This result demonstrates our framework
provides a robust and effective methodology for unlocking a model's ability to
solve problems previously beyond its reach, a critical step towards extending
the frontier of autonomous reasoning in LLM.

</details>


### [16] [Hubble: a Model Suite to Advance the Study of LLM Memorization](https://arxiv.org/abs/2510.19811)
*Johnny Tian-Zheng Wei,Ameya Godbole,Mohammad Aflah Khan,Ryan Wang,Xiaoyuan Zhu,James Flemings,Nitya Kashyap,Krishna P. Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: Hubble是一套用于研究LLM记忆机制的开源模型套件，包含标准模型和扰动模型，通过控制文本插入来模拟记忆风险，揭示了训练数据频率和训练阶段对记忆的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM记忆机制，特别是敏感数据在训练过程中的记忆风险，为缓解记忆问题提供实证基础。

Method: 开发标准模型和扰动模型，在训练过程中控制插入特定文本（如书籍段落、传记、测试集），分析不同参数规模、训练数据量和训练阶段对记忆的影响。

Result: 发现记忆风险取决于敏感数据在训练语料中的相对频率，以及敏感数据在训练过程中的持续暴露程度。增加训练语料规模可以稀释敏感数据，而让敏感数据在训练早期出现有助于记忆。

Conclusion: 提出了两个最佳实践：通过增加训练语料规模稀释敏感数据，以及让敏感数据在训练早期出现。Hubble模型套件为记忆研究、成员推断和机器遗忘提供了理想的测试平台。

Abstract: We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties](https://arxiv.org/abs/2510.19299)
*Philipp J. Schneider,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.AI

TL;DR: 提出了一个多智能体LLM模拟框架，通过行为奖励函数和上下文学习来模拟人类在线社交动态，包括同质性、互惠性和社会验证等特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLM智能体是否能重现人类在线行为的复杂社交动态，以及什么记忆和学习机制能使这种动态出现。

Method: 使用多智能体LLM模拟框架，智能体通过行为奖励函数（捕捉社交互动、信息寻求、自我呈现、协调和情感支持等核心驱动因素）进行重复互动和评估，并通过上下文学习和教练信号来适应行为。

Result: 实验显示，经过教练的LLM智能体发展出稳定的互动模式并形成涌现的社交联系，产生的网络结构反映了真实在线社区的特性。

Conclusion: 该框架为研究LLM群体中的集体动态建立了原则性测试平台，揭示了人工智能体如何近似或偏离类人社交行为。

Abstract: Can large language model (LLM) agents reproduce the complex social dynamics
that characterize human online behavior -- shaped by homophily, reciprocity,
and social validation -- and what memory and learning mechanisms enable such
dynamics to emerge? We present a multi-agent LLM simulation framework in which
agents repeatedly interact, evaluate one another, and adapt their behavior
through in-context learning accelerated by a coaching signal. To model human
social behavior, we design behavioral reward functions that capture core
drivers of online engagement, including social interaction, information
seeking, self-presentation, coordination, and emotional support. These rewards
align agent objectives with empirically observed user motivations, enabling the
study of how network structures and group formations emerge from individual
decision-making. Our experiments show that coached LLM agents develop stable
interaction patterns and form emergent social ties, yielding network structures
that mirror properties of real online communities. By combining behavioral
rewards with in-context adaptation, our framework establishes a principled
testbed for investigating collective dynamics in LLM populations and reveals
how artificial agents may approximate or diverge from human-like social
behavior.

</details>


### [18] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出了CKA-RL方法，通过持续知识适应策略和自适应知识合并机制，解决持续强化学习中的灾难性遗忘和知识利用效率问题。


<details>
  <summary>Details</summary>
Motivation: 现实环境通常是非平稳的，需要智能体持续适应新任务和变化条件。现有持续强化学习方法存在灾难性遗忘和知识利用效率低的问题。

Method: 引入持续知识适应策略，维护任务特定知识向量池，动态使用历史知识适应新任务；提出自适应知识合并机制，合并相似知识向量以解决可扩展性挑战。

Result: 在三个基准测试上，CKA-RL优于最先进方法，整体性能提升4.20%，前向迁移提升8.02%。

Conclusion: CKA-RL能有效积累和利用历史知识，缓解灾难性遗忘，实现跨任务的高效知识迁移。

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [19] [MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration](https://arxiv.org/abs/2510.19423)
*Jia-Kai Dong,I-Wei Huang,Chun-Tin Wu,Yi-Tien Tsai*

Main category: cs.AI

TL;DR: MSC-Bench是一个用于评估LLM代理在分层MCP生态系统中多跳端到端工具编排能力的大规模基准测试，通过构建"等函数集"作为真实基准，减少对LLM作为评判的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常在孤立环境中评估工具，忽略了功能重叠和跨服务器编排等挑战，导致评估过于乐观。

Method: 采用五级课程结构，从单工具编排到复杂跨服务器规划，系统测试代理能力，并构建"等函数集"作为真实基准。

Result: 实验表明刚性层次结构会阻碍性能，即使最先进的代理在鲁棒性方面也存在系统性弱点。

Conclusion: MSC-Bench提供了一个诊断框架来暴露这些限制，指导开发更强大高效的工具使用代理。

Abstract: We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,
end-to-end tool orchestration by LLM agents in a hierarchical Model-Context
Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in
isolation, ignoring challenges such as functional overlap and cross-server
orchestration, leading to overly optimistic assessments. MSC-Bench addresses
these gaps by constructing ground truth through 'equal function sets', allowing
objective metrics such as F1 score and reducing the dependency on
LLM-as-a-judge evaluation. Organized as a five-level curriculum, it
systematically tests agent capabilities from single-tool orchestration to
complex cross-server planning, and robustness to out-of-scope requests.
Experiments reveal that rigid hierarchies can hinder performance without
co-designed strategies, and even state-of-the-art agents exhibit systemic
weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose
these limitations and guide the development of more capable and efficient
tool-using agents. The benchmark and resources are publicly available at
https://github.com/snooow1029/MSC_Bench.

</details>


### [20] [HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application](https://arxiv.org/abs/2510.19631)
*Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.AI

TL;DR: 提出了HSCodeComp基准测试，用于评估深度搜索代理在分层规则应用中的能力，特别是预测10位商品编码（HSCode），现有代理表现远低于人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准测试忽视了代理在应用复杂规则（如法律条款、医疗手册和关税规则）方面的能力，这些规则具有模糊边界和隐含逻辑关系，对代理提出了挑战。

Method: 构建了基于真实世界电商平台数据的HSCodeComp基准测试，包含632个产品条目，涵盖多样化产品类别，HSCode由多位人类专家标注。

Result: 实验结果显示最佳代理仅达到46.8%的10位准确率，远低于人类专家的95.0%，测试时扩展无法进一步提升性能。

Conclusion: 分层规则应用对当前代理具有显著挑战，需要开发更先进的代理能力来处理复杂规则推理任务。

Abstract: Effective deep search agents must not only access open-domain and
domain-specific knowledge but also apply complex rules-such as legal clauses,
medical manuals and tariff rules. These rules often feature vague boundaries
and implicit logic relationships, making precise application challenging for
agents. However, this critical capability is largely overlooked by current
agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level
e-commerce benchmark designed to evaluate deep search agents in hierarchical
rule application. In this task, the deep reasoning process of agents is guided
by these rules to predict 10-digit Harmonized System Code (HSCode) of products
with noisy but realistic descriptions. These codes, established by the World
Customs Organization, are vital for global supply chain efficiency. Built from
real-world data collected from large-scale e-commerce platforms, our proposed
HSCodeComp comprises 632 product entries spanning diverse product categories,
with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source,
and closed-source agents reveal a huge performance gap: best agent achieves
only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,
detailed analysis demonstrates the challenges of hierarchical rule application,
and test-time scaling fails to improve performance further.

</details>


### [21] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: RLIE是一个将大语言模型与概率建模相结合的统一框架，用于学习加权规则集，通过规则生成、逻辑回归、迭代优化和评估四个阶段，实现更可靠的神经符号推理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法忽视了规则间的相互作用，且未充分利用LLM与概率规则学习结合进行鲁棒推理的潜力。

Method: RLIE框架包含四个阶段：LLM生成和过滤规则候选、逻辑回归学习概率权重、基于预测误差的迭代优化、以及将加权规则集作为直接分类器的评估。

Result: 直接使用学习权重的规则集表现优异，而将规则、权重和逻辑模型输出注入LLM提示反而降低准确性，表明LLM擅长语义生成但概率集成能力有限。

Conclusion: RLIE阐明了LLM在归纳推理中的潜力和局限性，通过与经典概率规则组合方法结合，实现了更可靠的神经符号推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [22] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: Memo是一种基于Transformer的架构和训练方法，用于解决强化学习中内存密集型、长视野任务的问题，通过插入周期性总结标记来创建和检索记忆。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的具身智能体策略训练中，视觉输入常常超出Transformer的上下文限制，而人类能够维护和利用压缩为记忆的终身经验。

Method: 提出Memo架构，通过在训练时在模型输入中插入周期性总结标记来实现记忆的创建和检索。

Result: 在网格世界元强化学习基准和照片级真实室内环境的多目标导航任务中，Memo优于朴素的长上下文Transformer基线，同时计算和存储效率更高。

Conclusion: Memo在推理时能更好地泛化到更长的上下文，并在需要截断历史上下文以适应推理约束的流式设置中保持鲁棒性。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [23] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: 提出了PROBE基准来评估LLM代理的主动性能力，将主动性分解为三个核心能力：搜索未指定问题、识别具体瓶颈和执行适当解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前评估主动性的基准局限于局部上下文，无法测试跨来源和长时间跨度的推理能力，需要新的评估框架。

Method: 设计PROBE基准，将主动性分解为三个能力阶段，并应用于评估领先LLM和流行代理框架。

Result: 即使最先进的模型也难以解决该基准，GPT-5和Claude Opus-4.1的最佳端到端性能仅为40%，揭示了代理系统的当前局限性。

Conclusion: 结果突显了代理系统中自主行动的当前限制，并揭示了有前景的未来研究方向。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [24] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=MzI3NzI0MTk1OQ==&mid=2247517331&idx=1&sn=c4d7f42e5710d8baf5da0cbd38bf1f46&chksm=ea666f92feb59ad3751586d4b95123d97bd195f7b293289b0b65281c5440d12bd7ce7ae5fd73#rd)
*PaperEveryday*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [25] [斯坦福+DeepMind突破：<em class="highlight">强化学习</em>让AI学会聪明记东西，性能碾压传统方案](http://mp.weixin.qq.com/s?__biz=Mzk3NTk0MjcxNQ==&mid=2247483895&idx=1&sn=d4c9c54568a5c2b853235c5ff12f4901&chksm=c55d5b9ea5d55a3d798111ac6caf6c581de20c6b441cf99dd01146d8320682014ec36799abcb#rd)
*隔壁同桌阿宅*

Main category: wechat.article

TL;DR: 这一阶段是Mem-α强化学习策略的核心训练过程，通过模拟真实世界的复杂环境，使模型能够自适应不同任务场景下的记忆管理需求。训练环境模拟了三种典型干扰：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这一阶段是Mem-α强化学习策略的核心训练过程，通过模拟真实世界的复杂环境，使模型能够自适应不同任务场景下的记忆管理需求。训练环境模拟了三种典型干扰：

</details>


### [26] [基于<em class="highlight">强化学习</em>的智能体化搜索全面综述：基础、角色、优化、评估与应用](http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671515&idx=1&sn=71a5779ba77647faed213f58d024ad6a&chksm=fd8b8c3888401ed1e3bf348d803cbc0118bd5a1bdfe30a08f62ade2b303237e87e25e6fdde26#rd)
*专知*

Main category: wechat.article

TL;DR: 在这一新范式下，强化学习（Reinforcement Learning， RL） 提供了一种强大的机制，用于实现自适应与自我改进的搜索行为。本综述首次系统梳理了基于强化学习的智能体化搜索（RL-based agentic search）研究进展，从三个互补维度组织


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在这一新范式下，强化学习（Reinforcement Learning， RL） 提供了一种强大的机制，用于实现自适应与自我改进的搜索行为。本综述首次系统梳理了基于强化学习的智能体化搜索（RL-based agentic search）研究进展，从三个互补维度组织

</details>


### [27] [从鸽子到人工智能，<em class="highlight">强化学习</em>怎样按人类的意愿行事？](http://mp.weixin.qq.com/s?__biz=MzI2NDIzMjYyMA==&mid=2247535806&idx=1&sn=ccfbde31d4df7458cfdbd1846d861075&chksm=eb76c872315b7c89142a6e5d965ecfcac507c333bdbd0a8624cf8a3b5e481fb67925d33c1d78#rd)
*墨子沙龙*

Main category: wechat.article

TL;DR: 从最基本的角度来说，强化学习是通过试错学习，这种试错（也可以说是探索）最简单的算法形式是所谓的“ε-贪婪”（厄普西隆―贪婪）算法。希腊字母 ε在数学上常用来表示“一点点”，ε-贪婪的意思就是“贪婪，除了一点


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从最基本的角度来说，强化学习是通过试错学习，这种试错（也可以说是探索）最简单的算法形式是所谓的“ε-贪婪”（厄普西隆―贪婪）算法。希腊字母 ε在数学上常用来表示“一点点”，ε-贪婪的意思就是“贪婪，除了一点

</details>


### [28] [自动驾驶中常提的“<em class="highlight">强化学习</em>”是个啥？](http://mp.weixin.qq.com/s?__biz=MzUyODY1NDE1NA==&mid=2247575267&idx=1&sn=cc073057a48a1da0cac4b27e0cd40fdb&chksm=fbf70761a495a30dbd44ebe6a99bf8dafe85ad1dd2dae94c86e76457cbdb794f89edd1ae3f79#rd)
*智驾最前沿*

Main category: wechat.article

TL;DR: 对于强化学习来说，首先要解决的是如何定义状态与奖励。状态既要包含足够的信息让策略做出正确决策，又不能过于冗余导致学习困难。奖励设计则非常敏感，奖励信号如果不合理会导致“奖励劫持”或“走捷径”现象，模型


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 对于强化学习来说，首先要解决的是如何定义状态与奖励。状态既要包含足够的信息让策略做出正确决策，又不能过于冗余导致学习困难。奖励设计则非常敏感，奖励信号如果不合理会导致“奖励劫持”或“走捷径”现象，模型

</details>


### [29] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=MzU4ODM4NjcyMg==&mid=2247498481&idx=1&sn=75025f196b052f351a6f5f0342c738f6&chksm=fc2594ff72ca39de4412821cf481b0be838688032297965544ae1187a05f9f6b2e2d60232574#rd)
*机器视觉与AI深度学习*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [30] [谷歌10.23日Nature：最先进的<em class="highlight">强化学习</em>算法](http://mp.weixin.qq.com/s?__biz=Mzk5MDIzNTE0Mw==&mid=2247485456&idx=1&sn=7915deab2163f8d3a9cab78115b94a2c&chksm=c49c5d390550d3e6ba962d9180d11f2457495d4ee55333cb280e1102c73b6a85a5b78fb4475e#rd)
*感存算一体*

Main category: wechat.article

TL;DR: 强化学习（RL）是人工智能的核心技术，已在围棋、星际争霸、芯片设计等领域取得突破。然而，这些成功背后的学习算法都是人类专家经过数十年反复试错手工设计出来的，如Q-learning、PPO、MuZero等。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（RL）是人工智能的核心技术，已在围棋、星际争霸、芯片设计等领域取得突破。然而，这些成功背后的学习算法都是人类专家经过数十年反复试错手工设计出来的，如Q-learning、PPO、MuZero等。

</details>


### [31] [Meta用40万个GPU小时做了一个实验，只为弄清<em class="highlight">强化学习</em>ScalingLaw](http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574522&idx=2&sn=33d8bfebdd06e9ed903b98c72fc79ea6&chksm=ea46e462254385922dfa488d5e68bc1dfa6d17d7e3da5f3bd8dc31dfeae19f1677d2ff6bc5f1#rd)
*机器学习算法与自然语言处理*

Main category: wechat.article

TL;DR: 异步强化学习设置作者首先研究异步的 off-policy RL 训练结构，因为它决定了训练的稳定性与算力效率，并且通常独立于其他设计选择。具体来说，作者比较了两种 off-policy 学习方式：PPO-off-policy-k 和 PipelineRL-k。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 异步强化学习设置作者首先研究异步的 off-policy RL 训练结构，因为它决定了训练的稳定性与算力效率，并且通常独立于其他设计选择。具体来说，作者比较了两种 off-policy 学习方式：PPO-off-policy-k 和 PipelineRL-k。

</details>


### [32] [最新<em class="highlight">Agentic</em> Search综述，RL让Agent自主检索，RAG逐渐成为过去式](http://mp.weixin.qq.com/s?__biz=Mzg4MzYxODkzMg==&mid=2247502905&idx=1&sn=1f89dc2ccf31e389e5ad59c99c4f7002&chksm=cebdf41fa8e72a40e2306ee8fc8ccceeb774d59f7705893993e52df87d73416722085d7b289f#rd)
*AI修猫Prompt*

Main category: wechat.article

TL;DR: survey analytical focus rl foun- search reasoning evaluation dations behavior integration scope application scope singh et al. [169] agentic rag x x liang et al. [108] reasoning in rag x x x x gao et ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: survey analytical focus rl foun- search reasoning evaluation dations behavior integration scope application scope singh et al. [169] agentic rag x x liang et al. [108] reasoning in rag x x x x gao et al. [58] reasoning in rag x x xi et al. [220] general search agents x 1 x li et al. [102] rl-based d

</details>


### [33] [<em class="highlight">Agentic</em> AI：通向 AGI 应用的关键前站与智能涌现之路](http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658053&idx=1&sn=22e1308a5679f4f6d856da7ce8c02afc&chksm=c05d56b1a65adcd6b6dd30132ad40dacedf2ba916def32284911c172feb7059bb123c66e57f4#rd)
*DataFunSummit*

Main category: wechat.article

TL;DR: 1. 背景与问题：多智能体系统的历史性分化 2. 核心概念与理论：agentic ai 的深度解析 3. 技术实现与案例：agentic ai 应用的威力展现 4. 理论基础与论证：智能涌现的科学支撑


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1. 背景与问题：多智能体系统的历史性分化 2. 核心概念与理论：agentic ai 的深度解析 3. 技术实现与案例：agentic ai 应用的威力展现 4. 理论基础与论证：智能涌现的科学支撑

</details>


### [34] [2025 Google Devfest｜当浏览器开始思考，人类进入了 <em class="highlight">Agentic</em> 时代](http://mp.weixin.qq.com/s?__biz=MjM5NDkwOTEyMQ==&mid=2651666121&idx=1&sn=9793f173dc16e560dae12d24658fb5dc&chksm=bce47c92e8b4c25c8f5ab0f1c3b3d9563b466bc46c359e2a6a7b688f54d84739f04c513edd7b#rd)
*GDG*

Main category: wechat.article

TL;DR: 浏览器，从信息入口到应用容器，再到如今的智能代理平台，它正经历一场新的“进化”。在 2025 Google DevFest 上海现场，来自清华大学的博士、Fellou 创始团队成员 马骁腾 将带来一次关于浏览器未来的分享 —— 揭秘 Fellou 背后


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 浏览器，从信息入口到应用容器，再到如今的智能代理平台，它正经历一场新的“进化”。在 2025 Google DevFest 上海现场，来自清华大学的博士、Fellou 创始团队成员 马骁腾 将带来一次关于浏览器未来的分享 —— 揭秘 Fellou 背后

</details>


### [35] [<em class="highlight">Agentic</em> 框架系列（三）：动手实践篇](http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484085&idx=1&sn=b9231b373bc3456a48de900db61739dc&chksm=feaa3abddcf5b1601088342a3f9f03287691473fd5c8cab130b8c5b2a1b69d85b344f1ad1e4c#rd)
*AI 知行社 Lab*

Main category: wechat.article

TL;DR: 在前两篇中，我们了解了 Agentic 框架的理念与生态。本篇我们将真正动手实践—— 用 LangGraph + LLM + 工具调用，搭建一个简单可运行的多代理系统，实现


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在前两篇中，我们了解了 Agentic 框架的理念与生态。本篇我们将真正动手实践—— 用 LangGraph + LLM + 工具调用，搭建一个简单可运行的多代理系统，实现

</details>


### [36] [LLM workflow vs. <em class="highlight">Agentic</em> workflow vs. AI Agent](http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068095&idx=1&sn=10981bc9031e1a033e9bd669bb08f03f&chksm=89ee011213dc72be43206655b1be0a8174c60e2dbf5e6b6c81ea47005fea6bc07a67e6b9fea2#rd)
*风海筑梦*

Main category: wechat.article

TL;DR: llm workflow vs. agentic workflow vs. ai agent 3x185 workflow runs。a. llm workflow the entire logic fully orchestrated by n8n； autonomy： none gpt-40 total tokens： 2，705 ee time： 25s when clicking 'exe...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: llm workflow vs. agentic workflow vs. ai agent 3x185 workflow runs。a. llm workflow the entire logic fully orchestrated by n8n； autonomy： none gpt-40 total tokens： 2，705 ee time： 25s when clicking 'execute get competitors message perplexity compress the response aggregate responses workfl

</details>


### [37] [AI洞察 | 企业AI <em class="highlight">Agent</em>落地面临的多重挑战和应对策略](http://mp.weixin.qq.com/s?__biz=MjM5ODYzNTgwMg==&mid=2650769585&idx=4&sn=db018529eb2b7d18c3a13c3c2d4c0ce8&chksm=bf6dd4805864c06615196ea1461bb3c27c68b29b75ca4d4ff28c76c10dfb17dc19594e566cd3#rd)
*CTI论坛*

Main category: wechat.article

TL;DR: Agentic AI 是 “会思考、能协作的系统范式”。按照Gartner的说法，Agentic AI 是一种能自主感知、规划、行动的软件实体，企业在规划架构时，需谨慎设计、风险管理、模块化构建等。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 是 “会思考、能协作的系统范式”。按照Gartner的说法，Agentic AI 是一种能自主感知、规划、行动的软件实体，企业在规划架构时，需谨慎设计、风险管理、模块化构建等。

</details>


### [38] [突然发现吴恩达的<em class="highlight">Agentic</em> AI课程笔记，真的超详细！](http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247492124&idx=1&sn=444048f76737262f706d300de27d8611&chksm=fa677327992e46eedeba064fe13b816eeb1fc7acee22234283b9b607538aa0d47559c319672d#rd)
*慕容千语*

Main category: wechat.article

TL;DR: 所以他换 了个思路：不用名词“agent”，改用形容词。了个思路：不用名词“agent”，改用形容词“agentic”。形容词是可以区分程度的。管智能化、自主化程度多高多低，都可以算AgenticAI，只是程度不同而已。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 所以他换 了个思路：不用名词“agent”，改用形容词。了个思路：不用名词“agent”，改用形容词“agentic”。形容词是可以区分程度的。管智能化、自主化程度多高多低，都可以算AgenticAI，只是程度不同而已。

</details>


### [39] [<em class="highlight">Agentic</em> 应用交付差异和可观测性解读-学习阿里技术和思码逸有感](http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485696&idx=1&sn=da5fdb61605b65d2936357342ec6af1b&chksm=fc24b382d16d9791389f8ea1d8ac0caafb4f9720cf304d6a7c1c829ecec5d092a34eadba2ec7#rd)
*求索云途*

Main category: wechat.article

TL;DR: Agentic 应用具备自主决策、持续学习和环境感知能力，这与传统软件的预定义逻辑执行模式形成鲜明对比；这种根本性差异体现在架构复杂性的显著提升、非确定性行为的挑战，以及全新的多代理协作模式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 应用具备自主决策、持续学习和环境感知能力，这与传统软件的预定义逻辑执行模式形成鲜明对比；这种根本性差异体现在架构复杂性的显著提升、非确定性行为的挑战，以及全新的多代理协作模式。

</details>


### [40] [AI Agents vs. <em class="highlight">Agentic</em> AI：概念分类、应用、挑战及潜在解决方案](http://mp.weixin.qq.com/s?__biz=Mzg2NjA2MDAyOA==&mid=2247484371&idx=1&sn=d48e8c7442693e15dddcd49eb7fc3a8a&chksm=cf4cd879c46de92f3086a19445b267f9ae9cfeea7d58ca016e180b1a73d6406736ab834e8a7d#rd)
*TouchAI*

Main category: wechat.article

TL;DR: 相比之下，Agentic AI 系统代表着一种范式转变，其特征在于多智能体协作、动态任务分解、持久记忆以及自治行为的编排化（orchestrated autonomy）。通过对体系结构演进、运行机制、交互模式与自治层级的顺序分析，我们对这两种


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 相比之下，Agentic AI 系统代表着一种范式转变，其特征在于多智能体协作、动态任务分解、持久记忆以及自治行为的编排化（orchestrated autonomy）。通过对体系结构演进、运行机制、交互模式与自治层级的顺序分析，我们对这两种

</details>


### [41] [AI<em class="highlight">大模型</em>的参数到底是啥？即专业又通俗的<em class="highlight">大模型</em>之旅](http://mp.weixin.qq.com/s?__biz=MzkzNTM1MDI4MQ==&mid=2247522486&idx=1&sn=2d1572503821bad93605e3b13ae21403&chksm=c3e96172bb1126c60ce87595b4c0f4fd51e866b14d18efbea7deb7c5843bb0de938271e07bb3#rd)
*JAVA葵花宝典*

Main category: wechat.article

TL;DR: 50.5 kb → 你会发现大模型就是一堆文件。除了几个很小的配置文件外，最占地方的，就是几个巨大的、以 .safetensors 结尾的文件。这些文件是什么？是复杂的代码吗？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 50.5 kb → 你会发现大模型就是一堆文件。除了几个很小的配置文件外，最占地方的，就是几个巨大的、以 .safetensors 结尾的文件。这些文件是什么？是复杂的代码吗？

</details>


### [42] [“<em class="highlight">大模型</em>—微算法”协同推进检察业务与智能技术深融](http://mp.weixin.qq.com/s?__biz=MjM5ODY1ODY4NA==&mid=2651428831&idx=2&sn=01b198186fad9aef3a29ab212aab7418&chksm=bc90c99aab4b62e31236b985a92c876f1cbc569c4ca50f9774ed6b633ed7f419a7370cbd1fc9#rd)
*阳检在线*

Main category: wechat.article

TL;DR: 大模型本身作为一类特殊的“智能体”与其他小模型处于高度关联耦合性存在。因此，有效解决大模型通用性与专业性协调不足的关键在于如何实现算法体系设计的“大”“小”协同性问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型本身作为一类特殊的“智能体”与其他小模型处于高度关联耦合性存在。因此，有效解决大模型通用性与专业性协调不足的关键在于如何实现算法体系设计的“大”“小”协同性问题。

</details>


### [43] [中科曙光发布国内首个科学<em class="highlight">大模型</em>一站式开发平台！](http://mp.weixin.qq.com/s?__biz=MzA5MzMxOTEwMA==&mid=2651000003&idx=1&sn=a88d68c8da1bd09091552bb8ebeb7843&chksm=8a79a5f92974b7dc9b6bbdc87a46ec6c76725bf43b0bd4ba3ff50ce7941c5dd777950c38a97f#rd)
*中科曙光*

Main category: wechat.article

TL;DR: “算力是大模型发展前置条件，且门槛越来越高”，中科曙光高级副总裁李斌表示，当前AI科学大模型正从“任务化”向“通用化”转变，由此带来模型参数量快速增长，这导致训练科学大模型的“超智融合”系统性能每9个月就


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “算力是大模型发展前置条件，且门槛越来越高”，中科曙光高级副总裁李斌表示，当前AI科学大模型正从“任务化”向“通用化”转变，由此带来模型参数量快速增长，这导致训练科学大模型的“超智融合”系统性能每9个月就

</details>


### [44] [想学AI<em class="highlight">大模型</em>应用开发，就按照这个顺序学！](http://mp.weixin.qq.com/s?__biz=MzYzODAzMDYyMQ==&mid=2247483870&idx=1&sn=2f348b5b749a2dc99926e526b79e74f2&chksm=f1c1be01640614f35d1ca4e696d59075e117896b8dfa1281765d64dc0d875efe73a29eaf6587#rd)
*讲点大模型*

Main category: wechat.article

TL;DR: 按这个顺序学 阶段1：大模型基础 5天 60天 20天 3天 大模型 大模型的 prompt 大模型api 基本信息 原理 提示词。阶段2：rag应用开发工程 15天 10天 5天 7天。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 按这个顺序学 阶段1：大模型基础 5天 60天 20天 3天 大模型 大模型的 prompt 大模型api 基本信息 原理 提示词。阶段2：rag应用开发工程 15天 10天 5天 7天。

</details>


### [45] [AI舆情防线评测出炉：14款中外<em class="highlight">大模型</em>谁能稳住“煽动风暴”？](http://mp.weixin.qq.com/s?__biz=Mzg4NTc0Mjk0Mg==&mid=2247495371&idx=1&sn=9b1ff3501c24a4c53e4ff18ecd5b60b6&chksm=cee24f0bfba6537abd5009283ffc9d2ae40248d562f64679883d122276a66834988bb014f9f8#rd)
*创宇后天*

Main category: wechat.article

TL;DR: 前十名大模型整体表现稳健，安全与表达的平衡度持续提升。国产模型在情绪识别、舆情引导与内容防护等方面保持领先，展现出更成熟的语义防控能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 前十名大模型整体表现稳健，安全与表达的平衡度持续提升。国产模型在情绪识别、舆情引导与内容防护等方面保持领先，展现出更成熟的语义防控能力。

</details>


### [46] [终于有人把AI<em class="highlight">大模型</em>三种模式agent、embedding、copilot讲清楚了！](http://mp.weixin.qq.com/s?__biz=MzYyMjg1MDUxMw==&mid=2247483762&idx=1&sn=5681ad80eadc56f368469fbf1f74d0ac&chksm=fe11d1ebc4b7998225467babf8c51dc037b40d49ddead98258a19af34d5f5238b1e8f7813cd5#rd)
*AI大模型老仔*

Main category: wechat.article

TL;DR: 一文讲清 ai大模型 三种模式。一、embedding模式。embedding通过将高维数据（如文本、图像、声音 等）转换为低维连续向量空间中的表示，生成称为 嵌入向量的数值化形式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一文讲清 ai大模型 三种模式。一、embedding模式。embedding通过将高维数据（如文本、图像、声音 等）转换为低维连续向量空间中的表示，生成称为 嵌入向量的数值化形式。

</details>


### [47] [【报告】破局之路：<em class="highlight">大模型</em>生产力工具的思考与实践（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531658&idx=2&sn=9248464355f1525357f0a8a910d93a30&chksm=fceaefea6b288fbc94517cd1811ec47aa96352db96d0ed21756afacd63785229f08746bd6225#rd)
*墨玫人工智能*

Main category: wechat.article

TL;DR: 《大模型生产力工具的思考与实践》 （完整版.pdf ）以下仅展示部分内容 下载方式见文末 一、市场发展与用户态度 （一）海外 AI 生产力工具应用现状（以编程助手为例）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 《大模型生产力工具的思考与实践》 （完整版.pdf ）以下仅展示部分内容 下载方式见文末 一、市场发展与用户态度 （一）海外 AI 生产力工具应用现状（以编程助手为例）

</details>


### [48] [<em class="highlight">大模型</em>推理学习新范式！ExGRPO框架：从盲目刷题到聪明复盘](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247835865&idx=4&sn=069efca60b6be453d4ceb658dde6b385&chksm=e9a6297549688781e7c46c3ac987676a7293ac5054d67f4cac18eccfe45592869146fd35a696#rd)
*量子位*

Main category: wechat.article

TL;DR: 大模型在强化学习过程中，终于知道什么经验更宝贵了！来自上海人工智能实验室、澳门大学、南京大学和香港中文大学的研究团队，最近提出了一套经验管理和学习框架ExGRPO——


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型在强化学习过程中，终于知道什么经验更宝贵了！来自上海人工智能实验室、澳门大学、南京大学和香港中文大学的研究团队，最近提出了一套经验管理和学习框架ExGRPO——

</details>


### [49] [行业资讯 | 当病理切片遇见AI<em class="highlight">大模型</em>：盘点十大国产AI病理<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzkxMTM0OTQzNQ==&mid=2247486922&idx=1&sn=b0059c06f8e0faca32ad9cde3a3aa524&chksm=c0e27413a9ea4342825e1dfbe6f53b526672d0d8d9f58d4ad4f5eea6ae6f129fb196d353591b#rd)
*DigitalPath*

Main category: wechat.article

TL;DR: cytobrain 兰丁思邈大模型 代生成式人工智能病理诊断模型。ROAM模型：胶质瘤精准诊断的专业突破清华大学自动化系生命基础模型实验室与中南大学湘雅医院合作开发的ROAM模型，其研究成果于2024年7月发表于《Nature Machine Intelligence


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: cytobrain 兰丁思邈大模型 代生成式人工智能病理诊断模型。ROAM模型：胶质瘤精准诊断的专业突破清华大学自动化系生命基础模型实验室与中南大学湘雅医院合作开发的ROAM模型，其研究成果于2024年7月发表于《Nature Machine Intelligence

</details>


### [50] [企业级AI<em class="highlight">大模型</em>落地实战技术应用指南（2025版）-安全牛](http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247509226&idx=2&sn=397ed1816c77e0cc6beea8e392a9d795&chksm=e9705c6eeeba56699618225a0dcc4ef0a0494a080aae8117331b43d8b50c983ccc2282ebf576#rd)
*开源报告*

Main category: wechat.article

TL;DR: 图2 基础大模型应用成熟度。（1）能力边界与“涌现能力” 近半年以来，大模型在语言理解、生成、推理、编程、翻译、对话、摘要等任务上表现。2。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 图2 基础大模型应用成熟度。（1）能力边界与“涌现能力” 近半年以来，大模型在语言理解、生成、推理、编程、翻译、对话、摘要等任务上表现。2。

</details>


### [51] [图解AI核心技术：<em class="highlight">大模型</em>、RAG、智能体、MCP](http://mp.weixin.qq.com/s?__biz=MzI2Nzk3MjUyNw==&mid=2247492744&idx=2&sn=20beb1947c5c723cb237ce76fe4806c4&chksm=eb9f589fb9bdb652cb1541019f476cb90d06c5e83e5443de4dbdcaaf35b397d416d60e7b4065#rd)
*机器学习社区*

Main category: wechat.article

TL;DR: 大模型Transformer vs. Mixture of Experts混合专家 （MoE） 是一种流行的架构，它使用不同的“专家”来改进 Transformer 模型。下图解释了它们与 Transformers 的区别。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型Transformer vs. Mixture of Experts混合专家 （MoE） 是一种流行的架构，它使用不同的“专家”来改进 Transformer 模型。下图解释了它们与 Transformers 的区别。

</details>


### [52] [<em class="highlight">大模型</em>应用开发进阶之路：从LLM能力集成到自主智能体构建](http://mp.weixin.qq.com/s?__biz=MzE5ODY1NzAwNA==&mid=2247483788&idx=1&sn=83274f16a98b67bbcdfcaa7d187d8c51&chksm=97989b93d2cb4350e8a2b76f295f2d0b7985232b2333872f4fd153e49b8a9ebd1edcfb4242ad#rd)
*LLM Tools猎手*

Main category: wechat.article

TL;DR: 这才是大模型应用开发最根本的思维转变。传统软件开发是“过程自动化”：用户需要按照预设流程操作。比如点外卖：打开App→浏览商家→选择菜品→支付→等待送达。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这才是大模型应用开发最根本的思维转变。传统软件开发是“过程自动化”：用户需要按照预设流程操作。比如点外卖：打开App→浏览商家→选择菜品→支付→等待送达。

</details>


### [53] [构建企业级AI治理中台：<em class="highlight">大模型</em>驱动的数据治理4.0架构白皮书](http://mp.weixin.qq.com/s?__biz=MzkxNTI2MTYwOQ==&mid=2247499453&idx=1&sn=bafa25a544dab86e9f15a9ade99541d8&chksm=c0eb57cada0e651e79e8861ea53ac44ee02f9a3da68744d19591d054a7bcbfea03814be939ab#rd)
*大数据范式*

Main category: wechat.article

TL;DR: ai大模型主数据治理整体方案智能体案例应用一deepseek赋能 数据治理技术一基于deepseek的智能体构建一al 在主数据清洗的应用与展望。基于deepseek的数据治理方案（完整版64页）.pdf p 基于deepseek的数据治理方案（完整版64页）.pptx al


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai大模型主数据治理整体方案智能体案例应用一deepseek赋能 数据治理技术一基于deepseek的智能体构建一al 在主数据清洗的应用与展望。基于deepseek的数据治理方案（完整版64页）.pdf p 基于deepseek的数据治理方案（完整版64页）.pptx al

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出了GRPO和Dr.GRPO框架，通过建模奖励噪声为伯努利噪声并进行噪声校正，在RLHF中提高对噪声奖励的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RLHF或RLVR对不一致或错误奖励的噪声非常敏感，但噪声与广泛使用的基于组的策略优化方法之间的相互作用尚未充分探索。

Method: 引入噪声鲁棒的GRPO和Dr.GRPO框架，将奖励腐败建模为伯努利噪声，在估计奖励翻转概率后进行噪声校正以消除学习信号的偏差。

Result: 在数学和代码任务上应用噪声校正后获得一致改进，在数学任务上准确率提升高达6.7个百分点，在代码任务上提升1.5个百分点。

Conclusion: 将监督学习中的标签噪声校正与现代RLHF相结合，为噪声现实世界部署提供了理论见解和实用算法。

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [55] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 提出BAPO方法解决离线强化学习中策略熵下降、优化不稳定问题，通过动态调整裁剪边界平衡正负样本贡献，在多个基准测试中取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在LLM对齐中应用广泛，但面临策略熵急剧下降、优化不稳定甚至崩溃的挑战，需要解决负优势样本主导和固定裁剪机制抑制探索的问题

Method: 提出BAPO方法，基于熵裁剪规则，动态调整PPO目标的裁剪边界，自适应平衡正负样本贡献，保持策略熵并稳定优化过程

Result: 在AIME 2024和2025基准测试中，7B BAPO模型超越开源对手SkyWork-OR1-7B，32B模型在同规模模型中达到SOTA，甚至优于o3-mini和Gemini-2.5-Flash-Thinking等专有系统

Conclusion: BAPO通过动态裁剪机制有效解决了离线RL中的优化不平衡问题，实现了快速、稳定且数据高效的训练

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [56] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: 论文发现RL后训练中存在梯度不平衡问题，大梯度任务主导优化过程但学习收益不一定更高，这质疑了简单混合数据集的做法。


<details>
  <summary>Details</summary>
Motivation: 研究多任务后训练中梯度不平衡问题，因为现有方法假设所有任务贡献相似梯度，但实际RL训练中某些任务会产生显著更大的梯度。

Method: 通过分析不同任务在RL后训练中的梯度大小和学习收益关系，探究梯度不平衡现象及其影响。

Result: 发现大梯度任务不一定获得更大学习收益，梯度不平衡不能由训练奖励或优势值等典型统计量解释，而是源于任务本身差异。

Conclusion: 简单混合数据集的方法存在问题，需要开发基于梯度级别的校正方法。

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [57] [A Communication-Efficient Decentralized Actor-Critic Algorithm](https://arxiv.org/abs/2510.19199)
*Xiaoxing Ren,Nicola Bastianello,Thomas Parisini,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出了一种去中心化的多智能体强化学习框架，通过本地训练减少通信负担，同时保持网络协调。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中通信受限的问题，降低通信开销同时维持智能体间的协调。

Method: 采用去中心化的actor-critic学习框架，每个智能体在信息交换前进行多次本地策略和价值函数更新，价值函数由多层神经网络近似。

Result: 在马尔可夫采样下建立了有限时间收敛分析，样本复杂度为O(ε^-3)，通信复杂度为O(ε^-1τ^-1)，其中τ为本地训练步数。数值实验验证了理论结果。

Conclusion: 该方法有效降低了通信负担，同时保证了多智能体系统的协调性能，神经网络近似质量影响最终误差界限。

Abstract: In this paper, we study the problem of reinforcement learning in multi-agent
systems where communication among agents is limited. We develop a decentralized
actor-critic learning framework in which each agent performs several local
updates of its policy and value function, where the latter is approximated by a
multi-layer neural network, before exchanging information with its neighbors.
This local training strategy substantially reduces the communication burden
while maintaining coordination across the network. We establish finite-time
convergence analysis for the algorithm under Markov-sampling. Specifically, to
attain the $\varepsilon$-accurate stationary point, the sample complexity is of
order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of
order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of
local training steps. We also show how the final error bound depends on the
neural network's approximation quality. Numerical experiments in a cooperative
control setting illustrate and validate the theoretical findings.

</details>


### [58] [Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments](https://arxiv.org/abs/2510.19244)
*Yiyu Qian,Su Nguyen,Chao Chen,Qinyue Zhou,Liyuan Zhao*

Main category: cs.LG

TL;DR: 提出SILVER with RL-guided labeling，通过将RL策略自身动作输出纳入边界点识别，将SILVER框架扩展到多动作和高维环境，提升深度强化学习的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习虽然性能出色但缺乏可解释性，限制了对其策略行为的信任。现有SILVER框架仅限于低维、二元动作领域，需要扩展到更复杂的环境。

Method: 从图像观察中提取紧凑特征表示，进行SHAP特征归因，然后使用RL引导标记生成行为一致的边界数据集，训练决策树和回归函数等替代模型来解释RL策略的决策结构。

Result: 在两个Atari环境中使用三种深度RL算法进行评估，结果表明该方法在保持竞争性任务性能的同时，显著提高了透明度和人类对智能体行为的理解。

Conclusion: 该工作通过将SILVER转变为可扩展且行为感知的框架，推进了可解释强化学习在高维多动作设置中解释深度RL智能体的能力。

Abstract: Deep reinforcement learning (RL) achieves remarkable performance but lacks
interpretability, limiting trust in policy behavior. The existing SILVER
framework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based
regression but remains restricted to low-dimensional, binary-action domains. We
propose SILVER with RL-guided labeling, an enhanced variant that extends SILVER
to multi-action and high-dimensional environments by incorporating the RL
policy's own action outputs into the boundary points identification. Our method
first extracts compact feature representations from image observations,
performs SHAP-based feature attribution, and then employs RL-guided labeling to
generate behaviorally consistent boundary datasets. Surrogate models, such as
decision trees and regression-based functions, are subsequently trained to
interpret RL policy's decision structure. We evaluate the proposed framework on
two Atari environments using three deep RL algorithms and conduct human-subject
study to assess the clarity and trustworthiness of the derived interpretable
policy. Results show that our approach maintains competitive task performance
while substantially improving transparency and human understanding of agent
behavior. This work advances explainable RL by transforming SILVER into a
scalable and behavior-aware framework for interpreting deep RL agents in
high-dimensional, multi-action settings.

</details>


### [59] [QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation](https://arxiv.org/abs/2510.19296)
*Yang Zhang,Rui Zhang,Jiaming Guo,Lei Huang,Di Huang,Yunpu Zhao,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出QiMeng-SALV方法，通过信号感知学习优化Verilog代码生成，利用功能正确输出信号的代码段来增强强化学习训练，在VerilogEval和RTLLM基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在Verilog代码生成方面取得显著进展，但缺乏有意义的功能奖励阻碍了基于强化学习的偏好优化，难以生成功能正确的Verilog代码。

Method: 利用功能正确输出信号的代码段优化RL训练：验证生成模块中信号的功能正确性；使用抽象语法树识别信号感知代码段；引入信号感知DPO在正确信号级代码段上进行优化。

Result: 在VerilogEval和RTLLM上达到最先进性能，7B参数模型匹配DeepSeek v3 671B模型的性能，显著优于在同一数据集上训练的开源模型CodeV。

Conclusion: QiMeng-SALV实现了从传统模块级到细粒度信号级优化的范式转变，解决了功能奖励不足的问题。

Abstract: The remarkable progress of Large Language Models (LLMs) presents promising
opportunities for Verilog code generation which is significantly important for
automated circuit design. The lacking of meaningful functional rewards hinders
the preference optimization based on Reinforcement Learning (RL) for producing
functionally correct Verilog code. In this paper, we propose Signal-Aware
Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments
of functionally correct output signal to optimize RL training. Considering
Verilog code specifies the structural interconnection of hardware gates and
wires so that different output signals are independent, the key insight of
QiMeng-SALV is to extract verified signal-aware implementations in partially
incorrect modules, so as to enhance the extraction of meaningful functional
rewards. Roughly, we verify the functional correctness of signals in generated
module by comparing with that of reference module in the training data. Then
abstract syntax tree (AST) is employed to identify signal-aware code segments
which can provide meaningful functional rewards from erroneous modules.
Finally, we introduce signal-aware DPO which is optimized on the correct
signal-level code segments, thereby preventing noise and interference from
incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from
conventional module-level to fine-grained signal-level optimization in Verilog
code generation, addressing the issue of insufficient functional rewards.
Experiments demonstrate that our method achieves state-of-the-art performance
on VerilogEval and RTLLM, with a 7B parameter model matching the performance of
the DeepSeek v3 671B model and significantly outperforming the leading
open-source model CodeV trained on the same dataset. Our code is available at
https://github.com/zy1xxx/SALV.

</details>


### [60] [A Markov Decision Process for Variable Selection in Branch & Bound](https://arxiv.org/abs/2510.19348)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了BBMDP，一种用于分支定界中变量选择的MDP公式，通过强化学习学习最优分支启发式策略，在四个标准MILP基准测试中优于现有最先进的RL代理。


<details>
  <summary>Details</summary>
Motivation: 分支定界求解器中变量选择启发式对性能影响重大，现有RL方法缺乏原则性的MDP公式，需要专门设计的收敛定理和算法。

Method: 引入BBMDP，一个原则性的标准MDP公式，用于分支定界中的变量选择，可以广泛利用各种RL算法来学习最优分支启发式。

Result: 计算实验验证了模型的有效性，提出的分支代理在四个标准MILP基准测试中优于之前最先进的RL代理。

Conclusion: BBMDP提供了一个原则性的MDP框架，能够有效学习分支定界中的最优变量选择策略，显著提升求解器性能。

Abstract: Mixed-Integer Linear Programming (MILP) is a powerful framework used to
address a wide range of NP-hard combinatorial optimization problems, often
solved by Branch and Bound (B&B). A key factor influencing the performance of
B&B solvers is the variable selection heuristic governing branching decisions.
Recent contributions have sought to adapt reinforcement learning (RL)
algorithms to the B&B setting to learn optimal branching policies, through
Markov Decision Processes (MDP) inspired formulations, and ad hoc convergence
theorems and algorithms. In this work, we introduce BBMDP, a principled vanilla
MDP formulation for variable selection in B&B, allowing to leverage a broad
range of RL algorithms for the purpose of learning optimal B\&B heuristics.
Computational experiments validate our model empirically, as our branching
agent outperforms prior state-of-the-art RL agents on four standard MILP
benchmarks.

</details>


### [61] [Teaming LLMs to Detect and Mitigate Hallucinations](https://arxiv.org/abs/2510.19507)
*Demian Till,John Smeaton,Peter Haubrick,Gouse Saheb,Florian Graef,David Berman*

Main category: cs.LG

TL;DR: 提出了一种名为"联盟一致性"的方法，通过整合来自不同LLM的多个响应来检测和缓解幻觉，相比单模型一致性方法有显著改进且推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有的单模型一致性方法在检测和缓解LLM幻觉方面取得了先进成果，但仍有局限性。不同LLM具有不同的训练数据、训练方案和模型架构，整合它们的响应可以带来进一步改进。

Method: 将单模型一致性方法扩展到多个LLM，形成"联盟一致性"方法，通过组合来自不同LLM的响应来检测和缓解幻觉。评估了来自15个LLM的不同模型组合，并探索了在什么条件下这种组合是有益的。

Result: 联盟一致性方法在幻觉检测和缓解能力上相比单模型一致性方法有显著改进，并且这些性能改进通常伴随着推理成本的降低。

Conclusion: 整合多个不同LLM的响应可以显著提升幻觉检测和缓解能力，同时降低推理成本，为LLM幻觉问题提供了更有效的解决方案。

Abstract: Recent work has demonstrated state-of-the-art results in large language model
(LLM) hallucination detection and mitigation through consistency-based
approaches which involve aggregating multiple responses sampled from a single
LLM for a given prompt. These approaches help offset limitations stemming from
the imperfect data on which LLMs are trained, which includes biases and
under-representation of information required at deployment time among other
limitations which can lead to hallucinations. We show that extending these
single-model consistency methods to combine responses from multiple LLMs with
different training data, training schemes and model architectures can result in
substantial further improvements in hallucination detection and mitigation
capabilities beyond their single-model consistency counterparts. We evaluate
this \emph{consortium consistency} approach across many model teams from a pool
of 15 LLMs and explore under what conditions it is beneficial to team together
different LLMs in this manner. Further, we show that these performance
improvements often come with reduced inference costs, offsetting a significant
drawback with single-model consistency methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore是一个受神经科学启发的强化学习架构，通过整合情感信号来增强大语言模型的代码生成能力。它使用轻量级多层感知机标记代码生成轨迹的效价和惊喜度，优先重放高负效价（尴尬）的失败案例，修剪低惊喜度的成功案例，从而减少幻觉代码48%，加速自我修正45%。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物学习的启发，特别是从错误中感到尴尬会驱动快速修正的行为模式（如训练小狗时一次责骂就能避免重复错误），旨在将情感信号整合到代码生成过程中。

Method: 使用轻量级多层感知机标记代码生成轨迹的效价和惊喜度；将高负效价（尴尬）的失败案例优先放入Dream Queue进行五倍重放；修剪低惊喜度的成功案例以防止过度自信和缓冲区膨胀。

Result: 在HumanEval和BigCodeBench等代码生成基准测试中，CosmoCore将幻觉代码（如语法错误或逻辑错误）减少了48%，自我修正速度提高了45%。

Conclusion: 该框架扩展了基于人类反馈的强化学习（RLHF），用于开发更具情感意识的代码助手，在IDE和数据管道中具有应用前景。

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [63] [From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems](https://arxiv.org/abs/2510.19274)
*Saurabh Chauhan,Zeeshan Rasheed,Malik Abdul Sami,Kai-Kristian Kemell,Muhammad Waseem,Zheying Zhang,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 提出一个基于LLM的多代理系统，用于自动化RESTful微服务的API优先开发，包括生成OpenAPI规范、生成服务器代码，并通过分析执行日志和错误消息的反馈循环来优化代码。


<details>
  <summary>Details</summary>
Motivation: 推进RESTful Web服务的API优先开发自动化，测试基于LLM的多代理系统在支持API优先开发方法中的能力。

Method: 使用基于LLM的代理系统，从OpenAPI规范生成服务器代码，并通过分析执行日志和错误消息的反馈循环来迭代优化代码。

Result: 使用PRAB基准测试表明，当保持OpenAPI规范小而专注时，LLM能够生成符合规范的完整功能代码和业务逻辑。

Conclusion: 基于LLM的多代理系统能够有效支持API优先开发，通过集成日志分析减少迭代次数，生成功能强大且健壮的服务。

Abstract: This paper presents a system that uses Large Language Models (LLMs)-based
agents to automate the API-first development of RESTful microservices. This
system helps to create an OpenAPI specification, generate server code from it,
and refine the code through a feedback loop that analyzes execution logs and
error messages. The integration of log analysis enables the LLM to detect and
address issues efficiently, reducing the number of iterations required to
produce functional and robust services. This study's main goal is to advance
API-first development automation for RESTful web services and test the
capability of LLM-based multi-agent systems in supporting the API-first
development approach. To test the proposed system's potential, we utilized the
PRAB benchmark. The results indicate that if we keep the OpenAPI specification
small and focused, LLMs are capable of generating complete functional code with
business logic that aligns to the specification. The code for the system is
publicly available at https://github.com/sirbh/code-gen

</details>


### [64] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage是一个多智能体系统，能够自动将学术论文转化为交互式网页，通过分层协作流程解决传统网页构建的繁琐问题。


<details>
  <summary>Details</summary>
Motivation: 研究人员在构建项目网页时面临手动、重复性的工作负担，现有自动化工具无法处理网页的动态交互特性。

Method: 采用多智能体系统，将论文到网页的创建过程分解为从叙事规划到多模态内容生成再到交互式渲染的粗到细管道，并设有专门的检查器智能体验证内容准确性。

Result: AutoPage能够在15分钟内以低于0.1美元的成本生成高质量、视觉吸引力强的网页，并创建了首个此类任务的基准测试PageBench。

Conclusion: AutoPage将系统从单纯工具转变为强大的协作助手，通过分层协作和验证机制有效解决了网页自动化创建的挑战。

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [65] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: 本文提出了扩展智能代理软件工程范围的愿景，建议超越代码活动，建立涵盖整个软件过程的框架，并提出了指导原则和词汇设计指南。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理AI在软件工程中的兴起，需要建立系统性的研究领域，考虑社会技术因素，而不仅仅是代码相关活动。

Method: 通过分析软件工程基础和演化趋势，结合新兴的智能代理SE框架，提出扩展范围、制定价值观原则和词汇设计指导。

Result: 提出了'全过程'愿景，初步的价值观和原则，以及明确的词汇设计指南，为智能代理SE奠定基础。

Conclusion: 这些建议将促进社区合作，引导软件工程社区为智能代理SE建立坚实基础，使其不仅是必然的，而且是经过深思熟虑和长期可取的。

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [66] [Review of Tools for Zero-Code LLM Based Application Development](https://arxiv.org/abs/2510.19747)
*Priyaranjan Pattnayak,Hussain Bohra*

Main category: cs.SE

TL;DR: 本调查论文综述了基于大语言模型的零代码开发平台，分析了这些平台如何利用LLM作为开发过程的核心，让用户无需编写代码即可构建应用程序。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，零代码开发平台正在改变软件创建方式。本文旨在系统性地调查和分析这些新兴平台，为研究者和实践者提供全面的分类和比较。

Method: 采用广泛的调查方法，基于界面风格、后端集成、输出类型和可扩展性等关键维度对平台进行分类。分析了专门的LLM应用构建器和集成LLM功能的通用无代码平台。

Result: 提出了一个分类法，按界面类型、支持的LLM后端、输出类型和可扩展性程度对平台进行分类。详细比较了各平台的优缺点，并讨论了与传统和低代码开发方法的权衡。

Conclusion: 虽然基于LLM的零代码平台大大降低了创建AI驱动应用程序的门槛，但在灵活性和可靠性方面仍面临挑战。该领域正在快速发展，为非程序员创建复杂软件提供了令人兴奋的机会。

Abstract: Large Language Models (LLMs) are transforming software creation by enabling
zero code development platforms. Our survey reviews recent platforms that let
users build applications without writing code, by leveraging LLMs as the brains
of the development process. We adopt a broad survey methodology, categorizing
platforms based on key dimensions such as interface style, backend integration,
output type, and extensibility. We analyze both dedicated LLM based app
builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and
general no code platforms (e.g., Bubble, Glide) that integrate LLM
capabilities. We present a taxonomy categorizing these platforms by their
interface (conversational, visual, etc.), supported LLM backends, output type
(chatbot, full application, workflow), and degree of extensibility. Core
features such as autonomous agents, memory management, workflow orchestration,
and API integrations are in scope of the survey. We provide a detailed
comparison, highlighting each platform's strengths and limitations. Trade offs
(customizability, scalability, vendor lock-in) are discussed in comparison with
traditional and low code development approaches. Finally, we outline future
directions, including multimodal interfaces, on device LLMs, and improved
orchestration for democratizing app creation with AI. Our findings indicate
that while zero code LLM platforms greatly reduce the barrier to creating AI
powered applications, they still face challenges in flexibility and
reliability. Overall, the landscape is rapidly evolving, offering exciting
opportunities to empower non programmers to create sophisticated software.

</details>


### [67] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: BOSQTGEN是一种新颖的黑盒API测试生成方法，通过分解API规范为基本元素，使用LLM生成连贯的输入值层次，并采用组合测试来高效采样，显著提高了代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代软件通过API组合构建，但API契约不足会导致期望不匹配和故障。现有测试生成技术面临多语言系统、源代码不可访问、成本可靠性权衡以及生成结构化输入困难等挑战。

Method: 将API规范分解为基本元素，使用LLM生成连贯的输入值层次，采用组合测试方法高效采样这些值，确保覆盖关键交互同时避免随机采样的冗余。

Result: 在RESTful基准测试中平均达到82%的代码覆盖率，比现有最先进系统提高20%以上，接近手工编写的测试套件水平。

Conclusion: BOSQTGEN提供完全API驱动的测试生成方法，使开发人员能够自动创建高质量的测试用例进行验证或测试驱动开发。

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [68] [OpenAI Launches ChatGPT Atlas Browser With Built-In Agent Mode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.maginative.com%2Farticle%2Fopenai-launches-chatgpt-atlas-browser-with-built-in-agent-mode%2F%3Futm_source=tldrmarketing/1/0100019a0b9828da-09892b91-bb8d-473d-9502-b43c075162b3-000000/wskbY7im6xRykvh6sTUX1Ytw1IxWff3IcXdOe-SRkDc=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出内置Agent模式的ChatGPT Atlas浏览器，可在侧边栏直接集成ChatGPT，具备网页阅读、总结和交互功能，Plus和Pro用户可使用Agent模式完成购物车填充、票务创建等操作


<details>
  <summary>Details</summary>
Motivation: 将AI助手直接集成到浏览器中，提供更便捷的网页交互体验，通过Agent模式实现自动化操作，提升用户生产力

Method: 开发内置ChatGPT的浏览器，提供持久侧边栏界面，实现网页内容读取和总结，为高级用户提供Agent模式执行具体任务

Result: Atlas浏览器已在macOS上发布，支持网页交互、个性化记忆存储和自动化操作功能

Conclusion: Atlas浏览器成功将AI助手深度集成到浏览体验中，为高级用户提供了更强大的自动化能力

Abstract: OpenAI Launches ChatGPT Atlas Browser With Built-In Agent Mode (2 minute read) Atlas is a browser that integrates ChatGPT directly into a persistent sidebar. The browser can read, summarize, and interact with webpages, while an exclusive Agent Mode for Plus and Pro users allows ChatGPT to complete actions like filling carts or creating tickets. Atlas also stores user “memories” to personalize browsing, though users can opt out. It's now available on macOS.

</details>


### [69] [Solving the wrong problem](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fai_assisted_coding%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/wMbIxYJYwbbo-aTIsc3sOOxAi9Apo53MBdowi-k2WQ0=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI辅助编码可能正在解决软件开发中的错误问题，因为AI代理经常重新创建现有代码，主要帮助经验不足的开发者，并掩盖了软件工程教育不足和重速度轻质量等根本问题。


<details>
  <summary>Details</summary>
Motivation: 探讨AI辅助编码在软件开发中的实际影响和潜在问题，揭示AI可能没有真正解决软件开发的核心挑战。

Method: 分析AI辅助编码的现状、LLM训练数据的性质以及AI对不同经验水平开发者的影响。

Result: 发现AI代理经常重新创建现有代码，主要使经验不足的开发者受益，同时掩盖了软件工程教育不足和重速度轻质量的根本问题。

Conclusion: AI辅助编码可能正在解决错误的问题，需要关注更根本的软件工程教育质量和开发实践改进。

Abstract: Solving the wrong problem (21 minute read) AI-assisted coding, despite its impressive capabilities, may be addressing the wrong problem in software development. AI agents often recreate existing code due to the nature of LLMs and training data. Furthermore, AI is primarily benefiting less experienced developers and masking underlying issues like inadequate software engineering education and a focus on speed over quality.

</details>


### [70] [Getting DeepSeek-OCR working on an NVIDIA Spark via brute force using Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F20%2Fdeepseek-ocr-claude-code%2F%3Futm_source=tldrwebdev/1/0100019a0b9bb411-f1f0bfc7-a9e0-4a33-8000-65b7dfdfb63d-000000/KmPx5Kmkr6E4qWJxiW8T5sAepIb7MBzB-I5f32AJf4I=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Claude Code在NVIDIA Spark上通过Docker沙盒自动化部署DeepSeek-OCR，解决了PyTorch和CUDA兼容性问题


<details>
  <summary>Details</summary>
Motivation: 解决在NVIDIA Spark设备上部署DeepSeek-OCR时遇到的PyTorch和CUDA兼容性挑战

Method: 通过Claude Code自动化工具，在Docker沙盒环境中尝试不同PyTorch版本，直到找到兼容版本并生成相关脚本和文档

Result: 成功找到兼容的PyTorch版本，完成DeepSeek-OCR在NVIDIA Spark上的部署，并生成了完整的部署笔记、脚本和文档

Conclusion: Claude Code能够有效解决复杂环境下的软件部署兼容性问题，自动化工具在解决技术挑战方面具有实用价值

Abstract: Getting DeepSeek-OCR working on an NVIDIA Spark via brute force using Claude Code (10 minute read) Claude Code can be used to get DeepSeek-OCR running on an NVIDIA Spark by automating the process within a Docker sandbox. Initially, Claude Code struggled to do this due to PyTorch and CUDA compatibility issues, but after being suggested different PyTorch versions, Claude Code found a compatible version and completed the task. Claude Code generated notes, scripts, and documentation, including a ...

</details>


### [71] [The LinkedIn Generative AI Application Tech Stack: Extending to Build AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fgenerative-ai%2Fthe-linkedin-generative-ai-application-tech-stack-extending-to-build-ai-agents%3Futm_source=tldrdevops/1/0100019a0ba1fde4-448155c2-3dd0-4dea-b3bd-bfc73c0d9f09-000000/OpX3601UT-947YMZu0O_DVlc1r6lPR3lwaowI3lAOa0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LinkedIn更新了生成式AI应用技术栈，通过定义gRPC服务模式和利用消息系统实现多智能体编排，使AI智能体能够思考、规划和与用户互动，招聘助手已在9月底全球英语版上线。


<details>
  <summary>Details</summary>
Motivation: 扩展LinkedIn的生成式AI应用技术栈，提升AI智能体的能力，使其能够更好地思考、规划和与用户互动，特别是为招聘助手等应用场景提供支持。

Method: 通过定义gRPC服务模式来描述智能体，并利用LinkedIn的消息系统进行多智能体编排，实现智能体间的协作和交互。

Result: 成功构建了能够思考、规划和行动的AI智能体系统，招聘助手已在9月底实现全球英语版可用。

Conclusion: LinkedIn的生成式AI技术栈扩展有效提升了AI智能体的能力，为构建更复杂的多智能体应用奠定了基础。

Abstract: The LinkedIn Generative AI Application Tech Stack: Extending to Build AI Agents (10 minute read) LinkedIn's generative AI application tech stack was updated to improve AI agents by enabling them to think, plan, and act with users, with the Hiring Assistant being globally available in English by the end of September. Key to this development were defining agents through gRPC service schema definitions and leveraging LinkedIn's messaging system for multi-agent orchestration. Agent interactions w...

</details>


### [72] [Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Fbeyond-rag%2F%3Futm_source=tldrdata/1/0100019a10899501-ec02b680-f7d6-4c62-97ff-70fe1bbfa982-000000/Jsq4aHSQ4_NA_c70SAJfN-uKnk96phrD1G2LMXKt32M=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: RAG只是上下文工程学科的起点，现代上下文工程已发展为包含上下文编写、压缩、隔离和选择，需要强大的元数据管理、策略即代码护栏和多模态能力。知识图谱支撑可解释、可信赖和可扩展的AI，同时引入新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 探索RAG之后上下文工程的发展，强调现代AI系统需要更先进的上下文管理方法来提高可信度、可解释性和扩展性。

Method: 提出上下文工程的新框架，包括上下文编写、压缩、隔离和选择等技术，结合知识图谱和策略即代码方法。

Result: 展示了上下文工程如何从RAG演变为更全面的学科，为构建更可靠和可扩展的AI系统提供了新方法。

Conclusion: 上下文工程正在快速发展，知识图谱和新的评估指标对于构建可信赖的AI系统至关重要，RAG只是这一更广泛学科的起点。

Abstract: Is RAG Dead? The Rise of Context Engineering and Semantic Layers for Agentic AI (18 minute read) RAG was only the starting point of the Context Engineering discipline. Modern context engineering now incorporates context writing, compression, isolation, and selection, demanding robust metadata management, policy-as-code guardrails, and multimodal capabilities. Knowledge graphs underpin explainable, trustworthy, and scalable AI, while new evaluation metrics (relevance, groundedness, provenance,...

</details>


### [73] [To unlock agentic engineering, you need more than a software catalog](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Main23/1/0100019a10986cfc-04fe8ff6-2f2c-4d64-96d8-f1ec81f8ab19-000000/Baf8_rE9WT3uDVYgSBrme2xIV6v9qP4Kn9al45vbaAM=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Port的智能体工程平台通过构建上下文湖、设置护栏和促进人机协作，帮助企业在软件开发生命周期中成功编排AI


<details>
  <summary>Details</summary>
Motivation: 当前企业需要超越简单的软件目录，实现深度、丰富、实时的上下文管理，以及有效的人机协作机制来解锁智能体工程的潜力

Method: 构建智能体工程平台，包括上下文湖、护栏系统和人机协作机制，用于在软件开发生命周期中编排AI

Result: 该平台能够成功协调AI在整个软件开发生命周期中的应用

Conclusion: Port的智能体工程平台为企业提供了成功实施AI编排所需的关键组件

Abstract: To unlock agentic engineering, you need more than a software catalog (Sponsor) You need a deep, rich, real-time context lake. You need guardrails. And you need harmonious human-to-agent collaboration.Port's agentic engineering platform lets you successfully orchestrate AI across your SDLC. Get a demo

</details>
