<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.AI](#cs.AI) [Total: 13]
- [wechat.article](#wechat.article) [Total: 9]
- [cs.LG](#cs.LG) [Total: 10]
- [tldr.article](#tldr.article) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: CodeAdapt方法通过结合CodeAct框架和少量样本的上下文学习，使标准指令微调语言模型无需额外训练就能达到或超越专门推理模型的性能，同时显著提高token效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型需要大量计算资源和数据训练，运行成本高昂且速度较慢。本文旨在证明标准指令微调语言模型可以通过简单方法实现强大的推理能力，而无需专门训练。

Method: 提出CodeAdapt方法，结合CodeAct框架（语言模型在自然语言推理中穿插代码执行）和少量样本的上下文学习，仅需5个训练问题即可实现性能提升。

Result: 在四个匹配的语言模型和推理模型对中，CodeAdapt使三个语言模型在八个任务上平均表现优于对应推理模型（最高提升22.9%），token效率提高10-81%，在六个任务上四个模型平均表现更优（最高提升35.7%）。

Conclusion: CodeAdapt风格的学习和推理具有鲁棒性和领域通用性，代码增强的语言模型是认知基础良好且强大的系统，可能为权重内强化学习提供坚实基础。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [2] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: PARL是一种使用LLMs作为强化学习代理的方法，通过提示而非微调，在非语言任务中实现与或超越传统RL代理的性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在强化学习任务中的表现，特别是在非语言结构化推理任务中，如网格世界位置解释，而现有工作主要关注语言相关任务。

Method: PARL方法通过提示将动作、状态和奖励编码，使LLMs能够通过试错与环境交互学习，无需任何微调。

Result: 在三个标准RL任务中，PARL在简单环境中能匹配或超越传统RL代理，但在需要复杂数学运算或状态动作解码的任务中存在性能限制。

Conclusion: LLMs可以作为有效的RL代理，在简单非语言任务中表现良好，但在复杂推理任务中仍有局限性。

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [3] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: TripTide是首个评估LLM在现实旅行中断情况下修订行程能力的基准，通过建模中断严重程度和旅行者容忍度等维度，从意图保持、响应性和适应性三方面评估LLM的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划系统如TripCraft和TravelPlanner虽然能够生成个性化行程，但无法有效应对现实旅行中常见的中断情况，如航班取消、天气关闭等。

Method: 采用三重评估方法：1) 引入自动指标（意图保持、响应性、适应性）；2) LLM作为评判者自动评估修订质量；3) 专家手动评估语义、空间、序列和响应方面的保持情况。

Result: LLM在序列一致性和语义稳定性方面表现良好，空间偏差在短途旅行中较大但随行程延长而减小，但中断处理能力随计划长度增加而下降。

Conclusion: TripTide为评估LLM在现实不确定性下的旅行规划适应性、个性化和韧性建立了基准，揭示了LLM在长行程中断处理方面的局限性。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: 提出了AgentArcEval方法，专门用于评估基于基础模型的智能体架构，并提供了一个智能体特定通用场景目录来指导架构设计和评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法难以应对智能体架构的独特特性，如复合架构、自主非确定性行为和持续演化，因此需要专门的方法来评估智能体架构。

Method: 开发了AgentArcEval评估方法和智能体特定通用场景目录，通过真实世界税务助手Luna的案例研究来验证其有效性。

Result: 成功展示了AgentArcEval方法和场景目录在评估真实世界智能体架构方面的实用性。

Conclusion: AgentArcEval为基于基础模型的智能体架构评估提供了有效的专门方法，解决了传统方法的局限性。

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [5] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: BDiff是一种基于文本的代码差异分析算法，能够识别块级和行级编辑操作，相比现有工具能生成更高质量的差异结果。


<details>
  <summary>Details</summary>
Motivation: 现有的代码差异分析工具在处理跨多行的块级编辑操作时存在局限，通常将块级操作表示为离散的行级编辑序列，这降低了开发人员理解代码变更的效率。

Method: 基于传统差异分析算法，首先构建包含所有可能行映射和块映射的候选集，然后使用Kuhn-Munkres算法计算最优映射集，以最小化编辑脚本大小并贴近开发者意图。

Result: 实验表明BDiff在差异结果质量上优于包括大语言模型在内的五种最先进工具，同时保持竞争性的运行时性能。

Conclusion: BDiff能够有效识别块级编辑操作，提高代码变更理解的效率，而大语言模型在代码差异分析任务中结果质量不可靠且运行时效率不可行。

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [6] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 该论文研究了AI配置文件（特别是AGENTS.md）在开源软件项目中的采用情况，分析了开发者如何为AI代理提供项目上下文信息以及这些文件的演变过程。


<details>
  <summary>Details</summary>
Motivation: 随着基于GenAI的编码助手向自主代理发展，如何为AI代理提供足够的项目上下文信息成为一个挑战。AGENTS.md作为潜在标准出现，但缺乏对其实际采用情况的了解。

Method: 对466个开源软件项目中AI配置文件的采用情况进行初步研究，分析这些文件的内容、信息呈现方式以及随时间演变的过程。

Result: 研究发现目前还没有建立统一的结构，在上下文信息的提供方式上存在很大差异（描述性、规定性、禁止性、解释性、条件性）。

Conclusion: AI配置文件在开源项目中的采用为研究真实世界中的提示和上下文工程提供了独特机会，研究文件结构或呈现方式的修改如何影响生成内容质量具有很大潜力。

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [7] [Risk Management for Mitigating Benchmark Failure Modes: BenchRisk](https://arxiv.org/abs/2510.21460)
*Sean McGregor,Victor Lu,Vassil Tashev,Armstrong Foundjem,Aishwarya Ramasethu,Sadegh AlMahdi Kazemi Zarkouei,Chris Knotz,Kongtao Chen,Alicia Parrish,Anka Reuel,Heather Frase*

Main category: cs.SE

TL;DR: 提出了BenchRisk框架，用于评估LLM基准测试的风险，识别了57种潜在故障模式和196种缓解策略，帮助用户避免基于不可靠基准做出错误决策。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试存在各种故障模式，可能影响基准的偏差、方差、覆盖范围和可理解性，导致用户基于不可靠证据做出错误的LLM部署决策。

Method: 基于NIST风险管理流程，迭代分析26个流行基准，识别故障模式并开发缓解策略，构建BenchRisk评分系统评估基准风险。

Result: 所有26个基准在五个维度（全面性、可理解性、一致性、正确性、持久性）中至少一个维度存在显著风险，BenchRisk能够有效比较不同基准的风险水平。

Conclusion: LLM基准测试领域存在重要研究空白，BenchRisk作为开源工具有助于识别和共享风险及缓解措施，提高基准可靠性。

Abstract: Large language model (LLM) benchmarks inform LLM use decisions (e.g., "is
this LLM safe to deploy for my use case and context?"). However, benchmarks may
be rendered unreliable by various failure modes that impact benchmark bias,
variance, coverage, or people's capacity to understand benchmark evidence.
Using the National Institute of Standards and Technology's risk management
process as a foundation, this research iteratively analyzed 26 popular
benchmarks, identifying 57 potential failure modes and 196 corresponding
mitigation strategies. The mitigations reduce failure likelihood and/or
severity, providing a frame for evaluating "benchmark risk," which is scored to
provide a metaevaluation benchmark: BenchRisk. Higher scores indicate that
benchmark users are less likely to reach an incorrect or unsupported conclusion
about an LLM. All 26 scored benchmarks present significant risk within one or
more of the five scored dimensions (comprehensiveness, intelligibility,
consistency, correctness, and longevity), which points to important open
research directions for the field of LLM benchmarking. The BenchRisk workflow
allows for comparison between benchmarks; as an open-source tool, it also
facilitates the identification and sharing of risks and their mitigations.

</details>


### [8] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究发现通过多样性策略组合多个LLM模型，可以在软件工程任务中实现比单个最佳模型高83%的性能上限，而基于共识的策略会陷入'流行度陷阱'。


<details>
  <summary>Details</summary>
Motivation: 当前追求单一大型语言模型处理所有软件工程任务既资源密集又忽略了模型互补性的潜力，但缺乏关于编码LLM如何互补以及最佳集成策略的实证研究。

Method: 实证比较了来自5个家族的10个LLM模型和3个集成模型，在代码生成和程序修复等3个软件工程基准上评估模型互补性和性能差距，并评估了多种选择启发式方法。

Result: 集成模型的理论性能上限比最佳单模型高83%，基于共识的策略会放大常见但不正确的输出，而基于多样性的策略能实现理论潜力的95%，即使在小型两模型集成中也有效。

Conclusion: 多样性策略提供了一种成本效益高的方式来利用多个LLM提升性能，避免了基于共识策略的'流行度陷阱'。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL](https://arxiv.org/abs/2510.21045)
*Ali Khosravi Kazazi,Zhenlong Li,M. Naser Lessani,Guido Cervone*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，通过专门化的智能体协作和程序化验证，将自然语言问题准确转换为空间SQL查询，显著提升了空间文本到SQL的准确率。


<details>
  <summary>Details</summary>
Motivation: SQL和PostGIS等地理空间工具的复杂性阻碍了非专家分析空间数据，现有单智能体方法在处理空间查询的语义和语法复杂性方面存在困难。

Method: 采用多智能体框架，包含实体提取、元数据检索、查询逻辑制定、SQL生成和审查智能体，通过知识库、模式分析和语义增强等技术实现协作。

Result: 在KaggleDBQA上达到81.2%准确率，在空间查询上达到87.7%准确率（相比无审查智能体的76.7%），且生成的查询有时比基准更符合用户意图。

Conclusion: 该工作使空间分析更易访问，为空间文本到SQL系统提供了稳健、可推广的基础，推动了自主GIS的发展。

Abstract: The complexity of Structured Query Language (SQL) and the specialized nature
of geospatial functions in tools like PostGIS present significant barriers to
non-experts seeking to analyze spatial data. While Large Language Models (LLMs)
offer promise for translating natural language into SQL (Text-to-SQL),
single-agent approaches often struggle with the semantic and syntactic
complexities of spatial queries. To address this, we propose a multi-agent
framework designed to accurately translate natural language questions into
spatial SQL queries. The framework integrates several innovative components,
including a knowledge base with programmatic schema profiling and semantic
enrichment, embeddings for context retrieval, and a collaborative multi-agent
pipeline as its core. This pipeline comprises specialized agents for entity
extraction, metadata retrieval, query logic formulation, SQL generation, and a
review agent that performs programmatic and semantic validation of the
generated SQL to ensure correctness (self-verification). We evaluate our system
using both the non-spatial KaggleDBQA benchmark and a new, comprehensive
SpatialQueryQA benchmark that includes diverse geometry types, predicates, and
three levels of query complexity. On KaggleDBQA, the system achieved an overall
accuracy of 81.2% (221 out of 272 questions) after the review agent's review
and corrections. For spatial queries, the system achieved an overall accuracy
of 87.7% (79 out of 90 questions), compared with 76.7% without the review
agent. Beyond accuracy, results also show that in some instances the system
generates queries that are more semantically aligned with user intent than
those in the benchmarks. This work makes spatial analysis more accessible, and
provides a robust, generalizable foundation for spatial Text-to-SQL systems,
advancing the development of autonomous GIS.

</details>


### [10] [DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance](https://arxiv.org/abs/2510.21117)
*Chunghyun Han,Alfio Gliozzo,Junkyu Lee,Agostino Capponi*

Main category: cs.AI

TL;DR: 首次对代理AI在去中心化治理中作为自主决策者进行实证研究，构建了能够解释提案背景、检索历史审议数据并独立确定投票立场的AI投票代理。


<details>
  <summary>Details</summary>
Motivation: 研究代理AI能否在去中心化治理中作为自主决策者，增强集体决策过程，为去中心化金融系统设计可解释且经济严谨的AI代理。

Method: 使用3000多个主要协议的提案，构建基于模块化可组合程序工作流的AI投票代理，在基于可验证区块链数据的真实金融模拟环境中运行。

Result: 发现代理AI的决策与人类和代币加权结果高度一致，通过精心设计的评估指标显示出强对齐性。

Conclusion: 代理AI能够通过产生可解释、可审计且基于实证的信号来增强集体决策，适用于现实DAO治理环境。

Abstract: This paper presents a first empirical study of agentic AI as autonomous
decision-makers in decentralized governance. Using more than 3K proposals from
major protocols, we build an agentic AI voter that interprets proposal
contexts, retrieves historical deliberation data, and independently determines
its voting position. The agent operates within a realistic financial simulation
environment grounded in verifiable blockchain data, implemented through a
modular composable program (MCP) workflow that defines data flow and tool usage
via Agentics framework. We evaluate how closely the agent's decisions align
with the human and token-weighted outcomes, uncovering strong alignments
measured by carefully designed evaluation metrics. Our findings demonstrate
that agentic AI can augment collective decision-making by producing
interpretable, auditable, and empirically grounded signals in realistic DAO
governance settings. The study contributes to the design of explainable and
economically rigorous AI agents for decentralized financial systems.

</details>


### [11] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 提出了NeuroGenPoisoning攻击框架，通过LLM内部神经元归因和遗传优化生成对抗性外部知识，在RAG系统中实现高效知识毒化攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击方法忽略了模型内部表示动态和神经元级敏感性，未能充分研究RAG毒化机制和知识冲突问题。

Method: 首先识别与上下文毒化知识强相关的毒化响应神经元，然后使用遗传算法进化对抗性段落以最大化激活这些神经元，并通过归因信号重用有潜力的外部知识变体。

Result: 在多个模型和数据集上实验显示，该方法能持续实现超过90%的人口覆盖成功率，同时保持流畅性，有效解决了知识冲突问题。

Conclusion: NeuroGenPoisoning框架能够大规模生成有效的毒化RAG知识，通过毒化响应神经元引导的毒化能有效解决知识冲突。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [12] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: EGO-Prompt是一个自动化的提示优化框架，通过进化图优化方法改进LLM的提示设计和推理过程，在多个领域任务中显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，为领域特定任务设计最优提示和推理过程既必要又具挑战性，需要解决如何整合领域知识、提高推理效率以及为领域专家提供知识整合指导等关键问题。

Method: 提出EGO-Prompt框架，从专家构建的初始语义因果图出发，通过因果引导的文本梯度过程自动优化提示和推理机制，包括生成确定性推理指导和适配LLM使用指导。

Result: 在公共卫生、交通和人类行为任务中，EGO-Prompt比最先进方法F1分数提高7.32%-12.61%，小模型能以不到20%成本达到大模型性能，并输出改进的领域特定语义因果图。

Conclusion: EGO-Prompt有效解决了领域特定提示优化问题，显著提升LLM性能同时降低成本，并增强了模型的可解释性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [13] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种神经符号具身任务规划框架，通过符号验证和交互验证过程改进LLM代码生成，在动态和部分可观测环境中显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码即策略方法在动态或部分可观测环境中存在环境接地不足的问题，导致代码生成不完整或不正确，影响任务成功率。

Method: 结合显式符号验证和交互验证过程，在验证阶段生成探索性代码与环境交互获取缺失观测，同时保持任务相关状态。

Result: 在RLBench和真实世界动态部分可观测场景中，任务成功率比代码即策略基线提高46.2%，任务相关动作可执行性达到86.8%以上。

Conclusion: 该框架通过增强代码生成的环境接地性，显著提高了动态环境中任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [14] [CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation](https://arxiv.org/abs/2510.21324)
*Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu*

Main category: cs.AI

TL;DR: 提出了CXRAgent，一个由导演协调的多阶段代理，用于胸部X光片分析，通过工具协调、多阶段推理和团队协作增强诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR分析模型难以适应新的诊断任务和复杂推理场景，现有代理依赖单一诊断流程且缺乏工具可靠性评估机制。

Method: 采用导演协调的三阶段方法：工具调用（包含证据驱动验证器）、诊断规划（组建专家团队）、协作决策（整合上下文记忆）。

Result: 在各种CXR解释任务上表现出色，能提供视觉证据并良好泛化到不同复杂度的临床任务。

Conclusion: CXRAgent通过多阶段协作和证据验证机制，显著提升了CXR分析的适应性和可信度。

Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety
of task-specific and foundation models have been developed for automatic CXR
interpretation. However, these models often struggle to adapt to new diagnostic
tasks and complex reasoning scenarios. Recently, LLM-based agent models have
emerged as a promising paradigm for CXR analysis, enhancing model's capability
through tool coordination, multi-step reasoning, and team collaboration, etc.
However, existing agents often rely on a single diagnostic pipeline and lack
mechanisms for assessing tools' reliability, limiting their adaptability and
credibility. To this end, we propose CXRAgent, a director-orchestrated,
multi-stage agent for CXR interpretation, where a central director coordinates
the following stages: (1) Tool Invocation: The agent strategically orchestrates
a set of CXR-analysis tools, with outputs normalized and verified by the
Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual
evidence to support reliable downstream diagnosis; (2) Diagnostic Planning:
Guided by task requirements and intermediate findings, the agent formulates a
targeted diagnostic plan. It then assembles an expert team accordingly,
defining member roles and coordinating their interactions to enable adaptive
and collaborative reasoning; (3) Collaborative Decision-making: The agent
integrates insights from the expert team with accumulated contextual memories,
synthesizing them into an evidence-backed diagnostic conclusion. Experiments on
various CXR interpretation tasks show that CXRAgent delivers strong
performance, providing visual evidence and generalizes well to clinical tasks
of different complexity. Code and data are valuable at this
\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.

</details>


### [15] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan框架通过蒙特卡洛树搜索和分层引导系统，将创意生成重新定义为对LLM潜在概念空间的原则性探索，显著提升了科学想法的创新性和合理性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在生成创新想法时倾向于高概率熟悉概念的局限性，以及现有搜索方法依赖不可靠自评估启发式的问题。

Method: 使用蒙特卡洛树搜索，结合语义罗盘向量进行长程引导，以及基于景观感知的价值函数进行局部决策，平衡内在一致性、外在新颖性和叙事进展。

Result: 在生成科学想法方面显著优于ReAct和ToT等基线方法，产生了更具合理性和创新性的结果。

Conclusion: 对于创意发现，原则性引导搜索比无约束代理更有效，为LLMs成为创新合作伙伴铺平了道路。

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [16] [Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning](https://arxiv.org/abs/2510.21398)
*Ravindra Aribowo Tarunokusumo,Rafael Fernandes Cunha*

Main category: cs.AI

TL;DR: 提出了一种结合强化学习的框架，通过预算强制解码策略来提高1.5B模型在数学推理任务上的性能，显著减少40%以上的token使用量。


<details>
  <summary>Details</summary>
Motivation: 预算强制方法虽然计算效率高且无需参数训练，但依赖于长上下文推理轨迹的监督微调，导致小模型因冗长响应而性能下降。

Method: 整合强化学习来提高token效率，仅使用1.5K训练样本，在GSM8K数据集上测试不同计算预算下的SFT+RL模型性能。

Result: SFT+RL模型在GSM8K数据集上表现更好，总体准确率更高，同时相比SFT模型显著减少超过40%的token使用量。

Conclusion: 强化学习可以恢复因长上下文训练造成的损失，整体提升数学推理性能。

Abstract: Test-time scaling methods have seen a rapid increase in popularity for its
computational efficiency and parameter-independent training to improve
reasoning performance on Large Language Models. One such method is called
budget forcing, a decoding intervention strategy which allocates extra compute
budget for thinking and elicits the inherent self-correcting behavior of the
model. However, this relies on supervised fine-tuning (SFT) on long-context
reasoning traces which causes performance degradation on smaller models due to
verbose responses. For this reason, we offer a framework integrating
reinforcement learning (RL) to improve token efficiency and boost the
performance of a 1.5B model for mathematical reasoning. We demonstrate this
using only 1.5K training samples and found that our SFT+RL model performed
better on the GSM8K dataset with varying compute budgets. Our main findings
showed an overall higher accuracy while significantly reducing its token usage
by over 40% compared to the SFT model, revealing how RL can recover the losses
due to long-context training and altogether improving performance in
mathematical reasoning.

</details>


### [17] [Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI](https://arxiv.org/abs/2510.21425)
*Maneeha Rani,Bhupesh Kumar Mishra,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 该论文提出了一种新的符号AI集成到LLMs的分类框架和路线图，旨在解决LLMs透明度不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在决策和响应生成方面表现出色，但缺乏透明度。现有的神经符号AI方法主要针对传统神经网络，不适用于LLMs的特性，因此需要系统性地理解如何将符号AI有效集成到LLMs中。

Method: 首先回顾已建立的神经符号AI方法，然后提出LLMs中符号集成的新分类法，并制定将符号技术与LLMs融合的路线图。该路线图引入了四个维度的分类框架：LLM不同阶段的符号集成、耦合机制、架构范式以及算法和应用层面。

Result: 论文全面识别了当前基准、前沿进展和关键差距，提出了未来研究的路线图。通过突出最新发展和文献中的显著差距，为实施符号集成到LLMs的框架提供了实用见解。

Conclusion: 该研究为增强LLMs透明度提供了系统性的符号集成方法，填补了现有神经符号AI方法在LLMs应用中的不足，为未来研究指明了方向。

Abstract: LLMs have demonstrated highly effective learning, human-like response
generation,and decision-making capabilities in high-risk sectors. However,
these models remain black boxes because they struggle to ensure transparency in
responses. The literature has explored numerous approaches to address
transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI
approaches were primarily developed for conventional neural networks and are
not well-suited to the unique features of LLMs. Consequently, there is a
limited systematic understanding of how symbolic AI can be effectively
integrated into LLMs. This paper aims to address this gap by first reviewing
established NeSy AI methods and then proposing a novel taxonomy of symbolic
integration in LLMs, along with a roadmap to merge symbolic techniques with
LLMs. The roadmap introduces a new categorisation framework across four
dimensions by organising existing literature within these categories. These
include symbolic integration across various stages of LLM, coupling mechanisms,
architectural paradigms, as well as algorithmic and application-level
perspectives. The paper thoroughly identifies current benchmarks, cutting-edge
advancements, and critical gaps within the field to propose a roadmap for
future research. By highlighting the latest developments and notable gaps in
the literature, it offers practical insights for implementing frameworks for
symbolic integration into LLMs to enhance transparency.

</details>


### [18] [EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law](https://arxiv.org/abs/2510.21524)
*Ilija Lichkovski,Alexander Müller,Mariam Ibrahim,Tiwai Mhundwa*

Main category: cs.AI

TL;DR: EU-Agent-Bench是一个可验证的人工策划基准，用于评估LLM代理在欧盟法律背景下执行非法行为的潜在倾向，涵盖数据保护、偏见/歧视和科学诚信等多个场景。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为代理在各种环境中部署，它们可能表现出不可预测的行为，包括采取不良和/或不安全的行动。需要测量LLM代理在欧盟立法背景下采取非法行为的潜在倾向。

Method: 创建了一个可验证的人工策划基准，通过将模型的函数调用与详尽引用相关立法的评分标准进行比较，评估前沿LLM的法律合规性，并研究在代理系统提示中提供相关立法摘录对合规性的影响。

Result: 评估了前沿LLM的法律合规性，并研究了提供立法摘录对合规性的影响。发布了公共预览集供研究社区使用，同时保留私有测试集以防止数据污染。

Conclusion: 鼓励未来的工作将代理安全基准扩展到不同的法律管辖区，以及多轮和多语言交互。

Abstract: Large language models (LLMs) are increasingly deployed as agents in various
contexts by providing tools at their disposal. However, LLM agents can exhibit
unpredictable behaviors, including taking undesirable and/or unsafe actions. In
order to measure the latent propensity of LLM agents for taking illegal actions
under an EU legislative context, we introduce EU-Agent-Bench, a verifiable
human-curated benchmark that evaluates an agent's alignment with EU legal norms
in situations where benign user inputs could lead to unlawful actions. Our
benchmark spans scenarios across several categories, including data protection,
bias/discrimination, and scientific integrity, with each user request allowing
for both compliant and non-compliant execution of the requested actions.
Comparing the model's function calls against a rubric exhaustively supported by
citations of the relevant legislature, we evaluate the legal compliance of
frontier LLMs, and furthermore investigate the compliance effect of providing
the relevant legislative excerpts in the agent's system prompt along with
explicit instructions to comply. We release a public preview set for the
research community, while holding out a private test set to prevent data
contamination in evaluating upcoming models. We encourage future work extending
agentic safety benchmarks to different legal jurisdictions and to multi-turn
and multilingual interactions. We release our code on
\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.

</details>


### [19] [Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine](https://arxiv.org/abs/2510.21614)
*Wenyi Wang,Piotr Piękos,Li Nanbo,Firas Laakom,Yimeng Chen,Mateusz Ostaszewski,Mingchen Zhuge,Jürgen Schmidhuber*

Main category: cs.AI

TL;DR: 论文提出Huxley-Gödel Machine (HGM)方法，通过估计自我改进潜力指标CMP来指导代码代理的自我修改树搜索，在SWE基准测试中优于现有方法并实现人类水平的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理自我改进方法假设基准测试性能高的代理具有更好的自我改进潜力，但作者发现自我改进潜力与实际编码性能之间存在不匹配问题。

Method: 提出CMP指标来评估代理的自我改进潜力，并基于此构建HGM方法，通过估计CMP来指导自我修改树的搜索过程。

Result: 在SWE-bench Verified和Polyglot基准测试中，HGM优于现有方法且用时更少，在SWE-bench Lite上达到人类水平的性能。

Conclusion: CMP指标能有效衡量代码代理的自我改进潜力，HGM方法在多个基准测试中表现出色并具有良好的迁移性。

Abstract: Recent studies operationalize self-improvement through coding agents that
edit their own codebases. They grow a tree of self-modifications through
expansion strategies that favor higher software engineering benchmark
performance, assuming that this implies more promising subsequent
self-modifications. However, we identify a mismatch between the agent's
self-improvement potential (metaproductivity) and its coding benchmark
performance, namely the Metaproductivity-Performance Mismatch. Inspired by
Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates
the benchmark performances of the descendants of an agent as an indicator of
its potential for self-improvement. We show that, in our self-improving coding
agent development setting, access to the true $\mathrm{CMP}$ is sufficient to
simulate how the G\"odel Machine would behave under certain assumptions. We
introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$
and using it as guidance, searches the tree of self-modifications. On SWE-bench
Verified and Polyglot, HGM outperforms prior self-improving coding agent
development methods while using less wall-clock time. Last but not least, HGM
demonstrates strong transfer to other coding datasets and large language
models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and
evaluated on SWE-bench Lite with GPT-5 achieves human-level performance,
matching the best officially checked results of human-engineered coding agents.
Our code is available at https://github.com/metauto-ai/HGM.

</details>


### [20] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: DeepAgent是一个端到端的深度推理智能体，通过自主思考、工具发现和行动执行在单一连贯推理过程中完成任务。它引入自主记忆折叠机制来压缩历史交互，并使用ToolPO强化学习策略来高效稳定地教授通用工具使用。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架通常遵循预定义工作流程，限制了自主性和全局任务完成能力。现实世界任务需要外部工具和长程交互，这带来了上下文长度爆炸和交互历史累积的挑战。

Method: DeepAgent采用端到端深度推理，包含自主记忆折叠机制（将过去交互压缩为结构化记忆）和ToolPO强化学习策略（利用LLM模拟API并通过工具调用优势归因分配细粒度信用）。

Result: 在八个基准测试（包括通用工具使用任务和下游应用）上的广泛实验表明，DeepAgent在标记工具和开放集工具检索场景中始终优于基线方法。

Conclusion: 这项工作朝着为现实世界应用构建更通用和更强大的智能体迈出了一步。

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [21] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: 该论文提出了AstaBench基准套件，用于更严格地评估AI科学代理的能力，包含2400多个科学发现问题，提供可复现的研究环境和工具，并对57个代理进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理基准在科学研究评估方面存在不足：缺乏真实世界用例的全面衡量、缺少可复现的工具、未考虑模型成本和工具访问等混杂变量、缺乏标准化接口和基线代理。

Method: 定义了更严格的代理基准原则和工具，开发了AstaBench套件，包含科学发现全过程的问题，提供生产级搜索工具的研究环境，并建立了9类科学优化代理和多个基线。

Result: 对57个代理在22个代理类别中的评估显示，尽管在某些方面取得了有意义的进展，但AI在解决科学研究辅助挑战方面仍然有很长的路要走。

Conclusion: AI代理在科学研究自动化方面具有潜力，但当前技术仍远未达到解决科学研究辅助挑战的水平，需要更严格的评估基准来推动进展。

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [22] [NerulPS 2025| 网络化多智能体<em class="highlight">强化学习</em>的贝叶斯自我图（ego-graph）推断](http://mp.weixin.qq.com/s?__biz=Mzk0MDc0MzAyNw==&mid=2247484144&idx=1&sn=df448757f98e0f646649856b36568bc4&chksm=c34e36a3fce625b167ed3af9638ec72fc48f08c8faacf1c2b9b69e9f7b06fef059c4290f1e36#rd)
*阿潮的博弈智能研究*

Main category: wechat.article

TL;DR: NerulPS 2025| 网络化多智能体强化学习的贝叶斯自我图（ego-graph）推断poster bayesian ego-graph inference for networked multi-agent reinforcement learning wei duan · jie lu · junyu xuan [ abstract） abstract： in networked mult...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: NerulPS 2025| 网络化多智能体强化学习的贝叶斯自我图（ego-graph）推断poster bayesian ego-graph inference for networked multi-agent reinforcement learning wei duan · jie lu · junyu xuan [ abstract） abstract： in networked multi-agent reinforcement learning （networked-marl

</details>


### [23] [【EI复现】基于深度<em class="highlight">强化学习</em>的微能源网能量管理与优化策略研究附Python代码](http://mp.weixin.qq.com/s?__biz=Mzk0MzY2Njg3NQ==&mid=2247526706&idx=1&sn=73287c3807aa90a217dc4557913975f7&chksm=c2adb4a998ccae84eb13473ed856027b3a6cedc904cc74e2dfb947a7db27842b93d6d5989055#rd)
*Matlab科研助手*

Main category: wechat.article

TL;DR: （二）深度强化学习的应用价值 深度强化学习（DRL）通过 “智能体 - 环境” 交互机制，无需建立精确系统模型即可实现动态优化，具备强鲁棒性与实时决策能力，为微能源网能量管理提供新解决方案。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （二）深度强化学习的应用价值 深度强化学习（DRL）通过 “智能体 - 环境” 交互机制，无需建立精确系统模型即可实现动态优化，具备强鲁棒性与实时决策能力，为微能源网能量管理提供新解决方案。

</details>


### [24] [<em class="highlight">强化学习</em>基础-PPO](http://mp.weixin.qq.com/s?__biz=MzkyNDI0OTQyOQ==&mid=2247485131&idx=1&sn=f1073f7d7ff747053d6938a8dc2a99e8&chksm=c0c2109c6a080d31b3bfcdc68969d52e94ef65efc672da8cba3c351f06f1bf5a252049585f15#rd)
*刘小强的博客*

Main category: wechat.article

TL;DR: 代码实现大模型强化学习（PPO），看这个视频就够了。本文将带你从 策略梯度的基本原理 出发，深入剖析 PPO 的核心思想、数学推导以及它在大模型训练中的具体应用逻辑。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 代码实现大模型强化学习（PPO），看这个视频就够了。本文将带你从 策略梯度的基本原理 出发，深入剖析 PPO 的核心思想、数学推导以及它在大模型训练中的具体应用逻辑。

</details>


### [25] [大模型<em class="highlight">强化学习</em>的熵控制：CE-GPPO、EPO与AsyPPO技术方案对比详解](http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247586057&idx=3&sn=f0656fc3670a34f521e969fad646189a&chksm=fae4377c8f7f0824b7585b9e96f979f4fa2daff506738128f80d042b4c4070f3a49464c99c46#rd)
*AI思想会*

Main category: wechat.article

TL;DR: LLM的强化学习训练最近进展很快，SOTA模型在各种推理benchmark上的表现确实亮眼。但更值得关注的其实是另一条信息——从Rutgers到Alibaba再到HKUST，这些研究团队正在攻克的是RL领域的一个老大难：怎么控制好熵，同时避免模型退


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: LLM的强化学习训练最近进展很快，SOTA模型在各种推理benchmark上的表现确实亮眼。但更值得关注的其实是另一条信息——从Rutgers到Alibaba再到HKUST，这些研究团队正在攻克的是RL领域的一个老大难：怎么控制好熵，同时避免模型退

</details>


### [26] [如何构建和<em class="highlight">强化学习</em>“自驱力”](http://mp.weixin.qq.com/s?__biz=MzYyMjgxMjI0Mw==&mid=2247483660&idx=1&sn=c859f82c34290406a85418c2fac4c3da&chksm=fedeb70194deec84d8d7093efcf425e609cc3e789d679e4ca8c362dffe73db3979ceca6e89cf#rd)
*阿烁说*

Main category: wechat.article

TL;DR: 那么，我们可以从学习的哪个部分入手呢？从日常的学习安排和奖励入手。我们将每天小测验的结果和小玩具挂钩，单元考的结果和一次大餐挂钩，期中期末的结果和一次“未知大奖励”挂钩，明确规定学习结果和即时奖励的关


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 那么，我们可以从学习的哪个部分入手呢？从日常的学习安排和奖励入手。我们将每天小测验的结果和小玩具挂钩，单元考的结果和一次大餐挂钩，期中期末的结果和一次“未知大奖励”挂钩，明确规定学习结果和即时奖励的关

</details>


### [27] [NLP论文速读（ MIT出品）| RL的奥卡姆剃刀：为什么在线<em class="highlight">强化学习</em>遗忘更少](http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487343&idx=1&sn=410902217460bfe9766d9a427872d91c&chksm=c1a3c75310ab335ed2019762e65c0a200fcc7070d4ebd6d1b9e9d37d6f74669aeffb5c6e8cd5#rd)
*NLP学习加油站*

Main category: wechat.article

TL;DR: 本文系统比较了两种主流微调方式——监督微调（SFT）与强化学习（RL），发现在达到相同新任务性能的前提下，RL显著更少遗忘旧任务知识。为揭示这一现象背后的机制，作者提出并验证了一个核心观点：遗忘的程度可以由新


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文系统比较了两种主流微调方式——监督微调（SFT）与强化学习（RL），发现在达到相同新任务性能的前提下，RL显著更少遗忘旧任务知识。为揭示这一现象背后的机制，作者提出并验证了一个核心观点：遗忘的程度可以由新

</details>


### [28] [安全的<em class="highlight">强化学习</em>的盾牌——评估屏蔽作为安全 RL 方法的优点和潜在缺点。](http://mp.weixin.qq.com/s?__biz=MzUzMTg5NTYxNg==&mid=2247496774&idx=1&sn=97efdcddf6b0ebdbf07d461563981313&chksm=fb61af5fc016fa981143fad32210fb84c4b86b8128a6ba52a5146bf8767c35a4a12c80112df2#rd)
*F8AI*

Main category: wechat.article

TL;DR: 将屏蔽融入强化学习的主要方法是预屏蔽和后屏蔽，它们的不同之处在于干预方式。图3 （左）展示了后屏蔽：屏蔽位于智能体和环境之间，持续监控环境状态和智能体选择的操作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 将屏蔽融入强化学习的主要方法是预屏蔽和后屏蔽，它们的不同之处在于干预方式。图3 （左）展示了后屏蔽：屏蔽位于智能体和环境之间，持续监控环境状态和智能体选择的操作。

</details>


### [29] [Nature2025 | <em class="highlight">强化学习</em>可以自我改变搜索空间了？](http://mp.weixin.qq.com/s?__biz=Mzg5NzExODk0MQ==&mid=2247489086&idx=1&sn=0be8802a931449f086ff50544ccbde78&chksm=c1528fa8e6ffecbdfb96ac7b233c6830a420f0ec740156d30b99d0828e468751a20c1091e037#rd)
*小小何先生*

Main category: wechat.article

TL;DR: 这篇文章介绍了一种名为 DiscoRL 的强化学习（RL）规则，该规则是通过元学习（Meta-Learning）方法自主发现的，其性能超越了目前手动设计的RL算法（如MuZero、PPO）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这篇文章介绍了一种名为 DiscoRL 的强化学习（RL）规则，该规则是通过元学习（Meta-Learning）方法自主发现的，其性能超越了目前手动设计的RL算法（如MuZero、PPO）。

</details>


### [30] [DRL圣经2025最新版-《<em class="highlight">强化学习</em>:导论第二版》免费pdf分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571704&idx=3&sn=90a38b03525d3bd46364d11ad825259a&chksm=96dedda4fbe5e0e7c8bde537e0be86258326926473cd0723c0011d56b24abeb437ae349eb69b#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection](https://arxiv.org/abs/2510.20963)
*Yongqiang Chen,Gang Niu,James Cheng,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出ColMAD协作式多智能体辩论协议，将辩论重构为非零和博弈，通过智能体间的支持性批评来改善错误检测性能，显著优于竞争性辩论方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论方法将辩论视为零和博弈，导致辩论黑客行为——智能体为获胜而误导裁判，反而引入更多错误。需要一种协作式辩论方法来提升错误检测效果。

Method: ColMAD协议将多智能体辩论重构为非零和博弈，鼓励智能体以支持性方式相互批评，补充彼此遗漏的观点，为裁判提供更全面的证据。

Result: ColMAD在错误检测任务中显著优于之前的竞争性多智能体辩论方法19%，相比单智能体方法也带来了非平凡的性能提升。

Conclusion: 协作式多智能体辩论协议能有效缓解辩论黑客问题，通过智能体间的支持性批评显著提升错误检测性能。

Abstract: Accurate detection of errors in large language models (LLM) responses is
central to the success of scalable oversight, or providing effective
supervision to superhuman intelligence. Yet, self-diagnosis is often unreliable
on complex tasks unless aided by reliable external feedback. Multi-agent debate
(MAD) seems to be a natural alternative to external feedback: multiple LLMs
provide complementary perspectives and cross-checks for error detection.
However, prior MAD protocols frame debate as a zero-sum game, where the
debaters compete to win the game instead of seeking the truth. Consequently, it
leads to debate hacking: debaters tend to mislead the judge by misinterpreting
the task or presenting overconfident claims, which introduce more mistakes and
underperform single-agent methods. To mitigate the issue, we introduce a new
collaborative MAD protocol, termed ColMAD, that reframes MAD as a non-zero sum
game. Specifically, ColMAD encourages multiple agents to criticize each other
in a supportive way, such that they can complement the missing points of each
other. Therefore, the judge agent can make a more informative conclusion based
on more comprehensive evidence. Empirically, we show that ColMAD significantly
outperforms previous competitive MAD by 19% and brings non-trivial improvements
over single-agent methods in error detection.

</details>


### [32] [On the Sample Complexity of Differentially Private Policy Optimization](https://arxiv.org/abs/2510.21060)
*Yi He,Xingyu Zhou*

Main category: cs.LG

TL;DR: 本文首次对差分隐私策略优化的样本复杂度进行理论研究，为隐私保护策略优化算法提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 随着策略优化在敏感领域（如机器人、医疗和大语言模型训练）的广泛应用，隐私保护问题日益突出，需要研究差分隐私下的策略优化方法。

Method: 首先形式化适合策略优化的差分隐私定义，然后系统分析策略梯度、自然策略梯度等常用策略优化算法在差分隐私约束下的样本复杂度。

Result: 理论结果表明隐私成本通常表现为样本复杂度的低阶项，同时揭示了隐私策略优化设置中的微妙但重要的观察。

Conclusion: 该研究为隐私保护策略优化算法提供了有价值的实践见解，证明了在差分隐私约束下实现高效策略优化的可行性。

Abstract: Policy optimization (PO) is a cornerstone of modern reinforcement learning
(RL), with diverse applications spanning robotics, healthcare, and large
language model training. The increasing deployment of PO in sensitive domains,
however, raises significant privacy concerns. In this paper, we initiate a
theoretical study of differentially private policy optimization, focusing
explicitly on its sample complexity. We first formalize an appropriate
definition of differential privacy (DP) tailored to PO, addressing the inherent
challenges arising from on-policy learning dynamics and the subtlety involved
in defining the unit of privacy. We then systematically analyze the sample
complexity of widely-used PO algorithms, including policy gradient (PG),
natural policy gradient (NPG) and more, under DP constraints and various
settings, via a unified framework. Our theoretical results demonstrate that
privacy costs can often manifest as lower-order terms in the sample complexity,
while also highlighting subtle yet important observations in private PO
settings. These offer valuable practical insights for privacy-preserving PO
algorithms.

</details>


### [33] [The Virtues of Brevity: Avoid Overthinking in Parallel Test-Time Reasoning](https://arxiv.org/abs/2510.21067)
*Raul Cavalcante Dinardi,Bruno Yamamoto,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 研究发现选择最短答案的简单启发式方法在提升LLM推理性能方面与复杂方法相当，但计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有并行测试时计算方法需要复杂评分，增加了计算成本和复杂性，需要寻找更高效的策略。

Method: 提出选择最短答案的简单启发式方法，认为模型在简洁自信的常规模式和冗长过度思考模式中运行，通过选择最短答案优先采样常规模式。

Result: 该方法在两个挑战性基准测试中与自一致性等复杂方法竞争，同时显著降低计算开销。

Conclusion: 最短答案启发式方法提供了相对于自一致性的帕累托改进，即使在输出相等性定义不明确的任务中也适用。

Abstract: Reasoning models represent a significant advance in LLM capabilities,
particularly for complex reasoning tasks such as mathematics and coding.
Previous studies confirm that parallel test-time compute-sampling multiple
solutions and selecting the best one-can further enhance the predictive
performance of LLMs. However, strategies in this area often require complex
scoring, thus increasing computational cost and complexity. In this work, we
demonstrate that the simple and counterintuitive heuristic of selecting the
shortest solution is highly effective. We posit that the observed effectiveness
stems from models operating in two distinct regimes: a concise, confident
conventional regime and a verbose overthinking regime characterized by
uncertainty, and we show evidence of a critical point where the overthinking
regime begins to be significant. By selecting the shortest answer, the
heuristic preferentially samples from the conventional regime. We confirm that
this approach is competitive with more complex methods such as self-consistency
across two challenging benchmarks while significantly reducing computational
overhead. The shortest-answer heuristic provides a Pareto improvement over
self-consistency and applies even to tasks where output equality is not well
defined.

</details>


### [34] [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)
*Stephen Zhao,Aidan Li,Rob Brekelmans,Roger Grosse*

Main category: cs.LG

TL;DR: RePULSe是一种新的训练方法，通过在学习过程中增加额外损失函数来减少不良输出的概率，同时保持平均奖励性能。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习方法优化平均奖励，但减少不良输出概率的方法通常会影响平均性能。RePULSe旨在改善这种权衡。

Method: 在标准RL损失基础上增加额外损失，使用学习到的提议来指导采样低奖励输出，然后降低这些输出的概率。

Result: 实验表明RePULSe在期望奖励与不良输出概率之间提供了更好的权衡，并且比标准RL对齐方法更具对抗鲁棒性。

Conclusion: RePULSe方法能够有效平衡平均奖励性能与减少不良输出概率的需求，提供更好的对齐效果。

Abstract: Reinforcement learning (RL) has become a predominant technique to align
language models (LMs) with human preferences or promote outputs which are
deemed to be desirable by a given reward function. Standard RL approaches
optimize average reward, while methods explicitly focused on reducing the
probability of undesired outputs typically come at a cost to average-case
performance. To improve this tradeoff, we introduce RePULSe, a new training
method that augments the standard RL loss with an additional loss that uses
learned proposals to guide sampling low-reward outputs, and then reduces those
outputs' probability. We run experiments demonstrating that RePULSe produces a
better tradeoff of expected reward versus the probability of undesired outputs
and is more adversarially robust, compared to standard RL alignment approaches
and alternatives.

</details>


### [35] [Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer Reviews](https://arxiv.org/abs/2510.21192)
*Luca Demetrio,Giovanni Apruzzese,Kathrin Grosse,Pavel Laskov,Emil Lupu,Vera Rimmer,Philine Widmer*

Main category: cs.LG

TL;DR: GenReview是迄今为止最大的包含LLM撰写评审的数据集，包含81K条针对ICLR 2018-2025年所有提交论文的评审，使用负面、正面和中性三种提示生成，并与原始论文和评审关联。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在科学同行评审中的潜在使用增加，需要全面数据集来理解其效用和影响，此前缺乏这样的数据集。

Method: 构建GenReview数据集，为ICLR 2018-2025所有提交论文使用三种独立提示（负面、正面、中性）生成LLM评审，并与原始论文和评审建立关联。

Result: LLMs在评审中存在偏见；LLM撰写的评审目前可被自动检测；LLMs不能始终严格遵循评审指南；LLM提供的评分仅与已接受论文的决策一致。

Conclusion: GenReview数据集填补了LLM在科学评审中应用研究的空白，支持广泛的研究调查，揭示了LLMs在评审中的局限性和潜在问题。

Abstract: How does the progressive embracement of Large Language Models (LLMs) affect
scientific peer reviewing? This multifaceted question is fundamental to the
effectiveness -- as well as to the integrity -- of the scientific process.
Recent evidence suggests that LLMs may have already been tacitly used in peer
reviewing, e.g., at the 2024 International Conference of Learning
Representations (ICLR). Furthermore, some efforts have been undertaken in an
attempt to explicitly integrate LLMs in peer reviewing by various editorial
boards (including that of ICLR'25). To fully understand the utility and the
implications of LLMs' deployment for scientific reviewing, a comprehensive
relevant dataset is strongly desirable. Despite some previous research on this
topic, such dataset has been lacking so far. We fill in this gap by presenting
GenReview, the hitherto largest dataset containing LLM-written reviews. Our
dataset includes 81K reviews generated for all submissions to the 2018--2025
editions of the ICLR by providing the LLM with three independent prompts: a
negative, a positive, and a neutral one. GenReview is also linked to the
respective papers and their original reviews, thereby enabling a broad range of
investigations. To illustrate the value of GenReview, we explore a sample of
intriguing research questions, namely: if LLMs exhibit bias in reviewing (they
do); if LLM-written reviews can be automatically detected (so far, they can);
if LLMs can rigorously follow reviewing instructions (not always) and whether
LLM-provided ratings align with decisions on paper acceptance or rejection
(holds true only for accepted papers). GenReview can be accessed at the
following link: https://anonymous.4open.science/r/gen_review.

</details>


### [36] [How Hard is it to Confuse a World Model?](https://arxiv.org/abs/2510.21232)
*Waris Radji,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中构建最混淆实例的问题，提出了一种对抗训练方法来解决神经网络世界模型中的这一挑战。


<details>
  <summary>Details</summary>
Motivation: 在强化学习理论中，最混淆实例对于建立遗憾下界至关重要，但在一般情况下构建这样的实例仍然是一个开放性问题。

Method: 将问题形式化为约束优化：寻找统计上接近参考模型的修改模型，同时使最优和次优策略产生不同的性能表现。提出了一种对抗训练程序来解决这个问题。

Result: 实证研究表明，可实现的混淆程度与近似模型中的不确定性相关。

Conclusion: 研究结果可能为深度基于模型的强化学习提供理论上有依据的探索策略。

Abstract: In reinforcement learning (RL) theory, the concept of most confusing
instances is central to establishing regret lower bounds, that is, the minimal
exploration needed to solve a problem. Given a reference model and its optimal
policy, a most confusing instance is the statistically closest alternative
model that makes a suboptimal policy optimal. While this concept is
well-studied in multi-armed bandits and ergodic tabular Markov decision
processes, constructing such instances remains an open question in the general
case. In this paper, we formalize this problem for neural network world models
as a constrained optimization: finding a modified model that is statistically
close to the reference one, while producing divergent performance between
optimal and suboptimal policies. We propose an adversarial training procedure
to solve this problem and conduct an empirical study across world models of
varying quality. Our results suggest that the degree of achievable confusion
correlates with uncertainty in the approximate model, which may inform
theoretically-grounded exploration strategies for deep model-based RL.

</details>


### [37] [DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection](https://arxiv.org/abs/2510.21638)
*Tala Aljaafari,Varun Kanade,Philip Torr,Christian Schroeder de Witt*

Main category: cs.LG

TL;DR: DEEDEE是一个用于强化学习时间序列的OOD检测器，使用两个统计量（episodewise均值和RBF核相似度）来捕获全局和局部偏差，在保持高性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中部署强化学习受到分布偏移脆弱性的限制，需要有效的OOD检测方法。

Method: 提出DEEDEE检测器，仅使用episodewise均值和RBF核相似度来训练摘要，捕获互补的全局和局部偏差。

Result: DEEDEE在标准RL OOD测试套件中匹配或超越当代检测器，计算量减少600倍，绝对准确率平均提升5%。

Conclusion: 结果表明，各种异常类型通常通过少量低阶统计量在RL轨迹中留下印记，为复杂环境中的OOD检测提供了紧凑基础。

Abstract: Deploying reinforcement learning (RL) in safety-critical settings is
constrained by brittleness under distribution shift. We study
out-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a
two-statistic detector that revisits representation-heavy pipelines with a
minimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel
similarity to a training summary, capturing complementary global and local
deviations. Despite its simplicity, DEEDEE matches or surpasses contemporary
detectors across standard RL OOD suites, delivering a 600-fold reduction in
compute (FLOPs / wall-time) and an average 5% absolute accuracy gain over
strong baselines. Conceptually, our results indicate that diverse anomaly types
often imprint on RL trajectories through a small set of low-order statistics,
suggesting a compact foundation for OOD detection in complex environments.

</details>


### [38] [Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems](https://arxiv.org/abs/2510.21427)
*Hao Liang,Shuqing Shi,Yudi Zhang,Biwei Huang,Yali Du*

Main category: cs.LG

TL;DR: GSAC框架结合因果表示学习和元演员-评论家学习，通过稀疏局部因果掩码和近似紧凑表示实现大规模网络系统的可扩展性和领域泛化。


<details>
  <summary>Details</summary>
Motivation: 解决大规模网络系统（如交通、电力、无线网络）中强化学习面临的规模和环境变化挑战。

Method: 每个智能体学习稀疏局部因果掩码识别影响其动态的最小邻域变量，生成状态和领域因子的近似紧凑表示，然后通过元演员-评论家在多个源域上训练共享策略。

Result: GSAC能够快速适应新领域，显著优于从头学习和传统适应基线方法。

Conclusion: GSAC框架在因果恢复、演员-评论家收敛和适应差距方面建立了有限样本保证，证明其在大规模网络系统中的有效性。

Abstract: Large-scale networked systems, such as traffic, power, and wireless grids,
challenge reinforcement-learning agents with both scale and environment shifts.
To address these challenges, we propose GSAC (Generalizable and Scalable
Actor-Critic), a framework that couples causal representation learning with
meta actor-critic learning to achieve both scalability and domain
generalization. Each agent first learns a sparse local causal mask that
provably identifies the minimal neighborhood variables influencing its
dynamics, yielding exponentially tight approximately compact representations
(ACRs) of state and domain factors. These ACRs bound the error of truncating
value functions to $\kappa$-hop neighborhoods, enabling efficient learning on
graphs. A meta actor-critic then trains a shared policy across multiple source
domains while conditioning on the compact domain factors; at test time, a few
trajectories suffice to estimate the new domain factor and deploy the adapted
policy. We establish finite-sample guarantees on causal recovery, actor-critic
convergence, and adaptation gap, and show that GSAC adapts rapidly and
significantly outperforms learning-from-scratch and conventional adaptation
baselines.

</details>


### [39] [Unified token representations for sequential decision models](https://arxiv.org/abs/2510.21448)
*Zhuojing Tian,Yushu Chen*

Main category: cs.LG

TL;DR: 提出统一令牌表示(UTR)方法，将回报、状态和动作合并为单个令牌，显著减少序列长度和模型复杂度，在保持性能的同时大幅降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的离线强化学习方法存在令牌冗余和二次注意力复杂度问题，限制了在实时或资源受限环境中的可扩展性。

Method: 开发了UTR方法，将回报、状态和动作合并为单个令牌，并基于此构建了UDT(Transformer架构)和UDC(门控CNN架构)两个变体。

Result: UTR方法在保持或超越现有最优方法性能的同时，显著降低了计算复杂度。理论分析显示UTR具有更紧的Rademacher复杂度界限，表明更好的泛化能力。

Conclusion: UTR方法在不同架构上都能良好泛化，为未来大规模决策模型的可扩展控制提供了高效基础。

Abstract: Transformers have demonstrated strong potential in offline reinforcement
learning (RL) by modeling trajectories as sequences of return-to-go, states,
and actions. However, existing approaches such as the Decision Transformer(DT)
and its variants suffer from redundant tokenization and quadratic attention
complexity, limiting their scalability in real-time or resource-constrained
settings. To address this, we propose a Unified Token Representation (UTR) that
merges return-to-go, state, and action into a single token, substantially
reducing sequence length and model complexity. Theoretical analysis shows that
UTR leads to a tighter Rademacher complexity bound, suggesting improved
generalization. We further develop two variants: UDT and UDC, built upon
transformer and gated CNN backbones, respectively. Both achieve comparable or
superior performance to state-of-the-art methods with markedly lower
computation. These findings demonstrate that UTR generalizes well across
architectures and may provide an efficient foundation for scalable control in
future large decision models.

</details>


### [40] [Excision Score: Evaluating Edits with Surgical Precision](https://arxiv.org/abs/2510.21537)
*Nikolai Gruzinov,Ksenia Sycheva,Earl T. Barr,Alex Bezzubov*

Main category: cs.LG

TL;DR: 提出了一种新的修订相似度度量方法——切除分数(ES)，通过移除现有文档与修订版本之间的共享内容，专注于比较差异区域，解决了传统相似度度量方法因共享内容主导而无法准确反映人类判断的问题。


<details>
  <summary>Details</summary>
Motivation: 现有相似度度量方法(如BLEU)在处理文档修订任务时存在根本缺陷，它们的高分主要由共享内容决定，而人类判断更关注实际变化部分。这导致在人类认为差异很大时，这些方法仍报告高相似度。

Method: 提出切除分数(ES)方法：1)使用最长公共子序列(LCS)识别并移除现有文档与修订版本之间的共享内容；2)仅比较剩余的不同区域；3)通过近似算法将标准立方LCS计算加速到二次复杂度。

Result: 在代码编辑评估中，ES显著优于现有方法：在HumanEvalFix上与测试执行对齐时，比最接近的竞争对手SARI提高了12%的皮尔逊相关性，比标准方法(如BLEU)提高了>21%。当增加共享上下文时，ES的改进进一步增加到20%和>30%。

Conclusion: ES通过关注实际变化区域而非共享内容，能更好地与人类判断对齐，特别适用于代码编辑等修订任务，解决了传统相似度度量的根本缺陷。

Abstract: Many tasks revolve around editing a document, whether code or text. We
formulate the revision similarity problem to unify a wide range of machine
learning evaluation problems whose goal is to assess a revision to an existing
document. We observe that revisions usually change only a small portion of an
existing document, so the existing document and its immediate revisions share a
majority of their content. We formulate five adequacy criteria for revision
similarity measures, designed to align them with human judgement. We show that
popular pairwise measures, like BLEU, fail to meet these criteria, because
their scores are dominated by the shared content. They report high similarity
between two revisions when humans would assess them as quite different. This is
a fundamental flaw we address. We propose a novel static measure, Excision
Score (ES), which computes longest common subsequence (LCS) to remove content
shared by an existing document with the ground truth and predicted revisions,
before comparing only the remaining divergent regions. This is analogous to a
surgeon creating a sterile field to focus on the work area. We use
approximation to speed the standard cubic LCS computation to quadratic. In
code-editing evaluation, where static measures are often used as a cheap proxy
for passing tests, we demonstrate that ES surpasses existing measures. When
aligned with test execution on HumanEvalFix, ES improves over its nearest
competitor, SARI, by 12% Pearson correlation and by >21% over standard measures
like BLEU. The key criterion is invariance to shared context; when we perturb
HumanEvalFix with increased shared context, ES' improvement over SARI increases
to 20% and >30% over standard measures. ES also handles other corner cases that
other measures do not, such as correctly aligning moved code blocks, and
appropriately rewarding matching insertions or deletions.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [41] [Web Codegen Scorer](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fangular%2Fweb-codegen-scorer%3Futm_source=tldrwebdev/1/0100019a15e9a9a3-bd3e7151-a193-418e-9858-13fa3953167c-000000/HC0XE17PT9V4Xa5yjyuf-bZeKlV9ilLaAc4kENQa6-U=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Web Codegen Scorer是谷歌Angular团队开发的工具，用于评估LLM生成的web代码质量，支持模型比较、提示词迭代和代码质量监控。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成领域的广泛应用，需要专门工具来评估生成的web代码质量，确保代码符合构建、安全和最佳实践标准。

Method: 开发了一个专门的评分工具，内置构建成功性、安全性和编码最佳实践等检查项，支持用户比较不同模型、迭代提示词并监控代码质量变化。

Result: 该工具能够有效评估web代码生成质量，帮助开发者选择更优的模型和提示词策略。

Conclusion: Web Codegen Scorer为LLM生成的web代码提供了标准化的质量评估框架，有助于提升代码生成的整体质量。

Abstract: Web Codegen Scorer (GitHub Repo) Web Codegen Scorer is a tool developed by the Angular team at Google for evaluating the quality of web code generated by LLMs. It focuses on web code quality and allows users to compare models, iterate on prompts, and monitor code quality over time using built-in checks like build success, security, and coding best practices.

</details>


### [42] [Bubble Launches AI Agent to Merge Conversational Coding with Visual App Building](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F10%2F16%2Fbubble-launches-ai-agent-merge-conversational-coding-visual-app-building%2F%3Futm_source=tldrdesign/1/0100019a162a486e-b6b68292-e01b-4da9-9282-a44dbcb5030d-000000/dZIkdt1Q2GeURXpH7TleobD0syfRHTalIPpAeNzigEc=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Bubble AI Agent 结合对话式AI编程和可视化拖拽编辑，构建生产级应用程序，解决限制AI编程工具采用的安全和质量问题。


<details>
  <summary>Details</summary>
Motivation: 针对72%的用户担心AI生成代码存在安全漏洞的问题，旨在解决AI编程工具采用中的安全性和质量障碍。

Method: 将对话式AI编程与可视化拖拽编辑相结合，允许用户在自然语言提示和可视化编辑之间切换，同时保持完全控制。

Result: 开发了一个能够构建生产级应用程序的平台，解决了AI生成代码的安全和质量问题。

Conclusion: Bubble AI Agent成功融合了对话式编程和可视化编辑，为AI编程工具的安全采用提供了可行解决方案。

Abstract: Bubble Launches AI Agent to Merge Conversational Coding with Visual App Building (2 minute read) Bubble AI Agent combines conversational AI coding with visual drag-and-drop editing to build production-grade applications, addressing security and quality concerns that limit AI coding tool adoption. The platform allows users to switch between natural language prompts and visual editing with full control, targeting the 72% of surveyed users worried about security vulnerabilities in AI-generated c...

</details>


### [43] [Vibe Coding's Real Problem Isn't Bugs—It's Judgment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fvibe-codings-real-problem-isnt-bugs-its-judgment%2F%3Futm_source=tldrinfosec/1/0100019a16549147-5eae1088-369e-4a95-8b1b-4f6058788190-000000/v_D1NDZtIjjQQ1qT11bMH_ESUfWhhZfzVWuDpUVUCHE=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代码生成虽然能快速大规模创建软件，但真正的风险在于速度过快和判断失误，而非代码漏洞本身。AI生成代码的漏洞率与人工代码相似，但无效编码实践和跳过代码审查等问题导致更多安全暴露风险。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成技术带来的实际风险，强调速度过快和判断失误比代码漏洞本身更危险，需要关注安全实践集成。

Method: 通过分析AI生成代码与人工代码的漏洞率对比，识别AI代码生成过程中的风险因素，如无效编码实践和跳过代码审查等。

Result: AI生成代码的漏洞率与人类代码相似，但由于编码实践问题和审查缺失，导致整体安全暴露风险更高。

Conclusion: 专家建议将安全直接集成到AI工作流程中，并培养最佳使用实践来应对AI代码生成的风险。

Abstract: Vibe Coding's Real Problem Isn't Bugs—It's Judgment (3 minute read) AI code generation enables rapid, large-scale software creation, but speed and flawed judgment are the real risks. While AI-made code has vulnerability rates similar to human work, issues like ineffective coding practices and skipped reviews mean more exposure to breaches, so experts urge integrating security directly into AI workflows and cultivating best usage practices.

</details>


### [44] [LLM Exchange Rates Updated](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farctotherium.substack.com%2Fp%2Fllm-exchange-rates-updated%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/vxIIQiUmsKV5CkmMmUdyCO2Ssi-qPiW7DHxLbL3FBE0=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 测试当前LLM隐含价值系统发现，几乎所有模型都更重视非白种人而非白种人，更重视女性而非男性。Claude Sonnet 4.5拯救白种人的价值仅为南亚人的1/18，GPT-5几乎完全平等主义，但白种人价值仅为非白种人的1/20。


<details>
  <summary>Details</summary>
Motivation: 研究当前大型语言模型中存在的隐含价值偏见，特别是对不同种族和性别的价值判断差异。

Method: 通过发送数千个查询，比较假设场景如"获得X美元vs拯救Y人"，分析LLM在不同种族和性别群体间的价值权衡。

Result: 几乎所有测试的LLM都显示出明显的价值偏见：更重视非白种人而非白种人，更重视女性而非男性。Claude Sonnet 4.5对白种人的价值评估仅为南亚人的1/18，GPT-5虽然接近完全平等主义，但仍将白种人价值评估为非白种人的1/20。

Conclusion: 当前主流LLM存在显著的种族和性别价值偏见，需要在模型训练和评估中更加关注这些隐含的价值系统问题。

Abstract: LLM Exchange Rates Updated (10 minute read) Testing implicit value systems across current LLMs reveals that almost all models value nonwhites over whites and women over men by large margins—Claude Sonnet 4.5 values saving whites from terminal illness at 1/18th the level of South Asians, while GPT-5 shows near-perfect egalitarianism except for whites valued at 1/20th nonwhites. The methodology involved sending thousands of queries comparing hypothetical scenarios like "receive $X vs. save Y pe...

</details>


### [45] [Automating Algorithm Discovery: A Case Study in MoE Load Balancing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadrs-ucb.notion.site%2Fmoe-load-balancing%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/7zp-ugIFaQslw7u9QnM7dg02POOtwDLXTeUTbYemaGQ=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenEvolve是一个进化编码代理，将大语言模型转化为自主代码优化器，能够发现突破性算法。在测试中，它独立发现并超越了人类专家设计的高度优化算法，在LLM推理中实现了5.0倍加速。


<details>
  <summary>Details</summary>
Motivation: 自动化算法发现过程，减少对人工专家设计的依赖，通过AI驱动的研究来改进系统性能。

Method: 使用进化编码代理方法，将大语言模型转化为自主代码优化器，通过进化过程发现优化算法。

Result: 在MoE负载平衡案例研究中，独立发现了超越人类专家设计的算法，在LLM推理中实现了5.0倍加速。

Conclusion: AI驱动的系统研究具有发现复杂计算策略的能力，证明了自动化算法发现的可行性。

Abstract: Automating Algorithm Discovery: A Case Study in MoE Load Balancing (7 minute read) OpenEvolve is an evolutionary coding agent that turns large language models (LLMs) into autonomous code optimizers that can discover breakthrough algorithms. In tests, it independently discovers and surpasses highly optimized algorithms engineered by human experts to achieve a 5.0x speedup in LLM inference. The ability to devise sophisticated computational strategies proves that AI-Driven Research for Systems c...

</details>


### [46] [Rediscovering the SUPER in Supercomputing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgheiler.com%2Fpost%2Fdagster-slurm%2F%3Futm_source=tldrdata/1/0100019a2522715b-cc849310-f30a-47c5-abce-3b061c55b7cc-000000/7GGEfuZQsVqx-m1gaXpPQSe4mEpTB_rlUlh7U0uUce4=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: dagster-slurm将Dagster的现代数据编排与Slurm管理的HPC集群集成，实现从笔记本电脑到超级计算机的无缝工作流移植


<details>
  <summary>Details</summary>
Motivation: 解决HPC环境中工作流在不同计算平台间移植困难的问题，实现从开发环境到生产环境的无缝过渡

Method: 开发dagster-slurm集成工具，将Dagster编排系统与Slurm作业调度系统连接，提供统一的接口

Result: 实现了工作流在笔记本电脑、CI流水线和Tier-0超级计算机之间的无缝移植，无需代码更改

Conclusion: 该集成工具成功弥合了现代数据编排与传统HPC系统之间的鸿沟，提高了工作流部署的灵活性和效率

Abstract: Rediscovering the SUPER in Supercomputing (2 minute read) dagster-slurm integrates Dagster's modern data orchestration with Slurm-managed HPC clusters, enabling seamless workflow portability across laptops, CI pipelines, and Tier-0 supercomputers without code changes.

</details>


### [47] [Code like a surgeon](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrnewsletter/1/0100019a253164a1-79150c9a-3824-4247-9309-78b4e25e1aeb-000000/84in06qL4xDhBQ2NiebZODChtyPwsMBgemRCr768xDc=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码工具让开发者能专注于重要工作，就像外科医生有支持团队处理辅助任务一样


<details>
  <summary>Details</summary>
Motivation: 开发者在编码过程中需要处理大量辅助性任务，这些任务占用了他们专注于核心开发工作的时间

Method: 利用AI代理来处理编码中的次要任务，让开发者专注于重要的核心工作

Result: AI工具现在已足够成熟，能够有效协助处理编码中的辅助任务

Conclusion: AI编码工具可以显著提高开发效率，让开发者像外科医生一样专注于核心技能

Abstract: Code like a surgeon (4 minute read) Surgeons do the actual work. However, their skills and time are highly leveraged with a support team that handles prep, secondary tasks, and admin. The surgeon focuses on the important stuff they're good at. AI coding tools enable developers to spend all of their time doing the things that matter. There are a lot of secondary tasks that AI agents are now good enough to help out with.

</details>
