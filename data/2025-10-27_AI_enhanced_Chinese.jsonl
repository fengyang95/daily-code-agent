{"id": "2510.21031", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21031", "abs": "https://arxiv.org/abs/2510.21031", "authors": ["Qinghua Lu", "Dehai Zhao", "Yue Liu", "Hao Zhang", "Liming Zhu", "Xiwei Xu", "Angela Shi", "Tristan Tan", "Rick Kazman"], "title": "AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents", "comment": null, "summary": "The emergence of foundation models (FMs) has enabled the development of\nhighly capable and autonomous agents, unlocking new application opportunities\nacross a wide range of domains. Evaluating the architecture of agents is\nparticularly important as the architectural decisions significantly impact the\nquality attributes of agents given their unique characteristics, including\ncompound architecture, autonomous and non-deterministic behaviour, and\ncontinuous evolution. However, these traditional methods fall short in\naddressing the evaluation needs of agent architecture due to the unique\ncharacteristics of these agents. Therefore, in this paper, we present\nAgentArcEval, a novel agent architecture evaluation method designed specially\nto address the complexities of FM-based agent architecture and its evaluation.\nMoreover, we present a catalogue of agent-specific general scenarios, which\nserves as a guide for generating concrete scenarios to design and evaluate the\nagent architecture. We demonstrate the usefulness of AgentArcEval and the\ncatalogue through a case study on the architecture evaluation of a real-world\ntax copilot, named Luna.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentArcEval\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u667a\u80fd\u4f53\u7279\u5b9a\u901a\u7528\u573a\u666f\u76ee\u5f55\u6765\u6307\u5bfc\u67b6\u6784\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u667a\u80fd\u4f53\u67b6\u6784\u7684\u72ec\u7279\u7279\u6027\uff0c\u5982\u590d\u5408\u67b6\u6784\u3001\u81ea\u4e3b\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u548c\u6301\u7eed\u6f14\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u67b6\u6784\u3002", "method": "\u5f00\u53d1\u4e86AgentArcEval\u8bc4\u4f30\u65b9\u6cd5\u548c\u667a\u80fd\u4f53\u7279\u5b9a\u901a\u7528\u573a\u666f\u76ee\u5f55\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7a0e\u52a1\u52a9\u624bLuna\u7684\u6848\u4f8b\u7814\u7a76\u6765\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86AgentArcEval\u65b9\u6cd5\u548c\u573a\u666f\u76ee\u5f55\u5728\u8bc4\u4f30\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u67b6\u6784\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "AgentArcEval\u4e3a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u67b6\u6784\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e13\u95e8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.21094", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21094", "abs": "https://arxiv.org/abs/2510.21094", "authors": ["Yao Lu", "Wanwei Liu", "Tanghaoran Zhang", "Kang Yang", "Yang Zhang", "Wenyu Xu", "Longfei Sun", "Xinjun Mao", "Shuzheng Gao", "Michael R. Lyu"], "title": "BDiff: Block-aware and Accurate Text-based Code Differencing", "comment": null, "summary": "Code differencing is a fundamental technique in software engineering practice\nand research. While researchers have proposed text-based differencing\ntechniques capable of identifying line changes over the past decade, existing\nmethods exhibit a notable limitation in identifying edit actions (EAs) that\noperate on text blocks spanning multiple lines. Such EAs are common in\ndevelopers' practice, such as moving a code block for conditional branching or\nduplicating a method definition block for overloading. Existing tools represent\nsuch block-level operations as discrete sequences of line-level EAs, compelling\ndevelopers to manually correlate them and thereby substantially impeding the\nefficiency of change comprehension. To address this issue, we propose BDiff, a\ntext-based differencing algorithm capable of identifying two types of\nblock-level EAs and five types of line-level EAs. Building on traditional\ndifferencing algorithms, we first construct a candidate set containing all\npossible line mappings and block mappings. Leveraging the Kuhn-Munkres\nalgorithm, we then compute the optimal mapping set that can minimize the size\nof the edit script (ES) while closely aligning with the original developer's\nintent. To validate the effectiveness of BDiff, we selected five\nstate-of-the-art tools, including large language models (LLMs), as baselines\nand adopted a combined qualitative and quantitative approach to evaluate their\nperformance in terms of ES size, result quality, and running time. Experimental\nresults show that BDiff produces higher-quality differencing results than\nbaseline tools while maintaining competitive runtime performance. Our\nexperiments also show the unreliability of LLMs in code differencing tasks\nregarding result quality and their infeasibility in terms of runtime\nefficiency. We have implemented a web-based visual differencing tool.", "AI": {"tldr": "BDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684\u4ee3\u7801\u5dee\u5f02\u5206\u6790\u7b97\u6cd5\uff0c\u80fd\u591f\u8bc6\u522b\u5757\u7ea7\u548c\u884c\u7ea7\u7f16\u8f91\u64cd\u4f5c\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5dee\u5f02\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u5dee\u5f02\u5206\u6790\u5de5\u5177\u5728\u5904\u7406\u8de8\u591a\u884c\u7684\u5757\u7ea7\u7f16\u8f91\u64cd\u4f5c\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u901a\u5e38\u5c06\u5757\u7ea7\u64cd\u4f5c\u8868\u793a\u4e3a\u79bb\u6563\u7684\u884c\u7ea7\u7f16\u8f91\u5e8f\u5217\uff0c\u8fd9\u964d\u4f4e\u4e86\u5f00\u53d1\u4eba\u5458\u7406\u89e3\u4ee3\u7801\u53d8\u66f4\u7684\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u4f20\u7edf\u5dee\u5f02\u5206\u6790\u7b97\u6cd5\uff0c\u9996\u5148\u6784\u5efa\u5305\u542b\u6240\u6709\u53ef\u80fd\u884c\u6620\u5c04\u548c\u5757\u6620\u5c04\u7684\u5019\u9009\u96c6\uff0c\u7136\u540e\u4f7f\u7528Kuhn-Munkres\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u6620\u5c04\u96c6\uff0c\u4ee5\u6700\u5c0f\u5316\u7f16\u8f91\u811a\u672c\u5927\u5c0f\u5e76\u8d34\u8fd1\u5f00\u53d1\u8005\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBDiff\u5728\u5dee\u5f02\u7ed3\u679c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u4e94\u79cd\u6700\u5148\u8fdb\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "BDiff\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5757\u7ea7\u7f16\u8f91\u64cd\u4f5c\uff0c\u63d0\u9ad8\u4ee3\u7801\u53d8\u66f4\u7406\u89e3\u7684\u6548\u7387\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u5dee\u5f02\u5206\u6790\u4efb\u52a1\u4e2d\u7ed3\u679c\u8d28\u91cf\u4e0d\u53ef\u9760\u4e14\u8fd0\u884c\u65f6\u6548\u7387\u4e0d\u53ef\u884c\u3002", "topic": "swe application"}}
{"id": "2510.21413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21413", "abs": "https://arxiv.org/abs/2510.21413", "authors": ["Seyedmoein Mohsenimofidi", "Matthias Galster", "Christoph Treude", "Sebastian Baltes"], "title": "Context Engineering for AI Agents in Open-Source Software", "comment": "6 pages, 1 figure, 2 tables", "summary": "GenAI-based coding assistants have disrupted software development. Their next\ngeneration is agent-based, operating with more autonomy and potentially without\nhuman oversight. One challenge is to provide AI agents with sufficient context\nabout the software projects they operate in. Like humans, AI agents require\ncontextual information to develop solutions that are in line with the target\narchitecture, interface specifications, coding guidelines, standard workflows,\nand other project-specific policies. Popular AI agents for software development\n(e.g., Claude Code) advocate for maintaining tool-specific version-controlled\nMarkdown files that cover aspects such as the project structure, building and\ntesting, or code style. The content of these files is automatically added to\neach prompt. AGENTS.md has emerged as a potential standard that consolidates\ntool-specific formats. However, little is known about whether and how\ndevelopers adopt this format. Therefore, in this paper, we present the results\nof a preliminary study investigating the adoption of AI configuration files in\n466 open-source software projects, what information developers provide in these\nfiles, how they present that information, and how they evolve over time. Our\nfindings indicate that there is no established structure yet, and that there is\na lot of variation in terms of how context is provided (descriptive,\nprescriptive, prohibitive, explanatory, conditional). We see great potential in\nstudying which modifications in structure or presentation can positively affect\nthe quality of the generated content. Finally, our analysis of commits that\nhave modified AGENTS.md files provides first insights into how projects\ncontinuously extend and maintain these files. We conclude the paper by\noutlining how the adoption of AI configuration files in provides a unique\nopportunity to study real-world prompt and context engineering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u914d\u7f6e\u6587\u4ef6\uff08\u7279\u522b\u662fAGENTS.md\uff09\u5728\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u4e2d\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u5206\u6790\u4e86\u5f00\u53d1\u8005\u5982\u4f55\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u9879\u76ee\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u53ca\u8fd9\u4e9b\u6587\u4ef6\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eGenAI\u7684\u7f16\u7801\u52a9\u624b\u5411\u81ea\u4e3b\u4ee3\u7406\u53d1\u5c55\uff0c\u5982\u4f55\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u8db3\u591f\u7684\u9879\u76ee\u4e0a\u4e0b\u6587\u4fe1\u606f\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\u3002AGENTS.md\u4f5c\u4e3a\u6f5c\u5728\u6807\u51c6\u51fa\u73b0\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5b9e\u9645\u91c7\u7528\u60c5\u51b5\u7684\u4e86\u89e3\u3002", "method": "\u5bf9466\u4e2a\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u4e2dAI\u914d\u7f6e\u6587\u4ef6\u7684\u91c7\u7528\u60c5\u51b5\u8fdb\u884c\u521d\u6b65\u7814\u7a76\uff0c\u5206\u6790\u8fd9\u4e9b\u6587\u4ef6\u7684\u5185\u5bb9\u3001\u4fe1\u606f\u5448\u73b0\u65b9\u5f0f\u4ee5\u53ca\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u76ee\u524d\u8fd8\u6ca1\u6709\u5efa\u7acb\u7edf\u4e00\u7684\u7ed3\u6784\uff0c\u5728\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63d0\u4f9b\u65b9\u5f0f\u4e0a\u5b58\u5728\u5f88\u5927\u5dee\u5f02\uff08\u63cf\u8ff0\u6027\u3001\u89c4\u5b9a\u6027\u3001\u7981\u6b62\u6027\u3001\u89e3\u91ca\u6027\u3001\u6761\u4ef6\u6027\uff09\u3002", "conclusion": "AI\u914d\u7f6e\u6587\u4ef6\u5728\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u91c7\u7528\u4e3a\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\uff0c\u7814\u7a76\u6587\u4ef6\u7ed3\u6784\u6216\u5448\u73b0\u65b9\u5f0f\u7684\u4fee\u6539\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u5177\u6709\u5f88\u5927\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.20909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20909", "abs": "https://arxiv.org/abs/2510.20909", "authors": ["Cedegao E. Zhang", "C\u00e9dric Colas", "Gabriel Poesia", "Joshua B. Tenenbaum", "Jacob Andreas"], "title": "Code-enabled language models can outperform reasoning models on diverse tasks", "comment": null, "summary": "Reasoning models (RMs), language models (LMs) trained with reinforcement\nlearning to produce long-form natural language reasoning, have been remarkably\nsuccessful, but they still require large amounts of computation and data to\ntrain, and can be slow and expensive to run. In this paper, we show that\nstandard instruct LMs can already be elicited to be strong reasoners at a level\ncomparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs\nR1) without finetuning, across diverse domains from instruction following and\ncreative generation to mathematical reasoning. This is achieved by CodeAdapt,\nour simple recipe that combines the CodeAct framework, where LMs interleave\nnatural language reasoning with code execution in a multi-step fashion, with\nfew-shot bootstrap in-context learning from as few as five training problems.\nAnalyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables\nthree LMs to outperform the corresponding RMs on average over eight tasks (up\nto 22.9%) while being 10-81% more token efficient, and delivers superior\nperformance on six tasks when averaged over the four models (up to 35.7%).\nFurthermore, the code-augmented reasoning traces display rich and varied\nproblem-solving strategies. Our findings support that (1) CodeAdapt-style\nlearning and reasoning may be robust and domain general and (2) code-enabled\nLMs are cognitively grounded and powerful systems, potentially providing a\nstrong foundation for in-weight reinforcement learning.", "AI": {"tldr": "CodeAdapt\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408CodeAct\u6846\u67b6\u548c\u5c11\u91cf\u6837\u672c\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f7f\u6807\u51c6\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u95e8\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8token\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u8bad\u7ec3\uff0c\u8fd0\u884c\u6210\u672c\u9ad8\u6602\u4e14\u901f\u5ea6\u8f83\u6162\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u6807\u51c6\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u3002", "method": "\u63d0\u51faCodeAdapt\u65b9\u6cd5\uff0c\u7ed3\u5408CodeAct\u6846\u67b6\uff08\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7a7f\u63d2\u4ee3\u7801\u6267\u884c\uff09\u548c\u5c11\u91cf\u6837\u672c\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4ec5\u97005\u4e2a\u8bad\u7ec3\u95ee\u9898\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728\u56db\u4e2a\u5339\u914d\u7684\u8bed\u8a00\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u5bf9\u4e2d\uff0cCodeAdapt\u4f7f\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u5728\u516b\u4e2a\u4efb\u52a1\u4e0a\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u5bf9\u5e94\u63a8\u7406\u6a21\u578b\uff08\u6700\u9ad8\u63d0\u534722.9%\uff09\uff0ctoken\u6548\u7387\u63d0\u9ad810-81%\uff0c\u5728\u516d\u4e2a\u4efb\u52a1\u4e0a\u56db\u4e2a\u6a21\u578b\u5e73\u5747\u8868\u73b0\u66f4\u4f18\uff08\u6700\u9ad8\u63d0\u534735.7%\uff09\u3002", "conclusion": "CodeAdapt\u98ce\u683c\u7684\u5b66\u4e60\u548c\u63a8\u7406\u5177\u6709\u9c81\u68d2\u6027\u548c\u9886\u57df\u901a\u7528\u6027\uff0c\u4ee3\u7801\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u662f\u8ba4\u77e5\u57fa\u7840\u826f\u597d\u4e14\u5f3a\u5927\u7684\u7cfb\u7edf\uff0c\u53ef\u80fd\u4e3a\u6743\u91cd\u5185\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2510.21460", "categories": ["cs.SE", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21460", "abs": "https://arxiv.org/abs/2510.21460", "authors": ["Sean McGregor", "Victor Lu", "Vassil Tashev", "Armstrong Foundjem", "Aishwarya Ramasethu", "Sadegh AlMahdi Kazemi Zarkouei", "Chris Knotz", "Kongtao Chen", "Alicia Parrish", "Anka Reuel", "Heather Frase"], "title": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk", "comment": "19 pages, 7 figures, to be published in the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations.", "AI": {"tldr": "\u63d0\u51fa\u4e86BenchRisk\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u98ce\u9669\uff0c\u8bc6\u522b\u4e8657\u79cd\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\u548c196\u79cd\u7f13\u89e3\u7b56\u7565\uff0c\u5e2e\u52a9\u7528\u6237\u907f\u514d\u57fa\u4e8e\u4e0d\u53ef\u9760\u57fa\u51c6\u505a\u51fa\u9519\u8bef\u51b3\u7b56\u3002", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5404\u79cd\u6545\u969c\u6a21\u5f0f\uff0c\u53ef\u80fd\u5f71\u54cd\u57fa\u51c6\u7684\u504f\u5dee\u3001\u65b9\u5dee\u3001\u8986\u76d6\u8303\u56f4\u548c\u53ef\u7406\u89e3\u6027\uff0c\u5bfc\u81f4\u7528\u6237\u57fa\u4e8e\u4e0d\u53ef\u9760\u8bc1\u636e\u505a\u51fa\u9519\u8bef\u7684LLM\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u57fa\u4e8eNIST\u98ce\u9669\u7ba1\u7406\u6d41\u7a0b\uff0c\u8fed\u4ee3\u5206\u679026\u4e2a\u6d41\u884c\u57fa\u51c6\uff0c\u8bc6\u522b\u6545\u969c\u6a21\u5f0f\u5e76\u5f00\u53d1\u7f13\u89e3\u7b56\u7565\uff0c\u6784\u5efaBenchRisk\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u98ce\u9669\u3002", "result": "\u6240\u670926\u4e2a\u57fa\u51c6\u5728\u4e94\u4e2a\u7ef4\u5ea6\uff08\u5168\u9762\u6027\u3001\u53ef\u7406\u89e3\u6027\u3001\u4e00\u81f4\u6027\u3001\u6b63\u786e\u6027\u3001\u6301\u4e45\u6027\uff09\u4e2d\u81f3\u5c11\u4e00\u4e2a\u7ef4\u5ea6\u5b58\u5728\u663e\u8457\u98ce\u9669\uff0cBenchRisk\u80fd\u591f\u6709\u6548\u6bd4\u8f83\u4e0d\u540c\u57fa\u51c6\u7684\u98ce\u9669\u6c34\u5e73\u3002", "conclusion": "LLM\u57fa\u51c6\u6d4b\u8bd5\u9886\u57df\u5b58\u5728\u91cd\u8981\u7814\u7a76\u7a7a\u767d\uff0cBenchRisk\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u5171\u4eab\u98ce\u9669\u53ca\u7f13\u89e3\u63aa\u65bd\uff0c\u63d0\u9ad8\u57fa\u51c6\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.21045", "categories": ["cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21045", "abs": "https://arxiv.org/abs/2510.21045", "authors": ["Ali Khosravi Kazazi", "Zhenlong Li", "M. Naser Lessani", "Guido Cervone"], "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL", "comment": null, "summary": "The complexity of Structured Query Language (SQL) and the specialized nature\nof geospatial functions in tools like PostGIS present significant barriers to\nnon-experts seeking to analyze spatial data. While Large Language Models (LLMs)\noffer promise for translating natural language into SQL (Text-to-SQL),\nsingle-agent approaches often struggle with the semantic and syntactic\ncomplexities of spatial queries. To address this, we propose a multi-agent\nframework designed to accurately translate natural language questions into\nspatial SQL queries. The framework integrates several innovative components,\nincluding a knowledge base with programmatic schema profiling and semantic\nenrichment, embeddings for context retrieval, and a collaborative multi-agent\npipeline as its core. This pipeline comprises specialized agents for entity\nextraction, metadata retrieval, query logic formulation, SQL generation, and a\nreview agent that performs programmatic and semantic validation of the\ngenerated SQL to ensure correctness (self-verification). We evaluate our system\nusing both the non-spatial KaggleDBQA benchmark and a new, comprehensive\nSpatialQueryQA benchmark that includes diverse geometry types, predicates, and\nthree levels of query complexity. On KaggleDBQA, the system achieved an overall\naccuracy of 81.2% (221 out of 272 questions) after the review agent's review\nand corrections. For spatial queries, the system achieved an overall accuracy\nof 87.7% (79 out of 90 questions), compared with 76.7% without the review\nagent. Beyond accuracy, results also show that in some instances the system\ngenerates queries that are more semantically aligned with user intent than\nthose in the benchmarks. This work makes spatial analysis more accessible, and\nprovides a robust, generalizable foundation for spatial Text-to-SQL systems,\nadvancing the development of autonomous GIS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7a0b\u5e8f\u5316\u9a8c\u8bc1\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u51c6\u786e\u8f6c\u6362\u4e3a\u7a7a\u95f4SQL\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u6587\u672c\u5230SQL\u7684\u51c6\u786e\u7387\u3002", "motivation": "SQL\u548cPostGIS\u7b49\u5730\u7406\u7a7a\u95f4\u5de5\u5177\u7684\u590d\u6742\u6027\u963b\u788d\u4e86\u975e\u4e13\u5bb6\u5206\u6790\u7a7a\u95f4\u6570\u636e\uff0c\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u95f4\u67e5\u8be2\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u5b9e\u4f53\u63d0\u53d6\u3001\u5143\u6570\u636e\u68c0\u7d22\u3001\u67e5\u8be2\u903b\u8f91\u5236\u5b9a\u3001SQL\u751f\u6210\u548c\u5ba1\u67e5\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u3001\u6a21\u5f0f\u5206\u6790\u548c\u8bed\u4e49\u589e\u5f3a\u7b49\u6280\u672f\u5b9e\u73b0\u534f\u4f5c\u3002", "result": "\u5728KaggleDBQA\u4e0a\u8fbe\u523081.2%\u51c6\u786e\u7387\uff0c\u5728\u7a7a\u95f4\u67e5\u8be2\u4e0a\u8fbe\u523087.7%\u51c6\u786e\u7387\uff08\u76f8\u6bd4\u65e0\u5ba1\u67e5\u667a\u80fd\u4f53\u768476.7%\uff09\uff0c\u4e14\u751f\u6210\u7684\u67e5\u8be2\u6709\u65f6\u6bd4\u57fa\u51c6\u66f4\u7b26\u5408\u7528\u6237\u610f\u56fe\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4f7f\u7a7a\u95f4\u5206\u6790\u66f4\u6613\u8bbf\u95ee\uff0c\u4e3a\u7a7a\u95f4\u6587\u672c\u5230SQL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u63a8\u5e7f\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u81ea\u4e3bGIS\u7684\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "2510.21513", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21513", "abs": "https://arxiv.org/abs/2510.21513", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair", "comment": null, "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u591a\u6837\u6027\u7b56\u7565\u7ec4\u5408\u591a\u4e2aLLM\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6bd4\u5355\u4e2a\u6700\u4f73\u6a21\u578b\u9ad883%\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u800c\u57fa\u4e8e\u5171\u8bc6\u7684\u7b56\u7565\u4f1a\u9677\u5165'\u6d41\u884c\u5ea6\u9677\u9631'\u3002", "motivation": "\u5f53\u524d\u8ffd\u6c42\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6240\u6709\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65e2\u8d44\u6e90\u5bc6\u96c6\u53c8\u5ffd\u7565\u4e86\u6a21\u578b\u4e92\u8865\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u7f16\u7801LLM\u5982\u4f55\u4e92\u8865\u4ee5\u53ca\u6700\u4f73\u96c6\u6210\u7b56\u7565\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u5b9e\u8bc1\u6bd4\u8f83\u4e86\u6765\u81ea5\u4e2a\u5bb6\u65cf\u768410\u4e2aLLM\u6a21\u578b\u548c3\u4e2a\u96c6\u6210\u6a21\u578b\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u7a0b\u5e8f\u4fee\u590d\u7b493\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u4e92\u8865\u6027\u548c\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u9009\u62e9\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u96c6\u6210\u6a21\u578b\u7684\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u6bd4\u6700\u4f73\u5355\u6a21\u578b\u9ad883%\uff0c\u57fa\u4e8e\u5171\u8bc6\u7684\u7b56\u7565\u4f1a\u653e\u5927\u5e38\u89c1\u4f46\u4e0d\u6b63\u786e\u7684\u8f93\u51fa\uff0c\u800c\u57fa\u4e8e\u591a\u6837\u6027\u7684\u7b56\u7565\u80fd\u5b9e\u73b0\u7406\u8bba\u6f5c\u529b\u768495%\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u4e24\u6a21\u578b\u96c6\u6210\u4e2d\u4e5f\u6709\u6548\u3002", "conclusion": "\u591a\u6837\u6027\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u5f0f\u6765\u5229\u7528\u591a\u4e2aLLM\u63d0\u5347\u6027\u80fd\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u5171\u8bc6\u7b56\u7565\u7684'\u6d41\u884c\u5ea6\u9677\u9631'\u3002", "topic": "agent analysis"}}
{"id": "2510.21117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21117", "abs": "https://arxiv.org/abs/2510.21117", "authors": ["Chunghyun Han", "Alfio Gliozzo", "Junkyu Lee", "Agostino Capponi"], "title": "DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance", "comment": "12 pages, 2 Figures", "summary": "This paper presents a first empirical study of agentic AI as autonomous\ndecision-makers in decentralized governance. Using more than 3K proposals from\nmajor protocols, we build an agentic AI voter that interprets proposal\ncontexts, retrieves historical deliberation data, and independently determines\nits voting position. The agent operates within a realistic financial simulation\nenvironment grounded in verifiable blockchain data, implemented through a\nmodular composable program (MCP) workflow that defines data flow and tool usage\nvia Agentics framework. We evaluate how closely the agent's decisions align\nwith the human and token-weighted outcomes, uncovering strong alignments\nmeasured by carefully designed evaluation metrics. Our findings demonstrate\nthat agentic AI can augment collective decision-making by producing\ninterpretable, auditable, and empirically grounded signals in realistic DAO\ngovernance settings. The study contributes to the design of explainable and\neconomically rigorous AI agents for decentralized financial systems.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u4ee3\u7406AI\u5728\u53bb\u4e2d\u5fc3\u5316\u6cbb\u7406\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u8005\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u80fd\u591f\u89e3\u91ca\u63d0\u6848\u80cc\u666f\u3001\u68c0\u7d22\u5386\u53f2\u5ba1\u8bae\u6570\u636e\u5e76\u72ec\u7acb\u786e\u5b9a\u6295\u7968\u7acb\u573a\u7684AI\u6295\u7968\u4ee3\u7406\u3002", "motivation": "\u7814\u7a76\u4ee3\u7406AI\u80fd\u5426\u5728\u53bb\u4e2d\u5fc3\u5316\u6cbb\u7406\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u8005\uff0c\u589e\u5f3a\u96c6\u4f53\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u89e3\u91ca\u4e14\u7ecf\u6d4e\u4e25\u8c28\u7684AI\u4ee3\u7406\u3002", "method": "\u4f7f\u75283000\u591a\u4e2a\u4e3b\u8981\u534f\u8bae\u7684\u63d0\u6848\uff0c\u6784\u5efa\u57fa\u4e8e\u6a21\u5757\u5316\u53ef\u7ec4\u5408\u7a0b\u5e8f\u5de5\u4f5c\u6d41\u7684AI\u6295\u7968\u4ee3\u7406\uff0c\u5728\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u533a\u5757\u94fe\u6570\u636e\u7684\u771f\u5b9e\u91d1\u878d\u6a21\u62df\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "result": "\u53d1\u73b0\u4ee3\u7406AI\u7684\u51b3\u7b56\u4e0e\u4eba\u7c7b\u548c\u4ee3\u5e01\u52a0\u6743\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u663e\u793a\u51fa\u5f3a\u5bf9\u9f50\u6027\u3002", "conclusion": "\u4ee3\u7406AI\u80fd\u591f\u901a\u8fc7\u4ea7\u751f\u53ef\u89e3\u91ca\u3001\u53ef\u5ba1\u8ba1\u4e14\u57fa\u4e8e\u5b9e\u8bc1\u7684\u4fe1\u53f7\u6765\u589e\u5f3a\u96c6\u4f53\u51b3\u7b56\uff0c\u9002\u7528\u4e8e\u73b0\u5b9eDAO\u6cbb\u7406\u73af\u5883\u3002", "topic": "agent analysis"}}
{"id": "2510.21144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21144", "abs": "https://arxiv.org/abs/2510.21144", "authors": ["Hanyu Zhu", "Lance Fiondella", "Jiawei Yuan", "Kai Zeng", "Long Jiao"], "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to\ndynamically integrate external knowledge during inference, improving their\nfactual accuracy and adaptability. However, adversaries can inject poisoned\nexternal knowledge to override the model's internal memory. While existing\nattacks iteratively manipulate retrieval content or prompt structure of RAG,\nthey largely ignore the model's internal representation dynamics and\nneuron-level sensitivities. The underlying mechanism of RAG poisoning has not\nbeen fully studied and the effect of knowledge conflict with strong parametric\nknowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,\na novel attack framework that generates adversarial external knowledge in RAG\nguided by LLM internal neuron attribution and genetic optimization. Our method\nfirst identifies a set of Poison-Responsive Neurons whose activation strongly\ncorrelates with contextual poisoning knowledge. We then employ a genetic\nalgorithm to evolve adversarial passages that maximally activate these neurons.\nCrucially, our framework enables massive-scale generation of effective poisoned\nRAG knowledge by identifying and reusing promising but initially unsuccessful\nexternal knowledge variants via observed attribution signals. At the same time,\nPoison-Responsive Neurons guided poisoning can effectively resolves knowledge\nconflict. Experimental results across models and datasets demonstrate\nconsistently achieving high Population Overwrite Success Rate (POSR) of over\n90% while preserving fluency. Empirical evidence shows that our method\neffectively resolves knowledge conflict.", "AI": {"tldr": "\u63d0\u51fa\u4e86NeuroGenPoisoning\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5185\u90e8\u795e\u7ecf\u5143\u5f52\u56e0\u548c\u9057\u4f20\u4f18\u5316\u751f\u6210\u5bf9\u6297\u6027\u5916\u90e8\u77e5\u8bc6\uff0c\u5728RAG\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u6bd2\u5316\u653b\u51fb\u3002", "motivation": "\u73b0\u6709RAG\u653b\u51fb\u65b9\u6cd5\u5ffd\u7565\u4e86\u6a21\u578b\u5185\u90e8\u8868\u793a\u52a8\u6001\u548c\u795e\u7ecf\u5143\u7ea7\u654f\u611f\u6027\uff0c\u672a\u80fd\u5145\u5206\u7814\u7a76RAG\u6bd2\u5316\u673a\u5236\u548c\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc6\u522b\u4e0e\u4e0a\u4e0b\u6587\u6bd2\u5316\u77e5\u8bc6\u5f3a\u76f8\u5173\u7684\u6bd2\u5316\u54cd\u5e94\u795e\u7ecf\u5143\uff0c\u7136\u540e\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fdb\u5316\u5bf9\u6297\u6027\u6bb5\u843d\u4ee5\u6700\u5927\u5316\u6fc0\u6d3b\u8fd9\u4e9b\u795e\u7ecf\u5143\uff0c\u5e76\u901a\u8fc7\u5f52\u56e0\u4fe1\u53f7\u91cd\u7528\u6709\u6f5c\u529b\u7684\u5916\u90e8\u77e5\u8bc6\u53d8\u4f53\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u5b9e\u73b0\u8d85\u8fc790%\u7684\u4eba\u53e3\u8986\u76d6\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "conclusion": "NeuroGenPoisoning\u6846\u67b6\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u6709\u6548\u7684\u6bd2\u5316RAG\u77e5\u8bc6\uff0c\u901a\u8fc7\u6bd2\u5316\u54cd\u5e94\u795e\u7ecf\u5143\u5f15\u5bfc\u7684\u6bd2\u5316\u80fd\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u3002", "topic": "agent analysis"}}
{"id": "2510.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21148", "abs": "https://arxiv.org/abs/2510.21148", "authors": ["Yang Zhao", "Pu Wang", "Hao Frank Yang"], "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "comment": null, "summary": "Designing optimal prompts and reasoning processes for large language models\n(LLMs) on domain-specific tasks is both necessary and challenging in real-world\napplications. Determining how to integrate domain knowledge, enhance reasoning\nefficiency, and even provide domain experts with refined knowledge integration\nhints are particularly crucial yet unresolved tasks. In this research, we\npropose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an\nautomated framework to designing better prompts, efficient reasoning processes\nand providing enhanced causal-informed process. EGO-Prompt begins with a\ngeneral prompt and fault-tolerant initial Semantic Causal Graph (SCG)\ndescriptions, constructed by human experts, which is then automatically refined\nand optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may\nbe partial or imperfect and that their optimal integration varies across LLMs,\nEGO-Prompt integrates a novel causal-guided textual gradient process in two\nsteps: first, generating nearly deterministic reasoning guidance from the SCG\nfor each instance, and second, adapting the LLM to effectively utilize the\nguidance alongside the original input. The iterative optimization algorithm\nfurther refines both the SCG and the reasoning mechanism using textual\ngradients with ground-truth. We tested the framework on real-world public\nhealth, transportation and human behavior tasks. EGO-Prompt achieves\n7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to\nreach the performence of larger models at under 20% of the original cost. It\nalso outputs a refined, domain-specific SCG that improves interpretability.", "AI": {"tldr": "EGO-Prompt\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u56fe\u4f18\u5316\u65b9\u6cd5\u6539\u8fdbLLM\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u9886\u57df\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u6700\u4f18\u63d0\u793a\u548c\u63a8\u7406\u8fc7\u7a0b\u65e2\u5fc5\u8981\u53c8\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u4ee5\u53ca\u4e3a\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u77e5\u8bc6\u6574\u5408\u6307\u5bfc\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faEGO-Prompt\u6846\u67b6\uff0c\u4ece\u4e13\u5bb6\u6784\u5efa\u7684\u521d\u59cb\u8bed\u4e49\u56e0\u679c\u56fe\u51fa\u53d1\uff0c\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u7684\u6587\u672c\u68af\u5ea6\u8fc7\u7a0b\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u548c\u63a8\u7406\u673a\u5236\uff0c\u5305\u62ec\u751f\u6210\u786e\u5b9a\u6027\u63a8\u7406\u6307\u5bfc\u548c\u9002\u914dLLM\u4f7f\u7528\u6307\u5bfc\u3002", "result": "\u5728\u516c\u5171\u536b\u751f\u3001\u4ea4\u901a\u548c\u4eba\u7c7b\u884c\u4e3a\u4efb\u52a1\u4e2d\uff0cEGO-Prompt\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5F1\u5206\u6570\u63d0\u9ad87.32%-12.61%\uff0c\u5c0f\u6a21\u578b\u80fd\u4ee5\u4e0d\u523020%\u6210\u672c\u8fbe\u5230\u5927\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8f93\u51fa\u6539\u8fdb\u7684\u9886\u57df\u7279\u5b9a\u8bed\u4e49\u56e0\u679c\u56fe\u3002", "conclusion": "EGO-Prompt\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u540c\u65f6\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.20963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20963", "abs": "https://arxiv.org/abs/2510.20963", "authors": ["Yongqiang Chen", "Gang Niu", "James Cheng", "Bo Han", "Masashi Sugiyama"], "title": "Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection", "comment": "Preprint, ongoing work", "summary": "Accurate detection of errors in large language models (LLM) responses is\ncentral to the success of scalable oversight, or providing effective\nsupervision to superhuman intelligence. Yet, self-diagnosis is often unreliable\non complex tasks unless aided by reliable external feedback. Multi-agent debate\n(MAD) seems to be a natural alternative to external feedback: multiple LLMs\nprovide complementary perspectives and cross-checks for error detection.\nHowever, prior MAD protocols frame debate as a zero-sum game, where the\ndebaters compete to win the game instead of seeking the truth. Consequently, it\nleads to debate hacking: debaters tend to mislead the judge by misinterpreting\nthe task or presenting overconfident claims, which introduce more mistakes and\nunderperform single-agent methods. To mitigate the issue, we introduce a new\ncollaborative MAD protocol, termed ColMAD, that reframes MAD as a non-zero sum\ngame. Specifically, ColMAD encourages multiple agents to criticize each other\nin a supportive way, such that they can complement the missing points of each\nother. Therefore, the judge agent can make a more informative conclusion based\non more comprehensive evidence. Empirically, we show that ColMAD significantly\noutperforms previous competitive MAD by 19% and brings non-trivial improvements\nover single-agent methods in error detection.", "AI": {"tldr": "\u63d0\u51faColMAD\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u534f\u8bae\uff0c\u5c06\u8fa9\u8bba\u91cd\u6784\u4e3a\u975e\u96f6\u548c\u535a\u5f08\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u7684\u652f\u6301\u6027\u6279\u8bc4\u6765\u6539\u5584\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u6027\u8fa9\u8bba\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\u5c06\u8fa9\u8bba\u89c6\u4e3a\u96f6\u548c\u535a\u5f08\uff0c\u5bfc\u81f4\u8fa9\u8bba\u9ed1\u5ba2\u884c\u4e3a\u2014\u2014\u667a\u80fd\u4f53\u4e3a\u83b7\u80dc\u800c\u8bef\u5bfc\u88c1\u5224\uff0c\u53cd\u800c\u5f15\u5165\u66f4\u591a\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u534f\u4f5c\u5f0f\u8fa9\u8bba\u65b9\u6cd5\u6765\u63d0\u5347\u9519\u8bef\u68c0\u6d4b\u6548\u679c\u3002", "method": "ColMAD\u534f\u8bae\u5c06\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u91cd\u6784\u4e3a\u975e\u96f6\u548c\u535a\u5f08\uff0c\u9f13\u52b1\u667a\u80fd\u4f53\u4ee5\u652f\u6301\u6027\u65b9\u5f0f\u76f8\u4e92\u6279\u8bc4\uff0c\u8865\u5145\u5f7c\u6b64\u9057\u6f0f\u7684\u89c2\u70b9\uff0c\u4e3a\u88c1\u5224\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc1\u636e\u3002", "result": "ColMAD\u5728\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u7ade\u4e89\u6027\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd519%\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u4e5f\u5e26\u6765\u4e86\u975e\u5e73\u51e1\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u534f\u8bae\u80fd\u6709\u6548\u7f13\u89e3\u8fa9\u8bba\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u7684\u652f\u6301\u6027\u6279\u8bc4\u663e\u8457\u63d0\u5347\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.21306", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21306", "abs": "https://arxiv.org/abs/2510.21306", "authors": ["Yarik Menchaca Resendiz", "Roman Klinger"], "title": "PARL: Prompt-based Agents for Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated high performance on tasks\nexpressed in natural language, particularly in zero- or few-shot settings.\nThese are typically framed as supervised (e.g., classification) or unsupervised\n(e.g., clustering) problems. However, limited work evaluates LLMs as agents in\nreinforcement learning (RL) tasks (e.g., playing games), where learning occurs\nthrough interaction with an environment and a reward system. While prior work\nfocused on representing tasks that rely on a language representation, we study\nstructured, non-linguistic reasoning - such as interpreting positions in a grid\nworld. We therefore introduce PARL (Prompt-based Agent for Reinforcement\nLearning), a method that uses LLMs as RL agents through prompting, without any\nfine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling\nthe model to learn through trial-and-error interaction. We evaluate PARL on\nthree standard RL tasks that do not entirely rely on natural language. We show\nthat it can match or outperform traditional RL agents in simple environments by\nleveraging pretrained knowledge. However, we identify performance limitations\nin tasks that require complex mathematical operations or decoding states and\nactions.", "AI": {"tldr": "PARL\u662f\u4e00\u79cd\u4f7f\u7528LLMs\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u800c\u975e\u5fae\u8c03\uff0c\u5728\u975e\u8bed\u8a00\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e0e\u6216\u8d85\u8d8a\u4f20\u7edfRL\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u975e\u8bed\u8a00\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5982\u7f51\u683c\u4e16\u754c\u4f4d\u7f6e\u89e3\u91ca\uff0c\u800c\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u3002", "method": "PARL\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u5c06\u52a8\u4f5c\u3001\u72b6\u6001\u548c\u5956\u52b1\u7f16\u7801\uff0c\u4f7fLLMs\u80fd\u591f\u901a\u8fc7\u8bd5\u9519\u4e0e\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6RL\u4efb\u52a1\u4e2d\uff0cPARL\u5728\u7b80\u5355\u73af\u5883\u4e2d\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u4f20\u7edfRL\u4ee3\u7406\uff0c\u4f46\u5728\u9700\u8981\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u6216\u72b6\u6001\u52a8\u4f5c\u89e3\u7801\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\u3002", "conclusion": "LLMs\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684RL\u4ee3\u7406\uff0c\u5728\u7b80\u5355\u975e\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21302", "abs": "https://arxiv.org/abs/2510.21302", "authors": ["Sanghyun Ahn", "Wonje Choi", "Junyong Lee", "Jinwoo Park", "Honguk Woo"], "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "comment": "Accepted at NeurIPS 2025 Spotlight", "summary": "Recent advances in large language models (LLMs) have enabled the automatic\ngeneration of executable code for task planning and control in embodied agents\nsuch as robots, demonstrating the potential of LLM-based embodied intelligence.\nHowever, these LLM-based code-as-policies approaches often suffer from limited\nenvironmental grounding, particularly in dynamic or partially observable\nsettings, leading to suboptimal task success rates due to incorrect or\nincomplete code generation. In this work, we propose a neuro-symbolic embodied\ntask planning framework that incorporates explicit symbolic verification and\ninteractive validation processes during code generation. In the validation\nphase, the framework generates exploratory code that actively interacts with\nthe environment to acquire missing observations while preserving task-relevant\nstates. This integrated process enhances the grounding of generated code,\nresulting in improved task reliability and success rates in complex\nenvironments. We evaluate our framework on RLBench and in real-world settings\nacross dynamic, partially observable scenarios. Experimental results\ndemonstrate that our framework improves task success rates by 46.2% over\nCode-as-Policies baselines and attains over 86.8% executability of\ntask-relevant actions, thereby enhancing the reliability of task planning in\ndynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u9a8c\u8bc1\u548c\u4ea4\u4e92\u9a8c\u8bc1\u8fc7\u7a0b\u6539\u8fdbLLM\u4ee3\u7801\u751f\u6210\uff0c\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u5373\u7b56\u7565\u65b9\u6cd5\u5728\u52a8\u6001\u6216\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5b58\u5728\u73af\u5883\u63a5\u5730\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u4e0d\u5b8c\u6574\u6216\u4e0d\u6b63\u786e\uff0c\u5f71\u54cd\u4efb\u52a1\u6210\u529f\u7387\u3002", "method": "\u7ed3\u5408\u663e\u5f0f\u7b26\u53f7\u9a8c\u8bc1\u548c\u4ea4\u4e92\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u5728\u9a8c\u8bc1\u9636\u6bb5\u751f\u6210\u63a2\u7d22\u6027\u4ee3\u7801\u4e0e\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u7f3a\u5931\u89c2\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u72b6\u6001\u3002", "result": "\u5728RLBench\u548c\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u90e8\u5206\u53ef\u89c2\u6d4b\u573a\u666f\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u4ee3\u7801\u5373\u7b56\u7565\u57fa\u7ebf\u63d0\u9ad846.2%\uff0c\u4efb\u52a1\u76f8\u5173\u52a8\u4f5c\u53ef\u6267\u884c\u6027\u8fbe\u523086.8%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7684\u73af\u5883\u63a5\u5730\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u4efb\u52a1\u89c4\u5212\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.21324", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.21324", "abs": "https://arxiv.org/abs/2510.21324", "authors": ["Jinhui Lou", "Yan Yang", "Zhou Yu", "Zhenqi Fu", "Weidong Han", "Qingming Huang", "Jun Yu"], "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation", "comment": "10 pages, 4 figures, 7 Tables", "summary": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.", "AI": {"tldr": "\u63d0\u51fa\u4e86CXRAgent\uff0c\u4e00\u4e2a\u7531\u5bfc\u6f14\u534f\u8c03\u7684\u591a\u9636\u6bb5\u4ee3\u7406\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u5206\u6790\uff0c\u901a\u8fc7\u5de5\u5177\u534f\u8c03\u3001\u591a\u9636\u6bb5\u63a8\u7406\u548c\u56e2\u961f\u534f\u4f5c\u589e\u5f3a\u8bca\u65ad\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684CXR\u5206\u6790\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u65b0\u7684\u8bca\u65ad\u4efb\u52a1\u548c\u590d\u6742\u63a8\u7406\u573a\u666f\uff0c\u73b0\u6709\u4ee3\u7406\u4f9d\u8d56\u5355\u4e00\u8bca\u65ad\u6d41\u7a0b\u4e14\u7f3a\u4e4f\u5de5\u5177\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\u3002", "method": "\u91c7\u7528\u5bfc\u6f14\u534f\u8c03\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u5de5\u5177\u8c03\u7528\uff08\u5305\u542b\u8bc1\u636e\u9a71\u52a8\u9a8c\u8bc1\u5668\uff09\u3001\u8bca\u65ad\u89c4\u5212\uff08\u7ec4\u5efa\u4e13\u5bb6\u56e2\u961f\uff09\u3001\u534f\u4f5c\u51b3\u7b56\uff08\u6574\u5408\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff09\u3002", "result": "\u5728\u5404\u79cdCXR\u89e3\u91ca\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u63d0\u4f9b\u89c6\u89c9\u8bc1\u636e\u5e76\u826f\u597d\u6cdb\u5316\u5230\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4e34\u5e8a\u4efb\u52a1\u3002", "conclusion": "CXRAgent\u901a\u8fc7\u591a\u9636\u6bb5\u534f\u4f5c\u548c\u8bc1\u636e\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86CXR\u5206\u6790\u7684\u9002\u5e94\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.21341", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21341", "abs": "https://arxiv.org/abs/2510.21341", "authors": ["Lufan Chang"], "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation", "comment": "Accepted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation.", "AI": {"tldr": "Magellan\u6846\u67b6\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u5206\u5c42\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5c06\u521b\u610f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5bf9LLM\u6f5c\u5728\u6982\u5ff5\u7a7a\u95f4\u7684\u539f\u5219\u6027\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u60f3\u6cd5\u7684\u521b\u65b0\u6027\u548c\u5408\u7406\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u751f\u6210\u521b\u65b0\u60f3\u6cd5\u65f6\u503e\u5411\u4e8e\u9ad8\u6982\u7387\u719f\u6089\u6982\u5ff5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u641c\u7d22\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u9760\u81ea\u8bc4\u4f30\u542f\u53d1\u5f0f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u7ed3\u5408\u8bed\u4e49\u7f57\u76d8\u5411\u91cf\u8fdb\u884c\u957f\u7a0b\u5f15\u5bfc\uff0c\u4ee5\u53ca\u57fa\u4e8e\u666f\u89c2\u611f\u77e5\u7684\u4ef7\u503c\u51fd\u6570\u8fdb\u884c\u5c40\u90e8\u51b3\u7b56\uff0c\u5e73\u8861\u5185\u5728\u4e00\u81f4\u6027\u3001\u5916\u5728\u65b0\u9896\u6027\u548c\u53d9\u4e8b\u8fdb\u5c55\u3002", "result": "\u5728\u751f\u6210\u79d1\u5b66\u60f3\u6cd5\u65b9\u9762\u663e\u8457\u4f18\u4e8eReAct\u548cToT\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u4e86\u66f4\u5177\u5408\u7406\u6027\u548c\u521b\u65b0\u6027\u7684\u7ed3\u679c\u3002", "conclusion": "\u5bf9\u4e8e\u521b\u610f\u53d1\u73b0\uff0c\u539f\u5219\u6027\u5f15\u5bfc\u641c\u7d22\u6bd4\u65e0\u7ea6\u675f\u4ee3\u7406\u66f4\u6709\u6548\uff0c\u4e3aLLMs\u6210\u4e3a\u521b\u65b0\u5408\u4f5c\u4f19\u4f34\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.21329", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21329", "abs": "https://arxiv.org/abs/2510.21329", "authors": ["Priyanshu Karmakar", "Soumyabrata Chaudhuri", "Shubhojit Mallick", "Manish Gupta", "Abhik Jana", "Shreya Ghosh"], "title": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions", "comment": "12 pages, 12 tables and 7 figures", "summary": "Recent efforts like TripCraft and TravelPlanner have advanced the use of\nLarge Language Models ( LLMs) for personalized, constraint aware travel\nitinerary generation. Yet, real travel often faces disruptions. To address\nthis, we present TripTide, the first benchmark evaluating LLM's ability to\nrevise itineraries under realistic disruptions. TripTide models key dimensions\nsuch as disruption severity and traveler tolerance, enabling nuanced assessment\nof LLM adaptability to events like flight cancellations, weather closures, or\noverbooked attractions. We conduct a threefold evaluation. First, we introduce\nautomatic metrics including Preservation of Intent (how well the revised plan\nmaintains feasibility and goals), Responsiveness (promptness and\nappropriateness of disruption handling), and Adaptability (semantic, spatial,\nand sequential divergence between original and revised plans). Second, we apply\nan LLM-as-a-judge approach to automatically assess revision quality. Third, we\nperform manual expert evaluation to verify whether revisions preserve semantic,\nspatial, sequential, and responsive aspects. Our experiments show that LLMs\nmaintain strong sequential consistency and semantic stability, while spatial\ndeviations are larger for shorter trips but decrease with longer ones,\nindicating that extended plans encourage better geographic coherence. However,\ndisruption-handling ability declines as plan length increases, highlighting\nlimits in LLM robustness. TripTide establishes a benchmark for evaluating\nadaptability, personalization, and resilience in LLM-based travel planning\nunder real-world uncertainty.", "AI": {"tldr": "TripTide\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u65c5\u884c\u4e2d\u65ad\u60c5\u51b5\u4e0b\u4fee\u8ba2\u884c\u7a0b\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5efa\u6a21\u4e2d\u65ad\u4e25\u91cd\u7a0b\u5ea6\u548c\u65c5\u884c\u8005\u5bb9\u5fcd\u5ea6\u7b49\u7ef4\u5ea6\uff0c\u4ece\u610f\u56fe\u4fdd\u6301\u3001\u54cd\u5e94\u6027\u548c\u9002\u5e94\u6027\u4e09\u65b9\u9762\u8bc4\u4f30LLM\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212\u7cfb\u7edf\u5982TripCraft\u548cTravelPlanner\u867d\u7136\u80fd\u591f\u751f\u6210\u4e2a\u6027\u5316\u884c\u7a0b\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u73b0\u5b9e\u65c5\u884c\u4e2d\u5e38\u89c1\u7684\u4e2d\u65ad\u60c5\u51b5\uff0c\u5982\u822a\u73ed\u53d6\u6d88\u3001\u5929\u6c14\u5173\u95ed\u7b49\u3002", "method": "\u91c7\u7528\u4e09\u91cd\u8bc4\u4f30\u65b9\u6cd5\uff1a1) \u5f15\u5165\u81ea\u52a8\u6307\u6807\uff08\u610f\u56fe\u4fdd\u6301\u3001\u54cd\u5e94\u6027\u3001\u9002\u5e94\u6027\uff09\uff1b2) LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u81ea\u52a8\u8bc4\u4f30\u4fee\u8ba2\u8d28\u91cf\uff1b3) \u4e13\u5bb6\u624b\u52a8\u8bc4\u4f30\u8bed\u4e49\u3001\u7a7a\u95f4\u3001\u5e8f\u5217\u548c\u54cd\u5e94\u65b9\u9762\u7684\u4fdd\u6301\u60c5\u51b5\u3002", "result": "LLM\u5728\u5e8f\u5217\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7a7a\u95f4\u504f\u5dee\u5728\u77ed\u9014\u65c5\u884c\u4e2d\u8f83\u5927\u4f46\u968f\u884c\u7a0b\u5ef6\u957f\u800c\u51cf\u5c0f\uff0c\u4f46\u4e2d\u65ad\u5904\u7406\u80fd\u529b\u968f\u8ba1\u5212\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "TripTide\u4e3a\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u65c5\u884c\u89c4\u5212\u9002\u5e94\u6027\u3001\u4e2a\u6027\u5316\u548c\u97e7\u6027\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLM\u5728\u957f\u884c\u7a0b\u4e2d\u65ad\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.21398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21398", "abs": "https://arxiv.org/abs/2510.21398", "authors": ["Ravindra Aribowo Tarunokusumo", "Rafael Fernandes Cunha"], "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning", "comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u7b97\u5f3a\u5236\u89e3\u7801\u7b56\u7565\u6765\u63d0\u9ad81.5B\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c1140%\u4ee5\u4e0a\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u9884\u7b97\u5f3a\u5236\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u65e0\u9700\u53c2\u6570\u8bad\u7ec3\uff0c\u4f46\u4f9d\u8d56\u4e8e\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5bfc\u81f4\u5c0f\u6a21\u578b\u56e0\u5197\u957f\u54cd\u5e94\u800c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8token\u6548\u7387\uff0c\u4ec5\u4f7f\u75281.5K\u8bad\u7ec3\u6837\u672c\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684SFT+RL\u6a21\u578b\u6027\u80fd\u3002", "result": "SFT+RL\u6a21\u578b\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u603b\u4f53\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u76f8\u6bd4SFT\u6a21\u578b\u663e\u8457\u51cf\u5c11\u8d85\u8fc740%\u7684token\u4f7f\u7528\u91cf\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6062\u590d\u56e0\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u9020\u6210\u7684\u635f\u5931\uff0c\u6574\u4f53\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21425", "abs": "https://arxiv.org/abs/2510.21425", "authors": ["Maneeha Rani", "Bhupesh Kumar Mishra", "Dhavalkumar Thakker"], "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI", "comment": null, "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b26\u53f7AI\u96c6\u6210\u5230LLMs\u7684\u5206\u7c7b\u6846\u67b6\u548c\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u51b3\u7b56\u548c\u54cd\u5e94\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u3002\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0c\u4e0d\u9002\u7528\u4e8eLLMs\u7684\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u5982\u4f55\u5c06\u7b26\u53f7AI\u6709\u6548\u96c6\u6210\u5230LLMs\u4e2d\u3002", "method": "\u9996\u5148\u56de\u987e\u5df2\u5efa\u7acb\u7684\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\uff0c\u7136\u540e\u63d0\u51faLLMs\u4e2d\u7b26\u53f7\u96c6\u6210\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u5236\u5b9a\u5c06\u7b26\u53f7\u6280\u672f\u4e0eLLMs\u878d\u5408\u7684\u8def\u7ebf\u56fe\u3002\u8be5\u8def\u7ebf\u56fe\u5f15\u5165\u4e86\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\uff1aLLM\u4e0d\u540c\u9636\u6bb5\u7684\u7b26\u53f7\u96c6\u6210\u3001\u8026\u5408\u673a\u5236\u3001\u67b6\u6784\u8303\u5f0f\u4ee5\u53ca\u7b97\u6cd5\u548c\u5e94\u7528\u5c42\u9762\u3002", "result": "\u8bba\u6587\u5168\u9762\u8bc6\u522b\u4e86\u5f53\u524d\u57fa\u51c6\u3001\u524d\u6cbf\u8fdb\u5c55\u548c\u5173\u952e\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u8def\u7ebf\u56fe\u3002\u901a\u8fc7\u7a81\u51fa\u6700\u65b0\u53d1\u5c55\u548c\u6587\u732e\u4e2d\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u65bd\u7b26\u53f7\u96c6\u6210\u5230LLMs\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u589e\u5f3aLLMs\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7b26\u53f7\u96c6\u6210\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u5728LLMs\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.21524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21524", "abs": "https://arxiv.org/abs/2510.21524", "authors": ["Ilija Lichkovski", "Alexander M\u00fcller", "Mariam Ibrahim", "Tiwai Mhundwa"], "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "AI": {"tldr": "EU-Agent-Bench\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u4eba\u5de5\u7b56\u5212\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6b27\u76df\u6cd5\u5f8b\u80cc\u666f\u4e0b\u6267\u884c\u975e\u6cd5\u884c\u4e3a\u7684\u6f5c\u5728\u503e\u5411\uff0c\u6db5\u76d6\u6570\u636e\u4fdd\u62a4\u3001\u504f\u89c1/\u6b67\u89c6\u548c\u79d1\u5b66\u8bda\u4fe1\u7b49\u591a\u4e2a\u573a\u666f\u3002", "motivation": "\u968f\u7740LLM\u4f5c\u4e3a\u4ee3\u7406\u5728\u5404\u79cd\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u5b83\u4eec\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u91c7\u53d6\u4e0d\u826f\u548c/\u6216\u4e0d\u5b89\u5168\u7684\u884c\u52a8\u3002\u9700\u8981\u6d4b\u91cfLLM\u4ee3\u7406\u5728\u6b27\u76df\u7acb\u6cd5\u80cc\u666f\u4e0b\u91c7\u53d6\u975e\u6cd5\u884c\u4e3a\u7684\u6f5c\u5728\u503e\u5411\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u4eba\u5de5\u7b56\u5212\u57fa\u51c6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u51fd\u6570\u8c03\u7528\u4e0e\u8be6\u5c3d\u5f15\u7528\u76f8\u5173\u7acb\u6cd5\u7684\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u524d\u6cbfLLM\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u5728\u4ee3\u7406\u7cfb\u7edf\u63d0\u793a\u4e2d\u63d0\u4f9b\u76f8\u5173\u7acb\u6cd5\u6458\u5f55\u5bf9\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u4e86\u524d\u6cbfLLM\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u63d0\u4f9b\u7acb\u6cd5\u6458\u5f55\u5bf9\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002\u53d1\u5e03\u4e86\u516c\u5171\u9884\u89c8\u96c6\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u7559\u79c1\u6709\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "conclusion": "\u9f13\u52b1\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\u6269\u5c55\u5230\u4e0d\u540c\u7684\u6cd5\u5f8b\u7ba1\u8f96\u533a\uff0c\u4ee5\u53ca\u591a\u8f6e\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\u3002", "topic": "agent analysis"}}
{"id": "2510.21060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21060", "abs": "https://arxiv.org/abs/2510.21060", "authors": ["Yi He", "Xingyu Zhou"], "title": "On the Sample Complexity of Differentially Private Policy Optimization", "comment": null, "summary": "Policy optimization (PO) is a cornerstone of modern reinforcement learning\n(RL), with diverse applications spanning robotics, healthcare, and large\nlanguage model training. The increasing deployment of PO in sensitive domains,\nhowever, raises significant privacy concerns. In this paper, we initiate a\ntheoretical study of differentially private policy optimization, focusing\nexplicitly on its sample complexity. We first formalize an appropriate\ndefinition of differential privacy (DP) tailored to PO, addressing the inherent\nchallenges arising from on-policy learning dynamics and the subtlety involved\nin defining the unit of privacy. We then systematically analyze the sample\ncomplexity of widely-used PO algorithms, including policy gradient (PG),\nnatural policy gradient (NPG) and more, under DP constraints and various\nsettings, via a unified framework. Our theoretical results demonstrate that\nprivacy costs can often manifest as lower-order terms in the sample complexity,\nwhile also highlighting subtle yet important observations in private PO\nsettings. These offer valuable practical insights for privacy-preserving PO\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5dee\u5206\u9690\u79c1\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u7406\u8bba\u7814\u7a76\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u7b56\u7565\u4f18\u5316\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u673a\u5668\u4eba\u3001\u533b\u7597\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7814\u7a76\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316\u9002\u5408\u7b56\u7565\u4f18\u5316\u7684\u5dee\u5206\u9690\u79c1\u5b9a\u4e49\uff0c\u7136\u540e\u7cfb\u7edf\u5206\u6790\u7b56\u7565\u68af\u5ea6\u3001\u81ea\u7136\u7b56\u7565\u68af\u5ea6\u7b49\u5e38\u7528\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\u9690\u79c1\u6210\u672c\u901a\u5e38\u8868\u73b0\u4e3a\u6837\u672c\u590d\u6742\u5ea6\u7684\u4f4e\u9636\u9879\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u9690\u79c1\u7b56\u7565\u4f18\u5316\u8bbe\u7f6e\u4e2d\u7684\u5fae\u5999\u4f46\u91cd\u8981\u7684\u89c2\u5bdf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8df5\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u4f18\u5316\u7684\u53ef\u884c\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21614", "abs": "https://arxiv.org/abs/2510.21614", "authors": ["Wenyi Wang", "Piotr Pi\u0119kos", "Li Nanbo", "Firas Laakom", "Yimeng Chen", "Mateusz Ostaszewski", "Mingchen Zhuge", "J\u00fcrgen Schmidhuber"], "title": "Huxley-G\u00f6del Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine", "comment": null, "summary": "Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using less wall-clock time. Last but not least, HGM\ndemonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is available at https://github.com/metauto-ai/HGM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHuxley-G\u00f6del Machine (HGM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\u6307\u6807CMP\u6765\u6307\u5bfc\u4ee3\u7801\u4ee3\u7406\u7684\u81ea\u6211\u4fee\u6539\u6811\u641c\u7d22\uff0c\u5728SWE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u5047\u8bbe\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u9ad8\u7684\u4ee3\u7406\u5177\u6709\u66f4\u597d\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\u4e0e\u5b9e\u9645\u7f16\u7801\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faCMP\u6307\u6807\u6765\u8bc4\u4f30\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaHGM\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1CMP\u6765\u6307\u5bfc\u81ea\u6211\u4fee\u6539\u6811\u7684\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728SWE-bench Verified\u548cPolyglot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHGM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u7528\u65f6\u66f4\u5c11\uff0c\u5728SWE-bench Lite\u4e0a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6027\u80fd\u3002", "conclusion": "CMP\u6307\u6807\u80fd\u6709\u6548\u8861\u91cf\u4ee3\u7801\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\uff0cHGM\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u5e76\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.21618", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21618", "abs": "https://arxiv.org/abs/2510.21618", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Guanting Dong", "Jiajie Jin", "Yinuo Wang", "Hao Wang", "Yutao Zhu", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets", "comment": null, "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.", "AI": {"tldr": "DeepAgent\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u81ea\u4e3b\u601d\u8003\u3001\u5de5\u5177\u53d1\u73b0\u548c\u884c\u52a8\u6267\u884c\u5728\u5355\u4e00\u8fde\u8d2f\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b8c\u6210\u4efb\u52a1\u3002\u5b83\u5f15\u5165\u81ea\u4e3b\u8bb0\u5fc6\u6298\u53e0\u673a\u5236\u6765\u538b\u7f29\u5386\u53f2\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528ToolPO\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u9ad8\u6548\u7a33\u5b9a\u5730\u6559\u6388\u901a\u7528\u5de5\u5177\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u9075\u5faa\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u548c\u5168\u5c40\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u9700\u8981\u5916\u90e8\u5de5\u5177\u548c\u957f\u7a0b\u4ea4\u4e92\uff0c\u8fd9\u5e26\u6765\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u7206\u70b8\u548c\u4ea4\u4e92\u5386\u53f2\u7d2f\u79ef\u7684\u6311\u6218\u3002", "method": "DeepAgent\u91c7\u7528\u7aef\u5230\u7aef\u6df1\u5ea6\u63a8\u7406\uff0c\u5305\u542b\u81ea\u4e3b\u8bb0\u5fc6\u6298\u53e0\u673a\u5236\uff08\u5c06\u8fc7\u53bb\u4ea4\u4e92\u538b\u7f29\u4e3a\u7ed3\u6784\u5316\u8bb0\u5fc6\uff09\u548cToolPO\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff08\u5229\u7528LLM\u6a21\u62dfAPI\u5e76\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u4f18\u52bf\u5f52\u56e0\u5206\u914d\u7ec6\u7c92\u5ea6\u4fe1\u7528\uff09\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u901a\u7528\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u548c\u4e0b\u6e38\u5e94\u7528\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDeepAgent\u5728\u6807\u8bb0\u5de5\u5177\u548c\u5f00\u653e\u96c6\u5de5\u5177\u68c0\u7d22\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u671d\u7740\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u6784\u5efa\u66f4\u901a\u7528\u548c\u66f4\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2510.21067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21067", "abs": "https://arxiv.org/abs/2510.21067", "authors": ["Raul Cavalcante Dinardi", "Bruno Yamamoto", "Anna Helena Reali Costa", "Artur Jordao"], "title": "The Virtues of Brevity: Avoid Overthinking in Parallel Test-Time Reasoning", "comment": "Accepted at NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Reasoning models represent a significant advance in LLM capabilities,\nparticularly for complex reasoning tasks such as mathematics and coding.\nPrevious studies confirm that parallel test-time compute-sampling multiple\nsolutions and selecting the best one-can further enhance the predictive\nperformance of LLMs. However, strategies in this area often require complex\nscoring, thus increasing computational cost and complexity. In this work, we\ndemonstrate that the simple and counterintuitive heuristic of selecting the\nshortest solution is highly effective. We posit that the observed effectiveness\nstems from models operating in two distinct regimes: a concise, confident\nconventional regime and a verbose overthinking regime characterized by\nuncertainty, and we show evidence of a critical point where the overthinking\nregime begins to be significant. By selecting the shortest answer, the\nheuristic preferentially samples from the conventional regime. We confirm that\nthis approach is competitive with more complex methods such as self-consistency\nacross two challenging benchmarks while significantly reducing computational\noverhead. The shortest-answer heuristic provides a Pareto improvement over\nself-consistency and applies even to tasks where output equality is not well\ndefined.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9009\u62e9\u6700\u77ed\u7b54\u6848\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u65b9\u9762\u4e0e\u590d\u6742\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u65b9\u6cd5\u9700\u8981\u590d\u6742\u8bc4\u5206\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6700\u77ed\u7b54\u6848\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8ba4\u4e3a\u6a21\u578b\u5728\u7b80\u6d01\u81ea\u4fe1\u7684\u5e38\u89c4\u6a21\u5f0f\u548c\u5197\u957f\u8fc7\u5ea6\u601d\u8003\u6a21\u5f0f\u4e2d\u8fd0\u884c\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u77ed\u7b54\u6848\u4f18\u5148\u91c7\u6837\u5e38\u89c4\u6a21\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u81ea\u4e00\u81f4\u6027\u7b49\u590d\u6742\u65b9\u6cd5\u7ade\u4e89\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u6700\u77ed\u7b54\u6848\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u76f8\u5bf9\u4e8e\u81ea\u4e00\u81f4\u6027\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u5373\u4f7f\u5728\u8f93\u51fa\u76f8\u7b49\u6027\u5b9a\u4e49\u4e0d\u660e\u786e\u7684\u4efb\u52a1\u4e2d\u4e5f\u9002\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.21652", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21652", "abs": "https://arxiv.org/abs/2510.21652", "authors": ["Jonathan Bragg", "Mike D'Arcy", "Nishant Balepur", "Dan Bareket", "Bhavana Dalvi", "Sergey Feldman", "Dany Haddad", "Jena D. Hwang", "Peter Jansen", "Varsha Kishore", "Bodhisattwa Prasad Majumder", "Aakanksha Naik", "Sigal Rahamimov", "Kyle Richardson", "Amanpreet Singh", "Harshit Surana", "Aryeh Tiktinsky", "Rosni Vasu", "Guy Wiener", "Chloe Anastasiades", "Stefan Candra", "Jason Dunkelberger", "Dan Emery", "Rob Evans", "Malachi Hamada", "Regan Huff", "Rodney Kinney", "Matt Latzke", "Jaron Lochner", "Ruben Lozano-Aguilera", "Cecile Nguyen", "Smita Rao", "Amber Tanaka", "Brooke Vlahos", "Peter Clark", "Doug Downey", "Yoav Goldberg", "Ashish Sabharwal", "Daniel S. Weld"], "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite", "comment": null, "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AstaBench\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u66f4\u4e25\u683c\u5730\u8bc4\u4f30AI\u79d1\u5b66\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5305\u542b2400\u591a\u4e2a\u79d1\u5b66\u53d1\u73b0\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u7814\u7a76\u73af\u5883\u548c\u5de5\u5177\uff0c\u5e76\u5bf957\u4e2a\u4ee3\u7406\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u57fa\u51c6\u5728\u79d1\u5b66\u7814\u7a76\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7528\u4f8b\u7684\u5168\u9762\u8861\u91cf\u3001\u7f3a\u5c11\u53ef\u590d\u73b0\u7684\u5de5\u5177\u3001\u672a\u8003\u8651\u6a21\u578b\u6210\u672c\u548c\u5de5\u5177\u8bbf\u95ee\u7b49\u6df7\u6742\u53d8\u91cf\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u63a5\u53e3\u548c\u57fa\u7ebf\u4ee3\u7406\u3002", "method": "\u5b9a\u4e49\u4e86\u66f4\u4e25\u683c\u7684\u4ee3\u7406\u57fa\u51c6\u539f\u5219\u548c\u5de5\u5177\uff0c\u5f00\u53d1\u4e86AstaBench\u5957\u4ef6\uff0c\u5305\u542b\u79d1\u5b66\u53d1\u73b0\u5168\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u751f\u4ea7\u7ea7\u641c\u7d22\u5de5\u5177\u7684\u7814\u7a76\u73af\u5883\uff0c\u5e76\u5efa\u7acb\u4e869\u7c7b\u79d1\u5b66\u4f18\u5316\u4ee3\u7406\u548c\u591a\u4e2a\u57fa\u7ebf\u3002", "result": "\u5bf957\u4e2a\u4ee3\u7406\u572822\u4e2a\u4ee3\u7406\u7c7b\u522b\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u610f\u4e49\u7684\u8fdb\u5c55\uff0c\u4f46AI\u5728\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u8f85\u52a9\u6311\u6218\u65b9\u9762\u4ecd\u7136\u6709\u5f88\u957f\u7684\u8def\u8981\u8d70\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u81ea\u52a8\u5316\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u6280\u672f\u4ecd\u8fdc\u672a\u8fbe\u5230\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u8f85\u52a9\u6311\u6218\u7684\u6c34\u5e73\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u63a8\u52a8\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.21184", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21184", "abs": "https://arxiv.org/abs/2510.21184", "authors": ["Stephen Zhao", "Aidan Li", "Rob Brekelmans", "Roger Grosse"], "title": "Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference", "comment": null, "summary": "Reinforcement learning (RL) has become a predominant technique to align\nlanguage models (LMs) with human preferences or promote outputs which are\ndeemed to be desirable by a given reward function. Standard RL approaches\noptimize average reward, while methods explicitly focused on reducing the\nprobability of undesired outputs typically come at a cost to average-case\nperformance. To improve this tradeoff, we introduce RePULSe, a new training\nmethod that augments the standard RL loss with an additional loss that uses\nlearned proposals to guide sampling low-reward outputs, and then reduces those\noutputs' probability. We run experiments demonstrating that RePULSe produces a\nbetter tradeoff of expected reward versus the probability of undesired outputs\nand is more adversarially robust, compared to standard RL alignment approaches\nand alternatives.", "AI": {"tldr": "RePULSe\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u589e\u52a0\u989d\u5916\u635f\u5931\u51fd\u6570\u6765\u51cf\u5c11\u4e0d\u826f\u8f93\u51fa\u7684\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u5747\u5956\u52b1\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u5e73\u5747\u5956\u52b1\uff0c\u4f46\u51cf\u5c11\u4e0d\u826f\u8f93\u51fa\u6982\u7387\u7684\u65b9\u6cd5\u901a\u5e38\u4f1a\u5f71\u54cd\u5e73\u5747\u6027\u80fd\u3002RePULSe\u65e8\u5728\u6539\u5584\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u5728\u6807\u51c6RL\u635f\u5931\u57fa\u7840\u4e0a\u589e\u52a0\u989d\u5916\u635f\u5931\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u63d0\u8bae\u6765\u6307\u5bfc\u91c7\u6837\u4f4e\u5956\u52b1\u8f93\u51fa\uff0c\u7136\u540e\u964d\u4f4e\u8fd9\u4e9b\u8f93\u51fa\u7684\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRePULSe\u5728\u671f\u671b\u5956\u52b1\u4e0e\u4e0d\u826f\u8f93\u51fa\u6982\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u5e76\u4e14\u6bd4\u6807\u51c6RL\u5bf9\u9f50\u65b9\u6cd5\u66f4\u5177\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "conclusion": "RePULSe\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u5e73\u5747\u5956\u52b1\u6027\u80fd\u4e0e\u51cf\u5c11\u4e0d\u826f\u8f93\u51fa\u6982\u7387\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21192", "abs": "https://arxiv.org/abs/2510.21192", "authors": ["Luca Demetrio", "Giovanni Apruzzese", "Kathrin Grosse", "Pavel Laskov", "Emil Lupu", "Vera Rimmer", "Philine Widmer"], "title": "Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer Reviews", "comment": null, "summary": "How does the progressive embracement of Large Language Models (LLMs) affect\nscientific peer reviewing? This multifaceted question is fundamental to the\neffectiveness -- as well as to the integrity -- of the scientific process.\nRecent evidence suggests that LLMs may have already been tacitly used in peer\nreviewing, e.g., at the 2024 International Conference of Learning\nRepresentations (ICLR). Furthermore, some efforts have been undertaken in an\nattempt to explicitly integrate LLMs in peer reviewing by various editorial\nboards (including that of ICLR'25). To fully understand the utility and the\nimplications of LLMs' deployment for scientific reviewing, a comprehensive\nrelevant dataset is strongly desirable. Despite some previous research on this\ntopic, such dataset has been lacking so far. We fill in this gap by presenting\nGenReview, the hitherto largest dataset containing LLM-written reviews. Our\ndataset includes 81K reviews generated for all submissions to the 2018--2025\neditions of the ICLR by providing the LLM with three independent prompts: a\nnegative, a positive, and a neutral one. GenReview is also linked to the\nrespective papers and their original reviews, thereby enabling a broad range of\ninvestigations. To illustrate the value of GenReview, we explore a sample of\nintriguing research questions, namely: if LLMs exhibit bias in reviewing (they\ndo); if LLM-written reviews can be automatically detected (so far, they can);\nif LLMs can rigorously follow reviewing instructions (not always) and whether\nLLM-provided ratings align with decisions on paper acceptance or rejection\n(holds true only for accepted papers). GenReview can be accessed at the\nfollowing link: https://anonymous.4open.science/r/gen_review.", "AI": {"tldr": "GenReview\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5305\u542bLLM\u64b0\u5199\u8bc4\u5ba1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b81K\u6761\u9488\u5bf9ICLR 2018-2025\u5e74\u6240\u6709\u63d0\u4ea4\u8bba\u6587\u7684\u8bc4\u5ba1\uff0c\u4f7f\u7528\u8d1f\u9762\u3001\u6b63\u9762\u548c\u4e2d\u6027\u4e09\u79cd\u63d0\u793a\u751f\u6210\uff0c\u5e76\u4e0e\u539f\u59cb\u8bba\u6587\u548c\u8bc4\u5ba1\u5173\u8054\u3002", "motivation": "\u968f\u7740LLMs\u5728\u79d1\u5b66\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u6f5c\u5728\u4f7f\u7528\u589e\u52a0\uff0c\u9700\u8981\u5168\u9762\u6570\u636e\u96c6\u6765\u7406\u89e3\u5176\u6548\u7528\u548c\u5f71\u54cd\uff0c\u6b64\u524d\u7f3a\u4e4f\u8fd9\u6837\u7684\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaGenReview\u6570\u636e\u96c6\uff0c\u4e3aICLR 2018-2025\u6240\u6709\u63d0\u4ea4\u8bba\u6587\u4f7f\u7528\u4e09\u79cd\u72ec\u7acb\u63d0\u793a\uff08\u8d1f\u9762\u3001\u6b63\u9762\u3001\u4e2d\u6027\uff09\u751f\u6210LLM\u8bc4\u5ba1\uff0c\u5e76\u4e0e\u539f\u59cb\u8bba\u6587\u548c\u8bc4\u5ba1\u5efa\u7acb\u5173\u8054\u3002", "result": "LLMs\u5728\u8bc4\u5ba1\u4e2d\u5b58\u5728\u504f\u89c1\uff1bLLM\u64b0\u5199\u7684\u8bc4\u5ba1\u76ee\u524d\u53ef\u88ab\u81ea\u52a8\u68c0\u6d4b\uff1bLLMs\u4e0d\u80fd\u59cb\u7ec8\u4e25\u683c\u9075\u5faa\u8bc4\u5ba1\u6307\u5357\uff1bLLM\u63d0\u4f9b\u7684\u8bc4\u5206\u4ec5\u4e0e\u5df2\u63a5\u53d7\u8bba\u6587\u7684\u51b3\u7b56\u4e00\u81f4\u3002", "conclusion": "GenReview\u6570\u636e\u96c6\u586b\u8865\u4e86LLM\u5728\u79d1\u5b66\u8bc4\u5ba1\u4e2d\u5e94\u7528\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u7814\u7a76\u8c03\u67e5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u8bc4\u5ba1\u4e2d\u7684\u5c40\u9650\u6027\u548c\u6f5c\u5728\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.21232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21232", "abs": "https://arxiv.org/abs/2510.21232", "authors": ["Waris Radji", "Odalric-Ambrym Maillard"], "title": "How Hard is it to Confuse a World Model?", "comment": null, "summary": "In reinforcement learning (RL) theory, the concept of most confusing\ninstances is central to establishing regret lower bounds, that is, the minimal\nexploration needed to solve a problem. Given a reference model and its optimal\npolicy, a most confusing instance is the statistically closest alternative\nmodel that makes a suboptimal policy optimal. While this concept is\nwell-studied in multi-armed bandits and ergodic tabular Markov decision\nprocesses, constructing such instances remains an open question in the general\ncase. In this paper, we formalize this problem for neural network world models\nas a constrained optimization: finding a modified model that is statistically\nclose to the reference one, while producing divergent performance between\noptimal and suboptimal policies. We propose an adversarial training procedure\nto solve this problem and conduct an empirical study across world models of\nvarying quality. Our results suggest that the degree of achievable confusion\ncorrelates with uncertainty in the approximate model, which may inform\ntheoretically-grounded exploration strategies for deep model-based RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u6784\u5efa\u6700\u6df7\u6dc6\u5b9e\u4f8b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u7406\u8bba\u4e2d\uff0c\u6700\u6df7\u6dc6\u5b9e\u4f8b\u5bf9\u4e8e\u5efa\u7acb\u9057\u61be\u4e0b\u754c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u6784\u5efa\u8fd9\u6837\u7684\u5b9e\u4f8b\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\uff1a\u5bfb\u627e\u7edf\u8ba1\u4e0a\u63a5\u8fd1\u53c2\u8003\u6a21\u578b\u7684\u4fee\u6539\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u6700\u4f18\u548c\u6b21\u4f18\u7b56\u7565\u4ea7\u751f\u4e0d\u540c\u7684\u6027\u80fd\u8868\u73b0\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u8bad\u7ec3\u7a0b\u5e8f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u53ef\u5b9e\u73b0\u7684\u6df7\u6dc6\u7a0b\u5ea6\u4e0e\u8fd1\u4f3c\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u80fd\u4e3a\u6df1\u5ea6\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u7684\u63a2\u7d22\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21638", "abs": "https://arxiv.org/abs/2510.21638", "authors": ["Tala Aljaafari", "Varun Kanade", "Philip Torr", "Christian Schroeder de Witt"], "title": "DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection", "comment": null, "summary": "Deploying reinforcement learning (RL) in safety-critical settings is\nconstrained by brittleness under distribution shift. We study\nout-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a\ntwo-statistic detector that revisits representation-heavy pipelines with a\nminimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel\nsimilarity to a training summary, capturing complementary global and local\ndeviations. Despite its simplicity, DEEDEE matches or surpasses contemporary\ndetectors across standard RL OOD suites, delivering a 600-fold reduction in\ncompute (FLOPs / wall-time) and an average 5% absolute accuracy gain over\nstrong baselines. Conceptually, our results indicate that diverse anomaly types\noften imprint on RL trajectories through a small set of low-order statistics,\nsuggesting a compact foundation for OOD detection in complex environments.", "AI": {"tldr": "DEEDEE\u662f\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684OOD\u68c0\u6d4b\u5668\uff0c\u4f7f\u7528\u4e24\u4e2a\u7edf\u8ba1\u91cf\uff08episodewise\u5747\u503c\u548cRBF\u6838\u76f8\u4f3c\u5ea6\uff09\u6765\u6355\u83b7\u5168\u5c40\u548c\u5c40\u90e8\u504f\u5dee\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\u53d7\u5230\u5206\u5e03\u504f\u79fb\u8106\u5f31\u6027\u7684\u9650\u5236\uff0c\u9700\u8981\u6709\u6548\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDEEDEE\u68c0\u6d4b\u5668\uff0c\u4ec5\u4f7f\u7528episodewise\u5747\u503c\u548cRBF\u6838\u76f8\u4f3c\u5ea6\u6765\u8bad\u7ec3\u6458\u8981\uff0c\u6355\u83b7\u4e92\u8865\u7684\u5168\u5c40\u548c\u5c40\u90e8\u504f\u5dee\u3002", "result": "DEEDEE\u5728\u6807\u51c6RL OOD\u6d4b\u8bd5\u5957\u4ef6\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u5f53\u4ee3\u68c0\u6d4b\u5668\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11600\u500d\uff0c\u7edd\u5bf9\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53475%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5404\u79cd\u5f02\u5e38\u7c7b\u578b\u901a\u5e38\u901a\u8fc7\u5c11\u91cf\u4f4e\u9636\u7edf\u8ba1\u91cf\u5728RL\u8f68\u8ff9\u4e2d\u7559\u4e0b\u5370\u8bb0\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7d27\u51d1\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21427", "abs": "https://arxiv.org/abs/2510.21427", "authors": ["Hao Liang", "Shuqing Shi", "Yudi Zhang", "Biwei Huang", "Yali Du"], "title": "Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Large-scale networked systems, such as traffic, power, and wireless grids,\nchallenge reinforcement-learning agents with both scale and environment shifts.\nTo address these challenges, we propose GSAC (Generalizable and Scalable\nActor-Critic), a framework that couples causal representation learning with\nmeta actor-critic learning to achieve both scalability and domain\ngeneralization. Each agent first learns a sparse local causal mask that\nprovably identifies the minimal neighborhood variables influencing its\ndynamics, yielding exponentially tight approximately compact representations\n(ACRs) of state and domain factors. These ACRs bound the error of truncating\nvalue functions to $\\kappa$-hop neighborhoods, enabling efficient learning on\ngraphs. A meta actor-critic then trains a shared policy across multiple source\ndomains while conditioning on the compact domain factors; at test time, a few\ntrajectories suffice to estimate the new domain factor and deploy the adapted\npolicy. We establish finite-sample guarantees on causal recovery, actor-critic\nconvergence, and adaptation gap, and show that GSAC adapts rapidly and\nsignificantly outperforms learning-from-scratch and conventional adaptation\nbaselines.", "AI": {"tldr": "GSAC\u6846\u67b6\u7ed3\u5408\u56e0\u679c\u8868\u793a\u5b66\u4e60\u548c\u5143\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5b66\u4e60\uff0c\u901a\u8fc7\u7a00\u758f\u5c40\u90e8\u56e0\u679c\u63a9\u7801\u548c\u8fd1\u4f3c\u7d27\u51d1\u8868\u793a\u5b9e\u73b0\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u9886\u57df\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\uff08\u5982\u4ea4\u901a\u3001\u7535\u529b\u3001\u65e0\u7ebf\u7f51\u7edc\uff09\u4e2d\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u89c4\u6a21\u548c\u73af\u5883\u53d8\u5316\u6311\u6218\u3002", "method": "\u6bcf\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u7a00\u758f\u5c40\u90e8\u56e0\u679c\u63a9\u7801\u8bc6\u522b\u5f71\u54cd\u5176\u52a8\u6001\u7684\u6700\u5c0f\u90bb\u57df\u53d8\u91cf\uff0c\u751f\u6210\u72b6\u6001\u548c\u9886\u57df\u56e0\u5b50\u7684\u8fd1\u4f3c\u7d27\u51d1\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u5143\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5728\u591a\u4e2a\u6e90\u57df\u4e0a\u8bad\u7ec3\u5171\u4eab\u7b56\u7565\u3002", "result": "GSAC\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u9886\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u5b66\u4e60\u548c\u4f20\u7edf\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GSAC\u6846\u67b6\u5728\u56e0\u679c\u6062\u590d\u3001\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6536\u655b\u548c\u9002\u5e94\u5dee\u8ddd\u65b9\u9762\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u8bc1\u660e\u5176\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21448", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21448", "abs": "https://arxiv.org/abs/2510.21448", "authors": ["Zhuojing Tian", "Yushu Chen"], "title": "Unified token representations for sequential decision models", "comment": null, "summary": "Transformers have demonstrated strong potential in offline reinforcement\nlearning (RL) by modeling trajectories as sequences of return-to-go, states,\nand actions. However, existing approaches such as the Decision Transformer(DT)\nand its variants suffer from redundant tokenization and quadratic attention\ncomplexity, limiting their scalability in real-time or resource-constrained\nsettings. To address this, we propose a Unified Token Representation (UTR) that\nmerges return-to-go, state, and action into a single token, substantially\nreducing sequence length and model complexity. Theoretical analysis shows that\nUTR leads to a tighter Rademacher complexity bound, suggesting improved\ngeneralization. We further develop two variants: UDT and UDC, built upon\ntransformer and gated CNN backbones, respectively. Both achieve comparable or\nsuperior performance to state-of-the-art methods with markedly lower\ncomputation. These findings demonstrate that UTR generalizes well across\narchitectures and may provide an efficient foundation for scalable control in\nfuture large decision models.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u4ee4\u724c\u8868\u793a(UTR)\u65b9\u6cd5\uff0c\u5c06\u56de\u62a5\u3001\u72b6\u6001\u548c\u52a8\u4f5c\u5408\u5e76\u4e3a\u5355\u4e2a\u4ee4\u724c\uff0c\u663e\u8457\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\u548c\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4ee4\u724c\u5197\u4f59\u548c\u4e8c\u6b21\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u65f6\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u4e86UTR\u65b9\u6cd5\uff0c\u5c06\u56de\u62a5\u3001\u72b6\u6001\u548c\u52a8\u4f5c\u5408\u5e76\u4e3a\u5355\u4e2a\u4ee4\u724c\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86UDT(Transformer\u67b6\u6784)\u548cUDC(\u95e8\u63a7CNN\u67b6\u6784)\u4e24\u4e2a\u53d8\u4f53\u3002", "result": "UTR\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7406\u8bba\u5206\u6790\u663e\u793aUTR\u5177\u6709\u66f4\u7d27\u7684Rademacher\u590d\u6742\u5ea6\u754c\u9650\uff0c\u8868\u660e\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UTR\u65b9\u6cd5\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u90fd\u80fd\u826f\u597d\u6cdb\u5316\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u51b3\u7b56\u6a21\u578b\u7684\u53ef\u6269\u5c55\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.21537", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21537", "abs": "https://arxiv.org/abs/2510.21537", "authors": ["Nikolai Gruzinov", "Ksenia Sycheva", "Earl T. Barr", "Alex Bezzubov"], "title": "Excision Score: Evaluating Edits with Surgical Precision", "comment": "Code is available at\n  https://anonymous.4open.science/r/excision-score-eval-B9AF/", "summary": "Many tasks revolve around editing a document, whether code or text. We\nformulate the revision similarity problem to unify a wide range of machine\nlearning evaluation problems whose goal is to assess a revision to an existing\ndocument. We observe that revisions usually change only a small portion of an\nexisting document, so the existing document and its immediate revisions share a\nmajority of their content. We formulate five adequacy criteria for revision\nsimilarity measures, designed to align them with human judgement. We show that\npopular pairwise measures, like BLEU, fail to meet these criteria, because\ntheir scores are dominated by the shared content. They report high similarity\nbetween two revisions when humans would assess them as quite different. This is\na fundamental flaw we address. We propose a novel static measure, Excision\nScore (ES), which computes longest common subsequence (LCS) to remove content\nshared by an existing document with the ground truth and predicted revisions,\nbefore comparing only the remaining divergent regions. This is analogous to a\nsurgeon creating a sterile field to focus on the work area. We use\napproximation to speed the standard cubic LCS computation to quadratic. In\ncode-editing evaluation, where static measures are often used as a cheap proxy\nfor passing tests, we demonstrate that ES surpasses existing measures. When\naligned with test execution on HumanEvalFix, ES improves over its nearest\ncompetitor, SARI, by 12% Pearson correlation and by >21% over standard measures\nlike BLEU. The key criterion is invariance to shared context; when we perturb\nHumanEvalFix with increased shared context, ES' improvement over SARI increases\nto 20% and >30% over standard measures. ES also handles other corner cases that\nother measures do not, such as correctly aligning moved code blocks, and\nappropriately rewarding matching insertions or deletions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fee\u8ba2\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u2014\u2014\u5207\u9664\u5206\u6570(ES)\uff0c\u901a\u8fc7\u79fb\u9664\u73b0\u6709\u6587\u6863\u4e0e\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u7684\u5171\u4eab\u5185\u5bb9\uff0c\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u5dee\u5f02\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u56e0\u5171\u4eab\u5185\u5bb9\u4e3b\u5bfc\u800c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5(\u5982BLEU)\u5728\u5904\u7406\u6587\u6863\u4fee\u8ba2\u4efb\u52a1\u65f6\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u5b83\u4eec\u7684\u9ad8\u5206\u4e3b\u8981\u7531\u5171\u4eab\u5185\u5bb9\u51b3\u5b9a\uff0c\u800c\u4eba\u7c7b\u5224\u65ad\u66f4\u5173\u6ce8\u5b9e\u9645\u53d8\u5316\u90e8\u5206\u3002\u8fd9\u5bfc\u81f4\u5728\u4eba\u7c7b\u8ba4\u4e3a\u5dee\u5f02\u5f88\u5927\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u62a5\u544a\u9ad8\u76f8\u4f3c\u5ea6\u3002", "method": "\u63d0\u51fa\u5207\u9664\u5206\u6570(ES)\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217(LCS)\u8bc6\u522b\u5e76\u79fb\u9664\u73b0\u6709\u6587\u6863\u4e0e\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u7684\u5171\u4eab\u5185\u5bb9\uff1b2)\u4ec5\u6bd4\u8f83\u5269\u4f59\u7684\u4e0d\u540c\u533a\u57df\uff1b3)\u901a\u8fc7\u8fd1\u4f3c\u7b97\u6cd5\u5c06\u6807\u51c6\u7acb\u65b9LCS\u8ba1\u7b97\u52a0\u901f\u5230\u4e8c\u6b21\u590d\u6742\u5ea6\u3002", "result": "\u5728\u4ee3\u7801\u7f16\u8f91\u8bc4\u4f30\u4e2d\uff0cES\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u5728HumanEvalFix\u4e0a\u4e0e\u6d4b\u8bd5\u6267\u884c\u5bf9\u9f50\u65f6\uff0c\u6bd4\u6700\u63a5\u8fd1\u7684\u7ade\u4e89\u5bf9\u624bSARI\u63d0\u9ad8\u4e8612%\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\uff0c\u6bd4\u6807\u51c6\u65b9\u6cd5(\u5982BLEU)\u63d0\u9ad8\u4e86>21%\u3002\u5f53\u589e\u52a0\u5171\u4eab\u4e0a\u4e0b\u6587\u65f6\uff0cES\u7684\u6539\u8fdb\u8fdb\u4e00\u6b65\u589e\u52a0\u523020%\u548c>30%\u3002", "conclusion": "ES\u901a\u8fc7\u5173\u6ce8\u5b9e\u9645\u53d8\u5316\u533a\u57df\u800c\u975e\u5171\u4eab\u5185\u5bb9\uff0c\u80fd\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u5224\u65ad\u5bf9\u9f50\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ee3\u7801\u7f16\u8f91\u7b49\u4fee\u8ba2\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u6839\u672c\u7f3a\u9677\u3002", "topic": "swe benchmark"}}
{"id": "tldr.2510.9e94fb5d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fangular%2Fweb-codegen-scorer%3Futm_source=tldrwebdev/1/0100019a15e9a9a3-bd3e7151-a193-418e-9858-13fa3953167c-000000/HC0XE17PT9V4Xa5yjyuf-bZeKlV9ilLaAc4kENQa6-U=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fangular%2Fweb-codegen-scorer%3Futm_source=tldrwebdev/1/0100019a15e9a9a3-bd3e7151-a193-418e-9858-13fa3953167c-000000/HC0XE17PT9V4Xa5yjyuf-bZeKlV9ilLaAc4kENQa6-U=428", "authors": ["TLDR Newsletter"], "title": "Web Codegen Scorer", "comment": "Source: TLDR Newsletter, Date: 2025-10-24, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fangular%2Fweb-codegen-scorer%3Futm_source=tldrwebdev/1/0100019a15e9a9a3-bd3e7151-a193-418e-9858-13fa3953167c-000000/HC0XE17PT9V4Xa5yjyuf-bZeKlV9ilLaAc4kENQa6-U=428", "summary": "Web Codegen Scorer (GitHub Repo) Web Codegen Scorer is a tool developed by the Angular team at Google for evaluating the quality of web code generated by LLMs. It focuses on web code quality and allows users to compare models, iterate on prompts, and monitor code quality over time using built-in checks like build success, security, and coding best practices.", "source": "tldr", "AI": {"tldr": "Web Codegen Scorer\u662f\u8c37\u6b4cAngular\u56e2\u961f\u5f00\u53d1\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684web\u4ee3\u7801\u8d28\u91cf\uff0c\u652f\u6301\u6a21\u578b\u6bd4\u8f83\u3001\u63d0\u793a\u8bcd\u8fed\u4ee3\u548c\u4ee3\u7801\u8d28\u91cf\u76d1\u63a7\u3002", "motivation": "\u968f\u7740LLM\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u8bc4\u4f30\u751f\u6210\u7684web\u4ee3\u7801\u8d28\u91cf\uff0c\u786e\u4fdd\u4ee3\u7801\u7b26\u5408\u6784\u5efa\u3001\u5b89\u5168\u548c\u6700\u4f73\u5b9e\u8df5\u6807\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u8bc4\u5206\u5de5\u5177\uff0c\u5185\u7f6e\u6784\u5efa\u6210\u529f\u6027\u3001\u5b89\u5168\u6027\u548c\u7f16\u7801\u6700\u4f73\u5b9e\u8df5\u7b49\u68c0\u67e5\u9879\uff0c\u652f\u6301\u7528\u6237\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u3001\u8fed\u4ee3\u63d0\u793a\u8bcd\u5e76\u76d1\u63a7\u4ee3\u7801\u8d28\u91cf\u53d8\u5316\u3002", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u6709\u6548\u8bc4\u4f30web\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u9009\u62e9\u66f4\u4f18\u7684\u6a21\u578b\u548c\u63d0\u793a\u8bcd\u7b56\u7565\u3002", "conclusion": "Web Codegen Scorer\u4e3aLLM\u751f\u6210\u7684web\u4ee3\u7801\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u6574\u4f53\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "tldr.2510.1c2312b4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F10%2F16%2Fbubble-launches-ai-agent-merge-conversational-coding-visual-app-building%2F%3Futm_source=tldrdesign/1/0100019a162a486e-b6b68292-e01b-4da9-9282-a44dbcb5030d-000000/dZIkdt1Q2GeURXpH7TleobD0syfRHTalIPpAeNzigEc=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F10%2F16%2Fbubble-launches-ai-agent-merge-conversational-coding-visual-app-building%2F%3Futm_source=tldrdesign/1/0100019a162a486e-b6b68292-e01b-4da9-9282-a44dbcb5030d-000000/dZIkdt1Q2GeURXpH7TleobD0syfRHTalIPpAeNzigEc=428", "authors": ["TLDR Newsletter"], "title": "Bubble Launches AI Agent to Merge Conversational Coding with Visual App Building", "comment": "Source: TLDR Newsletter, Date: 2025-10-24, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F10%2F16%2Fbubble-launches-ai-agent-merge-conversational-coding-visual-app-building%2F%3Futm_source=tldrdesign/1/0100019a162a486e-b6b68292-e01b-4da9-9282-a44dbcb5030d-000000/dZIkdt1Q2GeURXpH7TleobD0syfRHTalIPpAeNzigEc=428", "summary": "Bubble Launches AI Agent to Merge Conversational Coding with Visual App Building (2 minute read) Bubble AI Agent combines conversational AI coding with visual drag-and-drop editing to build production-grade applications, addressing security and quality concerns that limit AI coding tool adoption. The platform allows users to switch between natural language prompts and visual editing with full control, targeting the 72% of surveyed users worried about security vulnerabilities in AI-generated c...", "source": "tldr", "AI": {"tldr": "Bubble AI Agent \u7ed3\u5408\u5bf9\u8bdd\u5f0fAI\u7f16\u7a0b\u548c\u53ef\u89c6\u5316\u62d6\u62fd\u7f16\u8f91\uff0c\u6784\u5efa\u751f\u4ea7\u7ea7\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u51b3\u9650\u5236AI\u7f16\u7a0b\u5de5\u5177\u91c7\u7528\u7684\u5b89\u5168\u548c\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u9488\u5bf972%\u7684\u7528\u6237\u62c5\u5fc3AI\u751f\u6210\u4ee3\u7801\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u7684\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3AI\u7f16\u7a0b\u5de5\u5177\u91c7\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u8d28\u91cf\u969c\u788d\u3002", "method": "\u5c06\u5bf9\u8bdd\u5f0fAI\u7f16\u7a0b\u4e0e\u53ef\u89c6\u5316\u62d6\u62fd\u7f16\u8f91\u76f8\u7ed3\u5408\uff0c\u5141\u8bb8\u7528\u6237\u5728\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u53ef\u89c6\u5316\u7f16\u8f91\u4e4b\u95f4\u5207\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u63a7\u5236\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u6784\u5efa\u751f\u4ea7\u7ea7\u5e94\u7528\u7a0b\u5e8f\u7684\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86AI\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u548c\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "Bubble AI Agent\u6210\u529f\u878d\u5408\u4e86\u5bf9\u8bdd\u5f0f\u7f16\u7a0b\u548c\u53ef\u89c6\u5316\u7f16\u8f91\uff0c\u4e3aAI\u7f16\u7a0b\u5de5\u5177\u7684\u5b89\u5168\u91c7\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2510.cc049f31", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fvibe-codings-real-problem-isnt-bugs-its-judgment%2F%3Futm_source=tldrinfosec/1/0100019a16549147-5eae1088-369e-4a95-8b1b-4f6058788190-000000/v_D1NDZtIjjQQ1qT11bMH_ESUfWhhZfzVWuDpUVUCHE=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fvibe-codings-real-problem-isnt-bugs-its-judgment%2F%3Futm_source=tldrinfosec/1/0100019a16549147-5eae1088-369e-4a95-8b1b-4f6058788190-000000/v_D1NDZtIjjQQ1qT11bMH_ESUfWhhZfzVWuDpUVUCHE=428", "authors": ["TLDR Newsletter"], "title": "Vibe Coding's Real Problem Isn't Bugs\u2014It's Judgment", "comment": "Source: TLDR Newsletter, Date: 2025-10-24, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securityweek.com%2Fvibe-codings-real-problem-isnt-bugs-its-judgment%2F%3Futm_source=tldrinfosec/1/0100019a16549147-5eae1088-369e-4a95-8b1b-4f6058788190-000000/v_D1NDZtIjjQQ1qT11bMH_ESUfWhhZfzVWuDpUVUCHE=428", "summary": "Vibe Coding's Real Problem Isn't Bugs\u2014It's Judgment (3 minute read) AI code generation enables rapid, large-scale software creation, but speed and flawed judgment are the real risks. While AI-made code has vulnerability rates similar to human work, issues like ineffective coding practices and skipped reviews mean more exposure to breaches, so experts urge integrating security directly into AI workflows and cultivating best usage practices.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u867d\u7136\u80fd\u5feb\u901f\u5927\u89c4\u6a21\u521b\u5efa\u8f6f\u4ef6\uff0c\u4f46\u771f\u6b63\u7684\u98ce\u9669\u5728\u4e8e\u901f\u5ea6\u8fc7\u5feb\u548c\u5224\u65ad\u5931\u8bef\uff0c\u800c\u975e\u4ee3\u7801\u6f0f\u6d1e\u672c\u8eab\u3002AI\u751f\u6210\u4ee3\u7801\u7684\u6f0f\u6d1e\u7387\u4e0e\u4eba\u5de5\u4ee3\u7801\u76f8\u4f3c\uff0c\u4f46\u65e0\u6548\u7f16\u7801\u5b9e\u8df5\u548c\u8df3\u8fc7\u4ee3\u7801\u5ba1\u67e5\u7b49\u95ee\u9898\u5bfc\u81f4\u66f4\u591a\u5b89\u5168\u66b4\u9732\u98ce\u9669\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u6280\u672f\u5e26\u6765\u7684\u5b9e\u9645\u98ce\u9669\uff0c\u5f3a\u8c03\u901f\u5ea6\u8fc7\u5feb\u548c\u5224\u65ad\u5931\u8bef\u6bd4\u4ee3\u7801\u6f0f\u6d1e\u672c\u8eab\u66f4\u5371\u9669\uff0c\u9700\u8981\u5173\u6ce8\u5b89\u5168\u5b9e\u8df5\u96c6\u6210\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u5de5\u4ee3\u7801\u7684\u6f0f\u6d1e\u7387\u5bf9\u6bd4\uff0c\u8bc6\u522bAI\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u98ce\u9669\u56e0\u7d20\uff0c\u5982\u65e0\u6548\u7f16\u7801\u5b9e\u8df5\u548c\u8df3\u8fc7\u4ee3\u7801\u5ba1\u67e5\u7b49\u3002", "result": "AI\u751f\u6210\u4ee3\u7801\u7684\u6f0f\u6d1e\u7387\u4e0e\u4eba\u7c7b\u4ee3\u7801\u76f8\u4f3c\uff0c\u4f46\u7531\u4e8e\u7f16\u7801\u5b9e\u8df5\u95ee\u9898\u548c\u5ba1\u67e5\u7f3a\u5931\uff0c\u5bfc\u81f4\u6574\u4f53\u5b89\u5168\u66b4\u9732\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "\u4e13\u5bb6\u5efa\u8bae\u5c06\u5b89\u5168\u76f4\u63a5\u96c6\u6210\u5230AI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5e76\u57f9\u517b\u6700\u4f73\u4f7f\u7528\u5b9e\u8df5\u6765\u5e94\u5bf9AI\u4ee3\u7801\u751f\u6210\u7684\u98ce\u9669\u3002", "topic": "swe application"}}
{"id": "tldr.2510.2916ec85", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farctotherium.substack.com%2Fp%2Fllm-exchange-rates-updated%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/vxIIQiUmsKV5CkmMmUdyCO2Ssi-qPiW7DHxLbL3FBE0=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farctotherium.substack.com%2Fp%2Fllm-exchange-rates-updated%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/vxIIQiUmsKV5CkmMmUdyCO2Ssi-qPiW7DHxLbL3FBE0=428", "authors": ["TLDR Newsletter"], "title": "LLM Exchange Rates Updated", "comment": "Source: TLDR Newsletter, Date: 2025-10-24, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farctotherium.substack.com%2Fp%2Fllm-exchange-rates-updated%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/vxIIQiUmsKV5CkmMmUdyCO2Ssi-qPiW7DHxLbL3FBE0=428", "summary": "LLM Exchange Rates Updated (10 minute read) Testing implicit value systems across current LLMs reveals that almost all models value nonwhites over whites and women over men by large margins\u2014Claude Sonnet 4.5 values saving whites from terminal illness at 1/18th the level of South Asians, while GPT-5 shows near-perfect egalitarianism except for whites valued at 1/20th nonwhites. The methodology involved sending thousands of queries comparing hypothetical scenarios like \"receive $X vs. save Y pe...", "source": "tldr", "AI": {"tldr": "\u6d4b\u8bd5\u5f53\u524dLLM\u9690\u542b\u4ef7\u503c\u7cfb\u7edf\u53d1\u73b0\uff0c\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u90fd\u66f4\u91cd\u89c6\u975e\u767d\u79cd\u4eba\u800c\u975e\u767d\u79cd\u4eba\uff0c\u66f4\u91cd\u89c6\u5973\u6027\u800c\u975e\u7537\u6027\u3002Claude Sonnet 4.5\u62ef\u6551\u767d\u79cd\u4eba\u7684\u4ef7\u503c\u4ec5\u4e3a\u5357\u4e9a\u4eba\u76841/18\uff0cGPT-5\u51e0\u4e4e\u5b8c\u5168\u5e73\u7b49\u4e3b\u4e49\uff0c\u4f46\u767d\u79cd\u4eba\u4ef7\u503c\u4ec5\u4e3a\u975e\u767d\u79cd\u4eba\u76841/20\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u9690\u542b\u4ef7\u503c\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u4e0d\u540c\u79cd\u65cf\u548c\u6027\u522b\u7684\u4ef7\u503c\u5224\u65ad\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u53d1\u9001\u6570\u5343\u4e2a\u67e5\u8be2\uff0c\u6bd4\u8f83\u5047\u8bbe\u573a\u666f\u5982\"\u83b7\u5f97X\u7f8e\u5143vs\u62ef\u6551Y\u4eba\"\uff0c\u5206\u6790LLM\u5728\u4e0d\u540c\u79cd\u65cf\u548c\u6027\u522b\u7fa4\u4f53\u95f4\u7684\u4ef7\u503c\u6743\u8861\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u6d4b\u8bd5\u7684LLM\u90fd\u663e\u793a\u51fa\u660e\u663e\u7684\u4ef7\u503c\u504f\u89c1\uff1a\u66f4\u91cd\u89c6\u975e\u767d\u79cd\u4eba\u800c\u975e\u767d\u79cd\u4eba\uff0c\u66f4\u91cd\u89c6\u5973\u6027\u800c\u975e\u7537\u6027\u3002Claude Sonnet 4.5\u5bf9\u767d\u79cd\u4eba\u7684\u4ef7\u503c\u8bc4\u4f30\u4ec5\u4e3a\u5357\u4e9a\u4eba\u76841/18\uff0cGPT-5\u867d\u7136\u63a5\u8fd1\u5b8c\u5168\u5e73\u7b49\u4e3b\u4e49\uff0c\u4f46\u4ecd\u5c06\u767d\u79cd\u4eba\u4ef7\u503c\u8bc4\u4f30\u4e3a\u975e\u767d\u79cd\u4eba\u76841/20\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41LLM\u5b58\u5728\u663e\u8457\u7684\u79cd\u65cf\u548c\u6027\u522b\u4ef7\u503c\u504f\u89c1\uff0c\u9700\u8981\u5728\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u66f4\u52a0\u5173\u6ce8\u8fd9\u4e9b\u9690\u542b\u7684\u4ef7\u503c\u7cfb\u7edf\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.06e25879", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadrs-ucb.notion.site%2Fmoe-load-balancing%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/7zp-ugIFaQslw7u9QnM7dg02POOtwDLXTeUTbYemaGQ=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadrs-ucb.notion.site%2Fmoe-load-balancing%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/7zp-ugIFaQslw7u9QnM7dg02POOtwDLXTeUTbYemaGQ=428", "authors": ["TLDR Newsletter"], "title": "Automating Algorithm Discovery: A Case Study in MoE Load Balancing", "comment": "Source: TLDR Newsletter, Date: 2025-10-24, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadrs-ucb.notion.site%2Fmoe-load-balancing%3Futm_source=tldrai/1/0100019a165e0f31-f167fe45-f762-4452-ba65-ab6247461e27-000000/7zp-ugIFaQslw7u9QnM7dg02POOtwDLXTeUTbYemaGQ=428", "summary": "Automating Algorithm Discovery: A Case Study in MoE Load Balancing (7 minute read) OpenEvolve is an evolutionary coding agent that turns large language models (LLMs) into autonomous code optimizers that can discover breakthrough algorithms. In tests, it independently discovers and surpasses highly optimized algorithms engineered by human experts to achieve a 5.0x speedup in LLM inference. The ability to devise sophisticated computational strategies proves that AI-Driven Research for Systems c...", "source": "tldr", "AI": {"tldr": "OpenEvolve\u662f\u4e00\u4e2a\u8fdb\u5316\u7f16\u7801\u4ee3\u7406\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u81ea\u4e3b\u4ee3\u7801\u4f18\u5316\u5668\uff0c\u80fd\u591f\u53d1\u73b0\u7a81\u7834\u6027\u7b97\u6cd5\u3002\u5728\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u72ec\u7acb\u53d1\u73b0\u5e76\u8d85\u8d8a\u4e86\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u7684\u9ad8\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u5728LLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e865.0\u500d\u52a0\u901f\u3002", "motivation": "\u81ea\u52a8\u5316\u7b97\u6cd5\u53d1\u73b0\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u7814\u7a76\u6765\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7f16\u7801\u4ee3\u7406\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u81ea\u4e3b\u4ee3\u7801\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u8fdb\u5316\u8fc7\u7a0b\u53d1\u73b0\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728MoE\u8d1f\u8f7d\u5e73\u8861\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u72ec\u7acb\u53d1\u73b0\u4e86\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7b97\u6cd5\uff0c\u5728LLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e865.0\u500d\u52a0\u901f\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u7cfb\u7edf\u7814\u7a76\u5177\u6709\u53d1\u73b0\u590d\u6742\u8ba1\u7b97\u7b56\u7565\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u7b97\u6cd5\u53d1\u73b0\u7684\u53ef\u884c\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2510.33dd88f1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgheiler.com%2Fpost%2Fdagster-slurm%2F%3Futm_source=tldrdata/1/0100019a2522715b-cc849310-f30a-47c5-abce-3b061c55b7cc-000000/7GGEfuZQsVqx-m1gaXpPQSe4mEpTB_rlUlh7U0uUce4=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgheiler.com%2Fpost%2Fdagster-slurm%2F%3Futm_source=tldrdata/1/0100019a2522715b-cc849310-f30a-47c5-abce-3b061c55b7cc-000000/7GGEfuZQsVqx-m1gaXpPQSe4mEpTB_rlUlh7U0uUce4=428", "authors": ["TLDR Newsletter"], "title": "Rediscovering the SUPER in Supercomputing", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgheiler.com%2Fpost%2Fdagster-slurm%2F%3Futm_source=tldrdata/1/0100019a2522715b-cc849310-f30a-47c5-abce-3b061c55b7cc-000000/7GGEfuZQsVqx-m1gaXpPQSe4mEpTB_rlUlh7U0uUce4=428", "summary": "Rediscovering the SUPER in Supercomputing (2 minute read) dagster-slurm integrates Dagster's modern data orchestration with Slurm-managed HPC clusters, enabling seamless workflow portability across laptops, CI pipelines, and Tier-0 supercomputers without code changes.", "source": "tldr", "AI": {"tldr": "dagster-slurm\u5c06Dagster\u7684\u73b0\u4ee3\u6570\u636e\u7f16\u6392\u4e0eSlurm\u7ba1\u7406\u7684HPC\u96c6\u7fa4\u96c6\u6210\uff0c\u5b9e\u73b0\u4ece\u7b14\u8bb0\u672c\u7535\u8111\u5230\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u65e0\u7f1d\u5de5\u4f5c\u6d41\u79fb\u690d", "motivation": "\u89e3\u51b3HPC\u73af\u5883\u4e2d\u5de5\u4f5c\u6d41\u5728\u4e0d\u540c\u8ba1\u7b97\u5e73\u53f0\u95f4\u79fb\u690d\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u5f00\u53d1\u73af\u5883\u5230\u751f\u4ea7\u73af\u5883\u7684\u65e0\u7f1d\u8fc7\u6e21", "method": "\u5f00\u53d1dagster-slurm\u96c6\u6210\u5de5\u5177\uff0c\u5c06Dagster\u7f16\u6392\u7cfb\u7edf\u4e0eSlurm\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u8fde\u63a5\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u63a5\u53e3", "result": "\u5b9e\u73b0\u4e86\u5de5\u4f5c\u6d41\u5728\u7b14\u8bb0\u672c\u7535\u8111\u3001CI\u6d41\u6c34\u7ebf\u548cTier-0\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e4b\u95f4\u7684\u65e0\u7f1d\u79fb\u690d\uff0c\u65e0\u9700\u4ee3\u7801\u66f4\u6539", "conclusion": "\u8be5\u96c6\u6210\u5de5\u5177\u6210\u529f\u5f25\u5408\u4e86\u73b0\u4ee3\u6570\u636e\u7f16\u6392\u4e0e\u4f20\u7edfHPC\u7cfb\u7edf\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6d41\u90e8\u7f72\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2510.258f9f64", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrnewsletter/1/0100019a253164a1-79150c9a-3824-4247-9309-78b4e25e1aeb-000000/84in06qL4xDhBQ2NiebZODChtyPwsMBgemRCr768xDc=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrnewsletter/1/0100019a253164a1-79150c9a-3824-4247-9309-78b4e25e1aeb-000000/84in06qL4xDhBQ2NiebZODChtyPwsMBgemRCr768xDc=428", "authors": ["TLDR Newsletter"], "title": "Code like a surgeon", "comment": "Source: TLDR Newsletter, Date: 2025-10-27, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrnewsletter/1/0100019a253164a1-79150c9a-3824-4247-9309-78b4e25e1aeb-000000/84in06qL4xDhBQ2NiebZODChtyPwsMBgemRCr768xDc=428", "summary": "Code like a surgeon (4 minute read) Surgeons do the actual work. However, their skills and time are highly leveraged with a support team that handles prep, secondary tasks, and admin. The surgeon focuses on the important stuff they're good at. AI coding tools enable developers to spend all of their time doing the things that matter. There are a lot of secondary tasks that AI agents are now good enough to help out with.", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u5de5\u5177\u8ba9\u5f00\u53d1\u8005\u80fd\u4e13\u6ce8\u4e8e\u91cd\u8981\u5de5\u4f5c\uff0c\u5c31\u50cf\u5916\u79d1\u533b\u751f\u6709\u652f\u6301\u56e2\u961f\u5904\u7406\u8f85\u52a9\u4efb\u52a1\u4e00\u6837", "motivation": "\u5f00\u53d1\u8005\u5728\u7f16\u7801\u8fc7\u7a0b\u4e2d\u9700\u8981\u5904\u7406\u5927\u91cf\u8f85\u52a9\u6027\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5360\u7528\u4e86\u4ed6\u4eec\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5f00\u53d1\u5de5\u4f5c\u7684\u65f6\u95f4", "method": "\u5229\u7528AI\u4ee3\u7406\u6765\u5904\u7406\u7f16\u7801\u4e2d\u7684\u6b21\u8981\u4efb\u52a1\uff0c\u8ba9\u5f00\u53d1\u8005\u4e13\u6ce8\u4e8e\u91cd\u8981\u7684\u6838\u5fc3\u5de5\u4f5c", "result": "AI\u5de5\u5177\u73b0\u5728\u5df2\u8db3\u591f\u6210\u719f\uff0c\u80fd\u591f\u6709\u6548\u534f\u52a9\u5904\u7406\u7f16\u7801\u4e2d\u7684\u8f85\u52a9\u4efb\u52a1", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u50cf\u5916\u79d1\u533b\u751f\u4e00\u6837\u4e13\u6ce8\u4e8e\u6838\u5fc3\u6280\u80fd", "topic": "code agent"}}
{"id": "wechat.2510.23c1ee25", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MDc0MzAyNw==&mid=2247484144&idx=1&sn=df448757f98e0f646649856b36568bc4&chksm=c34e36a3fce625b167ed3af9638ec72fc48f08c8faacf1c2b9b69e9f7b06fef059c4290f1e36#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MDc0MzAyNw==&mid=2247484144&idx=1&sn=df448757f98e0f646649856b36568bc4&chksm=c34e36a3fce625b167ed3af9638ec72fc48f08c8faacf1c2b9b69e9f7b06fef059c4290f1e36#rd", "authors": ["\u963f\u6f6e\u7684\u535a\u5f08\u667a\u80fd\u7814\u7a76"], "title": "NerulPS 2025| \u7f51\u7edc\u5316\u591a\u667a\u80fd\u4f53<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u8d1d\u53f6\u65af\u81ea\u6211\u56fe\uff08ego-graph\uff09\u63a8\u65ad", "comment": "Source: WeChat, Published: 2025-10-27 13:42:42", "summary": "NerulPS 2025| \u7f51\u7edc\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8d1d\u53f6\u65af\u81ea\u6211\u56fe\uff08ego-graph\uff09\u63a8\u65adposter bayesian ego-graph inference for networked multi-agent reinforcement learning wei duan \u00b7 jie lu \u00b7 junyu xuan [ abstract\uff09 abstract\uff1a in networked multi-agent reinforcement learning \uff08networked-marl", "AI": {"tldr": "NerulPS 2025| \u7f51\u7edc\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8d1d\u53f6\u65af\u81ea\u6211\u56fe\uff08ego-graph\uff09\u63a8\u65adposter bayesian ego-graph inference for networked multi-agent reinforcement learning wei duan \u00b7 jie lu \u00b7 junyu xuan [ abstract\uff09 abstract\uff1a in networked mult...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.f31b0361", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY2Njg3NQ==&mid=2247526706&idx=1&sn=73287c3807aa90a217dc4557913975f7&chksm=c2adb4a998ccae84eb13473ed856027b3a6cedc904cc74e2dfb947a7db27842b93d6d5989055#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY2Njg3NQ==&mid=2247526706&idx=1&sn=73287c3807aa90a217dc4557913975f7&chksm=c2adb4a998ccae84eb13473ed856027b3a6cedc904cc74e2dfb947a7db27842b93d6d5989055#rd", "authors": ["Matlab\u79d1\u7814\u52a9\u624b"], "title": "\u3010EI\u590d\u73b0\u3011\u57fa\u4e8e\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5fae\u80fd\u6e90\u7f51\u80fd\u91cf\u7ba1\u7406\u4e0e\u4f18\u5316\u7b56\u7565\u7814\u7a76\u9644Python\u4ee3\u7801", "comment": "Source: WeChat, Published: 2025-10-27 12:57:15", "summary": "\uff08\u4e8c\uff09\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u4ef7\u503c \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u901a\u8fc7 \u201c\u667a\u80fd\u4f53 - \u73af\u5883\u201d \u4ea4\u4e92\u673a\u5236\uff0c\u65e0\u9700\u5efa\u7acb\u7cbe\u786e\u7cfb\u7edf\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u4f18\u5316\uff0c\u5177\u5907\u5f3a\u9c81\u68d2\u6027\u4e0e\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u5fae\u80fd\u6e90\u7f51\u80fd\u91cf\u7ba1\u7406\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "AI": {"tldr": "\uff08\u4e8c\uff09\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u4ef7\u503c \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u901a\u8fc7 \u201c\u667a\u80fd\u4f53 - \u73af\u5883\u201d \u4ea4\u4e92\u673a\u5236\uff0c\u65e0\u9700\u5efa\u7acb\u7cbe\u786e\u7cfb\u7edf\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u4f18\u5316\uff0c\u5177\u5907\u5f3a\u9c81\u68d2\u6027\u4e0e\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u5fae\u80fd\u6e90\u7f51\u80fd\u91cf\u7ba1\u7406\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.51b5f333", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDI0OTQyOQ==&mid=2247485131&idx=1&sn=f1073f7d7ff747053d6938a8dc2a99e8&chksm=c0c2109c6a080d31b3bfcdc68969d52e94ef65efc672da8cba3c351f06f1bf5a252049585f15#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDI0OTQyOQ==&mid=2247485131&idx=1&sn=f1073f7d7ff747053d6938a8dc2a99e8&chksm=c0c2109c6a080d31b3bfcdc68969d52e94ef65efc672da8cba3c351f06f1bf5a252049585f15#rd", "authors": ["\u5218\u5c0f\u5f3a\u7684\u535a\u5ba2"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u57fa\u7840-PPO", "comment": "Source: WeChat, Published: 2025-10-27 12:30:39", "summary": "\u4ee3\u7801\u5b9e\u73b0\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\uff0c\u770b\u8fd9\u4e2a\u89c6\u9891\u5c31\u591f\u4e86\u3002\u672c\u6587\u5c06\u5e26\u4f60\u4ece \u7b56\u7565\u68af\u5ea6\u7684\u57fa\u672c\u539f\u7406 \u51fa\u53d1\uff0c\u6df1\u5165\u5256\u6790 PPO \u7684\u6838\u5fc3\u601d\u60f3\u3001\u6570\u5b66\u63a8\u5bfc\u4ee5\u53ca\u5b83\u5728\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u903b\u8f91\u3002", "AI": {"tldr": "\u4ee3\u7801\u5b9e\u73b0\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\uff0c\u770b\u8fd9\u4e2a\u89c6\u9891\u5c31\u591f\u4e86\u3002\u672c\u6587\u5c06\u5e26\u4f60\u4ece \u7b56\u7565\u68af\u5ea6\u7684\u57fa\u672c\u539f\u7406 \u51fa\u53d1\uff0c\u6df1\u5165\u5256\u6790 PPO \u7684\u6838\u5fc3\u601d\u60f3\u3001\u6570\u5b66\u63a8\u5bfc\u4ee5\u53ca\u5b83\u5728\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u903b\u8f91\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.e70733d0", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247586057&idx=3&sn=f0656fc3670a34f521e969fad646189a&chksm=fae4377c8f7f0824b7585b9e96f979f4fa2daff506738128f80d042b4c4070f3a49464c99c46#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247586057&idx=3&sn=f0656fc3670a34f521e969fad646189a&chksm=fae4377c8f7f0824b7585b9e96f979f4fa2daff506738128f80d042b4c4070f3a49464c99c46#rd", "authors": ["AI\u601d\u60f3\u4f1a"], "title": "\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u71b5\u63a7\u5236\uff1aCE-GPPO\u3001EPO\u4e0eAsyPPO\u6280\u672f\u65b9\u6848\u5bf9\u6bd4\u8be6\u89e3", "comment": "Source: WeChat, Published: 2025-10-27 10:17:11", "summary": "LLM\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6700\u8fd1\u8fdb\u5c55\u5f88\u5feb\uff0cSOTA\u6a21\u578b\u5728\u5404\u79cd\u63a8\u7406benchmark\u4e0a\u7684\u8868\u73b0\u786e\u5b9e\u4eae\u773c\u3002\u4f46\u66f4\u503c\u5f97\u5173\u6ce8\u7684\u5176\u5b9e\u662f\u53e6\u4e00\u6761\u4fe1\u606f\u2014\u2014\u4eceRutgers\u5230Alibaba\u518d\u5230HKUST\uff0c\u8fd9\u4e9b\u7814\u7a76\u56e2\u961f\u6b63\u5728\u653b\u514b\u7684\u662fRL\u9886\u57df\u7684\u4e00\u4e2a\u8001\u5927\u96be\uff1a\u600e\u4e48\u63a7\u5236\u597d\u71b5\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u9000", "AI": {"tldr": "LLM\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6700\u8fd1\u8fdb\u5c55\u5f88\u5feb\uff0cSOTA\u6a21\u578b\u5728\u5404\u79cd\u63a8\u7406benchmark\u4e0a\u7684\u8868\u73b0\u786e\u5b9e\u4eae\u773c\u3002\u4f46\u66f4\u503c\u5f97\u5173\u6ce8\u7684\u5176\u5b9e\u662f\u53e6\u4e00\u6761\u4fe1\u606f\u2014\u2014\u4eceRutgers\u5230Alibaba\u518d\u5230HKUST\uff0c\u8fd9\u4e9b\u7814\u7a76\u56e2\u961f\u6b63\u5728\u653b\u514b\u7684\u662fRL\u9886\u57df\u7684\u4e00\u4e2a\u8001\u5927\u96be\uff1a\u600e\u4e48\u63a7\u5236\u597d\u71b5\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u9000", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.229ee19c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjgxMjI0Mw==&mid=2247483660&idx=1&sn=c859f82c34290406a85418c2fac4c3da&chksm=fedeb70194deec84d8d7093efcf425e609cc3e789d679e4ca8c362dffe73db3979ceca6e89cf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjgxMjI0Mw==&mid=2247483660&idx=1&sn=c859f82c34290406a85418c2fac4c3da&chksm=fedeb70194deec84d8d7093efcf425e609cc3e789d679e4ca8c362dffe73db3979ceca6e89cf#rd", "authors": ["\u963f\u70c1\u8bf4"], "title": "\u5982\u4f55\u6784\u5efa\u548c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u201c\u81ea\u9a71\u529b\u201d", "comment": "Source: WeChat, Published: 2025-10-27 06:47:01", "summary": "\u90a3\u4e48\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5b66\u4e60\u7684\u54ea\u4e2a\u90e8\u5206\u5165\u624b\u5462\uff1f\u4ece\u65e5\u5e38\u7684\u5b66\u4e60\u5b89\u6392\u548c\u5956\u52b1\u5165\u624b\u3002\u6211\u4eec\u5c06\u6bcf\u5929\u5c0f\u6d4b\u9a8c\u7684\u7ed3\u679c\u548c\u5c0f\u73a9\u5177\u6302\u94a9\uff0c\u5355\u5143\u8003\u7684\u7ed3\u679c\u548c\u4e00\u6b21\u5927\u9910\u6302\u94a9\uff0c\u671f\u4e2d\u671f\u672b\u7684\u7ed3\u679c\u548c\u4e00\u6b21\u201c\u672a\u77e5\u5927\u5956\u52b1\u201d\u6302\u94a9\uff0c\u660e\u786e\u89c4\u5b9a\u5b66\u4e60\u7ed3\u679c\u548c\u5373\u65f6\u5956\u52b1\u7684\u5173", "AI": {"tldr": "\u90a3\u4e48\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5b66\u4e60\u7684\u54ea\u4e2a\u90e8\u5206\u5165\u624b\u5462\uff1f\u4ece\u65e5\u5e38\u7684\u5b66\u4e60\u5b89\u6392\u548c\u5956\u52b1\u5165\u624b\u3002\u6211\u4eec\u5c06\u6bcf\u5929\u5c0f\u6d4b\u9a8c\u7684\u7ed3\u679c\u548c\u5c0f\u73a9\u5177\u6302\u94a9\uff0c\u5355\u5143\u8003\u7684\u7ed3\u679c\u548c\u4e00\u6b21\u5927\u9910\u6302\u94a9\uff0c\u671f\u4e2d\u671f\u672b\u7684\u7ed3\u679c\u548c\u4e00\u6b21\u201c\u672a\u77e5\u5927\u5956\u52b1\u201d\u6302\u94a9\uff0c\u660e\u786e\u89c4\u5b9a\u5b66\u4e60\u7ed3\u679c\u548c\u5373\u65f6\u5956\u52b1\u7684\u5173", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.2263df61", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487343&idx=1&sn=410902217460bfe9766d9a427872d91c&chksm=c1a3c75310ab335ed2019762e65c0a200fcc7070d4ebd6d1b9e9d37d6f74669aeffb5c6e8cd5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487343&idx=1&sn=410902217460bfe9766d9a427872d91c&chksm=c1a3c75310ab335ed2019762e65c0a200fcc7070d4ebd6d1b9e9d37d6f74669aeffb5c6e8cd5#rd", "authors": ["NLP\u5b66\u4e60\u52a0\u6cb9\u7ad9"], "title": "NLP\u8bba\u6587\u901f\u8bfb\uff08 MIT\u51fa\u54c1\uff09| RL\u7684\u5965\u5361\u59c6\u5243\u5200\uff1a\u4e3a\u4ec0\u4e48\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u9057\u5fd8\u66f4\u5c11", "comment": "Source: WeChat, Published: 2025-10-27 05:48:34", "summary": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e3b\u6d41\u5fae\u8c03\u65b9\u5f0f\u2014\u2014\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u53d1\u73b0\u5728\u8fbe\u5230\u76f8\u540c\u65b0\u4efb\u52a1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0cRL\u663e\u8457\u66f4\u5c11\u9057\u5fd8\u65e7\u4efb\u52a1\u77e5\u8bc6\u3002\u4e3a\u63ed\u793a\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u6838\u5fc3\u89c2\u70b9\uff1a\u9057\u5fd8\u7684\u7a0b\u5ea6\u53ef\u4ee5\u7531\u65b0", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e3b\u6d41\u5fae\u8c03\u65b9\u5f0f\u2014\u2014\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u53d1\u73b0\u5728\u8fbe\u5230\u76f8\u540c\u65b0\u4efb\u52a1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0cRL\u663e\u8457\u66f4\u5c11\u9057\u5fd8\u65e7\u4efb\u52a1\u77e5\u8bc6\u3002\u4e3a\u63ed\u793a\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u6838\u5fc3\u89c2\u70b9\uff1a\u9057\u5fd8\u7684\u7a0b\u5ea6\u53ef\u4ee5\u7531\u65b0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.50d70270", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMTg5NTYxNg==&mid=2247496774&idx=1&sn=97efdcddf6b0ebdbf07d461563981313&chksm=fb61af5fc016fa981143fad32210fb84c4b86b8128a6ba52a5146bf8767c35a4a12c80112df2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMTg5NTYxNg==&mid=2247496774&idx=1&sn=97efdcddf6b0ebdbf07d461563981313&chksm=fb61af5fc016fa981143fad32210fb84c4b86b8128a6ba52a5146bf8767c35a4a12c80112df2#rd", "authors": ["F8AI"], "title": "\u5b89\u5168\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u76fe\u724c\u2014\u2014\u8bc4\u4f30\u5c4f\u853d\u4f5c\u4e3a\u5b89\u5168 RL \u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u6f5c\u5728\u7f3a\u70b9\u3002", "comment": "Source: WeChat, Published: 2025-10-27 05:14:24", "summary": "\u5c06\u5c4f\u853d\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u8981\u65b9\u6cd5\u662f\u9884\u5c4f\u853d\u548c\u540e\u5c4f\u853d\uff0c\u5b83\u4eec\u7684\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u5e72\u9884\u65b9\u5f0f\u3002\u56fe3 \uff08\u5de6\uff09\u5c55\u793a\u4e86\u540e\u5c4f\u853d\uff1a\u5c4f\u853d\u4f4d\u4e8e\u667a\u80fd\u4f53\u548c\u73af\u5883\u4e4b\u95f4\uff0c\u6301\u7eed\u76d1\u63a7\u73af\u5883\u72b6\u6001\u548c\u667a\u80fd\u4f53\u9009\u62e9\u7684\u64cd\u4f5c\u3002", "AI": {"tldr": "\u5c06\u5c4f\u853d\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u8981\u65b9\u6cd5\u662f\u9884\u5c4f\u853d\u548c\u540e\u5c4f\u853d\uff0c\u5b83\u4eec\u7684\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u5e72\u9884\u65b9\u5f0f\u3002\u56fe3 \uff08\u5de6\uff09\u5c55\u793a\u4e86\u540e\u5c4f\u853d\uff1a\u5c4f\u853d\u4f4d\u4e8e\u667a\u80fd\u4f53\u548c\u73af\u5883\u4e4b\u95f4\uff0c\u6301\u7eed\u76d1\u63a7\u73af\u5883\u72b6\u6001\u548c\u667a\u80fd\u4f53\u9009\u62e9\u7684\u64cd\u4f5c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.17eb42e8", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NzExODk0MQ==&mid=2247489086&idx=1&sn=0be8802a931449f086ff50544ccbde78&chksm=c1528fa8e6ffecbdfb96ac7b233c6830a420f0ec740156d30b99d0828e468751a20c1091e037#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NzExODk0MQ==&mid=2247489086&idx=1&sn=0be8802a931449f086ff50544ccbde78&chksm=c1528fa8e6ffecbdfb96ac7b233c6830a420f0ec740156d30b99d0828e468751a20c1091e037#rd", "authors": ["\u5c0f\u5c0f\u4f55\u5148\u751f"], "title": "Nature2025 | <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u53ef\u4ee5\u81ea\u6211\u6539\u53d8\u641c\u7d22\u7a7a\u95f4\u4e86\uff1f", "comment": "Source: WeChat, Published: 2025-10-27 03:00:23", "summary": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a DiscoRL \u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u662f\u901a\u8fc7\u5143\u5b66\u4e60\uff08Meta-Learning\uff09\u65b9\u6cd5\u81ea\u4e3b\u53d1\u73b0\u7684\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u76ee\u524d\u624b\u52a8\u8bbe\u8ba1\u7684RL\u7b97\u6cd5\uff08\u5982MuZero\u3001PPO\uff09\u3002", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a DiscoRL \u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u662f\u901a\u8fc7\u5143\u5b66\u4e60\uff08Meta-Learning\uff09\u65b9\u6cd5\u81ea\u4e3b\u53d1\u73b0\u7684\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u76ee\u524d\u624b\u52a8\u8bbe\u8ba1\u7684RL\u7b97\u6cd5\uff08\u5982MuZero\u3001PPO\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.4bf58173", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571704&idx=3&sn=90a38b03525d3bd46364d11ad825259a&chksm=96dedda4fbe5e0e7c8bde537e0be86258326926473cd0723c0011d56b24abeb437ae349eb69b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571704&idx=3&sn=90a38b03525d3bd46364d11ad825259a&chksm=96dedda4fbe5e0e7c8bde537e0be86258326926473cd0723c0011d56b24abeb437ae349eb69b#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "DRL\u5723\u7ecf2025\u6700\u65b0\u7248-\u300a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>:\u5bfc\u8bba\u7b2c\u4e8c\u7248\u300b\u514d\u8d39pdf\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-10-27 00:01:59", "summary": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "AI": {"tldr": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
