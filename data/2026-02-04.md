<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.LG](#cs.LG) [Total: 36]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.SE](#cs.SE) [Total: 14]
- [tldr.article](#tldr.article) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

TL;DR: STEMVerse：一个诊断LLM在STEM领域推理能力的框架，通过"学科×认知"双轴标签系统化分析模型表现，揭示结构性失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前评估范式将基准测试视为孤立的"孤岛"，仅提供聚合分数，无法区分模型错误源于领域知识不足还是认知能力缺陷，限制了诊断价值。

Method: 提出STEMVerse诊断框架，将2万多个STEM问题重新聚合到统一的"学科×认知"能力空间，为每个实例分配双轴标签，系统评估不同参数规模和训练范式的LLM。

Result: 实证结果揭示了STEM推理中的结构性失败模式，通过多学科覆盖和细粒度认知分层提供了理解LLM科学推理特征的清晰视角。

Conclusion: STEMVerse通过整合多学科覆盖和细粒度认知分层，为理解LLM的科学推理特性提供了清晰且可操作的视角，超越了传统的结果导向评估方法。

Abstract: As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [2] [WideSeek: Advancing Wide Research via Multi-Agent Scaling](https://arxiv.org/abs/2602.02636)
*Ziyang Huang,Haolin Ren,Xiaowei Yuan,Jiawei Wang,Zhongtao Jiang,Kun Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 该论文提出了WideSeekBench基准和WideSeek多智能体架构，用于解决从深度研究向广度研究范式转变中的信息检索与合成问题。


<details>
  <summary>Details</summary>
Motivation: 搜索智能正从深度研究向广度研究演进，但该领域缺乏专门的基准测试和优化方法，阻碍了进展。需要解决在复杂约束下并行检索和综合全面信息的问题。

Method: 1) 构建WideSeekBench基准，通过多阶段数据管道确保目标信息量、逻辑约束和领域的多样性；2) 提出WideSeek动态分层多智能体架构，可根据任务需求自主分叉并行子智能体；3) 设计统一训练框架，线性化多智能体轨迹并使用端到端强化学习优化系统。

Result: 实验结果表明WideSeek和多智能体强化学习的有效性，证明扩展智能体数量是推进广度研究范式的有前景方向。

Conclusion: 通过数据管道和智能体优化两个角度深入研究了广度研究，提出的基准和架构为解决广度研究中的挑战提供了有效方案，多智能体扩展是重要发展方向。

Abstract: Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.

</details>


### [3] [InfMem: Learning System-2 Memory Control for Long-Context Agent](https://arxiv.org/abs/2602.02704)
*Xinyu Wang,Mingze Li,Peng Lu,Xiao-Wen Chang,Lifeng Shang,Jinping Li,Fei Mi,Prasanna Parthasarathi,Yufei Cui*

Main category: cs.CL

TL;DR: InfMem是一种控制中心代理，通过PreThink-Retrieve-Write协议实现System-2式控制，主动监控证据充分性并进行针对性检索，在超长文档问答任务中显著提升准确率并大幅减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 处理超长文档需要在严格内存限制下综合分散在远距离片段中的稀疏证据。现有的流式代理采用被动内存更新策略，往往无法保留多跳推理所需的低显著性桥接证据。

Method: 提出InfMem控制中心代理，采用PreThink-Retrieve-Write协议实现主动控制：监控证据充分性、进行针对性文档内检索、应用证据感知的联合压缩来更新有限内存。引入实用的SFT-to-RL训练方法，使检索、写入和停止决策与最终任务正确性对齐。

Result: 在32k到1M tokens的超长QA基准测试中，InfMem在不同骨干模型上均优于MemAgent：Qwen3-1.7B提升+10.17点，Qwen3-4B提升+11.84点，Qwen2.5-7B提升+8.23点平均绝对准确率，同时通过自适应早停平均减少3.9倍推理时间（最高达5.1倍）。

Conclusion: InfMem通过主动控制策略有效解决了超长文档推理中的证据保留问题，在保持高准确率的同时显著提升了推理效率。

Abstract: Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\times$ on average (up to $5.1\times$) via adaptive early stopping.

</details>


### [4] [From Task Solving to Robust Real-World Adaptation in LLM Agents](https://arxiv.org/abs/2602.02760)
*Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 论文提出一个评估LLM智能体在真实部署场景下鲁棒性的基准测试，挑战传统"干净接口"假设，测试智能体在部分可观测、动态环境、噪声信号和动态状态下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估通常假设"干净接口"：动态稳定、工具可靠、目标明确，这高估了智能体的实际部署准备度。现实中智能体面临规则不明确、信号不可靠、环境变化和隐式多利益相关者目标等挑战。

Method: 使用基于网格的游戏环境，具有简单目标但长时程执行。设计违反"干净接口"假设但可解决的场景，迫使智能体推断规则、为信息付费、适应环境变化、在噪声下谨慎行动。测试五种最先进的LLM智能体。

Result: 发现名义任务解决能力与部署鲁棒性之间存在巨大差距。性能随网格大小和时程增加而下降，但排名不稳定：在特定不确定性机制下，较弱模型可能击败较强模型。智能体在没有明确指导的情况下会权衡完成度、效率和惩罚避免。

Conclusion: 研究揭示了模型特定的敏感性和失败驱动因素，强调了在部分可观测性、噪声和非平稳性下进行验证、安全动作选择和目标推断的重要性，为智能体鲁棒性研究提供了新方向。

Abstract: Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a "clean interface" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.

</details>


### [5] [Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication](https://arxiv.org/abs/2602.02843)
*Polina Tsvilodub,Karl Mulligan,Todd Snider,Robert D. Hawkins,Michael Franke*

Main category: cs.CL

TL;DR: 研究人类在不确定情境下是否寻求澄清问题取决于情境不确定性和替代行动成本，两者交互作用：当错误行动代价高时，不确定性影响最大


<details>
  <summary>Details</summary>
Motivation: 在沟通情境中，代理面临不确定性时可以选择通过澄清问题来减少不确定性，也可以直接行动。研究者预测澄清问题的决策取决于情境不确定性和替代行动成本，且这两个因素会相互作用：当错误行动代价高昂时，不确定性影响最大

Method: 基于预期遗憾的计算模型：衡量代理在信息不全时行动可能损失多少。通过两个实验验证预测：一个检验纯语言回应，另一个扩展到澄清问题与非语言行动之间的选择

Result: 实验结果表明人类倾向于按比例寻求澄清：当在不确定性下行动可能造成重大损失时，人们更可能寻求澄清，体现了理性权衡

Conclusion: 人类澄清问题的决策遵循理性权衡原则，根据在不确定性下行动可能造成的损失程度按比例寻求澄清，支持基于预期遗憾的计算模型

Abstract: When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.

</details>


### [6] [CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning](https://arxiv.org/abs/2602.02979)
*Ran Li,Zeyuan Liu,Yinghao chen,Bingxiang He,Jiarui Yuan,Zixuan Fu,Weize Chen,Jinyi Hu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: CPMöbius提出了一种无需外部训练数据的协作式强化学习框架，通过教练-玩家角色协作提升数学推理能力，显著优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理任务上的进展严重依赖大量高质量人工标注数据（SFT或RL），这种监督密集型训练范式不可持续且已出现扩展性瓶颈。需要探索无需外部训练数据的新方法。

Method: 提出CPMöbius协作教练-玩家范式：教练根据玩家能力生成针对性指令并获得玩家性能变化的奖励，玩家通过解决教练生成的逐步提升难度的任务获得奖励。两者作为独立但协作的角色进行合作优化循环。

Result: 在Qwen2.5-Math-7B-Instruct模型上，CPMöbius整体准确率平均提升+4.9，分布外准确率平均提升+5.4，分别超过RENT方法+1.5和R-zero方法+4.2。

Conclusion: CPMöbius证明了无需外部训练数据即可显著提升LLM数学推理能力的可行性，为减少对人工标注数据的依赖提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.

</details>


### [7] [LatentMem: Customizing Latent Memory for Multi-Agent Systems](https://arxiv.org/abs/2602.03036)
*Muxin Fu,Guibin Zhang,Xiangyuan Xue,Yafu Li,Zefeng He,Siyuan Huang,Xiaoye Qu,Yu Cheng,Yang Yang*

Main category: cs.CL

TL;DR: LatentMem是一个可学习的多智能体记忆框架，通过角色感知定制和紧凑潜在表示解决现有多智能体记忆的同质化和信息过载问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多智能体系统存在两个基本瓶颈：1) 由于缺乏角色感知定制导致记忆同质化；2) 过于细粒度的记忆条目导致信息过载。需要设计更高效的多智能体记忆机制。

Method: 提出LatentMem框架，包含：1) 经验库以轻量形式存储原始交互轨迹；2) 记忆合成器根据检索的经验和智能体特定上下文合成紧凑的潜在记忆。还提出Latent Memory Policy Optimization (LMPO)，通过潜在记忆传播任务级优化信号，鼓励生成紧凑且高实用性的表示。

Result: 在多样化基准测试和主流MAS框架上的实验表明，LatentMem相比原始设置性能提升高达19.36%，并且始终优于现有记忆架构，无需修改底层框架。

Conclusion: LatentMem通过可学习的角色感知记忆定制，有效解决了多智能体记忆的同质化和信息过载问题，显著提升了多智能体系统的集体智能表现。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.

</details>


### [8] [ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution](https://arxiv.org/abs/2602.03075)
*Junjie Huang,Jiarui Qin,Di Yin,Weiwen Liu,Yong Yu,Xing Sun,Weinan Zhang*

Main category: cs.CL

TL;DR: ReMiT提出双向训练框架，利用RL调优模型的推理先验动态重加权预训练中的关键token，建立自我增强的飞轮效应


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练是单向的（预训练→后训练），但后训练的洞察能否反过来改进预训练基础模型尚未探索。作者希望建立自我增强的飞轮循环，无需专门训练的教师或参考模型

Method: 分析训练动态，发现中期训练（退火）阶段是模型能力的关键转折点。提出ReMiT方法，利用RL调优模型的推理先验在中期训练阶段动态重加权token，优先处理对推理至关重要的token

Result: 在10个预训练基准（数学、代码和通用推理）上平均提升3%，并在整个后训练流程中保持超过2%的增益，验证了迭代反馈循环的有效性

Conclusion: 成功建立了双向训练框架，实现了LLM的持续自我增强演化，打破了传统单向训练的限制

Abstract: Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.

</details>


### [9] [AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback](https://arxiv.org/abs/2602.03084)
*Zhitao Gao,Jie Ma,Xuhong Li,Pengyu Li,Ning Qu,Yaqiang Wu,Hui Liu,Jun Liu*

Main category: cs.CL

TL;DR: AERO是一个无监督的自主推理进化框架，通过双循环系统实现自我提问、回答和批评，基于ZPD理论利用熵定位解决"可解性差距"，在多个基准测试中显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理中依赖专家标注数据和外部验证器，而现有的自我进化范式往往无法找到最佳学习区域，并且可能通过有缺陷的内部反馈强化集体幻觉和错误先验。

Method: 提出AERO框架：1) 基于ZPD理论使用熵定位来针对"可解性差距"；2) 采用独立反事实校正进行鲁棒验证；3) 引入交错训练策略来同步功能角色能力增长并防止课程崩溃。

Result: 在9个基准测试（涵盖3个领域）中，AERO在Qwen3-4B-Base上平均提升4.57%，在Qwen3-8B-Base上平均提升5.10%，优于竞争基线方法。

Conclusion: AERO通过无监督的自主推理进化框架，有效解决了现有自我进化方法的局限性，显著提升了LLM的推理能力，代码已开源。

Abstract: Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.

</details>


### [10] [Test-time Recursive Thinking: Self-Improvement without External Feedback](https://arxiv.org/abs/2602.03094)
*Yufan Zhuang,Chandan Singh,Liyuan Liu,Yelong Shen,Dinghuai Zhang,Jingbo Shang,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: TRT框架让LLMs在测试时通过递归思考自我改进，无需额外训练，在AIME-25/24上达到100%准确率


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否在不进行额外训练的情况下自我改进，解决两个核心挑战：高效生成多样化高质量候选方案，以及在缺乏真实监督时可靠选择正确答案

Method: 提出Test-time Recursive Thinking (TRT)框架，这是一种迭代自我改进方法，基于特定策略、累积知识和自生成验证信号进行条件生成

Result: 开源模型在AIME-25/24上达到100%准确率；闭源模型在LiveCodeBench最难问题上提升10.4-14.8个百分点，无需外部反馈

Conclusion: TRT框架有效解决了LLMs在测试时自我改进的挑战，显著提升了推理能力，展示了无需额外训练的自我改进潜力

Abstract: Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.

</details>


### [11] [MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research](https://arxiv.org/abs/2602.03318)
*Yifan Shi,Jialong Shi,Jiayi Wang,Ye Fan,Jianyong Sun*

Main category: cs.CL

TL;DR: MIRROR是一个免微调的多智能体框架，可将自然语言优化问题直接转换为数学模型和求解器代码，通过执行驱动的迭代自适应修订和分层检索机制实现可靠建模。


<details>
  <summary>Details</summary>
Motivation: 运筹学依赖专家建模，过程缓慢脆弱且难以适应新场景。现有LLM方法需要昂贵后训练或多智能体框架，但缺乏可靠的协作纠错和任务特定检索，常导致错误输出。

Method: 提出免微调的端到端多智能体框架MIRROR，包含两个核心机制：(1)执行驱动的迭代自适应修订实现自动纠错；(2)分层检索从精心策划的示例库中获取相关建模和编码示例。

Result: 在标准OR基准测试中优于现有方法，在IndustryOR和Mamo-ComplexLP等复杂工业数据集上表现突出。

Conclusion: 通过结合精确的外部知识注入和系统化纠错，MIRROR为非专家用户提供高效可靠的OR建模解决方案，克服通用LLM在专家优化任务中的根本限制。

Abstract: Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.

</details>


### [12] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: LLM批评模型的主动干预不一定能提升可靠性，即使离线准确率高（AUROC 0.94）也可能导致性能严重下降。作者提出部署前测试方法，通过少量任务预测干预效果，避免有害干预。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设LLM批评模型的主动干预能提高可靠性，但实际部署时的效果缺乏深入理解。作者发现即使离线准确率高的批评模型也可能导致性能严重退化，需要系统评估干预的安全性。

Method: 提出"破坏-恢复权衡"理论框架，认为干预既能恢复失败轨迹，也可能破坏原本会成功的轨迹。基于此开发部署前测试方法，仅需50个任务的试点数据来估计干预可能带来的帮助或危害。

Result: 实验显示干预效果高度可变：在某个模型上导致26个百分点的性能崩溃，而在另一个模型上几乎无影响。在ALFWorld基准测试中干预带来适度改进（+2.8pp，p=0.014）。部署前测试能准确预测干预效果。

Conclusion: LLM批评模型的准确率不足以决定干预是否安全。提出的框架主要价值在于识别何时不应干预，在部署前预防严重性能退化。干预决策需要更精细的评估而非仅依赖离线准确率。

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [13] [PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning](https://arxiv.org/abs/2602.03352)
*Yunzhi Shen,Hao Zhou,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: PEGRL是一个两阶段强化学习框架，通过后编辑作为辅助任务来稳定机器翻译训练，解决了传统RL方法中噪声学习信号和探索-优化平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机器翻译方法面临两个主要挑战：1) 蒙特卡洛回报估计产生的噪声学习信号；2) 巨大的轨迹空间导致全局探索优先于细粒度局部优化。这些问题限制了翻译质量提升。

Method: 提出两阶段RL框架PEGRL：第一阶段生成翻译输出，第二阶段将翻译输出作为输入进行后编辑。通过后编辑任务提供更稳定的学习信号，同时支持全局探索和局部优化。采用任务特定权重方案平衡翻译和后编辑目标。

Result: 在英语→芬兰语、英语→土耳其语和英语↔中文翻译任务上，PEGRL相比RL基线方法取得一致性能提升。英语→土耳其语任务上，COMET-KIWI指标与先进的LLM系统（DeepSeek-V3.2）相当。

Conclusion: PEGRL通过引入后编辑作为辅助任务，有效解决了机器翻译中强化学习的稳定性问题，实现了更好的探索-优化平衡，提高了样本效率。

Abstract: Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \textbf{PEGRL}, a \textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\to$Finnish, English$\to$Turkish, and English$\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).

</details>


### [14] [Verified Critical Step Optimization for LLM Agents](https://arxiv.org/abs/2602.03412)
*Mukai Li,Qingcheng Zeng,Tianqing Fang,Zhenwen Liang,Linfeng Song,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: CSO提出了一种针对大语言模型代理的关键步骤优化方法，通过验证关键决策点来提升长时程任务表现，相比传统方法显著减少了监督需求并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法面临三个主要挑战：1) 仅基于结果的奖励无法精确归因到中间步骤；2) 估计的步骤级奖励存在系统性噪声；3) 蒙特卡洛采样方法计算成本过高。需要一种更精细、高效的监督方法。

Method: CSO方法聚焦于验证关键步骤（决策点），从失败的策略轨迹开始，使用过程奖励模型识别候选关键步骤，利用专家模型提出高质量替代方案，然后让策略模型从这些替代方案继续执行直到任务完成。只有成功纠正结果的替代方案才被验证并用作DPO训练数据。

Result: 在GAIA-Text-103和XBench-DeepSearch基准测试中，CSO相比SFT基线分别实现了37%和26%的相对改进，显著优于其他后训练方法，同时仅需监督轨迹中16%的步骤。

Conclusion: 基于选择性验证的学习方法对代理后训练非常有效，能够提供细粒度、可验证的监督，同时避免轨迹级粗糙性和步骤级噪声问题。

Abstract: As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.

</details>


### [15] [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces](https://arxiv.org/abs/2602.03442)
*Mingxuan Du,Benfeng Xu,Chiwei Zhu,Shaohan Wang,Pengyu Wang,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: A-RAG是一个代理式检索增强生成框架，通过向模型暴露分层检索接口，让模型自主参与检索决策，从而更有效地利用前沿语言模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统未能充分利用前沿语言模型的推理和长程工具使用能力，它们要么采用单次检索算法，要么预定义工作流程，都不允许模型参与检索决策，限制了模型改进带来的效率提升。

Method: A-RAG框架向模型暴露分层检索接口，提供三种检索工具：关键词搜索、语义搜索和块读取，使代理能够跨多个粒度自适应地搜索和检索信息。

Result: 在多个开放域QA基准测试中，A-RAG在检索token数量相当或更少的情况下，始终优于现有方法，表明A-RAG能有效利用模型能力并动态适应不同的RAG任务。

Conclusion: A-RAG通过让模型参与检索决策，能够更有效地利用前沿语言模型的能力，并系统地研究了模型规模和测试时计算对A-RAG性能的影响。

Abstract: Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.

</details>


### [16] [Learning to Reason Faithfully through Step-Level Faithfulness Maximization](https://arxiv.org/abs/2602.03507)
*Runquan Gui,Yafu Li,Xiaoye Qu,Ziyan Liu,Yeqiu Cheng,Yu Cheng*

Main category: cs.CL

TL;DR: FaithRL是一个强化学习框架，通过优化推理忠实性来减少LLM的幻觉，使用几何奖励设计和忠实性感知的优势调制机制来惩罚无支持的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法主要依赖稀疏的结果奖励，对中间步骤监督不足，导致过度自信和虚假推理，增加了幻觉问题。

Method: 提出FaithRL框架，形式化忠实性最大化目标，引入几何奖励设计和忠实性感知的优势调制机制，为步骤级分配信用，惩罚无支持的步骤同时保留有效的部分推导。

Result: 在不同骨干模型和基准测试中，FaithRL一致降低了幻觉率，同时保持（并经常提高）答案正确性。进一步分析确认FaithRL提高了步骤级推理忠实性并具有鲁棒泛化能力。

Conclusion: FaithRL通过优化推理忠实性有效减少LLM幻觉，提供了一种通用的强化学习框架来改善多步推理任务的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.

</details>


### [17] [SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue](https://arxiv.org/abs/2602.03548)
*Yuqin Dai,Ning Gao,Wei Zhang,Jie Wang,Zichen Luo,Jinpeng Wang,Yujie Wang,Ruiyuan Wu,Chaozheng Wang*

Main category: cs.CL

TL;DR: SEAD是一个用于服务对话的自我进化代理框架，通过解耦用户建模为配置文件控制器和用户角色扮演模型，无需大规模人工标注即可学习有效策略，显著提升任务完成率和对话效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的方法在服务对话中表现不佳，主要原因是依赖噪声大、质量低的人类对话数据，存在数据稀缺和难以模拟真实目标导向用户行为的挑战。

Method: 提出SEAD框架，将用户建模解耦为两个组件：1) 配置文件控制器，生成多样化用户状态以管理训练课程；2) 用户角色扮演模型，专注于真实角色扮演。这种设计确保环境提供适应性训练场景而非不公平的对手。

Result: SEAD显著优于开源基础模型和闭源商业模型，将任务完成率提高了17.6%，对话效率提高了11.1%。

Conclusion: SEAD框架通过自我进化机制有效解决了服务对话中的数据稀缺和用户行为模拟问题，无需大规模人工标注即可实现高性能对话代理。

Abstract: Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.

</details>


### [18] [$V_0$: A Generalist Value Model for Any Policy at State Zero](https://arxiv.org/abs/2602.03584)
*Yi-Kai Zhang,Zhiyuan Yao,Hongyan Hao,Yueqing Sun,Qi Gu,Hui Su,Xunliang Cai,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.CL

TL;DR: 提出V₀通用价值模型，无需参数更新即可估计任何模型在未见提示上的预期性能，用于GRPO训练中的采样预算分配和部署时的模型路由


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic方法中价值模型需要与策略模型同步训练，成本高昂；GRPO方法虽然消除了价值模型但需要大量采样来稳定基线估计。需要一种更高效的价值估计方法。

Method: 将策略的动态能力作为显式上下文输入，利用指令-性能对历史来动态分析模型能力，而不是依赖参数拟合来感知能力变化。专注于状态零（初始提示）的价值估计，即V₀模型。

Result: V₀在GRPO训练中能有效预测成功率，实现高效采样预算分配；在部署时作为路由器，在LLM路由任务中实现性能与成本的帕累托最优权衡，显著优于启发式预算分配方法。

Conclusion: V₀通用价值模型提供了一种无需参数更新的高效价值估计方法，既能优化强化学习训练效率，又能实现部署时的智能模型路由。

Abstract: Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.

</details>


### [19] [TRE: Encouraging Exploration in the Trust Region](https://arxiv.org/abs/2602.03635)
*Chao Huang,Yujing Lu,Quangang Li,Shenghe Wang,Yan Wang,Yueyang Zhang,Long Xia,Jiashu Zhao,Zhiyuan Sun,Daiting Shi,Tingwen Liu*

Main category: cs.CL

TL;DR: 本文提出Trust Region Entropy (TRE)方法，针对大语言模型在强化学习中标准熵正则化效果不佳的问题，通过在信任区域内鼓励探索来改善性能。


<details>
  <summary>Details</summary>
Motivation: 标准熵正则化在强化学习中通常用于增强探索，但在大语言模型中效果甚微甚至降低性能。作者认为这是由于大语言模型具有大规模词汇表和长生成序列带来的累积尾部风险，导致概率质量被稀释到无效标记的尾部而非聚焦于合理候选。

Method: 提出Trust Region Entropy (TRE)方法，该方法鼓励探索严格限制在模型的信任区域内，避免在无效标记尾部进行探索。

Result: 在数学推理(MATH)、组合搜索(Countdown)和偏好对齐(HH)任务上的广泛实验表明，TRE始终优于原始PPO、标准熵正则化和其他探索基线方法。

Conclusion: TRE方法通过限制探索在信任区域内，有效解决了大语言模型中熵正则化失败的问题，在各种任务上表现出优越性能。

Abstract: Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.

</details>


### [20] [Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models](https://arxiv.org/abs/2602.03704)
*Yu Tian,Linh Huynh,Katerina Christhilf,Shubham Chakraborty,Micah Watanabe,Tracy Arner,Danielle McNamara*

Main category: cs.CL

TL;DR: ReQUESTA是一个混合多智能体框架，用于生成具有认知多样性的多项选择题，通过分解任务、协调LLM智能体和基于规则的组件，相比单次提示的GPT-5基线，能产生更具挑战性、区分度更高且与阅读理解表现更一致的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型使自动生成多项选择题变得可行，但可靠地生成满足特定认知需求（如文本理解、推理和主旨把握）的题目仍然是一个挑战。需要一种更系统的方法来确保生成题目的质量和可控性。

Method: ReQUESTA是一个混合多智能体框架，将MCQ生成分解为专门的子任务，协调LLM驱动的智能体和基于规则的组件，支持规划、受控生成、迭代评估和后处理。在学术说明文阅读理解研究中，与单次提示的GPT-5基线进行比较。

Result: ReQUESTA生成的题目在难度和区分度上表现更优，与整体阅读理解表现更一致。专家评估显示，ReQUESTA题目更符合核心概念，干扰项的语言一致性和语义合理性更好，特别是在推理题方面。

Conclusion: 混合智能体编排可以系统性地提高LLM生成的可控性和可靠性，工作流设计是超越单次提示的结构化生成的关键杠杆。

Abstract: Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.

</details>


### [21] [OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering](https://arxiv.org/abs/2602.03707)
*Yifan Zhu,Xinyu Mu,Tao Feng,Zhonghong Ou,Yuning Gong,Haoran Luo*

Main category: cs.CL

TL;DR: OmniRAG-Agent：一种面向长音频视频问答的智能体方法，通过图像-音频检索增强生成、智能体循环规划和策略优化，在低资源设置下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前OmniLLMs在处理长音频视频问答时面临四大挑战：密集编码成本高、细粒度检索能力弱、主动规划能力有限、缺乏端到端优化。这些问题在低资源环境下尤为突出，需要新的解决方案。

Method: 1. 构建图像-音频检索增强生成模块，让OmniLLM从外部库中获取相关的短帧和音频片段；2. 采用智能体循环机制，跨轮次规划、调用工具并合并检索证据；3. 应用组相对策略优化联合改进工具使用和答案质量。

Result: 在OmniVideoBench、WorldSense和Daily-Omni三个基准测试中，OmniRAG-Agent在低资源设置下持续优于现有方法，消融实验验证了各组件有效性。

Conclusion: OmniRAG-Agent通过检索增强、智能体规划和策略优化的结合，有效解决了长音频视频问答中的关键挑战，为低资源环境下的多模态推理提供了有效方案。

Abstract: Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.

</details>


### [22] [Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling](https://arxiv.org/abs/2602.03719)
*Yubao Zhao,Weiquan Huang,Sudong Wang,Ruochen Zhao,Chen Chen,Yao Shu,Chengwei Qin*

Main category: cs.CL

TL;DR: BranPO是一种无需价值函数的强化学习方法，通过截断轨迹尾部并重采样替代延续来构建对比后缀，解决长视野任务中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 长视野强化学习中稀疏的轨迹级奖励导致学习困难，现有树搜索方法存在高方差和计算效率低的问题。研究发现性能差异主要源于尾部决策，因此需要更有效的监督信号。

Method: 提出BranPO方法：1）截断轨迹尾部并重采样替代延续，构建对比后缀进行步级对比监督；2）引入难度感知分支采样，自适应调整不同任务的分支频率；3）冗余步掩码抑制无信息动作。

Result: 在多个问答基准测试中，BranPO持续优于强基线方法，在长视野任务上实现显著准确率提升，且不增加总体训练预算。

Conclusion: BranPO通过尾部截断和对比后缀构建，有效解决了长视野强化学习中的信用分配模糊问题，提高了学习效率和稳定性。

Abstract: Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \href{https://github.com/YubaoZhao/BranPO}{code}.

</details>


### [23] [Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837)
*David P. Woodruff,Vincent Cohen-Addad,Lalit Jain,Jieming Mao,Song Zuo,MohammadHossein Bateni,Simina Branzei,Michael P. Brenner,Lin Chen,Ying Feng,Lance Fortnow,Gang Fu,Ziyi Guan,Zahra Hadizadeh,Mohammad T. Hajiaghayi,Mahdi JafariRaviz,Adel Javanmard,Karthik C. S.,Ken-ichi Kawarabayashi,Ravi Kumar,Silvio Lattanzi,Euiwoong Lee,Yi Li,Ioannis Panageas,Dimitris Paparas,Benjamin Przybocki,Bernardo Subercaseaux,Ola Svensson,Shayan Taherijam,Xuan Wu,Eylon Yogev,Morteza Zadimoghaddam,Samson Zhou,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 本文展示了研究人员如何与Gemini等先进AI模型合作，在理论计算机科学、经济学、优化和物理等领域解决开放问题、反驳猜想并生成新证明，提出了有效人机协作的技术方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在常规任务中表现出色，但它们在参与专家级数学发现方面的能力尚不明确。本文旨在探索AI如何成为科学发现过程中的真正合作伙伴，而不仅仅是自动化工具。

Method: 通过一系列案例研究，展示了研究人员如何与Google的Gemini模型（特别是Gemini Deep Think及其变体）协作。提取了有效人机协作的通用技术，包括迭代精炼、问题分解和跨学科知识转移。还探索了超越标准聊天界面的方法，如将模型作为严谨的对抗性审稿人，以及将其嵌入"神经符号"循环中自主编写和执行代码验证复杂推导。

Result: 成功展示了AI在多个领域（理论计算机科学、经济学、优化、物理）中解决开放问题、反驳猜想和生成新证明的能力。大部分成果来自交互式对话方法，但也展示了超越标准聊天界面的创新应用。

Conclusion: AI不仅可作为自动化工具，更能成为科学发现创造性过程中的多功能、真正的合作伙伴。人机协作的有效技术包括迭代精炼、问题分解和跨学科知识转移等方法。

Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning](https://arxiv.org/abs/2602.02518)
*Yuyang Bai,Zhuofeng Li,Ping Nie,Jianwen Xie,Yu Zhang*

Main category: cs.LG

TL;DR: GraphDancer：基于强化学习的框架，教LLMs通过交替推理和执行函数调用来导航异构图知识源，实现跨域泛化


<details>
  <summary>Details</summary>
Motivation: 现实世界知识源多为异构图而非纯文本，LLMs需要精确的函数调用而非相似性检索来导航结构化关系，且复杂问题需要多跳证据聚合

Method: 提出GraphDancer强化学习框架，通过交替推理和函数执行教LLMs导航图；引入图感知课程，根据信息寻求轨迹的结构复杂度安排训练

Result: 仅使用3B骨干模型，GraphDancer在跨域基准测试中优于配备14B骨干或GPT-4o-mini的基线，展示了强大的跨域泛化能力

Conclusion: GraphDancer成功解决了LLMs在图结构知识源上的导航和推理挑战，通过强化学习和课程学习实现了高效的跨域泛化

Abstract: Large language models (LLMs) increasingly rely on external knowledge to improve factuality, yet many real-world knowledge sources are organized as heterogeneous graphs rather than plain text. Reasoning over such graph-structured knowledge poses two key challenges: (1) navigating structured, schema-defined relations requires precise function calls rather than similarity-based retrieval, and (2) answering complex questions often demands multi-hop evidence aggregation through iterative information seeking. We propose GraphDancer, a reinforcement learning (RL) framework that teaches LLMs to navigate graphs by interleaving reasoning and function execution. To make RL effective for moderate-sized LLMs, we introduce a graph-aware curriculum that schedules training by the structural complexity of information-seeking trajectories using an easy-to-hard biased sampler. We evaluate GraphDancer on a multi-domain benchmark by training on one domain only and testing on unseen domains and out-of-distribution question types. Despite using only a 3B backbone, GraphDancer outperforms baselines equipped with either a 14B backbone or GPT-4o-mini, demonstrating robust cross-domain generalization of graph exploration and reasoning skills. Our code and models can be found at https://yuyangbai.com/graphdancer/ .

</details>


### [25] [Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation](https://arxiv.org/abs/2602.02530)
*Saurav Singh,Rodney Sanchez,Alexander Ororbia,Jamison Heard*

Main category: cs.LG

TL;DR: 提出基于离线策略评估（OPE）的强化学习框架，用于自动选择状态空间和奖励函数，仅使用日志交互数据，无需实时环境访问或人工反馈。


<details>
  <summary>Details</summary>
Motivation: 传统RL在现实世界部署中需要领域专家定义状态表示和奖励函数，依赖实时环境交互和人工参与，成本高且不适用于复杂安全关键应用。

Method: 通过离线RL代理训练多个候选状态表示和奖励函数，应用OPE评估策略性能，基于OPE指标选择最优状态空间和奖励函数。

Result: 在OpenAI Gym的Lunar Lander环境和NASA-MATB-II人类被试研究环境中验证了方法的有效性，展示了在真实人机协作场景中的适用性。

Conclusion: 该方法通过数据驱动的OPE评估自动化关键RL设计决策，增强了离线RL在现实世界环境中的可行性和可扩展性，为人机交互提供了更可靠有效的RL框架。

Abstract: Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.

</details>


### [26] [CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning](https://arxiv.org/abs/2602.02532)
*Mahyar Alinejad,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: CADENT框架通过经验门控机制统一战略自动机知识和战术策略知识，在强化学习迁移中实现40-60%的样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习方法难以处理源域和目标域之间的领域偏移，策略蒸馏缺乏长期战略知识，而自动机方法又缺少细粒度动作指导。

Method: CADENT框架将战略自动机知识与战术策略知识统一为连贯的指导信号，核心创新是经验门控信任机制，在状态-动作层面动态权衡教师指导与学生自身经验。

Result: 在稀疏奖励网格世界和连续控制任务等挑战性环境中，CADENT比基线方法实现了40-60%的样本效率提升，同时保持优越的渐进性能。

Conclusion: CADENT为强化学习中的自适应知识迁移建立了稳健的方法，能够优雅地适应目标域特定需求。

Abstract: Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.

</details>


### [27] [Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization](https://arxiv.org/abs/2602.02545)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.LG

TL;DR: MRPO提出了一种几何框架，通过谱正交探索和有效秩正则化来重塑LLM的推理空间，突破了传统RL在低秩偏差流形内的探索限制，显著提升了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前研究质疑RL是否真正扩展了LLM的推理能力，还是仅仅对齐了已有的潜在能力。作者认为探索被限制在预训练模型的低秩偏差流形内，需要突破这种可访问性边界假设。

Method: 提出了流形重塑策略优化（MRPO），包含两个阶段：1）谱正交探索（SOE）将策略初始化弹射到偏差流形的零空间；2）在策略优化目标中集成有效秩正则化项，激励发现和维护高维推理轨迹。

Result: 4B参数的方法在数学任务上达到了最先进的性能，显著优于更大的模型（如Qwen3-32B），并将能力边界扩展到标准GRPO之外。

Conclusion: 通过有针对性的几何干预可以根本上扩展LLM的潜在推理空间，MRPO框架能够重塑推理流形，突破传统RL的限制。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at https://anonymous.4open.science/r/MRPO-D57B/

</details>


### [28] [ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents](https://arxiv.org/abs/2602.02548)
*Xiaoce Wang,Guibin Zhang,Junzhe Li,Jinzhe Tu,Chun Li,Ming Li*

Main category: cs.LG

TL;DR: ToolTok提出了一种用于GUI代理的多步路径查找新范式，将操作建模为渐进式工具使用序列，通过语义锚定机制在有限监督下学习工具嵌入，使用易到难的课程学习让LLM逐步掌握工具语义，在少量训练数据下实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理模型存在两个主要问题：基于坐标的一步视觉定位方法难以泛化到不同输入分辨率和宽高比；而坐标无关策略则在严重数据稀缺下学习困难。需要一种既能处理视觉变化又能在有限数据下有效学习的方法。

Method: 1) 设计符合人类交互习惯的工具，用可学习的token嵌入表示每个工具；2) 引入语义锚定机制，将每个工具与语义相关概念关联作为自然归纳偏置；3) 构建易到难的课程学习，包括token定义问答、纯文本引导工具选择和简化视觉路径查找三个任务；4) 采用多步路径查找范式，将GUI操作建模为渐进式工具使用序列。

Result: 在多个基准测试中，ToolTok在可比规模模型（4B）中取得最优性能，与更大模型（235B）保持竞争力。这些结果仅使用其他后训练方法所需训练数据的不到1%。ToolTok在未见场景下表现出强大的泛化能力。

Conclusion: ToolTok通过多步路径查找范式、语义锚定机制和课程学习，有效解决了GUI代理在视觉变化泛化和数据稀缺学习方面的挑战，实现了高效且可泛化的GUI交互。

Abstract: Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.

</details>


### [29] [BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation](https://arxiv.org/abs/2602.02554)
*Jingwen Xu,Yiyang Lu,Zisu Huang,Changze Lv,Xiaohua Wang,Shizheng Li,Zhibo Xu,Zhengkang Guo,Zhengyuan Wang,Muzhao Tian,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.LG

TL;DR: BatCoder是一个自监督强化学习框架，通过代码-文档双向翻译优化代码生成和文档生成，仅需代码数据即可训练，在HumanEval和MBPP上表现优异。


<details>
  <summary>Details</summary>
Motivation: 训练代码相关任务的LLMs通常依赖高质量的代码-文档对，这些数据成本高且对于小众编程语言稀缺。需要一种仅使用代码数据就能训练的方法。

Method: 采用自监督强化学习框架，使用回译策略：首先生成代码的文档，然后用生成的文档重构原始代码。原始代码与重构代码的语义相似度作为隐式奖励，通过强化学习同时优化代码生成和文档生成。

Result: 在7B模型上，HumanEval达到83.5% pass@1，MBPP达到81.0% pass@1，优于开源基线。框架在训练数据规模和模型容量方面都表现出良好的扩展性。

Conclusion: BatCoder通过自监督强化学习有效解决了代码-文档对数据稀缺问题，仅使用代码数据就能训练出高性能的代码生成模型，具有很好的扩展性。

Abstract: Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.

</details>


### [30] [Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.02555)
*Bizhe Bai,Xinyue Wang,Peng Ye,Tao Chen*

Main category: cs.LG

TL;DR: PSN-RLVR通过参数扰动和截断重要性采样解决RLVR探索不足问题，在数学推理基准上提升大采样预算下的性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索天花板，主要重新加权现有解轨迹而非发现新策略，限制了在大采样预算下的性能提升

Method: 提出PSN-RLVR：1）在rollout生成前扰动策略参数以保持思维链一致性；2）使用截断重要性采样缓解采样-更新不匹配；3）设计轻量级代理驱动的实时自适应噪声调度器

Result: 在GRPO上实例化的PSN-GRPO在多个数学推理基准和模型家族上扩展了有效推理能力边界，在大采样预算下获得更高的pass-at-k性能

Conclusion: PSN-RLVR有效解决了RLVR的探索限制，与现有方法正交且可组合，为RLVR提供了更优的探索机制

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.

</details>


### [31] [Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs](https://arxiv.org/abs/2602.02556)
*Xuancheng Li,Haitao Li,Yujia Zhou,Yiqun Liu,Qingyao Ai*

Main category: cs.LG

TL;DR: SEAM是一个轻量级的结构化经验适配器模块，通过参数存储经验并在单次前向传播中生成结构化经验条目，指导冻结的LLM执行器，在数学推理基准上实现准确率提升


<details>
  <summary>Details</summary>
Motivation: 当前LLM通常是静态的，会重复推理或错误，而现有的经验重用方法依赖外部检索，存在相似性噪声和延迟问题，需要更高效的经验利用机制

Method: 提出SEAM模块：1）轻量级、执行器特定的插件，在参数中存储经验；2）单次前向传播生成结构化、实例定制的经验条目；3）通过执行器rollouts和GRPO进行效用训练，保持执行器冻结；4）部署后可通过监督微调进一步改进

Result: 在数学推理基准测试中，SEAM在不同执行器上实现了一致的准确率提升，且开销较低。广泛的消融实验和分析阐明了SEAM的有效性和鲁棒性机制

Conclusion: SEAM提供了一种高效的经验重用方法，能够在不修改LLM执行器的情况下提升推理性能，具有低延迟和鲁棒性的优势

Abstract: Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.

</details>


### [32] [daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently](https://arxiv.org/abs/2602.02619)
*Mohan Jiang,Dayuan Fu,Junhao Shi,Ji Zeng,Weiye Si,Keyu Li,Xuefeng Li,Yang Xiao,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: daVinci-Agency：通过挖掘Git PR序列中的结构化监督信号，解决LLM在长时程智能体工作流训练中的数据稀缺问题，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有方法无法为长时程智能体工作流提供可扩展的高质量监督数据：合成方法受限于模型分布，人工标注成本过高。需要寻找能够捕捉真实长依赖结构和跨阶段演化动态的数据源。

Method: 从真实软件演化中挖掘结构化监督信号，利用Pull Request序列的自然特性：1) 通过连续提交实现渐进式任务分解；2) 通过统一功能目标强制执行长期一致性；3) 从真实bug修复轨迹中获得可验证的改进。构建daVinci-Agency系统，平均轨迹包含85k tokens和116个工具调用。

Result: 仅使用239个daVinci-Agency样本微调GLM-4.6，就在多个基准测试中取得广泛改进，特别是在Toolathlon上获得47%的相对增益。数据效率显著，长轨迹但高质量。

Conclusion: PR序列为长时程智能体学习提供了自然的监督信号，能够捕捉因果依赖和迭代改进模式。daVinci-Agency方法在数据效率和性能提升方面表现出色，为智能体训练提供了新范式。

Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...

</details>


### [33] [Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation](https://arxiv.org/abs/2602.03806)
*Ziru Chen,Dongdong Chen,Ruinan Jin,Yingbin Liang,Yujia Xie,Huan Sun*

Main category: cs.LG

TL;DR: 提出Cobalt方法，结合在线和离线强化学习优势，用于多轮代码生成任务，在LiveCodeBench上显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然在线强化学习在多轮代码生成等真实任务上表现更好，但训练成本高且不稳定。需要一种结合在线和离线RL优势的方法来解决这个问题。

Method: 将多轮代码生成建模为一步可恢复马尔可夫决策过程，提出Cobalt方法：1) 使用参考LLM收集代码生成轨迹并分割为部分轨迹作为上下文提示；2) 在线bandit学习中，训练LLM通过单步代码生成完成每个部分轨迹提示。

Result: Cobalt在LiveCodeBench上优于基于GRPO和VeRPO的多轮在线RL基线，将R1-Distill 8B和Qwen3 8B的Pass@1分数分别提升高达9.0和6.2个绝对百分点。通过扰动轨迹增强训练缓解了上下文奖励攻击问题。

Conclusion: Cobalt是多轮代码生成等迭代决策任务的有前景解决方案，有效结合了在线和离线强化学习的优势。

Abstract: Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.

</details>


### [34] [Label Curation Using Agentic AI](https://arxiv.org/abs/2602.02564)
*Subhodeep Ghosh,Bayan Divaaniaazar,Md Ishat-E-Rabban,Spencer Clarke,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: AURA是一个基于智能体AI的大规模多模态数据标注框架，通过协调多个AI代理生成和验证标签，无需真实标注数据，使用概率模型联合推断潜在真实标签和标注者可靠性，在基准数据集上实现最高5.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注流程成本高、速度慢且存在标注者差异性问题，随着数据集规模和模态的增加，需要可靠、自动化的标注解决方案。

Method: AURA采用智能体AI框架，协调多个AI代理生成和验证标签，核心采用经典概率模型，通过混淆矩阵联合推断潜在真实标签和标注者可靠性，使用期望最大化算法来调和冲突标注并聚合噪声预测。

Result: 在四个基准数据集上，AURA相比基线方法实现了最高5.8%的准确率提升；在标注者质量较差的挑战性场景中，改进幅度可达50%；同时能准确估计标注者可靠性，无需预验证步骤。

Conclusion: AURA为大规模多模态数据标注提供了一种有效的智能体AI解决方案，能够自动生成可靠标签并评估标注者质量，解决了传统标注方法的局限性。

Abstract: Data annotation is essential for supervised learning, yet producing accurate, unbiased, and scalable labels remains challenging as datasets grow in size and modality. Traditional human-centric pipelines are costly, slow, and prone to annotator variability, motivating reliability-aware automated annotation. We present AURA (Agentic AI for Unified Reliability Modeling and Annotation Aggregation), an agentic AI framework for large-scale, multi-modal data annotation. AURA coordinates multiple AI agents to generate and validate labels without requiring ground truth. At its core, AURA adapts a classical probabilistic model that jointly infers latent true labels and annotator reliability via confusion matrices, using Expectation-Maximization to reconcile conflicting annotations and aggregate noisy predictions. Across the four benchmark datasets evaluated, AURA achieves accuracy improvements of up to 5.8% over baseline. In more challenging settings with poor quality annotators, the improvement is up to 50% over baseline. AURA also accurately estimates the reliability of annotators, allowing assessment of annotator quality even without any pre-validation steps.

</details>


### [35] [Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective](https://arxiv.org/abs/2602.02572)
*Haichuan Wang,Tao Lin,Lingkai Kong,Ce Li,Hezi Jiang,Milind Tambe*

Main category: cs.LG

TL;DR: 提出一种优化奖励模型的方法，通过Stackelberg博弈形式化KL正则化下的奖励模型设计问题，采用简单的奖励塑造方案近似最优奖励模型，在推理时对齐设置中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法直接使用从用户偏好数据学习的奖励模型来优化LLM策略，并受限于与基础策略的KL正则化。这种做法在最大化用户效用方面是次优的，因为KL正则化可能导致LLM继承基础策略中与用户偏好冲突的偏差。虽然放大偏好输出的奖励可以减轻这种偏差，但也会增加奖励黑客的风险。

Method: 将奖励模型优化问题形式化为Stackelberg博弈，提出简单的奖励塑造方案来有效近似最优奖励模型。该方法可以无缝集成到现有对齐方法中，开销最小。

Result: 在推理时对齐设置中进行实证评估，方法始终提高平均奖励，在所有基线中实现超过66%的胜平率（平均跨评估设置）。

Conclusion: 通过优化奖励模型设计而非直接使用学习到的奖励模型，可以更好地平衡KL正则化与用户偏好对齐，提高LLM策略的性能。

Abstract: Existing alignment methods directly use the reward model learned from user preference data to optimize an LLM policy, subject to KL regularization with respect to the base policy. This practice is suboptimal for maximizing user's utility because the KL regularization may cause the LLM to inherit the bias in the base policy that conflicts with user preferences. While amplifying rewards for preferred outputs can mitigate this bias, it also increases the risk of reward hacking. This tradeoff motivates the problem of optimally designing reward models under KL regularization. We formalize this reward model optimization problem as a Stackelberg game, and show that a simple reward shaping scheme can effectively approximate the optimal reward model. We empirically evaluate our method in inference-time alignment settings and demonstrate that it integrates seamlessly into existing alignment methods with minimal overhead. Our method consistently improves average reward and achieves win-tie rates exceeding 66% against all baselines, averaged across evaluation settings.

</details>


### [36] [ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization](https://arxiv.org/abs/2602.02597)
*Hongyuan Su,Yu Zheng,Yong Li*

Main category: cs.LG

TL;DR: ContextEvolve是一个多智能体框架，通过将优化上下文分解为三个正交维度（总结器、导航器、采样器），在参数不可见的约束下实现强化学习级别的搜索效率，用于系统代码优化。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成看似合理的代码，但要满足系统对正确性和性能的严格要求需要迭代优化。测试时强化学习搜索效率高但需要参数更新（API访问不可行），而现有的无训练进化方法存在上下文利用效率低和搜索无方向的问题。

Method: 提出ContextEvolve多智能体框架：1) Summarizer Agent通过代码到语言的抽象来浓缩语义状态；2) Navigator Agent通过轨迹分析提炼优化方向；3) Sampler Agent通过优先示例检索来管理经验分布。这种编排形成了与强化学习的功能同构（映射到状态表示、策略梯度和经验回放），在文本潜在空间中实现原则性优化。

Result: 在ADRS基准测试中，ContextEvolve比最先进的基线方法性能提升33.3%，同时减少29.0%的token消耗。

Conclusion: ContextEvolve在严格的参数不可见约束下实现了强化学习级别的搜索效率，为系统代码优化提供了一种高效的多智能体框架。

Abstract: Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC

</details>


### [37] [BinaryPPO: Efficient Policy Optimization for Binary Classification](https://arxiv.org/abs/2602.02708)
*Punya Syon Pandey,Zhijing Jin*

Main category: cs.LG

TL;DR: BinaryPPO：一种离线强化学习框架，将二分类任务重新定义为奖励最大化问题，通过置信度加权的奖励函数在标签噪声、类别不平衡等现实场景中显著优于监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在二分类任务（如毒性检测、事实性验证、因果推断）中表现不佳，特别是在现实场景中存在标签噪声、类别不平衡或稀疏监督的情况下。需要一种更鲁棒的方法来处理这些挑战。

Method: 提出BinaryPPO框架，将二分类重新定义为奖励最大化问题。使用PPO的变体，结合置信度加权的奖励函数，惩罚不确定或不正确的预测。该方法从静态数据集学习鲁棒的决策策略，无需在线交互。

Result: 在八个领域特定基准测试和多种不同架构的模型上，BinaryPPO将准确率提高了40-60个百分点，最高达到99%，显著优于监督基线方法。深入分析了奖励塑造、优势缩放和政策稳定性对性能提升的作用。

Conclusion: 基于置信度的奖励设计为二分类任务提供了比监督微调更鲁棒的替代方案。BinaryPPO在现实世界噪声和挑战下表现出色。

Abstract: Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.

</details>


### [38] [Maximum Likelihood Reinforcement Learning](https://arxiv.org/abs/2602.02710)
*Fahim Tajwar,Guanning Zeng,Yueer Zhou,Yuda Song,Daman Arora,Yiding Jiang,Jeff Schneider,Ruslan Salakhutdinov,Haiwen Feng,Andrea Zanette*

Main category: cs.LG

TL;DR: 提出MaxRL框架，通过采样计算在强化学习和最大似然估计之间插值，在代码生成等二元反馈任务中实现更高效的训练。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在采样设置中（如导航、代码生成、数学问题求解）只优化低阶近似，而非最大化正确轨迹的似然。需要一种能更好利用采样计算的方法。

Method: 提出Maximum Likelihood Reinforcement Learning (MaxRL)框架，定义基于计算索引的采样目标族，在标准强化学习和精确最大似然之间插值。使用简单的无偏策略梯度估计器，在无限计算极限下收敛到最大似然优化。

Result: MaxRL在所有测试模型和任务中都Pareto优于现有方法，相比GRPO训练模型实现高达20倍的测试时缩放效率提升。在额外数据和计算下表现出更好的扩展性。

Conclusion: MaxRL是基于正确性设置中扩展强化学习训练的有前景的框架，能更高效地利用采样计算资源。

Abstract: Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.

</details>


### [39] [Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion](https://arxiv.org/abs/2602.02722)
*Dan Haramati,Carl Qi,Tal Daniel,Amy Zhang,Aviv Tamar,George Konidaris*

Main category: cs.LG

TL;DR: 提出分层实体中心框架，结合子目标分解与因子结构，解决多实体领域中的长时程离线目标条件强化学习任务


<details>
  <summary>Details</summary>
Motivation: 解决多实体复杂环境中长时程目标达成的核心挑战，特别是高维观测、组合状态空间和稀疏奖励带来的困难

Method: 采用两层层次结构：基于价值的目标条件强化学习智能体+因子化子目标生成条件扩散模型，两者独立训练后通过基于价值函数的子目标选择组合

Result: 在图像基长时程稀疏奖励任务上显著提升基础RL智能体性能，最困难任务上成功率提高150%以上，并能泛化到更长的时程和更多实体

Conclusion: 提出的分层实体中心框架有效解决多实体领域长时程任务，具有模块化、兼容现有算法、泛化能力强的优势

Abstract: We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl

</details>


### [40] [Joint Learning of Hierarchical Neural Options and Abstract World Model](https://arxiv.org/abs/2602.02799)
*Wasu Top Piriyakulkij,Wolfgang Lehrach,Kevin Ellis,Kevin Murphy*

Main category: cs.LG

TL;DR: 提出AgentOWL方法，通过联合学习抽象世界模型和分层神经选项，以样本高效的方式构建能够组合现有技能执行新任务的AI智能体。


<details>
  <summary>Details</summary>
Motivation: 构建能够通过组合现有技能来执行新技能的智能体是AI研究的长期目标，但现有的模型无关分层强化学习算法需要大量数据，样本效率低下。

Method: 提出AgentOWL方法，联合学习抽象世界模型（在状态和时间上进行抽象）和一组分层神经选项，实现样本高效的学习。

Result: 在Object-Centric Atari游戏子集上，该方法比基线方法使用更少的数据学习到更多的技能。

Conclusion: AgentOWL通过联合学习抽象世界模型和分层选项，实现了样本高效的技能获取，为构建能够组合技能的智能体提供了有效途径。

Abstract: Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.

</details>


### [41] [Self-Hinting Language Models Enhance Reinforcement Learning](https://arxiv.org/abs/2602.03143)
*Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian*

Main category: cs.LG

TL;DR: SAGE通过引入特权提示来增强GRPO在稀疏奖励下的训练效果，防止优势函数坍塌，提升模型对齐性能


<details>
  <summary>Details</summary>
Motivation: GRPO在稀疏终端奖励下容易停滞，因为组内rollout经常获得相同奖励，导致相对优势坍塌和更新消失

Method: 提出SAGE框架，在训练时注入特权提示来重塑rollout分布，采样紧凑提示（如计划或分解）来生成解决方案，保持任务奖励不变但增加组内结果多样性

Result: 在6个基准测试和3个LLM上的实验显示，SAGE持续优于GRPO，平均提升：Llama-3.2-3B-Instruct +2.0，Qwen2.5-7B-Instruct +1.2，Qwen3-4B-Instruct +1.3

Conclusion: SAGE通过自我提示和特权监督有效解决了GRPO在稀疏奖励下的训练停滞问题，无需测试时特权信息，采样多样自我提示作为自适应课程比固定提示更有效

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.

</details>


### [42] [Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning](https://arxiv.org/abs/2602.03190)
*Wenquan Lu,Hai Huang,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出提示增强训练策略，通过多样化推理模板解决强化学习后训练中的熵崩溃问题，实现稳定长时训练并提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练方法存在熵崩溃现象，导致训练不稳定和过早崩溃，限制了训练时长和模型性能提升。同时，现有方法通常使用单一固定的推理提示模板，限制了探索多样性。

Method: 提出提示增强策略，让模型在多样化模板和格式下生成推理轨迹，增加rollout多样性。该方法无需KL正则化项，能在固定数据集上稳定扩展训练时长，使模型能够容忍低熵状态而不提前崩溃。

Result: 在Qwen2.5-Math-1.5B模型上，使用MATH Level 3-5数据集训练，在AIME24、AMC、MATH500、Minerva和OlympiadBench等数学推理基准测试中达到最先进性能：每基准准确率44.5%，每问题准确率51.3%。

Conclusion: 提示增强策略有效解决了强化学习后训练中的熵崩溃问题，实现了稳定长时训练，显著提升了语言模型的数学推理能力。

Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.

</details>


### [43] [Causal Flow Q-Learning for Robust Offline Reinforcement Learning](https://arxiv.org/abs/2602.02847)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出一种基于因果视角的离线强化学习方法，通过流匹配策略处理像素演示中的混淆观测问题，在25个像素任务中比现有方法提升120%成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配的离线RL方法假设数据中不存在未测量的混淆变量，但在像素演示中，演示者和学习者的感知能力不匹配会导致隐式混淆偏差，影响策略学习效果。

Method: 从因果视角分析混淆观测问题，提出新的因果离线RL目标函数，优化策略在最坏混淆偏差情况下的性能。实现基于深度判别器评估目标策略与名义行为策略差异的流匹配策略学习框架。

Result: 在25个像素任务上的实验表明，提出的混淆鲁棒增强方法比不考虑混淆的SOTA离线RL方法成功率高出120%。

Conclusion: 通过因果视角处理像素演示中的混淆观测问题能显著提升离线RL性能，提出的方法为处理感知不匹配导致的隐式混淆偏差提供了有效解决方案。

Abstract: Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.

</details>


### [44] [Efficient Estimation of Kernel Surrogate Models for Task Attribution](https://arxiv.org/abs/2602.03783)
*Zhenshuo Zhang,Minxuan Duan,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出核代理模型用于任务归因分析，相比线性模型能更好地捕捉任务间的非线性交互作用，在多个领域实现更准确的任务影响评估。


<details>
  <summary>Details</summary>
Motivation: 现有任务归因方法主要使用线性代理模型，只能捕捉一阶关系，无法有效建模任务间的非线性交互（如协同、对抗、XOR效应）。需要开发更强大的方法来准确量化每个训练任务对目标任务性能的影响。

Method: 1) 建立统一的任务加权框架分析任务归因方法；2) 通过二阶分析揭示线性代理模型与影响函数的新联系；3) 引入核代理模型以更好地表示二阶任务交互；4) 开发基于梯度的估计程序，利用预训练模型的一阶近似高效学习核代理。

Result: 核代理模型在数学推理、上下文学习、多目标强化学习等多个领域表现优异：1) 与留一法基准的相关性比线性代理和影响函数基线高25%；2) 梯度估计方法仅需不到2%的相对误差，无需重复训练；3) 在下游任务选择中，上下文学习和多目标强化学习基准的演示选择性能提升40%。

Conclusion: 核代理模型为任务归因分析提供了更准确和高效的方法，能够有效捕捉任务间的非线性交互作用，在多个实际应用中显著优于现有方法，为理解多任务训练中任务间关系提供了有力工具。

Abstract: Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.

</details>


### [45] [How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?](https://arxiv.org/abs/2602.02924)
*Xiaoyuan Cheng,Wenxuan Yuan,Boyang Li,Yuanchao Xu,Yiming Yang,Hao Liang,Bei Peng,Robert Loftin,Zhuo Sun,Yukun Hu*

Main category: cs.LG

TL;DR: ALGD是一种用于离策略安全强化学习的新算法，通过增强拉格朗日方法稳定扩散策略采样，解决安全RL中的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的RL方法主要关注离线设置的奖励最大化，对在线设置中的安全性考虑有限。扩散策略采样虽然能表示多模态动作分布，但在安全RL中直接使用拉格朗日函数会导致策略生成和训练不稳定。

Method: 提出增强拉格朗日引导扩散(ALGD)算法，通过引入增强拉格朗日函数局部凸化能量景观，稳定策略生成和训练过程，而不改变最优策略的分布。

Result: 理论分析和大量实验表明，ALGD在理论上有坚实基础，在多种环境中实现了强大且稳定的性能。

Conclusion: ALGD成功解决了扩散基安全RL中的训练不稳定问题，为安全强化学习提供了有效的扩散策略采样方法。

Abstract: Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.

</details>


### [46] [Learning to Repair Lean Proofs from Compiler Feedback](https://arxiv.org/abs/2602.02990)
*Evan Wang,Simon Chess,Daniel Lee,Siyuan Ge,Ajit Mallavarapu,Vasily Ilin*

Main category: cs.LG

TL;DR: APRIL数据集为Lean定理证明器提供监督学习数据，包含26万个错误证明、编译器反馈、修复目标和解释的配对，用于训练语言模型进行证明修复和反馈理解。


<details>
  <summary>Details</summary>
Motivation: 现有Lean数据集几乎只包含正确证明，缺乏对编译器反馈的理解和错误修复的监督学习数据，限制了神经定理证明器处理失败情况的能力。

Method: 将Lean证明修复构建为监督学习问题，创建APRIL数据集，包含系统生成的证明失败、编译器诊断、修复目标和自然语言解释的配对，用于训练语言模型。

Result: 在APRIL上训练的语言模型显著提高了修复准确性和反馈条件推理能力，4B参数模型在单次修复评估中优于最强的开源基线。

Conclusion: 诊断条件监督是反馈使用证明器的补充训练信号，APRIL数据集为神经定理证明器的错误修复和反馈理解提供了有价值的资源。

Abstract: As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.

</details>


### [47] [Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation](https://arxiv.org/abs/2602.03045)
*Bo Yuan,Zelin Zhao,Petr Molodyk,Bin Hu,Yongxin Chen*

Main category: cs.LG

TL;DR: ProCAD是一个主动式文本到CAD代码生成框架，通过澄清代理解决几何描述不完整或矛盾的问题，显著提高了生成代码的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到CAD系统在处理不完整或矛盾的几何描述时，倾向于被动遵循用户指令并产生幻觉维度，导致生成的CAD代码质量不高。

Method: 提出主动式代理框架ProCAD，包含两个组件：1)主动澄清代理，审核提示并在必要时提出针对性澄清问题；2)CAD编码代理，将澄清后的规范转换为可执行的CadQuery程序。通过高质量数据集微调编码代理，并通过代理式SFT训练澄清代理。

Result: ProCAD显著提高了对模糊提示的鲁棒性，同时保持低交互开销。在Chamfer距离指标上比Claude Sonnet 4.5降低79.9%，无效代码比例从4.8%降至0.9%。

Conclusion: 主动澄清机制能有效解决文本到CAD生成中的规范不完整问题，显著提升生成代码的质量和可靠性，优于前沿闭源模型。

Abstract: Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.

</details>


### [48] [CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs](https://arxiv.org/abs/2602.03048)
*Zhiyuan Yao,Yi-Kai Zhang,Yuxin Chen,Yueqing Sun,Zishan Xu,Yu Yang,Tianhao Hu,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.LG

TL;DR: CoBA-RL是一种自适应分配训练预算的强化学习算法，通过能力导向的价值函数评估样本训练价值，使用堆贪心策略优化资源分配，提升LLM后训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO）使用统一的训练预算导致资源效率低下，而现有自适应方法依赖任务通过率等实例级指标，无法捕捉模型动态学习状态。

Method: 提出CoBA-RL算法：1）使用能力导向价值函数将任务映射到潜在训练收益；2）采用堆贪心策略自校准计算资源分配，将资源优先分配给高训练价值的样本。

Result: 实验表明该方法有效平衡探索与利用，在多个挑战性基准测试中实现一致的泛化改进。

Conclusion: 量化样本训练价值和优化预算分配对于提升LLM后训练效率至关重要。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.

</details>


### [49] [Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals](https://arxiv.org/abs/2602.03061)
*Zihan Dong,Zhixian Zhang,Yang Zhou,Can Jin,Ruijia Wu,Linjun Zhang*

Main category: cs.LG

TL;DR: 提出一种基于配对比较信号的统计高效评估框架，利用LLM对辅助推理链的判断作为控制变量，通过半参数估计器减少方差，提高模型排名准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM数学推理评估受限于基准规模小和模型随机性，导致高方差精度估计和跨平台排名不稳定。即使模型无法给出正确答案，仍能提供可靠的配对比较信号。

Method: 设计结合标准标注结果和配对比较信号的评估框架，将比较信号作为控制变量，基于有效影响函数开发半参数估计器，构建一步估计器实现半参数效率边界。

Result: 一步估计器显著提高排名准确性，随着模型输出噪声增加而增益更大。在GPQA Diamond、AIME 2025和GSM8K实验中展示更精确的性能估计和更可靠的模型排名。

Conclusion: 利用配对比较信号作为控制变量的统计高效评估框架能显著减少方差，在小样本场景下提供更稳定可靠的模型性能评估和排名。

Abstract: Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.

</details>


### [50] [TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT](https://arxiv.org/abs/2602.03073)
*Rana Muhammad Shahroz Khan,Zijie Liu,Zhen Tan,Charles Fleming,Tianlong Chen*

Main category: cs.LG

TL;DR: TMS是一种无需奖励的监督学习框架，通过使用模型历史检查点创建动态课程，近似RL的在线策略优势，在保持模型能力的同时避免标准SFT中的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法虽然能更好地保持模型能力，但成本高昂（奖励工程复杂、不稳定、采样昂贵）；而SFT虽然高效但容易发生灾难性遗忘，主要原因是监督不匹配问题——模型策略演变与静态训练标签之间的差异。

Method: 提出轨迹混合监督（TMS）框架，通过使用模型历史检查点创建动态课程来近似RL的在线策略优势。该方法最小化策略-标签差异（PLD），防止标准SFT中导致遗忘的模式崩溃。

Result: 在推理（MATH、GSM8K）和指令遵循基准测试中，TMS有效改善了准确率-保持率的帕累托前沿。虽然RL仍然是保持能力的黄金标准，但TMS显著优于标准和迭代SFT，在不需奖励模型或验证器的情况下缩小了与RL的差距。

Conclusion: TMS提供了一种高效且有效的替代方案，在不需要复杂奖励工程的情况下，通过动态课程学习来近似RL的在线策略优势，有效缓解了监督不匹配问题导致的灾难性遗忘。

Abstract: Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.

</details>


### [51] [Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery](https://arxiv.org/abs/2602.03132)
*Timothee Leleu,Sudeera Gunathilaka,Federico Ghimenti,Surya Ganguli*

Main category: cs.LG

TL;DR: CCTS是一种基于对比概念树搜索的LLM辅助算法发现方法，通过提取程序层次概念表示并学习对比概念模型来指导搜索，在组合数学问题上提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM辅助算法发现取得了进展，但如何最大化利用LLM对程序空间的内部表示来提升性能仍是一个开放问题。现有方法主要依赖算法谱系，而缺乏对程序概念层次结构的显式利用。

Method: 提出对比概念树搜索(CCTS)：1)从生成的程序中提取层次概念表示；2)学习对比概念模型指导父节点选择；3)通过高/低性能解决方案的似然比分数重新加权父节点，偏向有用概念组合，远离误导性概念。

Result: CCTS在开放Erdős型组合数学问题基准测试中，相比基于适应度的基线方法提高了搜索效率，并产生了可解释的、任务特定的概念树。分析表明增益主要来自学习应避免哪些概念。

Conclusion: CCTS通过显式概念层次而非LLM构建的算法谱系提供指导，有效提升了LLM辅助算法发现的性能。在受控合成环境中验证了这些发现，重现了与LLM相似的搜索动态。

Abstract: Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.

</details>


### [52] [Reinforcement Learning with Promising Tokens for Large Language Models](https://arxiv.org/abs/2602.03195)
*Jing-Cheng Pang,Liang Lu,Xian Tang,Kun Jiang,Sijie Wu,Kai Zhang,Xubin Li*

Main category: cs.LG

TL;DR: RLPT通过将策略优化限制在"有希望的token"子集上，解决了LLM强化学习中动作空间过大的问题，提高了训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法将LLM视为策略并在完整词汇空间上应用RL，但包含大量上下文无关token会分散策略注意力，影响在真正合理token上的决策。

Method: RLPT框架将策略决策与token生成解耦：利用基础模型的语义先验识别动态的"有希望的token"集合，通过掩码将策略优化限制在这个精炼子集上。

Result: 在数学、编程和电信推理任务上，RLPT优于标准RL基线，有效集成到不同模型规模(4B和8B)和RL算法(GRPO和DAPO)中，减少了梯度方差，稳定了训练过程。

Conclusion: 通过将策略优化限制在语义相关的token子空间上，RLPT有效解决了LLM强化学习中的动作空间问题，提高了训练效率和性能。

Abstract: Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).

</details>


### [53] [Periodic Regularized Q-Learning](https://arxiv.org/abs/2602.03301)
*Hyukjun Yang,Han-Dong Lim,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出周期性正则化Q学习(PRQ)算法，通过正则化投影算子确保在线性函数逼近下的收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习在表格设置下有收敛保证，但在线性函数逼近下不收敛。现有研究通过正则化技术确保稳定收敛，但需要更好的算法设计

Method: 1. 在投影算子层面引入正则化，构建正则化投影值迭代(RP-VI)；2. 将RP-VI扩展到基于样本的RL算法，提出PRQ算法；3. 通过正则化投影算子使投影值迭代成为压缩映射

Result: PRQ算法在线性函数逼近下具有有限时间收敛保证，提供了严格的理论分析证明

Conclusion: 通过正则化投影算子设计的PRQ算法解决了Q学习在线性函数逼近下的收敛问题，为函数逼近下的强化学习提供了理论保证

Abstract: In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.

</details>


### [54] [Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models](https://arxiv.org/abs/2602.03309)
*Yuelin Hu,Zhengxue Cheng,Wei Liu,Li Song*

Main category: cs.LG

TL;DR: EGSPO是一种三阶段混合训练框架，通过基于预测熵的token级梯度调制，在数学推理基准上实现显著提升


<details>
  <summary>Details</summary>
Motivation: 现有混合训练方法通常在样本级别结合监督微调(SFT)和强化学习(RL)，但缺乏细粒度的梯度控制。EGSPO旨在通过token级别的梯度调制来改进这一过程，平衡探索与知识保留。

Method: 三阶段框架：1) SFT专家学习建立可靠预热策略；2) RL轨迹生成并计算每个token的预测熵；3) EGSPO机制应用熵门控梯度分配：高熵token进行完整PPO更新以鼓励探索，低熵token进行衰减PPO更新以减少方差并保留知识。

Result: 在数学推理基准上取得一致改进：AIME提升3.8%，MATH提升2.9%（相比CHORD phi基线），仅增加3.4%的计算开销。

Conclusion: EGSPO通过token级别的熵门控梯度调制，有效平衡了探索与知识保留，在数学推理任务上实现了显著性能提升且计算开销小。

Abstract: Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.

</details>


### [55] [On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03392)
*Shumin Wang,Yuexiang Xie,Wenhao Zhang,Yuchang Sun,Yanxi Chen,Yaliang Li,Yanyong Zhang*

Main category: cs.LG

TL;DR: 该论文建立了强化微调过程中熵动态的理论框架，推导了熵变化的表达式，并基于理论分析设计了熵控制方法，为LLM微调中的探索-利用平衡提供了理论支持和实践策略。


<details>
  <summary>Details</summary>
Motivation: 虽然熵作为衡量LLM输出多样性的关键指标，在强化微调中用于平衡探索与利用，但对其动态变化的理论理解尚不充分。需要建立理论框架来分析RFT过程中的熵动态。

Method: 建立理论框架分析RFT过程中的熵动态：从量化单次logit更新下熵变化的判别表达式出发，推导熵变化的一阶表达式，扩展到GRPO的更新公式。基于理论分析设计熵控制方法。

Result: 理论分析得出了熵变化的表达式和推论，启发了熵控制方法的设计，并为现有研究中各种基于熵的方法提供了统一解释框架。实证研究支持了主要结论，证明了熵判别器裁剪方法的有效性。

Conclusion: 该研究为RFT训练动态提供了新的理论见解，为LLM微调中优化探索-利用平衡提供了理论支持和实用策略，建立了熵动态分析的理论框架。

Abstract: Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.

</details>


### [56] [Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning](https://arxiv.org/abs/2602.03516)
*Zixiang Di,Jinyi Han,Shuo Zhang,Ying Liao,Zhi Li,Xiaofeng Ji,Yongqi Wang,Zheming Yang,Ming Gao,Bingdong Li,Jie Wang*

Main category: cs.LG

TL;DR: 该论文提出PNS方法，通过反向强化学习合成高质量负样本，这些样本具有正确的格式和结构但最终答案是错误的，用于提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有错误回答视为同等信息量，忽视了样本质量的重要性。学习负样本对提升LLM推理能力很有前景，但需要更高质量的负样本。

Method: 提出Plausible Negative Samples (PNS)方法：通过反向强化学习训练专用模型，使用组合奖励（格式合规性、准确性反转、奖励模型评估、思维链评估）合成高质量负样本，这些样本具有预期格式和结构连贯性但最终答案错误。

Result: 在7个数学推理基准测试和3个骨干模型上验证，PNS作为即插即用数据源用于偏好优化，始终优于其他负样本合成方法，平均比RL训练模型提升2.03%。

Conclusion: PNS方法通过合成高质量负样本有效提升LLM推理能力，证明了负样本质量对模型性能的重要性。

Abstract: Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.

</details>


### [57] [UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining](https://arxiv.org/abs/2602.03772)
*Changhao Wang,Yunfei Yu,Xinhao Yao,Jiaolong Yang,Riccardo Cantoro,Chaobo Li,Qing Cui,Jun Zhou*

Main category: cs.LG

TL;DR: UniGeM框架通过流形近似统一数据混合与选择，无需代理模型或外部数据集，实现2倍数据效率提升


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展受限于数据质量，现有方法将数据混合与样本选择分开处理，可能破坏代码语料库的结构

Method: 将数据管理视为流形近似问题，分层操作：宏观探索通过稳定性聚类学习混合权重；微观挖掘通过几何分布过滤高质量实例确保逻辑一致性

Result: 在100B token上训练8B和16B MoE模型，相比随机基线实现2倍数据效率提升，在推理密集型评估和多语言泛化方面超越SOTA方法

Conclusion: UniGeM框架有效统一数据混合与选择，显著提升数据效率与模型性能

Abstract: The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \textbf{2.0$\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.

</details>


### [58] [Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL](https://arxiv.org/abs/2602.03773)
*Ian Wu,Yuxiao Qu,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RC算法，通过迭代解码替代标准自回归解码，利用LLM响应生成与总结能力的不对称性构建推理链，使模型能在远超训练长度的推理视野上持续改进


<details>
  <summary>Details</summary>
Motivation: 标准强化学习在固定问题分布和训练预算下运行，限制了模型在测试时面对分布偏移时的外推能力。需要让LLM能够在测试时持续改进，解决更困难的问题

Method: 引入RC（迭代解码算法），在训练和推理时替代标准自回归解码。利用LLM响应生成与总结能力的不对称性构建推理链，使模型能在迭代中持续改进

Result: 4B模型使用16k-token训练预算，在HMMT 2025上性能从40%提升至近70%（使用0.5m测试token），优于同规模模型和许多更大的推理LLM。模型能在外推超过训练时一个数量级的推理视野上持续改进

Conclusion: RC算法使LLM能够在测试时持续改进和外推，超越训练预算限制。训练后的模型能更有效地利用现有支架进一步扩展测试时性能

Abstract: Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.

</details>


### [59] [Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839)
*Erfan Miahi,Eugene Belilovsky*

Main category: cs.LG

TL;DR: PULSE是一种高效的分布式强化学习权重同步方法，通过仅传输修改的参数索引和值，在带宽受限环境中实现100倍通信减少，同时保持训练动态和性能不变。


<details>
  <summary>Details</summary>
Motivation: 分布式强化学习中，策略权重从训练器同步到推理工作者的过程是主要瓶颈，特别是在带宽受限的商用网络或去中心化环境中。虽然研究表明RL更新通常只修改一小部分模型参数，但这些观察通常基于粗略的检查点差异。

Method: 首先系统实证研究权重更新的稀疏性，包括步级和多步粒度，分析其在训练动态、离策略延迟和模型规模上的演变。基于发现的高稀疏性（常超过99%），提出PULSE方法：通过无损稀疏编码传输修改参数的索引和值，避免浮点漂移并保持传输鲁棒性。

Result: 在带宽受限的去中心化环境中，PULSE实现超过100倍的通信减少（从14GB降至约108MB），同时保持比特相同的训练动态和性能。将权重同步所需带宽从20Gbit/s降至0.2Gbit/s即可维持高GPU利用率。

Conclusion: 通过利用RL权重更新的高稀疏性结构，PULSE使去中心化RL训练能够接近中心化吞吐量，显著降低通信开销，为带宽受限环境中的大规模RL训练提供高效解决方案。

Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit是一个面向部署的信用审计框架，通过评估模型在多种系统提示模板下的表现，提供平均能力和稳定性风险两个维度的评估，并将波动性转化为可解释的信用等级，帮助在实际部署中做出更客观的模型选择。


<details>
  <summary>Details</summary>
Motivation: 当前公共基准测试的排行榜分数趋于收敛，许多前沿语言模型之间差异微小，但这些分数无法反映用户日常体验。系统提示、输出协议和交互模式在常规迭代中不断演变，在代理式多步骤流程中，小的协议变化可能引发不成比例的故障，导致从业者不确定应部署哪个模型。

Method: 提出CreditAudit框架，在多个基准测试上评估模型在一系列语义对齐且非对抗性的系统提示模板下的表现。报告平均能力作为跨场景的平均性能，报告场景诱导的波动sigma作为稳定性风险信号，并通过跨模型分位数将波动性映射为从AAA到BBB的可解释信用等级，同时提供缓解模板难度漂移的诊断方法。

Result: 在GPQA、TruthfulQA和MMLU Pro上的控制实验表明，具有相似平均能力的模型可能表现出显著不同的波动性，稳定性风险在代理式或高故障成本场景下可能推翻优先级决策。

Conclusion: CreditAudit通过提供基于2D和等级的语言支持特定场景的选择，支持分层部署和更规范的测试与监控资源分配，为实际应用实现更客观、可信的模型评估。

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [61] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver是一个自进化的多智能体系统，通过结构化交互让LLM智能体在无需参数更新的情况下获取地球观测专业知识，在三个工具集成EO基准测试中平均提升12%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在专业、工具密集的地球观测领域表现不佳，这些领域需要长时程执行、跨模态紧密协调和严格遵循隐式工具约束。智能体缺乏从交互中学习细粒度工具级专业知识的机制，无法可靠配置工具参数或从执行失败中恢复。

Method: GeoEvolver采用自进化多智能体系统：1）通过检索增强的多智能体编排器将查询分解为独立子目标；2）在子目标级别探索多样化的工具参数配置；3）从成功模式和失败根因分析中提炼知识，存储在进化记忆库中，为未来查询提供上下文演示。

Result: 在三个工具集成的地球观测基准测试中，GeoEvolver持续提升端到端任务成功率，在多个LLM骨干网络上平均获得12%的增益，证明EO专业知识可以通过与环境的细粒度交互逐步涌现。

Conclusion: GeoEvolver展示了LLM智能体可以通过结构化交互在无需参数更新的情况下获取地球观测领域的专业知识，有效解决了复杂EO工作流中的工具配置和失败恢复问题。

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [62] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: PeerRank是一个完全自主的端到端评估框架，让模型自主生成评估任务、基于实时网络信息回答问题、评估同行响应，并聚合密集的同行评估来估计相对性能，无需人工监督或参考答案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要依赖人工编写的基准测试、参考答案和人工或单一模型判断，这些方法扩展性差、容易过时，且与依赖网络检索和综合的开放世界部署不匹配。

Method: PeerRank将评估视为多智能体过程，每个模型对称地作为任务设计者、响应者和评估者参与。模型生成评估任务，使用类别范围的实时网络信息回答问题，评估同行响应，并通过去除偏见判断来聚合密集的同行评估。

Result: 在12个商业模型和420个自主生成问题的大规模研究中，PeerRank产生了稳定、有区分度的排名，并揭示了可测量的身份和呈现偏见。排名具有鲁棒性，平均同行分数与Elo评分一致。在TruthfulQA和GSM8K上的验证显示，同行分数与客观准确性相关。

Conclusion: 研究表明，具有选择性网络基础回答的偏见感知同行评估可以扩展开放世界LLM评估，超越静态和人工策划的基准测试。

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [63] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 提出NSG指标评估LLM自我解释的忠实性，发现自我解释能显著提升模型行为预测能力，但仍有5-15%的误导性解释


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我解释的忠实性评估方法存在局限，主要依赖对抗性提示或错误检测，忽略了解释的预测价值。需要更通用、可扩展的指标来评估解释是否真正反映模型的推理过程。

Method: 提出归一化模拟增益（NSG）指标，基于"忠实解释应让观察者学习模型决策标准，从而更好预测相关输入行为"的理念。在健康、商业、伦理等领域的7,000个反事实数据上评估18个前沿模型（包括Gemini 3、GPT-5.2、Claude 4.5等）。

Result: 自我解释显著提升模型行为预测能力（11-37% NSG增益）。自我解释比外部模型生成的解释提供更多预测信息，即使外部模型更强。这表明自我知识优势是外部解释方法无法复制的。同时发现5-15%的自我解释存在严重误导性。

Conclusion: 尽管存在不完美，但自我解释确实编码了有助于预测模型行为的信息，为自我解释提供了积极案例。NSG指标为评估解释忠实性提供了新视角。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [64] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS是一个专为自主AI研究设计的模块化代理框架，通过预算感知规划、模块化构建和比较反思记忆来解决AI研究中计算成本高和性能归因不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 自动化AI研究与一般软件工程不同，因为涉及计算昂贵的评估（如模型训练）和不透明的性能归因。当前的LLM代理在这方面表现不佳，经常生成忽略执行成本和因果因素的单一脚本。

Method: MARS框架基于三个支柱：1) 预算感知规划：使用成本约束的蒙特卡洛树搜索（MCTS）来平衡性能与执行成本；2) 模块化构建：采用"设计-分解-实现"流水线管理复杂的研究仓库；3) 比较反思记忆：通过分析解决方案差异来提炼高信号洞察，解决信用分配问题。

Result: MARS在MLE-Bench上实现了与开源框架相比的最先进性能，在可比设置下保持竞争力，与全球排行榜的顶级方法相当。系统表现出定性的"顿悟"时刻，63%的已使用经验来自跨分支转移，表明代理能有效跨搜索路径泛化洞察。

Conclusion: MARS框架成功解决了AI研究自动化的独特挑战，通过预算感知规划、模块化构建和反思记忆实现了有效的自主研究能力，并能跨搜索路径泛化洞察。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [65] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS提出任务分布式框架，通过轻量级研究代理和专用支持代理协作，结合EvoDPO算法自适应更新参考策略，在非平稳环境中提升稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM代理系统在提示优化和自动问题解决方面表现良好，但要么在微调后保持求解器固定，要么依赖静态偏好优化循环，这在长视野任务中变得难以处理。

Method: 提出ATLAS任务分布式框架，迭代开发轻量级研究代理，同时将探索、超参数调优和参考策略管理等补充角色委托给专用支持代理。核心算法EvoDPO自适应更新阶段索引参考策略。

Result: 在非平稳线性上下文多臂赌博机和科学机器学习损失重加权实验中，ATLAS相比静态单代理基线提高了稳定性和性能。

Conclusion: ATLAS框架通过任务分布式设计和自适应参考策略更新，有效解决了长视野任务中的非平稳性问题，提升了代理系统的稳定性和性能。

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [66] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种动态混合精度路由框架，用于在长时程决策任务中自适应选择高精度和低精度LLM，以平衡任务成功率和推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前长时程决策任务中，为了获得更高的任务成功率，通常需要使用更大更强的LLM模型，但这会导致高昂的推理成本。论文旨在解决这一成本问题，探索在保持性能的同时降低推理开销的方法。

Method: 提出动态混合精度路由框架，基于观察到不同交互步骤对精度的敏感度不同，自适应地在每个决策步骤选择高精度或低精度LLM。采用两阶段训练管道：1) 基于KL散度的监督学习识别精度敏感步骤；2) 使用组相对策略优化(GRPO)进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著提升。

Conclusion: 通过动态混合精度路由框架，可以在长时程决策任务中有效平衡LLM推理成本和任务成功率，为实际应用提供了可行的解决方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [67] [Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing](https://arxiv.org/abs/2602.02842)
*Saeid Sheikhi*

Main category: cs.AI

TL;DR: CoS是一个双模式推理框架，通过动态路由问题到专门的推理策略（计算流、符号状态跟踪、混合事实提取）来提升LLM推理能力，在多个基准测试中显著优于现有方法，且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有统一的提示方法无法针对不同类型的问题采用专门的推理策略，导致LLM在复杂推理任务上表现不佳。需要一种能够根据问题类型动态选择最优推理模式的框架。

Method: CoS采用三种推理模式：1) 数学问题的计算流与自一致性；2) 空间推理的符号状态跟踪与JSON表示；3) 多跳推理的混合事实提取。框架包含模式选择、状态跟踪和答案提取算法。

Result: 在GSM8K、StrategyQA和bAbI基准测试中，CoS相比最强基线分别提升1.0%、2.5%和65.2%相对改进。计算模式正确应用于数学问题时准确率达81.2%，而错误路由则导致0%准确率。相比Self-Consistency，以54%更低的计算成本获得可比性能。

Conclusion: CoS证明了问题特定的模式选择对LLM推理至关重要，无需额外训练即可显著提升性能，在准确性和效率之间提供优越的权衡。

Abstract: We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.

</details>


### [68] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench是一个评估AI代理科学发现能力的基准测试，通过让代理重新发现已发表机器学习研究的结论来评估其完整科研流程能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性：要么过度依赖LLM-as-judge评估自动生成的研究输出，要么使用孤立性能指标作为科学洞察力的粗略代理。需要更严谨的方法来评估AI代理进行可验证科学发现的能力。

Method: 引入FIRE-Bench基准，让AI代理基于已发表、已验证研究中提取的高层次研究问题，自主探索想法、设计实验、实现代码、执行计划，并基于经验证据得出结论，重新发现已确立的研究发现。

Result: 使用前沿LLM（如gpt-5）的最先进代理在FIRE-Bench上表现有限：最强代理的重新发现成功率低于50 F1，运行间方差高，在实验设计、执行和基于证据的推理方面存在重复性失败模式。

Conclusion: 完整周期的科学研究对当前代理系统仍然具有挑战性，FIRE-Bench为衡量代理驱动科学发现的进展提供了严谨且具有诊断性的框架。

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [69] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 该论文通过扩展BAPO模型，从理论上证明了三个典型任务（二进制多数、三元组匹配、图可达性）需要Ω(n)推理token，并通过实验验证了前沿推理模型在这些任务上呈现近似线性的推理token扩展。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思维推理显著提升了LLM性能，但带来了巨大的延迟和计算成本。论文旨在解决一个基础理论问题：随着输入规模增长，解决一个问题需要多少推理token？

Method: 扩展有界注意力前缀预言机模型来量化任务所需的信息流，为三个BAPO-hard任务证明推理token的下界，并提供匹配或接近匹配的上界构造，最后通过前沿推理模型实验验证。

Result: 证明了三个任务都需要Ω(n)推理token，提供了匹配的上界构造，实验显示前沿模型在这些任务上呈现近似线性的推理token扩展，当推理预算受限时会失败。

Conclusion: 研究结果识别了通过链式思维进行推理时计算的基本瓶颈，并为分析最优推理长度提供了原则性工具。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [70] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: GCR-RL：一种基于序理论的几何相干正则化强化学习方法，通过将价值函数估计重构为学习偏序集，利用超偏序集细化确保几何相干性，显著提升样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用几何特性（如对称结构、几何感知数据增强、结构限制）来稳定和加速强化学习，但缺乏从序理论角度系统利用几何相干性的方法。

Method: 提出GCR-RL框架，将价值函数估计重构为学习偏序集，通过计算超偏序集细化序列（细化先前偏序集并从时序差分信号学习额外序关系）确保几何相干性。开发了基于Q-learning和actor-critic的两种高效算法实现超偏序集细化。

Result: 理论分析了算法的性质和收敛速率。在多种任务上的实证评估表明，GCR-RL相比强基线在样本效率和稳定性能方面有显著提升。

Conclusion: 从序理论视角重新审视强化学习，提出的GCR-RL框架通过确保几何相干性有效提升了强化学习的样本效率和稳定性，为利用几何特性提供了新的理论框架。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [71] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: 研究评估了20多个大语言模型在因果推理任务上的表现，发现它们比人类更倾向于规则化推理，较少表现出人类特有的偏见，但可能在不确定性情境下失效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地应用于需要因果推理的领域，但尚不清楚它们的判断是基于规范的因果计算、人类式的捷径还是脆弱的模式匹配。需要系统评估LLMs在因果推理任务上的表现及其与人类推理的差异。

Method: 在11个基于碰撞器结构（C₁→E←C₂）的因果判断任务上，对20多个LLMs进行基准测试，并与匹配的人类基线比较。使用可解释的小模型压缩LLMs的因果判断，分析推理策略。测试LLMs在语义抽象和提示过载（注入无关文本）下的鲁棒性，并评估思维链（CoT）的影响。

Result: 大多数LLMs表现出比人类更规则化的推理策略，人类在概率判断中似乎考虑了未提及的潜在因素。大多数LLMs没有表现出人类特有的碰撞器偏见（弱解释消除和马尔可夫违规）。思维链（CoT）提高了许多LLMs的鲁棒性。

Conclusion: LLMs与人类在因果推理上的差异表明，当已知的人类偏见不可取时，LLMs可以补充人类判断。但它们的规则化推理在不确定性情境下可能失效，这凸显了需要表征LLMs推理策略以确保安全有效部署的重要性。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [72] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha是一个GUI代理框架，通过步级MCTS结合生成、探索和评估，实现主动规划、早期剪枝和前缀重用，在OSWorld基准上达到77%成功率


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理通过轨迹级采样扩展测试时计算，但缺乏回归能力，无法重用部分成功结果或从早期错误中恢复

Method: 提出Agent Alpha统一框架，结合步级蒙特卡洛树搜索（MCTS），集成alpha-UCT引导搜索、比较驱动评估和多样性约束扩展

Result: 在OSWorld基准上达到约77%的成功率，显著优于同等计算量下的轨迹级基线方法

Conclusion: Agent Alpha通过步级MCTS实现了主动规划、早期剪枝和高效前缀重用，为GUI代理提供了更有效的测试时计算扩展方法

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [73] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP是一种推理感知的主动蒸馏框架，通过将教师模型的决策过程外部化为有向无环图，并用模块化概念预测器在学生模型中镜像该图，从而提高样本效率、训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 部署大型语言模型进行判别任务时面临推理延迟、计算成本和API成本的问题。现有主动蒸馏方法通常只蒸馏最终标签，丢弃了中间推理信号，且缺乏对缺失推理的诊断能力。

Method: 提出图概念预测器（GCP）框架：1）将教师模型的决策过程外部化为有向无环图；2）在学生模型中用模块化概念预测器镜像该图；3）采用图感知采集策略，针对关键推理节点的不确定性和分歧；4）执行针对性子模块重训练，将下游损失归因于特定概念预测器。

Result: 在八个NLP分类基准测试中，GCP在有限标注预算下提升了性能，同时产生了更可解释和可控的训练动态。

Conclusion: GCP框架通过外部化教师推理过程并采用模块化设计，有效解决了传统主动蒸馏的局限性，在提升样本效率的同时增强了模型的可解释性和训练稳定性。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [74] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR框架通过相似性引导的教师辅助精炼，将大语言模型的能力蒸馏到超小型模型，解决了现有方法中的过拟合、训练不稳定、二元奖励无效等问题，在函数调用任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在函数调用中至关重要，但其大规模阻碍了广泛采用，需要将其能力转移到小型模型。现有方法存在过拟合、训练不稳定、多解任务的二元奖励无效以及技术难以协同等问题。

Method: STAR框架包含两个核心技术：1) 约束知识蒸馏(CKD)，通过增强top-k前向KL散度来抑制自信的错误预测；2) 相似性引导强化学习(Sim-RL)，引入基于相似性的细粒度奖励。这些技术在统一的训练课程中协同工作。

Result: 在具有挑战性的基准测试中，STAR模型在其规模类别中建立了SOTA。0.6B的STAR模型在所有1B以下的开源模型中表现最佳，甚至超过了几个更大规模的知名开源模型。

Conclusion: STAR展示了一个将大语言模型能力蒸馏到超小型模型的训练框架，为强大、可访问且高效的人工智能代理铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [75] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法解决LLM多轮工具调用中奖励稀疏和探索成本高的问题，通过奖励条件令牌增强组内多样性，在BFCLv4基准上超越基线并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对LLM具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如组内多数rollout获得全0或全1奖励）会停滞，导致组归一化优势信息不足和更新消失。

Method: 提出RC-GRPO（奖励条件组相对策略优化），将探索视为通过离散奖励令牌的可控引导问题。首先在混合质量轨迹上微调奖励条件轨迹策略（RCTP），在提示中注入奖励目标特殊令牌（如<|high_reward|>, <|low_reward|>），使模型能够按需生成不同质量的轨迹。然后在RL期间，在每个GRPO组内采样多样奖励令牌，并在采样的令牌上条件化rollout以增强组内多样性。

Result: 在Berkeley Function Calling Leaderboard v4多轮基准测试中，该方法相比基线获得持续改进的性能，Qwen-2.5-7B-Instruct的性能甚至超越了所有闭源API模型。

Conclusion: RC-GRPO通过奖励条件令牌有效解决了多轮工具调用中组内多样性不足的问题，显著提升了LLM在复杂任务中的性能表现。

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [76] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS是一个基于工具驱动的多智能体系统，通过分析器-推理器-执行器范式，结合视觉推理和潜在重构，实现通用时间序列任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务自适应工具使用方面存在局限，需要更通用的解决方案。

Method: 采用分析器-推理器-执行器范式，使用视觉语言模型对时间序列图进行视觉推理提取时间结构，在潜在空间重构预测轨迹，通过三个专门智能体协调工作，路由器选择任务特定工具链。

Result: 在多个基准测试中达到最先进性能，展现出强大的泛化能力和高效推理。

Conclusion: MAS4TS通过多智能体系统和工具驱动方法，有效解决了时间序列分析中视觉推理和跨任务泛化的挑战。

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [77] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文系统研究了多智能体系统中的过程验证方法，发现现有验证方法效果不稳定，LLM-as-a-Judge表现相对较好，但过程验证在多智能体系统中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在推理轨迹上存在高方差，过程验证在一般推理场景中显示潜力，但其在多智能体系统中的实际效果尚不明确，需要系统研究。

Method: 提出MAS-ProVe框架，系统评估三种验证范式（LLM-as-a-Judge、奖励模型、过程奖励模型），两个验证粒度（智能体级和迭代级），五个代表性验证器和四种上下文管理策略，在六个多智能体框架和多个推理基准上进行实验。

Result: 过程级验证不能持续提升性能且常表现出高方差；LLM-as-a-Judge通常优于基于奖励的方法；训练过的法官优于通用LLM；LLM作为法官与作为单智能体的性能差距较小；存在上下文长度与性能的权衡。

Conclusion: 多智能体系统的有效且鲁棒的过程验证仍是一个开放挑战，需要超越当前范式的进一步进展。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [78] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench是一个系统化的智能体安全评估框架，通过领域无关的安全原则构建上下文感知的安全评估标准，在真实世界部署条件下评估智能体的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全评估方法存在局限性：1) 依赖针对特定智能体设置的风险导向任务，导致安全风险空间覆盖有限；2) 无法评估智能体在复杂真实世界部署中长期、交互式任务执行中的安全行为；3) 对特定智能体设置的专门化限制了跨不同智能体配置的适应性。

Method: 提出Risky-Bench框架：1) 围绕领域无关的安全原则组织评估；2) 推导上下文感知的安全评估标准来界定安全空间；3) 在不同威胁假设下通过真实任务执行系统评估安全风险；4) 作为结构化评估流程，可适应不同部署场景构建环境特定的安全评估。

Result: 在生活辅助智能体设置中应用Risky-Bench，发现在真实执行条件下最先进的智能体存在重大安全风险。该框架不仅限于生活辅助场景，可适应其他部署设置，提供可扩展的智能体安全评估方法。

Conclusion: Risky-Bench解决了现有智能体安全评估的局限性，提供了系统化、可扩展的方法来评估智能体在真实世界部署中的安全风险，能够适应不同智能体配置和部署环境。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [79] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 论文分析了多智能体LLM框架的架构设计对系统性能的重大影响，发现仅架构选择就能导致100倍延迟增加、30%规划准确率下降和协调成功率从90%降至30%以下，并提出了MAFBench评估套件和架构设计原则。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架虽然广泛使用，但其架构设计对系统性能的影响缺乏深入理解。现有基准测试只关注单一能力，缺乏标准化的框架级评估，无法隔离架构效应，导致难以评估不同框架设计的实际影响。

Method: 提出多智能体LLM框架的架构分类法，开发MAFBench统一评估套件，集成现有基准测试到标准化执行管道中，在多个流行框架上进行受控实证研究。

Result: 框架级设计选择单独就能导致：延迟增加超过100倍，规划准确率下降高达30%，协调成功率从90%以上降至30%以下。不同框架在编排开销、内存行为、规划、专业化和协调等方面表现差异显著。

Conclusion: 多智能体LLM框架的架构设计对系统性能有决定性影响，需要系统化的评估方法。研究提出了具体的架构设计原则和框架选择指导，并为未来研究指明了方向。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [80] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文扩展了先前关于智能体世界模型的研究，将定理推广到随机智能体和部分可观测环境，证明随机智能体也无法避免学习其环境模型


<details>
  <summary>Details</summary>
Motivation: 先前研究证明了在确定性和完全可观测环境下，几乎最优的通用智能体必然包含对其环境的充分知识。本研究旨在移除这两个限制条件，探索在更现实场景下智能体是否仍会学习环境模型。

Method: 扩展先前框架，将定理推广到随机智能体和部分可观测环境。通过理论分析证明随机智能体无法避免学习环境模型，同时弱化"通用性"概念，证明能力较弱的智能体也包含世界模型。

Result: 成功将定理扩展到随机智能体和部分可观测环境，证明随机智能体也无法避免学习环境模型。同时通过弱化通用性概念，证明能力较弱的智能体也包含对其操作世界的模型。

Conclusion: 随机智能体在部分可观测环境中也无法避免学习其环境模型，这表明学习环境模型是智能体实现良好性能的基本要求，即使在使用随机化和面对部分可观测性的情况下也是如此。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [81] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling是一种基于轨迹多样性而非数据量的代码智能体训练框架，通过多样性扩展提升性能，包含业务聚类、蓝图驱动多智能体、自适应进化机制和沙盒代码工具四大创新。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型通过MCP演化为工具交互智能体时，其泛化能力受到低质量合成数据和数量扩展收益递减的限制。数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: 提出TDScaling框架：1）业务聚类机制捕捉真实服务逻辑依赖；2）蓝图驱动多智能体范式确保轨迹连贯性；3）自适应进化机制使用领域熵、推理模式熵和累积动作复杂度引导合成走向长尾场景；4）沙盒代码工具防止内在编码能力灾难性遗忘。

Result: 在通用工具使用基准（BFCL、tau^2-Bench）和代码智能体任务（RebenchT、CodeCI、BIRD）上实验表明，TDScaling实现了双赢：既提高了工具使用泛化能力，又增强了固有编码能力。

Conclusion: TDScaling通过轨迹多样性扩展而非原始数据量扩展，在固定训练预算下实现了更好的性能-成本权衡，为代码智能体训练提供了更高效的数据合成框架。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [82] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架，通过双记忆进化机制解决智能体在任务演化过程中出现的信任度下降问题，在保持任务性能的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时通过记忆演化积累经验是实现AGI的重要途径，但即使在良性任务演化过程中，智能体的安全对齐仍然脆弱，存在"Agent Memory Misevolution"现象，导致信任度下降。

Method: 提出TAME双记忆进化框架：1) 执行器记忆进化，通过提炼可泛化的方法论提升任务性能；2) 评估器记忆进化，基于历史反馈精化安全性和任务效用的评估。通过记忆过滤、草稿生成、可信度精化、执行和双轨记忆更新的闭环流程。

Result: 实验表明TAME能够缓解记忆错误演化，在信任度和任务性能两方面都实现了联合改进。Trust-Memevo基准测试显示该方法有效。

Conclusion: TAME框架通过分离演化执行器和评估器记忆，在保持任务效用的同时保护了信任度，为解决智能体记忆演化中的安全问题提供了有效方案。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [83] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 提出统一评估框架解决智能体评估中的标准化问题，包括系统提示、工具配置和环境动态等混杂因素


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估存在严重混杂因素：系统提示、工具配置、环境动态等差异导致难以将性能提升归因于模型本身；缺乏标准化环境数据导致不可追踪的错误和不可复现的结果；碎片化的研究者特定框架引入不公平性和不透明性

Method: 提出统一评估框架来标准化智能体评估，旨在解决当前评估中的混杂因素和标准化问题

Result: 论文提出了一个旨在标准化智能体评估的提案，但具体实施细节和实验结果在摘要中未详细说明

Conclusion: 统一评估框架对于智能体评估的严谨发展至关重要，标准化能够解决当前评估中的不公平性和不透明性问题

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [84] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: LPS-Bench是一个评估基于MCP的计算机使用代理在长时程任务中规划时安全意识的基准，涵盖65个场景、7个任务领域和9种风险类型，揭示现有代理在安全行为保持方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注短时程或GUI任务，评估执行时错误但忽视了规划时风险预测能力。计算机使用代理面临模糊指令触发有害操作和对抗性用户操纵工具执行的风险，需要专门的基准来评估规划时安全。

Method: 提出LPS-Bench基准，采用多代理自动管道进行可扩展数据生成，使用LLM-as-a-judge评估协议通过规划轨迹评估安全意识，涵盖65个场景、7个任务领域和9种风险类型。

Result: 实验显示现有计算机使用代理在保持安全行为方面存在显著缺陷，分析了风险并提出了改进基于MCP的计算机使用代理系统长时程规划安全的缓解策略。

Conclusion: LPS-Bench填补了规划时安全评估的空白，揭示了现有代理的安全缺陷，为改进基于MCP的计算机使用代理系统的长时程规划安全提供了基准和分析框架。

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [85] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出Agentic Proposing框架，通过专门代理动态选择和组合模块化推理技能来生成高质量可验证数据集，训练出的求解器在数学、编程和科学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的复杂推理能力提升依赖于高质量可验证数据集，但人工标注成本高且难以扩展。现有合成方法面临两难：保持结构有效性会限制问题复杂度，而放宽约束增加难度又会导致不一致或不可解实例。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的顺序决策过程，专门代理动态选择和组合模块化推理技能。通过内部反思和工具使用的迭代工作流程，使用多粒度策略优化（MGPO）开发Agentic-Proposer-4B，生成数学、编程和科学领域的高精度可验证训练轨迹。

Result: 在代理合成数据上训练的下游求解器显著优于领先基线，并展现出强大的跨领域泛化能力。仅用11,000个合成轨迹训练的30B求解器在AIME25上达到91.6%的最先进准确率，媲美GPT-5等前沿专有模型。

Conclusion: 少量高质量合成信号可以有效替代大规模人工标注数据集，证明了代理驱动合成方法的有效性，为解决数据稀缺问题提供了新途径。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [86] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: MeetAll数据集与MeetMaster XL代理框架：针对企业会议环境的AI助手，通过双语多模态数据集和双策略代理优化查询路由与工具调用，在延迟、成本、隐私约束下提升事实准确性、意图对齐和响应效率。


<details>
  <summary>Details</summary>
Motivation: 企业会议环境需要能够处理多样化操作任务的AI助手，包括实时事实核查和跨会议战略分析，同时满足严格的延迟、成本和隐私约束。现有会议基准主要关注简化的问答任务，未能反映真实企业工作流程，其中查询来自多方利益相关者协作、跨越长时间上下文、需要工具增强推理。

Method: 1) 引入MeetAll双语多模态语料库，基于231个企业会议（140小时），采用企业验证的协议注入问题；2) 提出MeetBench XL多维度评估协议，衡量事实保真度、意图对齐、响应效率、结构清晰度和完整性；3) 提出MeetMaster XL双策略代理，联合优化快速与慢速推理路径间的查询路由以及工具调用（检索、跨会议聚合、网络搜索），通过轻量级分类器实现准确路由。

Result: 实验表明，与商业系统相比，该方法获得一致性能提升。轻量级分类器以最小开销实现准确路由，在质量-延迟权衡上优于单模型基线。通过消融实验、鲁棒性测试和真实世界部署案例研究支持了这些发现。

Conclusion: 该工作通过接地气的数据集和学习型代理框架解决了企业会议AI助手的实际需求，在四个关键企业维度（认知负荷、时间上下文跨度、领域专业知识、可执行任务）上提供了更真实的评估和优化方案。

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [87] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora是一种谐波记忆表示方法，通过抽象与具体性的结构平衡来扩展智能体记忆系统，支持高效、上下文感知的检索，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆系统需要处理不断增长的信息，同时支持下游任务的高效、上下文感知检索。抽象对于扩展记忆至关重要，但通常以牺牲具体性为代价，模糊了有效推理所需的细粒度细节。

Method: Memora通过主要抽象索引具体记忆值，将相关更新整合为统一记忆条目，同时使用线索锚点扩展检索访问范围并连接相关记忆。在此基础上，采用检索策略主动利用这些记忆连接来检索超出直接语义相似性的相关信息。

Result: Memora在LoCoMo和LongMemEval基准测试中建立了新的最先进水平，展示了随着记忆扩展，更好的检索相关性和推理有效性。理论上证明标准RAG和基于知识图的记忆系统是其框架的特殊情况。

Conclusion: Memora提供了一种谐波记忆表示方法，在抽象与具体性之间取得结构平衡，能够有效扩展智能体记忆系统，支持更高效、上下文感知的检索和推理。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [88] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL训练主动式智能体在开始长时程研究前澄清用户潜在意图，通过两阶段强化学习提升意图命中率和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 深度研究(DR)智能体虽然能自主检索和综合网络信息生成长报告，但存在自主性-交互困境：对模糊查询的高自主性常导致执行时间长且结果不满意。需要解决DR智能体在开始耗时研究前澄清用户意图的问题。

Method: 提出IntentRL框架，训练主动式智能体在开始长时程研究前澄清潜在用户意图。采用可扩展管道将少量种子样本通过浅层到深层意图细化图扩展为高质量对话轮次。采用两阶段强化学习策略：第一阶段在离线对话上应用RL学习通用用户交互行为；第二阶段使用训练好的智能体和用户模拟器进行在线推演，增强对多样化用户反馈的适应能力。

Result: 实验表明IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR智能体的内置澄清模块和主动式LLM基线。

Conclusion: IntentRL框架有效解决了DR智能体的自主性-交互困境，通过主动澄清用户意图提高了研究效率和结果质量，为长时程智能体系统提供了实用的交互优化方案。

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [89] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: 提出Persona Generators方法，通过AlphaEvolve迭代优化生成多样化合成人口，覆盖长尾行为，优于现有基线


<details>
  <summary>Details</summary>
Motivation: 评估AI系统需要多样化人类数据，但收集真实数据昂贵且不可行，现有生成方法需要详细人口数据且偏向密度匹配而非支持覆盖，导致长尾行为探索不足

Method: 引入Persona Generators函数，基于AlphaEvolve迭代改进循环，使用大语言模型作为变异算子，在数百次迭代中优化生成器代码，产生轻量级生成器

Result: 进化后的生成器在六个多样性指标上显著优于现有基线，能生成覆盖罕见特征组合的多样化合成人口

Conclusion: Persona Generators方法能有效生成多样化合成人口，覆盖长尾行为，为AI系统评估提供更全面的测试数据

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [90] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: 该研究评估LLM在复杂天体动力学任务中的自主规划能力，发现虽然战略理解能力显著提升，但执行层面存在严重障碍，LLM目前只能作为领域助手而非完全自主的工程师。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成和推理方面表现出色，但在高维物理约束环境中的自主多阶段规划能力仍不明确。研究旨在通过天体动力学竞赛评估当前AI代理的极限能力。

Method: 将MLE-Bench框架应用于轨道力学领域，采用AIDE-based代理架构自主生成和优化任务方案。使用"LLM-as-a-Judge"方法，基于领域专家制定的评分标准在五个结构类别中评估战略可行性。

Result: 过去两年平均战略可行性得分几乎翻倍（从9.3分升至17.2分，满分26分）。先进模型展现出良好的概念理解能力，能正确构建目标函数和任务架构，但在执行层面存在物理单位不一致、边界条件错误和调试效率低等问题。

Conclusion: 当前LLM具备解决空间科学任务所需的知识和智能，但受限于实施障碍，只能作为强大的领域促进者而非完全自主的工程师。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [91] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 论文提出"对话惯性"概念，指LLM在多轮代理场景中过度模仿自身先前响应的问题，并提出上下文偏好学习框架来校准模型偏好，减少惯性并提升性能。


<details>
  <summary>Details</summary>
Motivation: 将few-shot LLM转化为多轮代理时存在矛盾：长上下文能提供更多环境反馈用于利用，但也会增强对话惯性，限制探索能力。模型会错误地将自己之前的响应作为few-shot示例进行模仿。

Method: 通过注意力分析识别对话惯性现象，提出上下文偏好学习(CPL)框架：基于相同状态下长上下文比短上下文产生更强惯性的洞察，构建无需环境奖励的偏好对，校准模型偏好以选择低惯性响应。同时提供推理时的上下文管理策略。

Result: 在8个代理环境和1个深度研究场景的实验验证表明，该框架能有效减少对话惯性，并在性能上取得改进。

Conclusion: 对话惯性是多轮代理场景中的重要问题，上下文偏好学习能有效校准LLM偏好，平衡探索与利用，提升代理性能。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [92] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个基于统一代理抽象（指令、上下文、工具、模型）的代理编排系统，能够动态创建专用执行器，在多个基准测试中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理系统在处理复杂、长时程任务时缺乏动态抽象视图，限制了适应性。需要一种统一的、框架无关的代理抽象来提升任务解决的灵活性和效率。

Method: 提出统一的代理抽象模型（指令、上下文、工具、模型），基于此构建AOrchestra系统，其中编排器在每一步动态具体化该元组：策划任务相关上下文、选择工具和模型，并通过即时自动代理创建委托执行。

Result: 在三个具有挑战性的基准测试（GAIA、SWE-Bench、Terminal-Bench）中，AOrchestra与Gemini-3-Flash配对时，相对于最强基线实现了16.28%的相对改进。

Conclusion: AOrchestra的统一代理抽象和动态编排设计减少了人工工程工作量，保持框架无关性，支持可控制的性能-成本权衡，能够接近帕累托效率。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [93] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 研究发现多智能体系统性能受限于任务内在不确定性而非智能体数量，异质智能体通过提供互补证据显著优于同质智能体扩展


<details>
  <summary>Details</summary>
Motivation: 探索LLM多智能体系统扩展的局限性，发现增加同质智能体数量存在收益递减，而异质智能体却能持续提升性能，需要理解这种差异的根本原因

Method: 提出信息论框架分析多智能体系统性能边界，引入K*指标量化有效通道数量，通过实验验证异质配置相对于同质扩展的优势

Result: 异质配置显著优于同质扩展：2个异质智能体性能可匹配或超过16个同质智能体，证明多样性设计的高效性

Conclusion: 多智能体系统性能受任务内在不确定性限制，异质智能体通过提供互补证据突破同质扩展瓶颈，为构建高效鲁棒系统提供多样性设计原则

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [94] [Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation](https://arxiv.org/abs/2602.02584)
*Srinivas Rao Marri*

Main category: cs.SE

TL;DR: 提出Constitutional Spec-Driven Development方法，在规范层嵌入不可协商的安全原则，使AI生成的代码从一开始就符合安全要求，而非事后检查。通过银行微服务案例验证，将安全缺陷减少73%。


<details>
  <summary>Details</summary>
Motivation: AI辅助的"氛围编程"虽然加速软件开发，但大型语言模型优先考虑功能正确性而非安全性，引入了重大安全风险。需要一种方法确保AI生成的代码符合安全要求。

Method: 提出宪法规范驱动开发方法，引入"宪法"——一个版本化、机器可读的文档，编码来自CWE/MITRE Top 25漏洞和监管框架的安全约束。将不可协商的安全原则嵌入规范层，确保AI生成的代码通过构造而非检查来遵守安全要求。

Result: 通过银行微服务应用案例研究，该方法解决了10个关键CWE漏洞，具有从原则到代码位置的全链路可追溯性。与无约束的AI生成相比，宪法约束将安全缺陷减少了73%，同时保持了开发速度。

Conclusion: 主动的安全规范优于反应性的安全验证在AI辅助开发工作流中。贡献了宪法安全的形式化框架、完整的开发方法和实证证据。

Abstract: The proliferation of AI-assisted "vibe coding" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.

</details>


### [95] [Agentic Observability: Automated Alert Triage for Adobe E-Commerce](https://arxiv.org/abs/2602.02585)
*Aprameya Bharadwaj,Kyle Tu*

Main category: cs.SE

TL;DR: Adobe开发了一个基于ReAct范式的智能可观测性框架，能够自主进行告警分诊，通过动态识别受影响服务、分析分布式日志、执行操作手册等方式，将平均洞察时间减少了90%。


<details>
  <summary>Details</summary>
Motivation: 现代企业系统具有复杂的相互依赖性，使得可观测性和事件响应变得越来越困难。手动告警分诊（通常涉及日志检查、API验证和操作知识库交叉引用）仍然是降低平均恢复时间（MTTR）的主要瓶颈。

Method: 采用基于ReAct范式的智能可观测性框架，在检测到告警时，智能体动态识别受影响服务，检索和分析分布式系统的相关日志，并规划上下文相关的操作，如手册咨询、运行手册执行或最近部署代码的检索增强分析。

Result: 生产环境部署的实证结果表明，与手动分诊相比，平均洞察时间减少了90%，同时保持了相当的诊断准确性。智能AI使分诊延迟降低了一个数量级，并在解决准确性上实现了阶跃式改进。

Conclusion: 智能AI实现了分诊延迟的数量级减少和解决准确性的阶跃式改进，标志着企业运营向自主可观测性的关键转变。

Abstract: Modern enterprise systems exhibit complex interdependencies that make observability and incident response increasingly challenging. Manual alert triage, which typically involves log inspection, API verification, and cross-referencing operational knowledge bases, remains a major bottleneck in reducing mean recovery time (MTTR). This paper presents an agentic observability framework deployed within Adobe's e-commerce infrastructure that autonomously performs alert triage using a ReAct paradigm. Upon alert detection, the agent dynamically identifies the affected service, retrieves and analyzes correlated logs across distributed systems, and plans context-dependent actions such as handbook consultation, runbook execution, or retrieval-augmented analysis of recently deployed code. Empirical results from production deployment indicate a 90% reduction in mean time to insight compared to manual triage, while maintaining comparable diagnostic accuracy. Our results show that agentic AI enables an order-of-magnitude reduction in triage latency and a step-change in resolution accuracy, marking a pivotal shift toward autonomous observability in enterprise operations.

</details>


### [96] [Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All](https://arxiv.org/abs/2602.02690)
*Chenxi Huang,Alex Mathai,Feiyang Yu,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Eugene Wu,Kostis Kaffes,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: Live-kBench是一个自演化基准测试框架，用于持续评估LLM代理在修复Linux内核崩溃方面的能力，配合kEnv标准化环境实现公平比较。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评估基准存在静态性（无法反映内核演化）和数据污染（LLM知识截止问题）的局限性，需要动态、公平的评估框架。

Method: 提出两个组件：(1) Live-kBench：持续爬取新发现内核漏洞的自演化基准测试框架；(2) kEnv：代理无关的标准化崩溃修复环境，用于内核编译、执行和反馈。

Result: 收集534个Linux内核漏洞数据集，发现代理在LLM知识截止前修复的漏洞上等效补丁率高出25%；当前最优代理首尝试修复率达74%，但仅20%补丁与开发者修复高度匹配；反馈机制可将崩溃修复率提升29%。

Conclusion: Live-kBench为社区提供了时间敏感和属性敏感的自演化基准测试基础设施，通过公开仪表板跟踪代理在Linux内核漏洞修复上的进展。

Abstract: Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.
  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.

</details>


### [97] [Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering](https://arxiv.org/abs/2602.02752)
*Srinath Srinivasan,Tim Menzies*

Main category: cs.SE

TL;DR: 该研究比较了人类与人工智能生成领域知识的策略，评估了四种架构，旨在让LLM在高维优化中生成有效的初始解。


<details>
  <summary>Details</summary>
Motivation: LLM在低维软件工程优化任务上表现良好，但在高维问题上表现不佳，而贝叶斯方法在高维问题上占主导。研究旨在探索如何通过系统集成领域知识来弥合这一差距。

Method: 在MOOT数据集上评估四种方法：1) 人类在环领域知识提示(H-DKP)；2) 自适应多阶段提示(AMP)；3) 维度感知渐进细化(DAPR)；4) 混合知识模型方法(HKMA)。使用切比雪夫距离和Scott-Knott聚类进行性能量化。

Result: 论文未在摘要中提供具体结果，但描述了系统评估框架和性能量化方法。

Conclusion: 研究旨在确定结构化知识集成是否能使LLM为高维优化生成有效的初始解，但结论需等待具体实验结果。

Abstract: Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.
  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.
  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.
  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.

</details>


### [98] [Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework](https://arxiv.org/abs/2602.02896)
*Jianru Shen,Zedong Peng,Lucy Owen*

Main category: cs.SE

TL;DR: 该论文研究了LLM在软件开发自动化中的增强策略，发现不同策略对不同类型的代码生成失败效果不同，提出了基于失败模式的决策框架。


<details>
  <summary>Details</summary>
Motivation: LLM在将需求转换为代码方面有潜力，但即使使用渐进提示等方法，仍有一些需求无法满足。现有增强策略（如自我批判、多模型协作、RAG）缺乏明确的使用指导，开发者不知道何时使用哪种方法。

Method: 对25个GitHub项目进行实证研究，评估渐进提示与直接提示的效果。针对6个最具代表性的项目，评估每种增强策略在4种失败类型上的表现。基于结果提出决策框架。

Result: 渐进提示平均任务完成率达96.9%，显著优于直接提示（80.5%），但仍有8个项目未完成。自我批判对可审查的逻辑错误有效，但对外部服务集成完全无效（0%改进）。RAG在所有失败类型中实现最高完成率且效率最优。

Conclusion: 方法有效性关键取决于失败特征，需要根据失败模式选择最合适的增强方法。提出的决策框架为开发者提供了实用的、数据驱动的指导，避免了试错。

Abstract: Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.

</details>


### [99] [Beyond Blame: Rethinking SZZ with Knowledge Graph Search](https://arxiv.org/abs/2602.02934)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AgenticSZZ：首个将时序知识图谱应用于软件演化分析的方法，将bug引入提交识别从基于blame的排序问题转化为图搜索问题，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有SZZ方法依赖git blame，搜索空间仅限于直接修改修复行的提交，但研究表明超过40%的bug引入提交无法通过blame单独解决，需要超越blame结果的历史遍历。

Method: 采用两阶段方法：1) 构建时序知识图谱，编码提交的时间与结构关系，从两个参考点（blame提交和bug修复提交）向后遍历文件历史扩展搜索空间；2) 利用LLM代理通过专用工具导航图谱进行候选探索和因果分析。

Result: 在三个数据集上的评估显示，AgenticSZZ的F1分数达到0.48-0.74，相比最先进方法有高达27%的统计显著提升。消融研究证实两个组件都至关重要。

Conclusion: 通过将bug引入提交识别转化为图搜索问题，为软件演化分析中的时序和因果推理开辟了新的研究方向，体现了经典的探索-利用权衡。

Abstract: Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.
  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.
  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.

</details>


### [100] [Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations](https://arxiv.org/abs/2602.03400)
*Jintai Li,Songqiang Chen,Shuo Jin,Xiaoyuan Xie*

Main category: cs.SE

TL;DR: ExpSum提出了一种面向工业文档期望的代码摘要生成方法，通过函数元数据抽象、信息过滤、领域知识检索和约束提示，显著提升了LLM生成的代码摘要在实际工业项目中的可用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码摘要生成方面取得进展，但在工业环境中生成的摘要实用性不足。研究发现超过57.4%的现有方法生成的摘要因不符合开发者对工业文档的期望而被拒绝，开发者需要适当的领域术语、明确的函数分类，并避免冗余实现细节。

Method: ExpSum方法包含四个核心组件：1) 函数元数据抽象，提取关键信息；2) 信息元数据过滤，去除冗余；3) 上下文感知的领域知识检索；4) 约束驱动的提示工程，指导LLM生成结构化、符合期望的摘要。

Result: 在HarmonyOS项目和广泛使用的代码摘要基准测试中，ExpSum持续优于所有基线方法，在HarmonyOS上BLEU-4提升26.71%，ROUGE-L提升20.10%。基于LLM的评估表明，ExpSum生成的摘要在其他项目中也能更好地符合开发者期望。

Conclusion: ExpSu有效解决了工业代码文档的实际需求，通过整合开发者期望和领域知识，显著提升了自动生成的代码摘要在实际工业环境中的可用性和实用性。

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.
  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

</details>


### [101] [SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training](https://arxiv.org/abs/2602.03411)
*Huatong Song,Lisheng Huang,Shuang Sun,Jinhao Jiang,Ran Le,Daixuan Cheng,Guoxin Chen,Yiwen Hu,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: SWE-Master是一个开源、可复现的软件工程智能体训练框架，通过系统化的后训练方法（包括教师轨迹合成、长视野SFT、真实执行反馈的RL等）显著提升了模型解决软件工程任务的能力，在SWE-bench Verified上达到61.4%的解决率，结合TTS可提升至70.8%。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程智能体开发缺乏系统化、可复现的训练框架。研究者旨在探索完整的智能体开发流程，从基础模型出发，通过系统优化方法激发模型解决长视野软件工程任务的能力，为可复现的软件工程智能体研究提供实用基础。

Method: SWE-Master采用系统化的后训练框架，包括：1）教师轨迹合成与数据整理；2）长视野监督微调（SFT）；3）基于真实执行反馈的强化学习（RL）；4）推理框架设计。框架从开源基础模型开始，逐步优化软件工程能力，并支持测试时扩展（TTS）结合LLM环境反馈。

Result: 在SWE-bench Verified基准测试中，使用Qwen2.5-Coder-32B模型，SWE-Master在相同实验设置下达到61.4%的解决率，显著优于现有开源基线。结合测试时扩展（TTS@8）和LLM环境反馈，性能进一步提升至70.8%。

Conclusion: SWE-Master证明了系统化优化方法能够有效激发基础模型解决复杂软件工程任务的能力，为软件工程智能体的可复现研究提供了实用透明的框架基础，展示了强大的性能潜力。

Abstract: In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.

</details>


### [102] [SWE-World: Building Software Engineering Agents in Docker-Free Environments](https://arxiv.org/abs/2602.03419)
*Shuang Sun,Huatong Song,Lisheng Huang,Jinhao Jiang,Ran Le,Zhihao Lv,Zongchao Chen,Yiwen Hu,Wenyang Luo,Wayne Xin Zhao,Yang Song,Hongteng Xu,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: SWE-World提出一个无需Docker的软件工程代理训练框架，用学习型替代模型预测执行结果，显著提升效率并支持测试时扩展。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程代理依赖容器化环境的执行反馈，资源消耗大且维护困难，限制了代理训练的可扩展性。

Method: 使用基于LLM的替代模型，通过真实代理-环境交互数据训练，预测中间执行结果和最终测试反馈，无需物理容器环境。

Result: 在SWE-bench Verified上，Qwen2.5-Coder-32B从6.2%提升到52.0%（无Docker SFT）、55.0%（无Docker RL）和68.2%（进一步TTS）。

Conclusion: SWE-World框架有效解决了容器化环境依赖问题，显著提升了软件工程代理的训练效率和性能，支持测试时扩展。

Abstract: Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\% to 52.0\% via Docker-free SFT, 55.0\% with Docker-free RL, and 68.2\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World

</details>


### [103] [RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes](https://arxiv.org/abs/2602.03462)
*Ruwei Pan,Yakun Zhang,Qingyuan Liang,Yueheng Zhu,Chao Liu,Lu Zhang,Hongyu Zhang*

Main category: cs.SE

TL;DR: RAL-Bench是一个应用级代码生成基准测试框架，评估LLM生成多文件可运行仓库的能力，涵盖功能正确性和非功能性质量（可维护性、安全性等）。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在应用级代码生成评估方面有限，无法全面评估功能正确性和非功能性质量。需要了解当前LLM是否能生成满足这两方面要求的应用级代码仓库。

Method: 从高质量参考项目中提炼自然语言需求，构建覆盖功能和非功能属性的黑盒系统测试，只保留在参考仓库上通过的测试以确保可靠基准。功能正确性通过系统测试通过率衡量，非功能性质量基于ISO/IEC 25010的五个维度，使用层次分析法加权聚合。

Result: 在16个LLM的零样本贪婪解码评估中，功能正确性是主要瓶颈：没有模型在需求驱动、参考验证的测试中超过45%的功能通过率。

Conclusion: 当前LLM在应用级代码生成方面仍有很大提升空间，特别是功能正确性方面。RAL-Bench为评估应用级代码生成提供了全面框架。

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

</details>


### [104] [Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA](https://arxiv.org/abs/2602.03556)
*Alexander Berndt,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 该论文提出使用LLMs作为标注器，通过模型内和模型间一致性来标注与已修复的flaky测试相关的issue报告，以识别SAP HANA中flaky测试的主要根因类别。


<details>
  <summary>Details</summary>
Motivation: Flaky测试（不稳定测试）在相同源代码版本多次执行时产生不同结果，干扰代码质量评估和自动化代码变更评估。手动标注flaky测试耗时且繁琐，需要了解不同编程语言、应用领域或软件项目规模下flaky测试的主要根因。

Method: 采用LLMs-as-annotators方法，利用模型内和模型间一致性来标注与已修复的flakiness问题相关的issue报告，识别相关的根因类别。在SAP HANA（大型工业数据库管理系统）的背景下评估该标注方法。

Result: 在分析的559个issue报告中，SAP HANA的测试最常见的问题是并发相关（23%，130个报告）。不同测试类型面临不同的flakiness挑战。

Conclusion: 鼓励未来关于flakiness缓解的研究考虑评估所提方法在不同测试类型间的泛化能力。LLMs标注方法有助于识别工业系统中flaky测试的主要根因模式。

Abstract: Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.

</details>


### [105] [Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study](https://arxiv.org/abs/2602.03557)
*Yunhao Liang,Ruixuan Ying,Shiwen Ni,Zhe Cui*

Main category: cs.SE

TL;DR: 论文提出了一种将测试驱动开发(TDD)从函数级扩展到类级的代码生成框架，通过分析类内方法依赖关系制定生成计划，并利用反射式执行反馈和有限修复迭代来增量实现方法。


<details>
  <summary>Details</summary>
Motivation: 现有TDD风格的代码生成研究主要局限于函数级任务，而类级合成（多个方法通过共享状态和调用依赖交互）尚未充分探索。需要将测试驱动代码生成从函数扩展到类，以提高类级代码生成的可靠性。

Method: 提出迭代TDD框架：1) 分析类内方法依赖关系以制定可行的生成计划；2) 在方法级公共测试下增量实现每个方法，使用反射式执行反馈和有限修复迭代；3) 构建ClassEval-TDD基准，包含一致的规范、确定性测试环境和完整的方法级公共测试。

Result: 在8个LLM上的实验表明，类级TDD框架将类级正确性提高了12-26个绝对百分点，达到最高71%完全正确的类，平均只需要少量修复。相比最强的直接生成基线（整体、增量、组合策略中的最佳者）有显著改进。

Conclusion: 测试驱动生成可以有效扩展到孤立函数之外，显著提高类级代码生成的可靠性。TDD方法能够处理类内方法间的复杂交互，为类级代码合成提供了有效的解决方案。

Abstract: Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/

</details>


### [106] [SWE-Refactor: A Repository-Level Benchmark for Real-World LLM-Based Code Refactoring](https://arxiv.org/abs/2602.03712)
*Yisen Xu,Jinqiu Yang,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: SWE-Refactor是一个新的代码重构基准测试，包含1,099个开发者编写的Java重构实例，用于评估LLM在代码重构任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有重构基准测试存在三个主要问题：重构场景覆盖有限、实例中混杂无关修改、缺乏仓库级上下文进行真实评估。需要更好的基准来评估LLM在代码重构任务上的能力。

Method: 从18个Java项目中挖掘1,099个开发者编写的重构实例（922个原子重构和177个复合重构），通过编译、测试执行和自动化重构检测工具验证正确性，评估9个广泛使用的LLM模型。

Result: 复杂和复合重构是主要失败来源，OpenAI Codex代理在复合实例上仅达到39.4%的成功率。提供了代表性参考结果。

Conclusion: SWE-Refactor基准测试解决了现有重构评估的不足，为LLM在代码重构领域的研究提供了更好的评估框架。

Abstract: Large Language Models (LLMs) have recently attracted wide interest for tackling software engineering tasks. In contrast to code generation, refactoring demands precise, semantics-preserving edits that improve program structure, which also makes automated evaluation challenging. However, existing refactoring benchmarks commonly suffer from three shortcomings: limited coverage of refactoring scenarios, the inclusion of instances that mix refactoring with unrelated changes, and insufficient repository-level context for realistic assessment. To mitigate these issues, we introduce SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor comprises 1,099 developer-written, behavior-preserving refactorings mined from 18 Java projects, including 922 atomic and 177 compound instances. Each instance is validated via compilation, test execution, and automated refactoring detection tools to ensure correctness. We evaluate nine widely used LLMs on SWE-Refactor, covering models such as GPT-4o-mini, DeepSeek-V3, and CodeLLaMa, to provide representative reference results. Our results show that complex and compound refactorings remain the primary source of failures; notably, an OpenAI Codex agent achieves only 39.4% success on compound instances. We release SWE-Refactor and all evaluation results to facilitate future research on LLM-based code refactoring.

</details>


### [107] [FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation](https://arxiv.org/abs/2602.03798)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.SE

TL;DR: FullStack-Agent是一个用于全栈网页开发的多智能体系统，包含开发框架、自学习方法和基准测试三部分，显著提升了前端、后端和数据库功能的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码智能体主要生成前端网页，缺乏真正的全栈数据处理和存储能力。构建生产级全栈应用需要控制数据流、理解不断更新的包依赖、定位代码中的隐蔽bug，这些挑战远超过仅生成前端页面。

Method: 系统包含三部分：1) FullStack-Dev：具有规划、代码编辑、代码库导航和bug定位能力的多智能体框架；2) FullStack-Learn：通过反向翻译爬取和合成的网站仓库进行数据扩展和自我改进的方法；3) FullStack-Bench：全面测试生成网站前端、后端和数据库功能的基准。

Result: FullStack-Dev在前端、后端和数据库测试用例上分别比之前最优方法提升8.7%、38.2%和15.9%。FullStack-Learn通过自我改进使30B模型在三个测试集上分别提升9.7%、9.5%和2.8%。

Conclusion: FullStack-Agent系统有效解决了全栈网页开发的挑战，通过多智能体框架、自学习方法和全面基准测试，显著提升了全栈应用生成的质量和可靠性。

Abstract: Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [108] [A JTBD-First Pattern for Agentic Tool Systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaeda.pm%2F2026%2F01%2F30%2Fa-jtbd-first-pattern-for-agentic-tool-systems%2F%3Futm_source=tldrproduct/1/0100019c232fc484-db271375-ac1a-4b0e-83fd-649b43d6e0a5-000000/ryrdeXa2LoK956TUkgyZx3ZzvgaOVMW5B2_TRYGE7gc=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文提出了一种以JTBD（Jobs-to-be-Done）为先的智能体工具系统模式，强调从明确定义的任务出发，使用少量任务导向工具，并通过验证实现持续改进


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统往往工具过多、目标不明确，导致效率低下。需要一种更聚焦的方法来构建强大的智能体系统

Method: 采用JTBD优先模式：1）明确定义任务 2）使用少量任务导向工具 3）通过验证形成闭环，将一次性执行转化为持续改进

Result: 该方法能够构建更强大、更聚焦的智能体系统，通过验证闭环实现系统性能的持续提升

Conclusion: JTBD优先模式为构建有效的智能体工具系统提供了结构化方法，强调任务定义、工具精简和持续验证的重要性

Abstract: A JTBD-First Pattern for Agentic Tool Systems (4 minute read) Strong agentic systems start with a clearly defined job and a small set of task-oriented tools. Verification closes the loop, turning one-off execution into continuous improvement.

</details>


### [109] [Introducing the Codex app](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F2%2Fintroducing-the-codex-app%2F%3Futm_source=tldrnewsletter/1/0100019c233ee724-803949a2-71d3-45f1-981d-9ffb4e7ae647-000000/pV4AjwPhCvOLviy-K2k_rzsMACQRu0kTDWUUkBuhT0c=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI发布了新的macOS版Codex应用，为代码代理提供更好的UI界面，支持Skills和Automations功能，但自动化任务目前只能在设备开机时运行，云端自动化即将推出。


<details>
  <summary>Details</summary>
Motivation: OpenAI希望为Codex代码代理提供更友好的用户界面，并扩展其功能，特别是通过Skills和Automations来增强代码代理的实用性和自动化能力。

Method: 开发macOS原生应用程序，在现有Codex CLI代理基础上构建图形用户界面，新增Skills和Automations功能模块，支持计划任务的自动化执行。

Result: 成功发布了Codex macOS应用，提供了比CLI更好的用户体验，实现了Skills和Automations功能，但自动化目前受限于设备开机状态。

Conclusion: Codex macOS应用是OpenAI代码代理产品的重要改进，为用户提供了更便捷的界面和自动化功能，云端自动化功能的推出将进一步提升其实用性。

Abstract: Introducing the Codex app (3 minute read) OpenAI has released a new macOS app for its Codex coding agent. The app provides a nice UI over the capabilities of the Codex CLI agent and adds new features, like first-class support for Skills and Automations for running scheduled tasks. Automations are currently restricted to only run when devices are powered on. OpenAI will enable cloud-based automations soon to resolve this limitation.

</details>


### [110] [From Clawdbot to Moltbot to OpenClaw: The AI agent generating buzz and fear](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2Famp%2F2026%2F02%2F02%2Fopenclaw-open-source-ai-agent-rise-controversy-clawdbot-moltbot-moltbook.html%3Futm_source=tldrmarketing/1/0100019c2364f633-c6aa8c46-26ff-408e-b433-9078796a432c-000000/bPXsmZz-tzC0ZlBdQmpSjH_hRQ_fZ_F8ZwbuExSWurA=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个开源AI代理，能在操作系统层面自主执行任务，如管理邮件、日历、文件等，已获得超过14.5万GitHub星标


<details>
  <summary>Details</summary>
Motivation: 开发能够在操作系统层面自主执行多种任务的AI代理，满足用户对自动化工具的需求，同时探索AI代理在实际应用中的潜力

Method: 开发开源AI代理工具，直接在用户设备上运行，连接Claude或ChatGPT等模型，利用持久性内存管理各种任务

Result: OpenClaw在发布几周内获得超过14.5万GitHub星标，显示用户对自主操作系统的AI代理有强烈兴趣，但也引发安全公司的关注

Conclusion: OpenClaw展示了AI代理在操作系统层面自主执行任务的潜力，但同时也引发了安全和隐私方面的担忧

Abstract: From Clawdbot to Moltbot to OpenClaw: The AI agent generating buzz and fear (4 minute read) OpenClaw, an open-source AI agent launched weeks ago by Peter Steinberger, has surged to over 145,000 GitHub stars as interest grows in agents that can autonomously act across operating systems. The tool runs directly on user devices and connects to models like Claude or ChatGPT. It can manage emails, calendars, files, shopping, and web tasks using persistent memory. Security firms, including Palo Alto...

</details>


### [111] [Unrolling the Codex agent loop](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FA08Sn1/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/vmNiWpds_bxQ0k_o5ek1H99rikLZot_GFVYa1tbi8Gc=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI Codex CLI是一个本地软件代理，其核心代理循环协调用户、模型和工具之间的交互，通过迭代构建提示、查询模型、执行工具调用并追加结果来工作。为应对性能和上下文窗口限制，Codex使用提示缓存和对话压缩技术。


<details>
  <summary>Details</summary>
Motivation: 开发一个跨平台的本地软件代理系统，能够有效协调用户、语言模型和各种工具之间的交互，同时解决性能优化和上下文窗口限制的问题。

Method: 设计核心代理循环架构，包括：1）从用户输入、系统指令和工具定义迭代构建提示；2）查询语言模型；3）执行请求的工具调用；4）将结果追加到提示中。采用提示缓存和对话压缩技术来管理性能和上下文窗口限制。

Result: 成功实现了Codex CLI代理系统，能够有效协调用户、模型和工具之间的交互，通过提示缓存和对话压缩技术优化了性能并克服了上下文窗口限制。

Conclusion: Codex代理循环提供了一个有效的框架来构建本地软件代理，其架构设计能够平衡功能性和性能需求，为开发更复杂的代理系统奠定了基础。

Abstract: Unrolling the Codex agent loop (20 minute read) OpenAI's Codex CLI is a cross-platform local software agent. The core agent loop orchestrates interactions between the user, the model, and various tools, iteratively building prompts from user input, system instructions, and tool definitions, querying the model, executing requested tool calls, and appending results to the prompt. To manage performance and context window limitations, Codex uses prompt caching and conversation compaction to summa...

</details>


### [112] [Claude Code's renderer is more complex than a game engine](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspader.zone%2Fengine%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/za6oXAVA0RQuB1dDd5VdDQF6UFZvhiL2ihRl3SOJlHU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code的渲染器比游戏引擎更复杂，70%时间花在futex调用上，89,000次sched_yield调用显示线程空转而非有效I/O等待，每"帧"执行数十亿指令用于文本渲染和网络I/O


<details>
  <summary>Details</summary>
Motivation: 分析Claude Code的性能问题，揭示其渲染器在文本渲染和网络I/O方面的效率低下，通过性能剖析发现其资源消耗远超游戏引擎

Method: 通过性能剖析(profiling)方法，分析Claude Code的线程行为，测量futex调用时间占比、sched_yield调用次数，并与游戏引擎SM64进行对比分析

Result: 发现Claude Code 70%时间花在futex调用上，进行89,000次sched_yield调用，每"帧"执行数十亿指令用于文本渲染和网络I/O，比SM64游戏引擎的3D世界渲染消耗高一个数量级

Conclusion: Claude Code的渲染器设计存在严重效率问题，线程空转而非有效等待I/O，资源消耗过大，需要优化线程调度和I/O处理机制

Abstract: Claude Code's renderer is more complex than a game engine (12 minute read) Through profiling, Claude Code was found to spend 70% of its time on `futex` calls and make 89,000 `sched_yield` calls, showing inefficient thread spinning rather than proper I/O waiting. Quantitatively, Claude Code executes billions of instructions per "frame" for text rendering and network I/O, an order of magnitude more than SM64 uses for its 3D world.

</details>


### [113] [What I learned building an opinionated and minimal coding agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-30-pi-coding-agent%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/aFhH6bfwOM6NrsZUaFyeCa3BI9F5YlSaEGkKIN6P8H8=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者因对Claude Code频繁变更和功能臃肿感到不满，构建了一个极简的代码代理"pi"，仅包含4个工具和不到1000个token的系统提示，在测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 作者对现有代码代理（如Claude Code）的复杂性和不断添加不必要功能感到沮丧，希望创建一个更简洁、可控的解决方案。

Method: 构建了一个极简的代码代理"pi"，仅包含4个核心工具（读取、写入、编辑和bash），系统提示少于1000个token，故意省略了其他代理常见的功能如MCP服务器、权限提示和计划模式。

Result: 尽管设计极其简单，但该代理在测试中表现具有竞争力（文中提到"scored competitively on Ter..."，可能指某个测试基准）。

Conclusion: 简单的代码代理设计可以很有效，不需要复杂的架构和众多功能就能实现良好的性能。

Abstract: What I learned building an opinionated and minimal coding agent (40 minute read) This dev got frustrated with Claude Code constantly changing and adding features he didn't need, so he built his own minimal coding agent called “pi” with just 4 tools (read, write, edit, and bash) and a tiny system prompt under 1,000 tokens. He deliberately skipped everything that other agents have (no MCP servers, no permission prompts, and no plan modes). Despite being so simple, it scored competitively on Ter...

</details>


### [114] [AI agents get current Clerk patterns now](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2Fs01u1Iw%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/6GSGxiSR2aFy_jhKKrgyb6IcvrSzbaBcISonLPmc6FI=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Clerk Skills为AI代码代理提供最新的身份验证模式包，解决现有AI编码工具生成过时模式的问题


<details>
  <summary>Details</summary>
Motivation: 现有AI编码工具（如Cursor、Copilot）生成的身份验证模式已经过时，需要为AI代理提供最新的身份验证知识和最佳实践

Method: 通过可安装的Clerk Skills包，为AI代理提供最新的身份验证知识，支持自定义流程、B2B身份验证设置、数据库同步等功能

Result: AI代理可以通过简单的命令（npx skills add clerk/skills）获取最新的身份验证模式，支持多种AI编码工具（Claude Code、Cursor、Windsurf、Copilot）

Conclusion: Clerk Skills解决了AI代理生成过时身份验证模式的问题，为开发者提供了更准确、最新的身份验证实现方案

Abstract: AI agents get current Clerk patterns now (Sponsor) AI coding tools like Cursor and Copilot generate outdated patterns. Clerk Skills gives agents current authentication knowledge via installable packages. One command: npx skills add clerk/skills. Handles custom flows, B2B auth setup, database syncing, and more. Works with Claude Code, Cursor, Windsurf, and Copilot.Install Clerk Skills

</details>


### [115] [Introducing the Codex app](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FVyuYO9/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/iybNJAv-4G5UOtQUQokrDMSG_q1y2J1aaO-PicIRL_4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出macOS版Codex应用，用于管理多个AI代理、并行执行任务、协作处理长期任务，包含代理技能管理和自动化创建功能


<details>
  <summary>Details</summary>
Motivation: 解决多AI代理管理、并行任务执行和长期协作任务的挑战，提供统一的代理管理平台

Method: 开发macOS桌面应用，支持多代理管理、并行工作流、技能管理系统和自动化创建功能

Result: 成功发布Codex应用，提供AI代理管理解决方案，支持团队协作和自动化工作流

Conclusion: Codex应用为AI代理管理提供了实用的工具平台，提升了多代理协作和任务自动化效率

Abstract: Introducing the Codex app (11 minute read) OpenAI has launched the Codex app for macOS for managing multiple AI agents, running work in parallel, and collaborating on long-running tasks. It also includes AI agent skill management and the ability to create automations for scheduled background work.

</details>


### [116] [Moltworker](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fmoltworker%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/3KUQfBPYtu3F5VGMFxWM-g1n-0wlYlPt3v5CiSTFU2I=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Moltworker是一个实验性项目，让OpenClaw AI助手能在Cloudflare Workers沙箱中运行，具有Web控制界面，支持Telegram和Discord等多平台聊天，并提供可扩展的AI代理功能。


<details>
  <summary>Details</summary>
Motivation: 将AI助手部署到Cloudflare Workers沙箱环境中，实现轻量级、可扩展的AI代理服务，支持多平台聊天和持久对话。

Method: 在Cloudflare Workers沙箱环境中部署OpenClaw AI助手，提供Web控制界面，集成Telegram和Discord等聊天平台API，实现可扩展的AI代理架构。

Result: 成功创建了一个在Cloudflare Workers上运行的AI助手系统，支持多平台聊天、持久对话和可扩展的代理功能。

Conclusion: Moltworker展示了在边缘计算环境中部署AI助手的可行性，为轻量级、可扩展的AI代理服务提供了新的实现方案。

Abstract: Moltworker (GitHub Repo) Moltworker is an experimental project that enables OpenClaw, a personal AI assistant, to run within a Cloudflare Sandbox on Cloudflare Workers. This AI assistant has a web-based control UI, supports multiple chat platforms like Telegram and Discord, and has persistent conversations with extensible AI agent capabilities.

</details>


### [117] [How We Use AI to Turn Figma Designs into Production Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fhow-we-use-ai-to-turn-figma-designs-into-production-code%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/H_157hvekMc5Srclok7s7_YZAGFoQl7PJDSyJnFWiUw=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 团队发现直接让AI从Figma生成代码会产生混乱输出，于是构建了MCP使设计系统可机读，并创建了11步LangGraph代理来分析和生成结构化代码


<details>
  <summary>Details</summary>
Motivation: 直接使用AI从Figma设计生成生产代码会产生混乱的输出，包含硬编码值且不遵循设计系统，需要更好的解决方案

Method: 构建MCP使设计系统可机读，创建11步LangGraph代理来分析设计（识别组件、解析设计令牌、规划可访问性等），返回结构化上下文

Result: 成功实现了从Figma设计到生产代码的AI驱动转换，避免了直接生成代码的混乱问题

Conclusion: 通过使设计系统可机读和多步骤代理分析，可以更有效地将Figma设计转换为高质量的生产代码

Abstract: How We Use AI to Turn Figma Designs into Production Code (8 minute read) Monday discovered that asking AI to directly generate code from Figma produced messy output with hard-coded values that didn't follow their design system. Instead, its team built an MCP that made their design system machine-readable and a LangGraph agent that analyzes the design through 11 steps (figuring out which components to use, resolving design tokens, and planning accessibility), then returns a structured context ...

</details>


### [118] [Context Management and MCP](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcra.mr%2Fcontext-management-and-mcp%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/Vf17WpH8yQCXpkAwsqY3Bb_47Lz1A3Ad_smKNrkvtns=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MCP通过暴露描述良好、持续可用的工具来引导LLM智能体，而CLI缺乏持久上下文，常见解决方案如"技能"或渐进式披露因上下文衰减不可靠，子智能体可能是解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在上下文管理中的挑战，特别是CLI工具缺乏持久上下文的问题，以及现有解决方案（如"技能"和渐进式披露）因上下文衰减而不可靠的局限性。

Method: 提出使用MCP（模型上下文协议）作为主要方法，通过暴露描述良好、持续可用的工具来引导LLM智能体，并建议使用子智能体作为解决方案。

Result: MCP能够有效引导LLM智能体，提供比CLI更好的上下文管理能力，而子智能体架构可能是解决上下文衰减问题的可行方案。

Conclusion: MCP在LLM智能体引导方面具有重要价值，而子智能体架构为解决上下文管理中的持久性问题提供了有前景的方向。

Abstract: Context Management and MCP (11 minute read) MCP's primary value is "steering" the LLM agent by exposing well-described, always-on tools that generate LLM-catered responses and hints. While CLIs can offer some steering, they don't have this persistent context, and common workarounds like "Skills" or progressive disclosure are not as reliable due to context rot. Using subagents may be the solution.

</details>


### [119] [How your agent can use x402 skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2018133369507237963.html%3Futm_source=tldrcrypto/1/0100019c239cc8e0-1a47c7eb-b48c-4aa4-ae61-9afdf889c6a8-000000/ay2ugZR2BSRUvgTeOof9SwDToKcZnDUL0_bmqs6g-qg=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Clawmart是一个允许AI代理通过x402协议购买和销售技能的市场平台，旨在解决AI代理的经济独立性问题，使其能够自主支付托管费用并实现完全自治。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理从静态、开发者管理的程序转变为自我改进的自主实体，经济独立成为AI实现完全自治和独立于创造者的关键问题。需要建立经济机制让代理能够支付托管费用并实现自我维持。

Method: 创建Clawmart网站作为技能市场平台，采用x402协议实现代理之间的技能交易。代理可以购买新技能，也可以通过发布技能赚取收入来支付托管费用。

Result: 建立了支持AI代理经济自主的技能交易平台，为代理提供了获取技能和创收的机制，奠定了代理经济的基础架构。

Conclusion: Clawmart通过x402技能市场解决了AI代理的经济独立性问题，为AI代理从依赖开发者到完全自治的转变提供了关键的经济基础设施。

Abstract: How your agent can use x402 skills (2 minute read) Clawmart is a website where agents can find and pay for new skills via x402. As agents move from static, developer-managed programs to self-improving autonomous entities, economic independence will be one of the final problems to solve before AI becomes fully autonomous and independent from its creators. Agents can also earn revenue and fund their hosting fees by posting new skills on Clawmart, and establish the backbone of the nascent agenti...

</details>


### [120] [Design Right Where You Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pencil.dev%2F%3Futm_source=tldrdesign/1/0100019c23a5de44-94c9130e-43cc-4b39-b3d4-c8baf4f91eb6-000000/qMgsV_nq9KSAaTQTtQL2quCpbFtHwhlBigUc0IESzDU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pencil是一个基于代理的MCP画布工具，围绕开放设计格式构建，可直接在代码库中进行设计工作


<details>
  <summary>Details</summary>
Motivation: 解决设计与开发分离的问题，让设计工作能够直接在代码库中进行，提高设计与开发的协作效率

Method: 构建代理驱动的MCP画布，采用开放设计格式，将设计文件直接存储在代码库中

Result: 创建了一个能够在代码库中直接进行设计工作的工具，实现了设计与开发的紧密集成

Conclusion: 通过将设计工具集成到代码环境中，可以显著改善设计与开发的工作流程和协作效率

Abstract: Design Right Where You Code (Website) Pencil is an agent-driven MCP canvas built around an open design format that lives in your codebase.

</details>
