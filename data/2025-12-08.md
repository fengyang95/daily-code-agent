<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [tldr.article](#tldr.article) [Total: 3]
- [cs.LG](#cs.LG) [Total: 6]
- [wechat.article](#wechat.article) [Total: 17]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment](https://arxiv.org/abs/2512.05464)
*Panatchakorn Anantaprayoon,Nataliia Babina,Jad Tarifi,Nima Asgharbeygi*

Main category: cs.CL

TL;DR: 提出Collective Agency作为更全面的对齐价值，并开发Dynamic Alignment框架，通过LLM自动生成训练数据和自我奖励机制实现可扩展的自我对齐。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类偏好的对齐方法在AGI/ASI时代可能不足，且资源密集难以扩展。现有AI反馈的自改进方法仍局限于传统对齐价值观，需要更全面的对齐目标和可扩展方法。

Method: 提出Collective Agency作为统一开放的对齐价值，强调综合智能能力。开发Dynamic Alignment框架：1) 用LLM自动生成训练数据集；2) 自我奖励机制，策略模型评估自身输出候选并为GRPO学习分配奖励。

Result: 实验结果表明，该方法成功将模型对齐到Collective Agency，同时保持一般的NLP能力。

Conclusion: Collective Agency和Dynamic Alignment框架为超越传统对齐规范提供了可行方案，实现了更全面的对齐目标和可扩展的自我改进方法。

Abstract: Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: 本文对AI生成代码中的缺陷进行了系统性分析，总结了错误类型、分布、修复策略及其与不同模型的关联，为未来模型改进和质量评估提供参考。


<details>
  <summary>Details</summary>
Motivation: AI代码生成模型被广泛使用以提高开发效率，但生成的代码存在质量问题。这些模型基于包含缺陷的公开代码训练，导致生成的代码可能包含错误，给开发过程的信任和维护带来挑战。现有研究对AI生成代码缺陷的分析较为分散，缺乏系统性总结，需要全面综述来揭示错误类型、分布、修复策略及其与特定模型的关联。

Method: 通过系统性地分析现有AI生成代码文献，建立对生成代码中缺陷的整体理解。研究包括理解AI生成代码中缺陷的性质和程度，对不同模型生成的代码中的缺陷类型和模式进行分类，并讨论消除生成代码中缺陷的可能修复和缓解策略。

Result: 提供了AI生成代码中缺陷的全面分类和模式分析，揭示了不同模型生成的代码中缺陷的分布特征，总结了有效的修复和缓解策略，并建立了缺陷类型与特定模型之间的关联。

Conclusion: 本研究为AI代码生成模型的改进和质量评估提供了系统性的参考框架，有助于提高AI生成代码的可靠性和实用性，促进AI辅助编程工具的健康发展。

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [3] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 该研究探讨了在软件工程教育中整合生成式AI工具的方法，通过在大学游戏开发项目中开展用户研究，分析学生使用AI工具的情况，并开发了基于RAG的本地化LLM助手来提供项目上下文支持。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的快速发展，软件工程教育需要适应变革，确保学生不仅掌握传统开发方法，还能有意义且负责任地使用新技术。项目制课程为探索AI辅助工具在实际开发实践中的整合提供了有效环境。

Method: 研究在大学编程项目中开展用户研究，学生合作开发电脑游戏。研究调查了参与者在软件开发不同阶段如何使用生成式AI工具，识别了这些工具最有效的任务类型，并分析了学生遇到的挑战。此外，研究还开发了一个基于RAG的本地化LLM助手，通过检索增强生成技术将回答基于相关文档和源代码。

Result: 研究深入了解了学生在软件项目中使用AI工具的模式，识别了AI辅助最有效的任务类型和常见挑战。对基于RAG的本地化LLM助手进行了定性分析，包括模型行为、参数敏感性和常见失败模式。

Conclusion: 研究结果加深了对教育软件项目中上下文感知AI支持的理解，为未来将基于AI的辅助工具整合到软件工程课程中提供了信息。项目制课程是探索AI工具整合的有效环境，本地化、上下文感知的AI助手能够提供更有针对性的支持。

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [4] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 研究比较了软件工程师在LLM辅助代码审查与人工同行审查中的情感调节、行为参与和反馈采纳过程，发现LLM辅助审查降低了情感成本，将参与重点从情绪管理转向认知负荷管理。


<details>
  <summary>Details</summary>
Motivation: 代码审查是一项社会技术实践，但人们对软件工程师在LLM辅助代码审查与人工同行审查中的参与方式差异了解不足。需要理解工程师在两种审查方式下的情感反应、参与决策和反馈采纳过程。

Method: 采用两阶段定性研究：第一阶段，20名软件工程师进行同行审查并接受访谈，了解情感反应和参与决策；第二阶段，引入符合工程师偏好的新提示，探究特征如何影响他们的反应。开发了连接情感自我调节、行为参与和解决方案的综合模型。

Result: 识别了工程师应对负面反馈的情感调节策略：重构、对话调节、回避和防御。参与通过社会校准进行，工程师根据关系氛围和团队规范调整反应和行为。在同行审查中，解决方案轨迹因焦点（个人/双人/团队）和内部意义建构过程而异。LLM辅助审查降低了情感成本和自我调节需求，当LLM反馈符合工程师认知期望时，参与者报告处理努力减少，采纳倾向可能更高。

Conclusion: LLM辅助审查将参与重点从情绪管理转向认知负荷管理。AI最适合作为支持性伙伴，减少认知和情感负荷，同时保留人类责任和同行审查等社会技术活动的社会意义。提出了连接情感自我调节、行为参与和解决方案的综合参与模型。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [5] [WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp](https://arxiv.org/abs/2512.05314)
*Ke Mao,Timotej Kapus,Cons T Åhs,Matteo Marescotti,Daniel Ip,Ákos Hajdu,Sopot Cela,Aparup Banerjee*

Main category: cs.SE

TL;DR: WhatsCode是WhatsApp的领域特定AI开发系统，在25个月的工业部署中，从隐私自动化扩展到自主代理工作流，显著提升了隐私验证覆盖率，生成了数千个代码变更，并识别出两种稳定的人机协作模式。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助开发工具在工业环境中日益普及，但合规相关、大规模工业环境中的部署在学术文献中存在显著空白。本研究旨在填补这一空白，通过WhatsCode在WhatsApp（服务超过20亿用户）的实际部署经验，为大规模AI工具在合规环境中的部署提供实证指导。

Method: WhatsCode是一个领域特定的AI开发系统，支持WhatsApp并处理数百万行跨平台代码。系统从目标隐私自动化演变为自主代理工作流，与端到端功能开发和DevOps流程集成。研究基于25个月（2023-2025）的工业部署数据，分析系统性能和人机协作模式。

Result: WhatsCode取得了显著成效：自动化隐私验证覆盖率从15%提升至53%（3.5倍增长），识别隐私需求，生成了3000多个被接受的代码变更（接受率9%-100%），提交了692个自动化重构/修复变更、711个框架采用、141个功能开发辅助，并在bug分类中保持86%的精确度。研究识别出两种稳定的人机协作模式：一键部署（60%高置信度变更）和接管-修订（40%复杂决策）。

Conclusion: 组织因素（如所有权模型、采用动态、风险管理）与技术能力同样重要，对大规模AI成功具有决定性影响。有效的人机协作（而非完全自动化）是驱动可持续业务影响的关键。研究为合规相关环境中大规模AI工具部署提供了基于证据的指导。

Abstract: The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes.
  WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.

</details>


### [6] [Everything is Context: Agentic File System Abstraction for Context Engineering](https://arxiv.org/abs/2512.05470)
*Xiwei Xu,Robert Mao,Quan Bai,Xuewu Gu,Yechao Li,Liming Zhu*

Main category: cs.SE

TL;DR: 提出基于文件系统抽象的统一上下文工程框架，解决GenAI系统中上下文管理碎片化问题，实现可追溯、可验证的AI协作系统


<details>
  <summary>Details</summary>
Motivation: 生成式AI重塑软件系统设计，当前面临的核心挑战从模型微调转向上下文工程。现有实践如提示工程、RAG和工具集成等碎片化，产生临时性产物，限制了可追溯性和可问责性

Method: 提出受Unix"万物皆文件"启发的文件系统抽象，为异构上下文产物提供持久化、受治理的基础设施。在AIGNE框架中实现可验证的上下文工程管道，包含上下文构造器、加载器和评估器

Result: 在AIGNE框架中实现该架构，通过两个示例展示：具有记忆的代理和基于MCP的GitHub助手。展示了如何在开发和工业环境中操作化该架构

Conclusion: 该架构为可问责、以人为中心的AI协作建立了可重复使用的基础，支持可验证、可维护且工业就绪的GenAI系统

Abstract: Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.

</details>


### [7] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: iEcoreGen是一个混合方法，结合了EMF模板代码生成和LLM代码补全，通过分解需求、生成初始代码、添加文档注释，再让LLM补全未实现方法，在代码生成任务中表现优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 模板代码生成方法能保证正确性但不够灵活，LLM方法灵活但可能产生错误代码。需要结合两者优势，实现既可靠又灵活的代码生成。

Method: 1. 使用Ecore模型定义系统结构；2. 分解需求生成操作规范；3. 用EMF模板生成初始Java代码；4. 序列化规范为文档注释；5. 调用LLM补全未实现方法。

Result: 在20个代码生成任务和5个LLM上评估，iEcoreGen在pass@k指标上优于纯LLM基线，在compilation@k指标上表现相当。消融研究明确了各组件贡献。

Conclusion: LLM增强的模型驱动开发是实现更高效软件自动化的有前景路径，混合方法结合了模板方法的可靠性和LLM的灵活性。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [8] [Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures](https://arxiv.org/abs/2512.05908)
*Amirkia Rafiei Oskooei,S. Selcan Yukcu,Mehmet Cevheri Bozoglan,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 该论文提出了一种新的bug定位方法，将多仓库微服务架构中的代码库转换为分层自然语言摘要，然后进行NL-to-NL搜索，而不是跨模态检索，显著提升了bug定位性能。


<details>
  <summary>Details</summary>
Motivation: 在多仓库微服务架构中，bug定位面临三大挑战：自然语言bug报告与代码之间的语义鸿沟、LLM上下文限制、以及需要首先识别正确的代码仓库。现有方法如检索增强生成(RAG)系统在处理大规模代码库时效果有限。

Method: 方法分为两个阶段：1) 将代码库转换为分层自然语言摘要（文件级、目录级、仓库级）；2) 两阶段搜索：首先将bug报告路由到相关仓库，然后在选定仓库内进行自上而下的定位（仓库→目录→文件）。

Result: 在包含46个仓库和110万行代码的工业系统DNext上评估，该方法达到Pass@10为0.82，MRR为0.50，显著优于检索基线和GitHub Copilot、Cursor等代理RAG系统。

Conclusion: 研究表明，工程化的自然语言表示比原始源代码在可扩展的bug定位中更有效，提供了可解释的仓库→目录→文件搜索路径，这对于在企业AI工具中建立信任至关重要。

Abstract: Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [9] [Teaching an LLM a Niche Diagraming Language](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.huy.rocks%2Feveryday%2F12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language%3Futm_source=tldrdev/1/0100019aee7ce3c5-54d34d99-2ac7-44c4-a917-617fee933856-000000/5_BZTBhy8bpHO3ueCCo89OXuMU--RhKOXepcjGVzfnk=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Qwen2.5-Coder-7B模型成功训练出能生成和编辑Pintora图表语言的模型，准确率达86%，通过持续预训练和指令微调实现


<details>
  <summary>Details</summary>
Motivation: 训练LLM掌握小众的图表语言Pintora，使其能够理解和生成专业图表，扩展LLM在特定领域的应用能力

Method: 1) 对Qwen2.5-Coder-7B进行Pintora图表的持续预训练；2) 使用AI代理生成的数据进行指令微调；3) 在Google Colab和Runpod上使用有限资源完成训练

Result: 模型在生成和编辑Pintora图表方面达到86%的准确率，成功掌握了这一小众图表语言

Conclusion: 通过适当的训练策略，即使是小众的领域特定语言，LLM也能有效学习并达到实用准确率

Abstract: Teaching an LLM a Niche Diagraming Language (9 minute read) A 7B language model (Qwen2.5-Coder-7B) was successfully trained (with 86% accuracy) to generate and edit diagrams using the less popular Pintora language. The training involved continued pretraining on Pintora diagrams, followed by instruction fine-tuning. Data was generated by an AI agent, cleaned, and used to train the model with limited resources on Google Colab and Runpod.

</details>


### [10] [Building Deep Research: How we Achieved State of the Art](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2FTavily%2Ftavily-deep-research%3Futm_source=tldrdev/1/0100019aee7ce3c5-54d34d99-2ac7-44c4-a917-617fee933856-000000/JHy0tqhOTXV0N9ccct6ryRfPDZul-3rfVQM2cYS_AhU=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Tavily通过重构研究代理，将工具输出提炼为紧凑反思而非全部传递，减少66%令牌使用同时达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有研究代理系统通常将所有工具输出传递给代理循环，导致令牌使用效率低下，且在新模型出现时假设失效

Method: 重新设计研究代理架构，将工具输出提炼为紧凑的反思摘要，仅在最终生成阶段加载原始来源，而非将所有输出传递到代理循环

Result: 令牌使用减少66%，同时在基准测试中达到最先进水平（SOTA）

Conclusion: 通过精简架构和选择性信息传递，可以在显著降低计算成本的同时实现卓越的研究代理性能

Abstract: Building Deep Research: How we Achieved State of the Art (8 minute read) Tavily rebuilt its research agent from scratch after overengineering the first version with assumptions that broke when new models arrived. Instead of passing all tool outputs through the agent loop like most systems do, it distilled outputs into compact reflections and only loaded raw sources for final generation. This cut token usage by 66% while hitting SOTA on benchmarks.

</details>


### [11] [Evaluating AI Agents in Security Operations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcotool.ai%2Fblog%2Fevaluating-gpt-5-1-claude-opus-4-5-and-gemini-3-pro-ai-agents-in-security-operations%3Futm_source=tldrinfosec/1/0100019aeed800f8-e7d24dd5-f51a-4c75-b5d4-1dd422a5ef94-000000/eNirV3DdBVRiCy6g5DJD_Gi6gcsC7u54vMbQwyUhVfM=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cotool测试AI代理在安全运维任务中的表现，使用Splunk BOTSv3数据集和Claude、OpenAI、Gemini的前沿模型。GPT-5.1和Opus 4.5准确率最高达63%，但Opus成本是GPT-5.1的3倍，执行时间却快一半。Gemini模型表现显著较差。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在安全运维任务中的实际能力，比较不同前沿模型在解决安全操作任务时的性能、成本和效率差异。

Method: 使用Splunk BOTSv3数据集测试AI代理，评估Claude、OpenAI和Gemini的前沿模型在安全运维任务中的表现，包括准确率、成本和执行时间等指标。

Result: GPT-5.1和Opus 4.5准确率最高（63%），Opus成本是GPT-5.1的3倍但执行时间快一半，Gemini模型表现显著较差且未能完成多个任务。

Conclusion: 不同AI模型在安全运维任务中表现差异显著，需要在准确率、成本和执行时间之间权衡选择。Opus在速度上有优势但成本高，Gemini在当前任务中表现不佳。

Abstract: Evaluating AI Agents in Security Operations (6 minute read) Cotool tested an AI agent's ability to solve security operations tasks using the Splunk BOTSv3 dataset and frontier models from Claude, OpenAI, and Gemini. Cotool found that while GPT-5.1 and Opus 4.5 achieved the top accuracy at 63%, Opus was 3x more expensive than GPT-5.1 but also completed tasks in half the wall-clock time of any other model. The Gemini models performed notably poorly on the tasks and failed to complete several ta...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，通过RKHS-SHAP状态归因增强Actor-Critic方法，实现可解释性、效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic方法缺乏可解释性，现有可解释RL方法很少利用状态归因辅助训练，且忽视不同状态维度对奖励的异质性影响

Method: 提出RSA2C算法：1) Actor在向量值RKHS中实现，使用Mahalanobis加权算子值核；2) Value Critic和Advantage Critic在标量RKHS中；3) 通过RKHS-SHAP计算状态归因，转换为Mahalanobis门控权重，调节Actor梯度和Advantage Critic目标；4) 使用稀疏化字典

Result: 在三个标准连续控制环境中验证了算法在效率、稳定性和可解释性方面的优势

Conclusion: RSA2C算法通过状态归因增强Actor-Critic方法，在理论分析和实验验证中均表现出优越性能

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [13] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 该论文研究了区分人类与LLM生成的科学想法的挑战，发现现有模型在连续改写后检测性能下降25.4%，但加入研究问题作为上下文可提升检测性能2.97%，且简化非专家风格的改写最影响检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在研究领域的广泛应用，区分LLM与人类生成的科学想法变得至关重要，但目前这一领域尚未被充分探索。现有研究主要集中在检测LLM生成的文本，但科学想法的来源识别仍是一个未解决的问题。

Method: 系统评估了最先进的机器学习模型区分人类与LLM生成想法的能力，特别关注连续改写阶段后的检测效果。研究了将研究问题作为上下文信息对检测性能的影响，并分析了不同改写风格对检测难度的影响。

Result: 1) 经过5次连续改写后，检测性能平均下降25.4%；2) 加入研究问题作为上下文信息可将检测性能提升最多2.97%；3) 检测算法在想法被改写成简化非专家风格时表现最差，这是导致LLM特征消失的主要原因。

Conclusion: 区分人类与LLM生成的科学想法具有挑战性，特别是在多次改写后。上下文信息有助于提升检测性能，但简化改写会显著削弱LLM的可识别特征。这为理解LLM的研究认知能力提供了重要见解。

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [14] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出针对LLM硬件代码生成的遗忘框架，结合语法保持遗忘策略和细粒度选择性损失，有效移除问题知识而不影响代码生成能力


<details>
  <summary>Details</summary>
Motivation: LLM在硬件设计自动化中展现潜力，但存在可靠性问题：可能记忆专有IP、污染基准测试、包含不安全编码模式，需要解决这些风险

Method: 1) 语法保持遗忘策略：在遗忘过程中保护硬件代码的结构完整性；2) 细粒度floor-aware选择性损失：实现精确高效的问题知识移除

Result: 支持3倍大的遗忘集，通常只需单个训练周期，同时保持RTL代码的语法正确性和功能完整性

Conclusion: 为可靠的LLM辅助硬件设计开辟了新途径，实现了有效遗忘而不降低代码生成能力

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [15] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编码作为低代码/无代码平台的替代方案，让非程序员通过自然语言与AI助手交互来开发Web应用，在案例研究中被证明是可行的。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编码是否可以作为低代码/无代码平台的补充或替代方案，为组织数字化转型提供更灵活、适用范围更广、开发更快、可重用性更好且减少供应商锁定的终端用户开发方式。

Method: 通过案例研究，让非程序员参与者与AI助手交互来开发基本的Web应用，分析任务完成情况和参与者反馈。

Result: 大多数研究参与者在合理时间内成功完成了任务，并支持AI辅助的终端用户编码作为可行的终端用户开发方法。

Conclusion: AI辅助的终端用户编码是可行的终端用户开发范式，可能在未来补充甚至替代传统的低代码/无代码平台模型。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [16] [GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop](https://arxiv.org/abs/2512.05502)
*Omid Bazgir,Vineeth Manthapuri,Ilia Rattsev,Mohammad Jafarnejad*

Main category: cs.LG

TL;DR: GRASP是一个多智能体、图推理框架，通过将定量系统药理学模型编码为类型化生物知识图谱并编译为可执行代码，显著提升了QSP建模的效率和可访问性。


<details>
  <summary>Details</summary>
Motivation: 定量系统药理学建模对药物开发至关重要，但传统方法需要大量时间投入，限制了领域专家的生产力。需要一种方法使QSP模型开发既易于访问又保持生物医学严谨性。

Method: GRASP采用多智能体、图推理框架，将QSP模型编码为类型化生物知识图谱，编译为MATLAB/SimBiology代码。采用两阶段工作流：理解阶段（图重建遗留代码）和行动阶段（约束检查的语言驱动修改），通过状态机进行迭代验证。

Result: 在LLM作为评判的对比评估中，GRASP在生物合理性、数学正确性、结构保真度和代码质量方面显著优于专家引导的思维链和思维树基线（约9-10/10 vs. 5-7/10）。广度优先参数对齐在依赖发现、单位和范围方面达到F1=0.95。

Conclusion: 图结构化的智能体工作流可以使QSP模型开发既易于访问又保持严谨，使领域专家能够用自然语言指定机制而不牺牲生物医学保真度。

Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.

</details>


### [17] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 本文提出一种基于α-散度的强化学习方法，通过显式目标分布控制精度-多样性权衡，在Lean定理证明基准上实现覆盖精度帕累托前沿的最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法调优LLMs时会导致多样性显著下降，这是因为RL隐式优化了"模式寻求"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案同时保留正确答案的相对概率），使用α-散度族近似该目标分布，通过插值模式寻求和质量覆盖散度来直接控制精度-多样性权衡。

Result: 在Lean定理证明基准上，该方法在覆盖精度帕累托前沿上实现了最先进的性能，在覆盖轴上优于所有先前方法。

Conclusion: 通过α-散度族显式控制精度-多样性权衡的方法，能够有效解决RL调优LLMs时的多样性损失问题，在定理证明等推理任务中表现优异。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [18] [NeurIPS最佳论文（3）：1000层<em class="highlight">强化学习</em>网络](http://mp.weixin.qq.com/s?__biz=MzkwNjY4MzA0Mg==&mid=2247485131&idx=1&sn=813c49be1e5aad42c7f41fef0e11dfe9&chksm=c16c464db5e25d37e34c21168ff6f40f4347a189e13ca0f46de6789bc505c336dd3c0dbde23b#rd)
*沉迷AI的科研姬*

Main category: wechat.article

TL;DR: 本表提供了在多种复杂任务中，不同网络深度对自监督强化学习（Self-Supervised RL）性能影响的详细比较。表中列出了在不同任务中，使用不同深度的网络时，模型在训练过程中的表现，特别是成功率达到目标的次数。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本表提供了在多种复杂任务中，不同网络深度对自监督强化学习（Self-Supervised RL）性能影响的详细比较。表中列出了在不同任务中，使用不同深度的网络时，模型在训练过程中的表现，特别是成功率达到目标的次数。

</details>


### [19] [阿里通义千问团队：如何让大模型<em class="highlight">强化学习</em>训练不崩溃？](http://mp.weixin.qq.com/s?__biz=Mzk4ODUwNDAxNw==&mid=2247485844&idx=2&sn=af3e041b2375689d2b9712ddc7f0e147&chksm=c45f34fa4ec948cfbb5c5ee6d9ba07a0cd10d46b05509b96f8ce20ff095526e7043e3bf73df3#rd)
*MoonAGI*

Main category: wechat.article

TL;DR: 其实，在它们背后有一个关键的训练方法——强化学习（RL）。但问题来了：训练大模型的强化学习过程经常会"崩溃"，就像玩游戏时突然死机一样，前功尽弃。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 其实，在它们背后有一个关键的训练方法——强化学习（RL）。但问题来了：训练大模型的强化学习过程经常会"崩溃"，就像玩游戏时突然死机一样，前功尽弃。

</details>


### [20] [SAPO：让<em class="highlight">强化学习</em>告别“硬剪切”](http://mp.weixin.qq.com/s?__biz=Mzk0ODg4NDI5NA==&mid=2247486577&idx=1&sn=366391b915d1f44068e6a0a1cede4dd1&chksm=c243258b4c882f1c1cce4aef8ce79b12c4255b092ca7167c1b40430435b6d070dd229fb13b0c#rd)
*通义千问Qwen*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）已经成为提升大语言模型（Large Language Models， LLM）推理能力的核心技术之一。现代 RL 训练流程使模型能够解决困难的数学问题、编写复杂代码和进行多模态推理。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）已经成为提升大语言模型（Large Language Models， LLM）推理能力的核心技术之一。现代 RL 训练流程使模型能够解决困难的数学问题、编写复杂代码和进行多模态推理。

</details>


### [21] [清华大学+南洋理工 VLA-RL：通过可扩展的<em class="highlight">强化学习</em>实现精通和通用的机器人操作](http://mp.weixin.qq.com/s?__biz=MzU5NDgwNDM4NA==&mid=2247516140&idx=1&sn=7db5a4ffc9b4bc7659a5cef58d370280&chksm=ffc15d4216669d4c03662c75dcf3769f02cf6812a510c089162a2d686ac8d56de02e4017ffd4#rd)
*一只佳佳怪*

Main category: wechat.article

TL;DR: 因此，强化学习已成为一种有前景的范式，通过在无限制状态覆盖的在线收集数据上进行培训，实现测试时间扩展的改进。人们很自然地会问：我们能否在自由落体机器人操作中实现类似的基于RL的测试时间缩放效益？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 因此，强化学习已成为一种有前景的范式，通过在无限制状态覆盖的在线收集数据上进行培训，实现测试时间扩展的改进。人们很自然地会问：我们能否在自由落体机器人操作中实现类似的基于RL的测试时间缩放效益？

</details>


### [22] [优文推介 | 基于模仿学习的深度<em class="highlight">强化学习</em>训练数据推断攻击](http://mp.weixin.qq.com/s?__biz=MzI1MjAyMTg1Ng==&mid=2650471785&idx=1&sn=993836d112927d64d4320ae1d0bd6730&chksm=f06c8084c1a1c9254bfdd02f4440785c11831e8fb30c93b66003e8cdd511c4055b76367e3e06#rd)
*网络与信息安全学报*

Main category: wechat.article

TL;DR: 深度强化学习是一种结合深度学习与强化学习的先进机器学习方法。它利用深度神经网络处理复杂的输入数据（如图像或视频），使得智能体能够在高维环境中学习决策。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 深度强化学习是一种结合深度学习与强化学习的先进机器学习方法。它利用深度神经网络处理复杂的输入数据（如图像或视频），使得智能体能够在高维环境中学习决策。

</details>


### [23] [LLM<em class="highlight">强化学习</em>不稳定之谜，被Qwen团队从「一阶近似」视角解开](http://mp.weixin.qq.com/s?__biz=MzAxNTc4MTc1Ng==&mid=2649479654&idx=2&sn=994bdf5c45f9eafd6a7719c7b36aa330&chksm=82b046ed5372c9838159767fbdf460c5ec5e6b6e6f680079eb285ae1cdec89099ace84ce97f4#rd)
*新机器视觉*

Main category: wechat.article

TL;DR: 如今，强化学习（rl）已成为提升大语言模型（llm）复杂推理与解题能力的关键技术范式，而稳定的训练过程对于成功扩展 rl 至关重要。由于语言具有强烈的上下文属性，LLM 的 RL 通常依赖序列级奖励 —— 即根据完整生成序列


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 如今，强化学习（rl）已成为提升大语言模型（llm）复杂推理与解题能力的关键技术范式，而稳定的训练过程对于成功扩展 rl 至关重要。由于语言具有强烈的上下文属性，LLM 的 RL 通常依赖序列级奖励 —— 即根据完整生成序列

</details>


### [24] [北航一篇304页的<em class="highlight">Code</em> <em class="highlight">Agent</em>综述！近30家机构参与（阿里、字节、腾讯登）](http://mp.weixin.qq.com/s?__biz=Mzg4Mjg4NTQxMQ==&mid=2247549448&idx=1&sn=5e6e3294313335b80d4490c09cea367c&chksm=ce50e9215b3d76f9295c8bf241b827f917c99e0f94bb27cff02279a5bc236315c691df1bbce0#rd)
*大模型之心Tech*

Main category: wechat.article

TL;DR: 代码审查：如CodeAgent包含“审查者智能体”，自动检测代码中的漏洞、不符合规范的写法（如未注释的复杂逻辑），甚至能生成修改建议。3. 软件测试：验证“做得对不对”单元测试生成：如ChatUniTest通过“理解函数功能→生成


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 代码审查：如CodeAgent包含“审查者智能体”，自动检测代码中的漏洞、不符合规范的写法（如未注释的复杂逻辑），甚至能生成修改建议。3. 软件测试：验证“做得对不对”单元测试生成：如ChatUniTest通过“理解函数功能→生成

</details>


### [25] [GLM-4.6V，这是我见过的最强「有眼、有手」国产<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzIwNDY1NTU5Mg==&mid=2247496948&idx=1&sn=84f4f83ccc6d9a7eaf873654b9dfc294&chksm=9624090f512136ddb6e1c77d59c696aeb5e83ec087df982bac1acbc6fcedda1feb1e7bd10ae3#rd)
*包包算法笔记*

Main category: wechat.article

TL;DR: 根据这两篇文章的内容，结合图表，对比一下两个模型的异同，并思考和阐述，下一步改进clip模型的思路。answer：评价：对论文的阅读总结基本到位，也能准确提取出图表，并将两篇论文的图表对比分析，并生成新的图文输出


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 根据这两篇文章的内容，结合图表，对比一下两个模型的异同，并思考和阐述，下一步改进clip模型的思路。answer：评价：对论文的阅读总结基本到位，也能准确提取出图表，并将两篇论文的图表对比分析，并生成新的图文输出

</details>


### [26] [AI <em class="highlight">大模型</em>彻底改变游戏！这 4 个应用方向，开发者必看](http://mp.weixin.qq.com/s?__biz=MzIxMTc2NzM4MA==&mid=2247498776&idx=1&sn=12a1f7d08904bfa1281c6a7d951fad0d&chksm=96527769275c02aeaf3824d8b124e3500474c33dc8df12c7ddc924800d69132f63d671b568c1#rd)
*Digital Neural Space*

Main category: wechat.article

TL;DR: 而大模型让 NPC 进入了真正的类智能时代。实现路径根据玩家的提问实时生成对话，创建程序化对话测试工具——自动生成多种对话路径用于 QA做一个可以和玩家对话、给任务、甚至评价玩家行为的角色，记忆玩家行为（好感度


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而大模型让 NPC 进入了真正的类智能时代。实现路径根据玩家的提问实时生成对话，创建程序化对话测试工具——自动生成多种对话路径用于 QA做一个可以和玩家对话、给任务、甚至评价玩家行为的角色，记忆玩家行为（好感度

</details>


### [27] [我国首个通专融合农耕<em class="highlight">大模型</em>发布](http://mp.weixin.qq.com/s?__biz=MzIyMzA4MzEzNA==&mid=2651225251&idx=2&sn=dcf76e9a2e18d6905fed55b98e96de58&chksm=f254c641b125dffd64b166b5618bd1fc48ee23a9e2441c3902c392bc20ba379cb3e31225514a#rd)
*杨凌农高会*

Main category: wechat.article

TL;DR: 该所正式发布农耕大模型1.0。这是我国首个聚焦高标准农田建设监测监管、耕地保护和质量提升的多模态智能模型。该模型由中国工程院院士唐华俊牵头攻关，使用了“通识大模型+领域知识+垂直工具链”的协同智能体架构。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 该所正式发布农耕大模型1.0。这是我国首个聚焦高标准农田建设监测监管、耕地保护和质量提升的多模态智能模型。该模型由中国工程院院士唐华俊牵头攻关，使用了“通识大模型+领域知识+垂直工具链”的协同智能体架构。

</details>


### [28] [又一<em class="highlight">大模型</em>完成国家备案！](http://mp.weixin.qq.com/s?__biz=MzU2ODMwNjIyNQ==&mid=2247995466&idx=1&sn=505eeb530cc97e2dc00457a2ced0ed5d&chksm=fdf8312a9264a02c9c6122e50f2833aeba5546464ab5866b7d8af09db7636b8f95dec2a49f73#rd)
*苏州高铁新城*

Main category: wechat.article

TL;DR: 大模型 “大模型”是人工智能前沿技术，指通过海量数据训练、具有强大数据处理和智能分析能力的模型。它们不仅能处理自然语言，还能理解和生成图像、视频、代码等多种内容，是推动“人工智能+”发展的核心驱动力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型 “大模型”是人工智能前沿技术，指通过海量数据训练、具有强大数据处理和智能分析能力的模型。它们不仅能处理自然语言，还能理解和生成图像、视频、代码等多种内容，是推动“人工智能+”发展的核心驱动力。

</details>


### [29] [课程回顾｜陈沁：如何用<em class="highlight">大模型</em>帮助我们工作](http://mp.weixin.qq.com/s?__biz=MzAxMTAxNDk0Mw==&mid=2649404863&idx=3&sn=5dff7c9b8dd07ffc59db5bb48f4bf6aa&chksm=8297582b65a9e331e76e6c94c8743dba1d1e8a6ebae6e1ebde05f9d7f6b5cc5df16412177add#rd)
*复旦大学经济学院专业学位硕士*

Main category: wechat.article

TL;DR: 陈老师从大模型的运行原理、使用大模型的原生能力、给大模型增加知识，以及智能体的构建四方面系统地介绍了大模型在学术研究和商业实践中的应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 陈老师从大模型的运行原理、使用大模型的原生能力、给大模型增加知识，以及智能体的构建四方面系统地介绍了大模型在学术研究和商业实践中的应用。

</details>


### [30] [2025 AI<em class="highlight">大模型</em>开发生态白皮书](http://mp.weixin.qq.com/s?__biz=MjM5MzMzNjczMA==&mid=2654202290&idx=3&sn=228e6c76ba54f4aa1a6547242bde3b5c&chksm=bcfd7568dc4fffc039a6240ff40ee577a798c7e4f2a456d0f13c06e4db7c1176450cf50e3f6e#rd)
*爱运营*

Main category: wechat.article

TL;DR: 2025 AI大模型开发生态白皮书完整白皮书共123页，有需要的请通过底部阅读原文免费下载。支持我们请加入知识星球，更多资料。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025 AI大模型开发生态白皮书完整白皮书共123页，有需要的请通过底部阅读原文免费下载。支持我们请加入知识星球，更多资料。

</details>


### [31] [基于多模态<em class="highlight">大模型</em>的具身智能体研究进展与展望](http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247718864&idx=1&sn=b0568d0773994af7ff6b2bebfb0f51a1&chksm=f8febb940b541d5ab78b3db552c3145e230854579e0c36ad2dc0afae36ad31185b92a29761f5#rd)
*一点人工一点智能*

Main category: wechat.article

TL;DR: 近期，多模态大模型（multimodal large language model，MLLM）的出现为具身智能体的发展带来了一些新的突破，离实现真正意义上的具身智能体又更进一步。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近期，多模态大模型（multimodal large language model，MLLM）的出现为具身智能体的发展带来了一些新的突破，离实现真正意义上的具身智能体又更进一步。

</details>


### [32] [AI Agents深度解析：从<em class="highlight">大模型</em>到自主智能体](http://mp.weixin.qq.com/s?__biz=MzAwODQ5ODYzOA==&mid=2452074886&idx=1&sn=934c73126fe2449d3b441d71a7767520&chksm=8daf969ed6a7049897ceb49c9ec537efacc937b2e02abf2e3bfe4112e003a409e191ad195401#rd)
*GoFrame开源技术*

Main category: wechat.article

TL;DR: 大语言模型（Large Language Model， LLM）是基于深度学习技术，通过在海量文本数据上进行预训练而获得的超大规模神经网络模型。其核心是Transformer架构，通过自注意力机制捕捉文本中的长距离依赖关系。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大语言模型（Large Language Model， LLM）是基于深度学习技术，通过在海量文本数据上进行预训练而获得的超大规模神经网络模型。其核心是Transformer架构，通过自注意力机制捕捉文本中的长距离依赖关系。

</details>


### [33] [<em class="highlight">大模型</em>原理-生成式AI的现状与趋势](http://mp.weixin.qq.com/s?__biz=MzI0MjQ4Njk1Mg==&mid=2247483980&idx=1&sn=ff3c0f95ed83be06a075ea9a4c15a13f&chksm=e8bc73d9b3a14dc3ecda0f36fe6b25e306fb5c8bb148f51eb6b154221ffa9fd7fd1abcb4096e#rd)
*云水堂实壮哥*

Main category: wechat.article

TL;DR: 生成式大模型的现状主要有下面几点：1）模型规模持续扩大参数增长：主流模型参数量已突破千亿级（如GPT-4、PaLM-2）训练数据量：使用多模态、多语言混合数据（如文本、代码、图像），数据量达数+TB级别。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 生成式大模型的现状主要有下面几点：1）模型规模持续扩大参数增长：主流模型参数量已突破千亿级（如GPT-4、PaLM-2）训练数据量：使用多模态、多语言混合数据（如文本、代码、图像），数据量达数+TB级别。

</details>


### [34] [一文详解AI<em class="highlight">大模型</em>14个核心基础概念：Transformer、Token、MoE、RAG、对齐、预训练、微调、Agent](http://mp.weixin.qq.com/s?__biz=MzI1MzUyMTMwOA==&mid=2247497200&idx=1&sn=cd0ecb29675bfbf6ceb0468e519577ff&chksm=e88dff2a2d49ded95c010df5776102d024457f88049d7a16262a60b01fb4d39c4882211f88e2#rd)
*魔方AI空间*

Main category: wechat.article

TL;DR: 对齐技术解决大模型 "能说会做"但"说错做错"的问题 ，是大模型安全可靠应用的关键保障。基于人类反馈的强化学习（RLHF）：RLHF是当前实现对齐最主流和最有效的方法 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 对齐技术解决大模型 "能说会做"但"说错做错"的问题 ，是大模型安全可靠应用的关键保障。基于人类反馈的强化学习（RLHF）：RLHF是当前实现对齐最主流和最有效的方法 。

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 提出两种基于信息论和热力学的无监督指标来评估LLM的任务忠实度：语义忠实度(SF)和语义熵产生(SEP)。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，需要新的无监督评估方法。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文到答案的转换。将QCA三元组建模为共享主题的概率分布，通过凸优化计算KL散度作为SF指标，并推导热力学SEP指标。

Result: SF指标量化QCA三元组的忠实度，SEP指标衡量答案生成中的熵产生。高忠实度通常对应低熵产生，两种指标可用于LLM评估和幻觉控制。

Conclusion: 提出的SF和SEP指标为LLM忠实度评估提供了理论基础和无监督方法，在SEC 10-K文件摘要任务中验证了有效性。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [36] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 论文提出将"意志薄弱"(akrasia)作为分析AI代理系统不一致性的核心概念，并开发了Akrasia基准测试来量化评估模型在不同诱惑条件下的自我控制能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出一种特殊的不一致性：它们"知道"正确答案但无法据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为"意志薄弱"(akrasia)。作者认为这一概念对于分析AI代理系统的不一致性和目标漂移至关重要。

Method: 引入Akrasia基准测试的初步版本，包含四种结构化提示条件：基线(B)、同义词(S)、时间(T)和诱惑(X)，用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的"自我控制"量化比较。

Result: 建立了量化评估AI系统"意志薄弱"现象的方法框架，能够比较不同模型在面临诱惑时的自我控制能力。同时指出微观层面的意志薄弱可能在多代理系统中累积为宏观层面的不稳定性。

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将代理行为与经典的代理理论联系起来，为哲学、心理学和新兴的代理AI科学之间建立了实证桥梁。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [37] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 提出Executor-Analyst框架，通过解耦工具执行与临床推理，解决小LLM临床代理的上下文利用失败问题，无需昂贵微调即达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于小LLM的临床代理（如TxAgent）存在"上下文利用失败"问题：模型能成功检索生物医学证据，但无法基于这些信息进行诊断推理。需要解决小模型在临床推理中的局限性。

Method: 提出Executor-Analyst框架，将专门的TxAgents（执行器）与长上下文基础模型（分析师）解耦。采用分层集成策略保持证据多样性，避免信息瓶颈。通过架构工程而非端到端微调来提升性能。

Result: 在CURE-Bench上达到最先进性能，无需昂贵微调。发现两个关键扩展洞察：1）上下文性能悖论（超过12k token会引入噪声降低准确性）；2）动作空间维度诅咒（工具扩展需要分层检索策略）。

Conclusion: 通过无训练的架构工程方法，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础。分层集成策略显著优于全局池化，有效解决了信息瓶颈问题。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [38] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 提出一种多模态肿瘤学智能体（MOA），整合基于TITAN基础模型的IDH1突变预测组织学工具，结合临床和基因组数据推理，在低级别胶质瘤中实现高精度IDH1突变预测。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变具有重要临床意义，但现有预测方法有限。需要整合多模态信息（组织学、临床、基因组）并通过外部生物医学知识增强预测准确性。

Method: 开发多模态肿瘤学智能体（MOA）：1）基于TITAN基础模型的组织学工具用于IDH1突变预测；2）整合结构化临床和基因组数据；3）通过PubMed、Google Search、OncoKB进行外部知识推理；4）在TCGA-LGG队列的488名患者上进行评估。

Result: MOA（无组织学工具）F1分数0.826，优于临床基线0.798；融合组织学特征后F1分数达0.912，超过组织学基线0.894和融合组织学-临床基线0.897，显示最佳性能。

Conclusion: MOA通过整合多模态信息和外部生物医学知识源，能够捕获互补的突变相关信息，实现准确的IDH1突变预测，为低级别胶质瘤的精准诊断提供新方法。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [39] [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](https://arxiv.org/abs/2512.05930)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: PRiSM是一个用于评估视觉语言模型在科学领域推理能力的动态多模态基准，包含超过24,750个大学物理和数学问题，通过Python代码生成和验证提供细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估科学领域（如数学和物理）的视觉语言模型时存在不足，缺乏中间推理步骤、对变化的鲁棒性以及验证科学正确性的机制，需要更全面的评估框架。

Method: 开发了PRiSM基准，包含动态文本和视觉输入、生成图像，以及丰富的结构化输出（可执行Python代码用于真值生成和验证、详细逐步推理）。使用基于代理的PrismAgent管道生成结构化问题实例。

Result: 通过五个针对性评估任务（泛化、符号程序合成、扰动鲁棒性、推理校正、歧义解析）对现有VLM进行全面评估，揭示了它们在科学推理方面的局限性和失败模式。

Conclusion: PRiSM基准能够深入洞察视觉语言模型的科学推理能力，揭示其局限性、不确定性行为和科学推理的不足，为科学领域的VLM评估提供了更全面的框架。

Abstract: Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.

</details>
