{"id": "2602.16819", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16819", "abs": "https://arxiv.org/abs/2602.16819", "authors": ["Yiqing Xie", "Emmy Liu", "Gaokai Zhang", "Nachiket Kotalwar", "Shubham Gandhi", "Sathwik Acharya", "Xingyao Wang", "Carolyn Rose", "Graham Neubig", "Daniel Fried"], "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks", "comment": null, "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Hybrid-Gym\u8bad\u7ec3\u73af\u5883\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u5408\u6210\u4efb\u52a1\uff08\u5982\u51fd\u6570\u5b9a\u4f4d\u3001\u4f9d\u8d56\u641c\u7d22\uff09\u6765\u6559\u6388\u8bed\u8a00\u6a21\u578b\u53ef\u8fc1\u79fb\u7684\u7f16\u7a0b\u6280\u80fd\uff0c\u4f7f\u4ee3\u7406\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u7f16\u7a0b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u57fa\u51c6\uff08\u5982SWE-Bench\uff09\u4e3b\u8981\u5173\u6ce8\u89e3\u51b3GitHub\u4e0a\u7684\u5355\u4e00\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u4f7f\u7528\u4e2d\u4ee3\u7406\u9700\u8981\u5904\u7406\u66f4\u590d\u6742\u591a\u6837\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u4ee3\u7801\u5e93\u63a2\u7d22\u3001\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u67b6\u6784\u8bbe\u8ba1\u7b49\u6280\u80fd\u3002\u9700\u8981\u5f00\u53d1\u80fd\u6559\u6388\u8fd9\u4e9b\u53ef\u8fc1\u79fb\u6280\u80fd\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u8f68\u8ff9\u8bc6\u522b\u53ef\u8fc1\u79fb\u6280\u80fd\uff0c\u5236\u5b9a\u8bbe\u8ba1\u8f85\u52a9\u8bad\u7ec3\u4efb\u52a1\u7684\u539f\u5219\u3002\u57fa\u4e8e\u8fd9\u4e9b\u539f\u5219\u521b\u5efaHybrid-Gym\u8bad\u7ec3\u73af\u5883\uff0c\u5305\u542b\u53ef\u6269\u5c55\u7684\u5408\u6210\u4efb\u52a1\uff08\u51fd\u6570\u5b9a\u4f4d\u3001\u4f9d\u8d56\u641c\u7d22\u7b49\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u638c\u63e1\u8fd9\u4e9b\u6280\u80fd\u3002", "result": "\u5728Hybrid-Gym\u4e0a\u8bad\u7ec3\u7684\u4ee3\u7406\u80fd\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff1a\u5728SWE-Bench Verified\u4e0a\u76f8\u5bf9\u57fa\u7840\u6a21\u578b\u63d0\u534725.4%\uff0c\u5728SWT-Bench Verified\u4e0a\u63d0\u53477.9%\uff0c\u5728Commit-0 Lite\u4e0a\u63d0\u53475.1%\u3002Hybrid-Gym\u8fd8\u80fd\u8865\u5145\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u96c6\uff08\u5982\u5728SWT-Bench Verified\u4e0a\u63d0\u5347SWE-Play 4.9%\uff09\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u4efb\u52a1\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u6559\u6388\u8bed\u8a00\u6a21\u578b\u53ef\u8fc1\u79fb\u7684\u7f16\u7a0b\u6280\u80fd\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u7f16\u7a0b\u4efb\u52a1\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u4e0e\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "code agent"}}
{"id": "2602.16802", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16802", "abs": "https://arxiv.org/abs/2602.16802", "authors": ["Kejian Shi", "Yixin Liu", "Peifeng Wang", "Alexander R. Fabbri", "Shafiq Joty", "Arman Cohan"], "title": "References Improve LLM Alignment in Non-Verifiable Domains", "comment": "ICLR 2026 Camera Ready", "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.", "AI": {"tldr": "\u63d0\u51fa\u53c2\u8003\u5f15\u5bfc\u7684LLM\u8bc4\u4f30\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982LLM\u5bf9\u9f50\uff09\uff0c\u901a\u8fc7\u53c2\u8003\u8f93\u51fa\u6765\u589e\u5f3aLLM\u8bc4\u4f30\u5668\u51c6\u786e\u6027\uff0c\u5e76\u7528\u4e8e\u6a21\u578b\u81ea\u6539\u8fdb\uff0c\u5728AlpacaEval\u548cArena-Hard\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u9a8c\u8bc1\u5668\u7684\u975e\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982LLM\u5bf9\u9f50\uff09\u3002\u9700\u8981\u63a2\u7d22\u53c2\u8003\u5f15\u5bfc\u7684LLM\u8bc4\u4f30\u5668\u662f\u5426\u80fd\u4f5c\u4e3a\u8f6f\"\u9a8c\u8bc1\u5668\"\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4f7f\u7528\u53c2\u8003\u8f93\u51fa\u589e\u5f3aLLM\u8bc4\u4f30\u5668\u7684\u8bc4\u4f30\u534f\u8bae\uff1b\u901a\u8fc7\u53c2\u8003\u5f15\u5bfc\u65b9\u6cd5\u63d0\u5347LLM\u8bc4\u4f30\u5668\u51c6\u786e\u6027\uff1b\u5229\u7528\u6539\u8fdb\u7684\u8bc4\u4f30\u5668\u8fdb\u884c\u5bf9\u9f50\u8c03\u4f18\uff0c\u8ba9LLM\u4f7f\u7528\u53c2\u8003\u5f15\u5bfc\u7684\u8bc4\u4f30\u5668\u8fdb\u884c\u81ea\u6539\u8fdb\u3002", "result": "\u53c2\u8003\u5f15\u5bfc\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u8bc4\u4f30\u5668\u51c6\u786e\u6027\uff1b\u53c2\u8003\u5f15\u5bfc\u7684\u81ea\u6539\u8fdb\u5728AlpacaEval\u548cArena-Hard\u57fa\u51c6\u4e0a\u4f18\u4e8e\u76f4\u63a5SFT\u84b8\u998f\u548c\u65e0\u53c2\u8003\u81ea\u6539\u8fdb\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528ArMoRM\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0cLlama-3-8B-Instruct\u5728AlpacaEval\u548cArena-Hard\u4e0a\u5206\u522b\u8fbe\u523073.1%\u548c58.7%\uff0cQwen2.5-7B\u5206\u522b\u8fbe\u523070.0%\u548c74.1%\u3002", "conclusion": "\u53c2\u8003\u5f15\u5bfc\u7684LLM\u8bc4\u4f30\u5668\u80fd\u591f\u5728\u975e\u53ef\u9a8c\u8bc1\u9886\u57df\u5b9e\u73b0\u6709\u6548\u7684LLM\u540e\u8bad\u7ec3\uff0c\u4e3aLLM\u5bf9\u9f50\u7b49\u7f3a\u4e4f\u771f\u5b9e\u9a8c\u8bc1\u5668\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.17037", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17037", "abs": "https://arxiv.org/abs/2602.17037", "authors": ["Rahul Nanda", "Chandra Maddila", "Smriti Jha", "Euna Mehnaz Khan", "Matteo Paltenghi", "Satish Chandra"], "title": "Wink: Recovering from Misbehaviors in Coding Agents", "comment": null, "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.\n  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.", "AI": {"tldr": "\u63d0\u51faWink\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u81ea\u6211\u5e72\u9884\u81ea\u52a8\u6062\u590d\u7f16\u7801\u4ee3\u7406\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6210\u529f\u89e3\u51b390%\u9700\u8981\u5355\u6b21\u5e72\u9884\u7684\u5f02\u5e38", "motivation": "\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u884c\u4e1a\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u504f\u79bb\u6307\u4ee4\u3001\u9677\u5165\u91cd\u590d\u5faa\u73af\u3001\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u7b49\u5f02\u5e38\u884c\u4e3a\uff0c\u8fd9\u4e9b\u6545\u969c\u4f1a\u4e2d\u65ad\u5f00\u53d1\u6d41\u7a0b\u5e76\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884", "method": "\u57fa\u4e8e\u751f\u4ea7\u6d41\u91cf\u5206\u6790\u5efa\u7acb\u5f02\u5e38\u884c\u4e3a\u5206\u7c7b\u6cd5\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5f02\u6b65\u81ea\u6211\u5e72\u9884\u7cfb\u7edfWink\uff0c\u901a\u8fc7\u89c2\u5bdf\u4ee3\u7406\u8f68\u8ff9\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u4fee\u6b63\u6307\u5bfc\u6765\u5f15\u5bfc\u4ee3\u7406\u56de\u5230\u6b63\u786e\u8def\u5f84", "result": "\u5728\u8d85\u8fc710,000\u4e2a\u771f\u5b9e\u4e16\u754c\u4ee3\u7406\u8f68\u8ff9\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u6210\u529f\u89e3\u51b390%\u9700\u8981\u5355\u6b21\u5e72\u9884\u7684\u5f02\u5e38\uff1b\u751f\u4ea7\u73af\u5883A/B\u6d4b\u8bd5\u663e\u793a\u5de5\u5177\u8c03\u7528\u5931\u8d25\u3001\u4f1a\u8bdd\u4ee4\u724c\u6570\u548c\u5de5\u7a0b\u5e08\u5e72\u9884\u6b21\u6570\u663e\u8457\u51cf\u5c11", "conclusion": "Wink\u7cfb\u7edf\u80fd\u6709\u6548\u6062\u590d\u4ee3\u7406\u5f02\u5e38\u884c\u4e3a\uff0c\u4e3a\u6784\u5efa\u5927\u89c4\u6a21\u5f39\u6027\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u7ecf\u9a8c\u548c\u89c1\u89e3", "topic": "code agent"}}
{"id": "2602.17091", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17091", "abs": "https://arxiv.org/abs/2602.17091", "authors": ["Kan Watanabe", "Tatsuya Shirai", "Yutaro Kashiwa", "Hajimu Iida"], "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation", "comment": null, "summary": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.", "AI": {"tldr": "\u63d0\u51fa\u9884\u6d4b\u6a21\u578b\u8bc6\u522bPR\u5ba1\u67e5\u4e2d\u53ef\u80fd\u88ab\u5220\u9664\u7684\u51fd\u6570\uff0c\u5e2e\u52a9\u5ba1\u9605\u8005\u4f18\u5148\u5904\u7406\u91cd\u8981\u4ee3\u7801", "motivation": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08\u5982GitHub Copilot\uff09\u867d\u7136\u52a0\u901f\u4e86\u5f00\u53d1\uff0c\u4f46\u4ea7\u751f\u4e86\u66f4\u591a\u9700\u8981\u5ba1\u67e5\u7684\u4ee3\u7801\uff0c\u5176\u4e2d\u76f8\u5f53\u4e00\u90e8\u5206\u6700\u7ec8\u4f1a\u88ab\u5220\u9664\uff0c\u589e\u52a0\u4e86\u5ba1\u9605\u8005\u7684\u8d1f\u62c5\u3002\u76ee\u524d\u6ca1\u6709\u7814\u7a76\u5e2e\u52a9\u5ba1\u9605\u8005\u9ad8\u6548\u8bc6\u522b\u5c06\u88ab\u5220\u9664\u7684\u4ee3\u7801\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u6a21\u578b\u6765\u8bc6\u522b\u5728PR\u5ba1\u67e5\u4e2d\u53ef\u80fd\u88ab\u5220\u9664\u7684\u51fd\u6570\u3002\u7814\u7a76\u53d1\u73b0\u56e0\u4e0d\u540c\u539f\u56e0\u88ab\u5220\u9664\u7684\u51fd\u6570\u5177\u6709\u4e0d\u540c\u7684\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8687.1%\u7684AUC\uff0c\u8868\u660e\u9884\u6d4b\u65b9\u6cd5\u80fd\u6709\u6548\u5e2e\u52a9\u5ba1\u9605\u8005\u4f18\u5148\u5904\u7406\u91cd\u8981\u4ee3\u7801\u3002", "conclusion": "\u9884\u6d4b\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u53ef\u80fd\u88ab\u5220\u9664\u7684\u51fd\u6570\uff0c\u5e2e\u52a9\u5ba1\u9605\u8005\u4f18\u5316\u5ba1\u67e5\u6d41\u7a0b\uff0c\u5c06\u7cbe\u529b\u96c6\u4e2d\u5728\u771f\u6b63\u91cd\u8981\u7684\u4ee3\u7801\u4e0a\u3002", "topic": "code agent"}}
{"id": "2602.16763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16763", "abs": "https://arxiv.org/abs/2602.16763", "authors": ["Mubashara Akhtar", "Anka Reuel", "Prajna Soni", "Sanchit Ahuja", "Pawan Sasanka Ammanamanchi", "Ruchit Rawal", "Vil\u00e9m Zouhar", "Srishti Yadav", "Chenxi Whitehouse", "Dayeon Ki", "Jennifer Mickel", "Leshem Choshen", "Marek \u0160uppa", "Jan Batzner", "Jenny Chim", "Jeba Sania", "Yanan Long", "Hossein A. Rahmani", "Christina Knight", "Yiyang Nan", "Jyoutir Raj", "Yu Fan", "Shubham Singh", "Subramanyam Sahoo", "Eliya Habba", "Usman Gohar", "Siddhesh Pawar", "Robert Scholz", "Arjun Subramonian", "Jingwei Ni", "Mykel Kochenderfer", "Sanmi Koyejo", "Mrinmaya Sachan", "Stella Biderman", "Zeerak Talat", "Avijit Ghosh", "Irene Solaiman"], "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation", "comment": null, "summary": "Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.", "AI": {"tldr": "\u5206\u679060\u4e2aLLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u9971\u548c\u73b0\u8c61\uff0c\u53d1\u73b0\u8fd1\u534a\u6570\u57fa\u51c6\u5df2\u9971\u548c\uff0c\u9971\u548c\u7387\u968f\u65f6\u95f4\u589e\u52a0\uff0c\u4e13\u5bb6\u7b56\u5212\u7684\u57fa\u51c6\u6bd4\u4f17\u5305\u57fa\u51c6\u66f4\u6297\u9971\u548c\uff0c\u9690\u85cf\u6d4b\u8bd5\u6570\u636e\u65e0\u4fdd\u62a4\u6548\u679c", "motivation": "AI\u57fa\u51c6\u6d4b\u8bd5\u5728\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u51b3\u7b56\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u8bb8\u591a\u57fa\u51c6\u5f88\u5feb\u9971\u548c\uff0c\u65e0\u6cd5\u533a\u5206\u6700\u4f73\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u957f\u671f\u4ef7\u503c\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u57fa\u51c6\u9971\u548c\u73b0\u8c61\u53ca\u5176\u9a71\u52a8\u56e0\u7d20", "method": "\u4ece\u4e3b\u8981\u6a21\u578b\u5f00\u53d1\u5546\u7684\u6280\u672f\u62a5\u544a\u4e2d\u9009\u53d660\u4e2aLLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u4efb\u52a1\u8bbe\u8ba1\u3001\u6570\u636e\u6784\u5efa\u548c\u8bc4\u4f30\u683c\u5f0f\u4e09\u4e2a\u7ef4\u5ea6\u5b9a\u4e4914\u4e2a\u5c5e\u6027\u7279\u5f81\uff0c\u6d4b\u8bd55\u4e2a\u5047\u8bbe\u6765\u68c0\u9a8c\u6bcf\u4e2a\u5c5e\u6027\u5bf9\u9971\u548c\u7387\u7684\u5f71\u54cd", "result": "\u8fd1\u534a\u6570\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u51fa\u9971\u548c\u73b0\u8c61\uff0c\u9971\u548c\u7387\u968f\u57fa\u51c6\u5e74\u9f84\u589e\u52a0\u800c\u589e\u52a0\uff1b\u9690\u85cf\u6d4b\u8bd5\u6570\u636e\uff08\u516c\u5f00vs\u79c1\u6709\uff09\u6ca1\u6709\u4fdd\u62a4\u6548\u679c\uff1b\u4e13\u5bb6\u7b56\u5212\u7684\u57fa\u51c6\u6bd4\u4f17\u5305\u57fa\u51c6\u66f4\u80fd\u62b5\u6297\u9971\u548c", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u80fd\u5ef6\u957f\u57fa\u51c6\u6d4b\u8bd5\u7684\u5bff\u547d\uff0c\u4e3a\u6784\u5efa\u66f4\u6301\u4e45\u7684\u8bc4\u4f30\u7b56\u7565\u63d0\u4f9b\u4e86\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bbe\u8ba1", "topic": "swe benchmark"}}
{"id": "2602.17183", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17183", "abs": "https://arxiv.org/abs/2602.17183", "authors": ["Kishan Maharaj", "Nandakishore Menon", "Ashita Saxena", "Srikanth Tamilselvam"], "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering", "comment": "11 pages, 4 Figures, 5 Tables, Work in Progress", "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4ee3\u7801\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7b54\u6848\u683c\u5f0f\u53d8\u5316\u3001\u5e72\u6270\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u89c4\u6a21\u53d8\u5316\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u9700\u8981\u5904\u7406\u957f\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u4ee3\u7801\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5bf9\u7b54\u6848\u683c\u5f0f\u3001\u5e72\u6270\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u89c4\u6a21\u7684\u654f\u611f\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u63a7\u5236\u6027\u6d88\u878d\u5b9e\u9a8c\u6d4b\u8bd5\u6a21\u578b\u654f\u611f\u6027\uff1a1\uff09\u6253\u4e71\u591a\u9879\u9009\u62e9\u9009\u9879\u987a\u5e8f\uff1b2\uff09\u5f00\u653e\u6027\u95ee\u9898\u8bbe\u7f6e\uff1b3\uff09\"\u5927\u6d77\u635e\u9488\"\u4e0a\u4e0b\u6587\uff08\u5305\u542b\u76f8\u5173\u548c\u5bf9\u6297\u6027\u65e0\u5173\u4fe1\u606f\uff09\u3002\u6269\u5c55\u4e86LongCodeBench Python\u6570\u636e\u96c6\uff0c\u65b0\u589eCOBOL\u548cJava\u95ee\u7b54\u96c6\uff0c\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u5728\u4e09\u79cd\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5728\u6253\u4e71\u7684\u591a\u9879\u9009\u62e9\u9009\u9879\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b2\uff09\u5f00\u653e\u6027\u95ee\u9898\u4e2d\u6027\u80fd\u4e5f\u5927\u5e45\u4e0b\u964d\uff1b3\uff09\u5728\u5b58\u5728\u65e0\u5173\u7ebf\u7d22\u65f6\u8868\u73b0\u51fa\u8106\u5f31\u884c\u4e3a\u3002\u8fd9\u8868\u660e\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4ee3\u7801\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u7b54\u6848\u683c\u5f0f\u53d8\u5316\u548c\u5b58\u5728\u5e72\u6270\u4fe1\u606f\u65f6\u3002\u7814\u7a76\u4e3a\u8bc4\u4f30\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u4f20\u7edf\u548c\u73b0\u4ee3\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2602.16745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16745", "abs": "https://arxiv.org/abs/2602.16745", "authors": ["Zhangyi Liu", "Huaizhi Qu", "Xiaowei Yin", "He Sun", "Yanjun Han", "Tianlong Chen", "Zhun Deng"], "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency", "comment": null, "summary": "Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.", "AI": {"tldr": "PETS\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u6d4b\u8bd5\u65f6\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u5206\u914d\u63a8\u7406\u8f68\u8ff9\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u76f8\u6bd4\u5747\u5300\u5206\u914d\u663e\u8457\u51cf\u5c11\u91c7\u6837\u9700\u6c42\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u53ef\u4ee5\u901a\u8fc7\u805a\u5408\u968f\u673a\u63a8\u7406\u8f68\u8ff9\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u6709\u9650\u9884\u7b97\u4e0b\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u81ea\u4e00\u81f4\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u63d0\u51faPETS\u6846\u67b6\uff0c\u5f15\u5165\u81ea\u4e00\u81f4\u6027\u7387\u4f5c\u4e3a\u65b0\u5ea6\u91cf\uff0c\u5c06\u8f68\u8ff9\u5206\u914d\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4f18\u5316\u95ee\u9898\u3002\u7814\u7a76\u79bb\u7ebf\u548c\u5728\u7ebf\u4e24\u79cd\u8bbe\u7f6e\uff1a\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u5c06\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u4f17\u5305\u5de5\u4f5c\u8005\uff0c\u5229\u7528\u591a\u6570\u6295\u7968\u7b97\u6cd5\uff1b\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u63d0\u51fa\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u6839\u636e\u95ee\u9898\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u9884\u7b97\u3002", "result": "\u5728GPQA\u6570\u636e\u96c6\u4e0a\uff0cPETS\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u90fd\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u81ea\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u76f8\u6bd4\u5747\u5300\u5206\u914d\u51cf\u5c11\u4e8675%\uff08\u79bb\u7ebf\uff09\u548c55%\uff08\u5728\u7ebf\uff09\u7684\u91c7\u6837\u9884\u7b97\u3002", "conclusion": "PETS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u3001\u7406\u8bba\u4fdd\u8bc1\u7684\u6d4b\u8bd5\u65f6\u81ea\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6709\u9650\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.16805", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16805", "abs": "https://arxiv.org/abs/2602.16805", "authors": ["Yonatan Gideoni", "Sebastian Risi", "Yarin Gal"], "title": "Simple Baselines are Competitive with Code Evolution", "comment": null, "summary": "Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.", "AI": {"tldr": "\u7b80\u5355\u57fa\u51c6\u65b9\u6cd5\u5728\u4ee3\u7801\u6f14\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7801\u6f14\u5316\u7814\u7a76\u5728\u8bc4\u4f30\u65b9\u6cd5\u548c\u95ee\u9898\u8bbe\u8ba1\u4e0a\u7684\u4e0d\u8db3", "motivation": "\u5f53\u524d\u4ee3\u7801\u6f14\u5316\u6280\u672f\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u7b80\u5355\u57fa\u51c6\u65b9\u6cd5\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u9700\u8981\u9a8c\u8bc1\u590d\u6742\u65b9\u6cd5\u7684\u5b9e\u9645\u4f18\u52bf", "method": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6570\u5b66\u8fb9\u754c\u4f18\u5316\u3001\u667a\u80fd\u4f53\u811a\u624b\u67b6\u8bbe\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u7ade\u8d5b\uff09\u6d4b\u8bd5\u7b80\u5355\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u4e0e\u590d\u6742\u4ee3\u7801\u6f14\u5316\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u5206\u6790", "result": "\u7b80\u5355\u57fa\u51c6\u65b9\u6cd5\u5728\u6240\u6709\u4e09\u4e2a\u9886\u57df\u90fd\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u590d\u6742\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4ee3\u7801\u6f14\u5316\u7814\u7a76\u4e2d\u7684\u8bc4\u4f30\u504f\u5dee\u548c\u8bbe\u8ba1\u95ee\u9898", "conclusion": "\u4ee3\u7801\u6f14\u5316\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u8bbe\u8ba1\u826f\u597d\u7684\u641c\u7d22\u7a7a\u95f4\u800c\u975e\u641c\u7d22\u7b97\u6cd5\u672c\u8eab\uff0c\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u5e76\u5efa\u7acb\u6700\u4f73\u5b9e\u8df5", "topic": "code agent"}}
{"id": "2602.16812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16812", "abs": "https://arxiv.org/abs/2602.16812", "authors": ["Zhongcan Xiao", "Leyi Zhang", "Guannan Zhang", "Xiaoping Wang"], "title": "NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography", "comment": null, "summary": "Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.", "AI": {"tldr": "NeuDiff Agent\u662f\u4e00\u4e2a\u53d7\u6cbb\u7406\u7684AI\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u4e2d\u5b50\u884d\u5c04\u6570\u636e\u5206\u6790\uff0c\u5c06\u5904\u7406\u65f6\u95f4\u4ece435\u5206\u949f\u51cf\u5c11\u5230\u7ea690\u5206\u949f\uff084.6-5.0\u500d\u52a0\u901f\uff09\uff0c\u540c\u65f6\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6676\u4f53\u7ed3\u6784\u6587\u4ef6\u3002", "motivation": "\u5927\u578b\u79d1\u5b66\u8bbe\u65bd\u9762\u4e34\u5206\u6790\u548c\u62a5\u544a\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7ed3\u6784\u590d\u6742\u7684\u6837\u54c1\uff0c\u4f20\u7edf\u624b\u52a8\u5904\u7406\u6d41\u7a0b\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u79d1\u5b66\u4ea7\u51fa\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53d7\u6cbb\u7406\u7684\u5de5\u5177\u4f7f\u7528AI\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u767d\u540d\u5355\u5de5\u5177\u9650\u5236\u64cd\u4f5c\uff0c\u5728\u5173\u952e\u5de5\u4f5c\u6d41\u8fb9\u754c\u5b9e\u65bd\u6545\u969c\u5173\u95ed\u9a8c\u8bc1\u95e8\uff0c\u5e76\u6355\u83b7\u5b8c\u6574\u7684\u6eaf\u6e90\u4fe1\u606f\u7528\u4e8e\u68c0\u67e5\u548c\u5ba1\u8ba1\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeuDiff Agent\u5c06\u5904\u7406\u65f6\u95f4\u4ece435\u5206\u949f\uff08\u624b\u52a8\uff09\u51cf\u5c11\u523086.5-94.4\u5206\u949f\uff084.6-5.0\u500d\u52a0\u901f\uff09\uff0c\u540c\u65f6\u751f\u6210\u65e0A\u6216B\u7ea7\u8b66\u62a5\u7684\u9a8c\u8bc1CIF\u6587\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u8bbe\u65bd\u6676\u4f53\u5b66\u4e2d\u90e8\u7f72\u667a\u80fdAI\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u8ffd\u6eaf\u6027\u548c\u9762\u5411\u51fa\u7248\u7684\u9a8c\u8bc1\u8981\u6c42\u3002", "topic": "code agent"}}
{"id": "2602.17365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17365", "abs": "https://arxiv.org/abs/2602.17365", "authors": ["Yiming Guan", "Rui Yu", "John Zhang", "Lu Wang", "Chaoyun Zhang", "Liqun Li", "Bo Qiao", "Si Qin", "He Huang", "Fangkai Yang", "Pu Zhao", "Lukas Wutschitz", "Samuel Kessler", "Huseyin A Inan", "Robert Sim", "Saravan Rajmohan", "Qingwei Lin", "Dongmei Zhang"], "title": "Computer-Using World Model", "comment": "35 pages, 7 figures", "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.", "AI": {"tldr": "CUWM\u662f\u4e00\u4e2a\u684c\u9762\u8f6f\u4ef6\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u89e3\u9884\u6d4bUI\u72b6\u6001\u53d8\u5316\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u52a8\u4f5c\u641c\u7d22\uff0c\u63d0\u5347\u529e\u516c\u4efb\u52a1\u51b3\u7b56\u8d28\u91cf\u548c\u6267\u884c\u9c81\u68d2\u6027\u3002", "motivation": "\u590d\u6742\u8f6f\u4ef6\u73af\u5883\u4e2d\uff0c\u5355\u4e2a\u9519\u8bef\u7684UI\u64cd\u4f5c\u53ef\u80fd\u7834\u574f\u957f\u671f\u5de5\u4f5c\u6d41\uff0c\u800c\u771f\u5b9e\u6267\u884c\u4e0d\u652f\u6301\u53cd\u4e8b\u5b9e\u63a2\u7d22\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u8bd5\u9519\u5b66\u4e60\u548c\u89c4\u5212\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u89e3\uff1a\u5148\u9884\u6d4b\u6587\u672c\u63cf\u8ff0\u7684\u72b6\u6001\u53d8\u5316\uff0c\u518d\u53ef\u89c6\u5316\u5408\u6210\u4e0b\u4e00\u622a\u56fe\u3002\u5728\u79bb\u7ebfUI\u8f6c\u6362\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u9884\u6d4b\u4e0e\u8ba1\u7b97\u673a\u4f7f\u7528\u73af\u5883\u7684\u7ed3\u6784\u8981\u6c42\u3002", "result": "\u901a\u8fc7\u6d4b\u8bd5\u65f6\u52a8\u4f5c\u641c\u7d22\u8bc4\u4f30\uff0c\u4e16\u754c\u6a21\u578b\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u9ad8\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u6267\u884c\u9c81\u68d2\u6027\uff0c\u5728\u4e00\u7cfb\u5217Office\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "CUWM\u4e3a\u684c\u9762\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4bUI\u72b6\u6001\u53d8\u5316\u652f\u6301\u667a\u80fd\u51b3\u7b56\uff0c\u6539\u5584\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u4ee3\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.17003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17003", "abs": "https://arxiv.org/abs/2602.17003", "authors": ["Serin Kim", "Sangam Lee", "Dongha Lee"], "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History", "comment": null, "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.", "AI": {"tldr": "Persona2Web\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u5f00\u653e\u7f51\u7edc\u4e0a\u8bc4\u4f30\u4e2a\u6027\u5316\u7f51\u9875\u4ee3\u7406\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\"\u6f84\u6e05\u4ee5\u4e2a\u6027\u5316\"\u539f\u5219\uff0c\u8981\u6c42\u4ee3\u7406\u6839\u636e\u7528\u6237\u5386\u53f2\u800c\u975e\u660e\u786e\u6307\u4ee4\u6765\u89e3\u51b3\u6a21\u7cca\u67e5\u8be2\u3002", "motivation": "\u5f53\u524d\u7f51\u9875\u4ee3\u7406\u7f3a\u4e4f\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u800c\u7528\u6237\u5f88\u5c11\u8be6\u7ec6\u8bf4\u660e\u610f\u56fe\u7684\u6bcf\u4e2a\u7ec6\u8282\u3002\u5b9e\u7528\u7684\u7f51\u9875\u4ee3\u7406\u9700\u8981\u80fd\u591f\u901a\u8fc7\u63a8\u65ad\u7528\u6237\u504f\u597d\u548c\u4e0a\u4e0b\u6587\u6765\u89e3\u91ca\u6a21\u7cca\u67e5\u8be2\u3002", "method": "\u6784\u5efaPersona2Web\u57fa\u51c6\uff0c\u5305\u542b\uff1a1) \u5728\u957f\u65f6\u95f4\u8de8\u5ea6\u5185\u9690\u542b\u63ed\u793a\u504f\u597d\u7684\u7528\u6237\u5386\u53f2\uff1b2) \u9700\u8981\u4ee3\u7406\u63a8\u65ad\u9690\u542b\u7528\u6237\u504f\u597d\u7684\u6a21\u7cca\u67e5\u8be2\uff1b3) \u80fd\u591f\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4e2a\u6027\u5316\u7684\u63a8\u7406\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5bf9\u5404\u79cd\u4ee3\u7406\u67b6\u6784\u3001\u9aa8\u5e72\u6a21\u578b\u3001\u5386\u53f2\u8bbf\u95ee\u65b9\u6848\u548c\u4e0d\u540c\u6a21\u7cca\u5ea6\u67e5\u8be2\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u4e2a\u6027\u5316\u7f51\u9875\u4ee3\u7406\u884c\u4e3a\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "Persona2Web\u4e3a\u4e2a\u6027\u5316\u7f51\u9875\u4ee3\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u771f\u5b9e\u5f00\u653e\u7f51\u7edc\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "topic": "agent analysis"}}
{"id": "2602.17022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17022", "abs": "https://arxiv.org/abs/2602.17022", "authors": ["Takyoung Kim", "Jinseok Nam", "Chandrayee Basu", "Xing Fan", "Chengyuan Ma", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "ReIn: Conversational Error Recovery with Reasoning Inception", "comment": "ICLR 2026", "summary": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.", "AI": {"tldr": "ReIn\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u6a21\u5757\u8bc6\u522b\u5bf9\u8bdd\u9519\u8bef\u5e76\u751f\u6210\u6062\u590d\u8ba1\u5212\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u7cfb\u7edf\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6062\u590d\u8ba1\u5212\u690d\u5165\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5bf9\u8bdd\u4ee3\u7406\u7684\u9519\u8bef\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u5728\u56fa\u5b9a\u4efb\u52a1\u5bfc\u5411\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7528\u6237\u5f15\u53d1\u7684\u610f\u5916\u9519\u8bef\u5f88\u8106\u5f31\u3002\u7531\u4e8e\u5fae\u8c03\u6216\u4fee\u6539\u63d0\u793a\u7684\u6210\u672c\u548c\u65f6\u95f4\u8981\u6c42\u9ad8\uff0c\u9700\u8981\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u7cfb\u7edf\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba9\u4ee3\u7406\u80fd\u591f\u4ece\u4e0a\u4e0b\u6587\u9519\u8bef\u7684\u4ea4\u4e92\u4e2d\u6062\u590d\u3002", "method": "\u63d0\u51faReasoning Inception (ReIn)\u65b9\u6cd5\uff1a\u4f7f\u7528\u5916\u90e8inception\u6a21\u5757\u8bc6\u522b\u9884\u5b9a\u4e49\u7684\u5bf9\u8bdd\u9519\u8bef\u5e76\u751f\u6210\u6062\u590d\u8ba1\u5212\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6062\u590d\u8ba1\u5212\u6574\u5408\u5230\u4ee3\u7406\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6307\u5bfc\u5176\u91c7\u53d6\u7ea0\u6b63\u884c\u52a8\uff0c\u800c\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u7cfb\u7edf\u63d0\u793a\u3002", "result": "\u5728\u6a21\u62df\u7528\u6237\u6a21\u7cca\u548c\u4e0d\u652f\u6301\u7684\u8bf7\u6c42\u7b49\u5bf9\u8bdd\u5931\u8d25\u573a\u666f\u4e2d\uff0cReIn\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9519\u8bef\u7c7b\u578b\u3002\u5b83\u59cb\u7ec8\u4f18\u4e8e\u663e\u5f0f\u63d0\u793a\u4fee\u6539\u65b9\u6cd5\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u9ad8\u6548\u5373\u65f6\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ReIn\u662f\u4e00\u79cd\u5b89\u5168\u6709\u6548\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u8054\u5408\u5b9a\u4e49\u6062\u590d\u5de5\u5177\uff0c\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u9aa8\u5e72\u6a21\u578b\u6216\u7cfb\u7edf\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u5bf9\u8bdd\u4ee3\u7406\u7684\u6062\u590d\u80fd\u529b\u3002\u5176\u64cd\u4f5c\u673a\u5236\u5206\u6790\u8868\u660e\uff0c\u7279\u522b\u662f\u4e0e\u6307\u4ee4\u5c42\u6b21\u7ed3\u6784\u76f8\u5173\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2602.16855", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16855", "abs": "https://arxiv.org/abs/2602.16855", "authors": ["Haiyang Xu", "Xi Zhang", "Haowei Liu", "Junyang Wang", "Zhaozai Zhu", "Shengjie Zhou", "Xuhao Hu", "Feiyu Gao", "Junjie Cao", "Zihua Wang", "Zhiyuan Chen", "Jitong Liao", "Qi Zheng", "Jiahui Zeng", "Ze Xu", "Shuai Bai", "Junyang Lin", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents", "comment": "25 pages, 11 figures, 11 tables", "summary": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.", "AI": {"tldr": "GUI-Owl-1.5\u662f\u4e00\u4e2a\u539f\u751fGUI\u4ee3\u7406\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u5e73\u53f0\u548c\u5c3a\u5bf8\uff0c\u572820\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u98de\u8f6e\u3001\u7edf\u4e00\u80fd\u529b\u589e\u5f3a\u548c\u591a\u5e73\u53f0\u73af\u5883RL\u6269\u5c55\u7b49\u521b\u65b0\u5b9e\u73b0\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8de8\u591a\u79cd\u5e73\u53f0\uff08\u684c\u9762\u3001\u79fb\u52a8\u3001\u6d4f\u89c8\u5668\u7b49\uff09\u5de5\u4f5c\u7684GUI\u4ee3\u7406\u6a21\u578b\uff0c\u652f\u6301\u4e91\u8fb9\u534f\u4f5c\u548c\u5b9e\u65f6\u4ea4\u4e92\uff0c\u89e3\u51b3\u73b0\u6709GUI\u4ee3\u7406\u5728\u591a\u5e73\u53f0\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u9650\u5236\u3002", "method": "1. \u6df7\u5408\u6570\u636e\u98de\u8f6e\uff1a\u7ed3\u5408\u6a21\u62df\u73af\u5883\u548c\u4e91\u6c99\u7bb1\u73af\u5883\u6784\u5efaUI\u7406\u89e3\u548c\u8f68\u8ff9\u751f\u6210\u7684\u6570\u636e\u7ba1\u9053\uff1b2. \u7edf\u4e00\u80fd\u529b\u589e\u5f3a\uff1a\u4f7f\u7528\u7edf\u4e00\u601d\u7ef4\u5408\u6210\u7ba1\u9053\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u5de5\u5177/MCP\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u591a\u4ee3\u7406\u9002\u5e94\uff1b3. \u591a\u5e73\u53f0\u73af\u5883RL\u6269\u5c55\uff1a\u63d0\u51faMRPO\u7b97\u6cd5\u89e3\u51b3\u591a\u5e73\u53f0\u51b2\u7a81\u548c\u957f\u89c6\u91ce\u4efb\u52a1\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "result": "\u572820\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\uff1aGUI\u81ea\u52a8\u5316\u4efb\u52a1\uff08OSWorld 56.5\uff0cAndroidWorld 71.6\uff0cWebArena 48.4\uff09\uff0c\u63a5\u5730\u4efb\u52a1\uff08ScreenSpotPro 80.3\uff09\uff0c\u5de5\u5177\u8c03\u7528\u4efb\u52a1\uff08OSWorld-MCP 47.6\uff0cMobileWorld 46.8\uff09\uff0c\u8bb0\u5fc6\u548c\u77e5\u8bc6\u4efb\u52a1\uff08GUI-Knowledge Bench 75.5\uff09\u3002", "conclusion": "GUI-Owl-1.5\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u6536\u96c6\u3001\u80fd\u529b\u589e\u5f3a\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u5e73\u53f0GUI\u4ee3\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6a21\u578b\u5df2\u5f00\u6e90\u5e76\u63d0\u4f9b\u5728\u7ebf\u6f14\u793a\u3002", "topic": "code agent"}}
{"id": "2602.16891", "categories": ["cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16891", "abs": "https://arxiv.org/abs/2602.16891", "authors": ["Hongwei Li", "Zhun Wang", "Qinrun Dai", "Yuzhou Nie", "Jinjun Peng", "Ruitong Liu", "Jingyang Zhang", "Kaijie Zhu", "Jingxuan He", "Lun Wang", "Yangruibo Ding", "Yueqi Chen", "Wenbo Guo", "Dawn Song"], "title": "OpenSage: Self-programming Agent Generation Engine", "comment": null, "summary": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.", "AI": {"tldr": "OpenSage\u662f\u9996\u4e2a\u80fd\u8ba9LLM\u81ea\u52a8\u521b\u5efa\u5177\u6709\u81ea\u751f\u6210\u62d3\u6251\u7ed3\u6784\u548c\u5de5\u5177\u96c6\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u5957\u4ef6\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5185\u5b58\u652f\u6301\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709ADK\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u5f00\u53d1\u5957\u4ef6\u8981\u4e48\u529f\u80fd\u652f\u6301\u4e0d\u8db3\uff0c\u8981\u4e48\u4f9d\u8d56\u4eba\u5de5\u624b\u52a8\u8bbe\u8ba1\u62d3\u6251\u3001\u5de5\u5177\u548c\u5185\u5b58\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6574\u4f53\u6027\u80fd\u3002", "method": "OpenSage\u8ba9LLM\u81ea\u52a8\u521b\u5efa\u5177\u6709\u81ea\u751f\u6210\u62d3\u6251\u7ed3\u6784\u548c\u5de5\u5177\u96c6\u7684\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u5206\u5c42\u56fe\u7ed3\u6784\u5185\u5b58\u7cfb\u7edf\uff0c\u5e76\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e13\u95e8\u5de5\u5177\u5305\u3002", "result": "\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u591a\u79cd\u9aa8\u5e72\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eOpenSage\u4f18\u4e8e\u73b0\u6709ADK\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "OpenSage\u80fd\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u5f00\u53d1\u94fa\u5e73\u9053\u8def\uff0c\u5c06\u7126\u70b9\u4ece\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u4ee5AI\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2602.17045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17045", "abs": "https://arxiv.org/abs/2602.17045", "authors": ["Jared Moore", "Rasmus Overmark", "Ned Cooper", "Beba Cibralic", "Nick Haber", "Cameron R. Jones"], "title": "Large Language Models Persuade Without Planning Theory of Mind", "comment": null, "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8bf4\u670d\u4efb\u52a1\u8bc4\u4f30LLM\u548c\u4eba\u7c7b\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u9700\u8981\u591a\u6b65\u89c4\u5212\u63a8\u65ad\u4ed6\u4eba\u5fc3\u7406\u72b6\u6001\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u4f7f\u7528\u4fee\u8f9e\u7b56\u7565\u8bf4\u670d\u4eba\u7c7b\u65b9\u9762\u4f18\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u73b0\u6709\u5fc3\u667a\u7406\u8bba\u8bc4\u4f30\u4e3b\u8981\u4f7f\u7528\u9759\u6001\u95ee\u7b54\u57fa\u51c6\uff0c\u4f46\u7406\u8bba\u7814\u7a76\u8868\u660e\u7b2c\u4e00\u4eba\u79f0\u4e92\u52a8\u662f\u5fc3\u667a\u7406\u8bba\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u800c\u9884\u6d4b\u6027\u65c1\u89c2\u4efb\u52a1\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u8fd9\u4e00\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u65b0\u9896\u7684\u8bf4\u670d\u4efb\u52a1\uff0c\u8981\u6c42\u8bf4\u670d\u8005\u901a\u8fc7\u7b56\u7565\u6027\u62ab\u9732\u4fe1\u606f\uff0c\u8ba9\u76ee\u6807\u4ece\u4e09\u4e2a\u653f\u7b56\u63d0\u6848\u4e2d\u9009\u62e9\u4e00\u4e2a\u3002\u5b9e\u9a8c\u5206\u4e3a\u5fc3\u7406\u72b6\u6001\"\u63ed\u793a\"\u548c\"\u9690\u85cf\"\u4e24\u79cd\u6761\u4ef6\uff0c\u5728\u9690\u85cf\u6761\u4ef6\u4e0b\u8bf4\u670d\u8005\u9700\u8981\u8be2\u95ee\u6216\u63a8\u65ad\u76ee\u6807\u7684\u5fc3\u7406\u72b6\u6001\u3002\u8fdb\u884c\u4e86\u4e09\u4e2a\u5b9e\u9a8c\uff1a\u5b9e\u9a8c1\u4f7f\u7528\u7406\u6027\u63a8\u7406\u673a\u5668\u4eba\u76ee\u6807\uff0c\u5b9e\u9a8c2\u4f7f\u7528\u4eba\u7c7b\u626e\u6f14\u673a\u5668\u4eba\u76ee\u6807\uff0c\u5b9e\u9a8c3\u6d4b\u91cf\u4eba\u7c7b\u76ee\u6807\u7684\u771f\u5b9e\u4fe1\u5ff5\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c1\u4e2d\uff0cLLM\u5728\u5fc3\u7406\u72b6\u6001\u63ed\u793a\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9690\u85cf\u6761\u4ef6\u4e0b\u8868\u73b0\u4f4e\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u8868\u660e\u5176\u5728\u9700\u8981\u591a\u6b65\u89c4\u5212\u6765\u83b7\u53d6\u548c\u4f7f\u7528\u5fc3\u7406\u72b6\u6001\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u4eba\u7c7b\u5728\u4e24\u4e2a\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u4e2d\u7b49\u3002\u5b9e\u9a8c2\u548c3\u4e2d\uff0cLLM\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u4f18\u4e8e\u4eba\u7c7b\u8bf4\u670d\u8005\uff0c\u8868\u660e\u6709\u6548\u7684\u8bf4\u670d\u53ef\u4ee5\u4e0d\u4f9d\u8d56\u663e\u5f0f\u7684\u5fc3\u667a\u7406\u8bba\u63a8\u7406\uff08\u5982\u901a\u8fc7\u4fee\u8f9e\u7b56\u7565\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8b66\u793a\u4e0d\u8981\u5c06\u7c7b\u4eba\u5fc3\u667a\u7406\u8bba\u5f52\u56e0\u4e8eLLM\uff0c\u540c\u65f6\u7a81\u663e\u4e86LLM\u5728\u5f71\u54cd\u4eba\u4eec\u4fe1\u5ff5\u548c\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6709\u6548\u7684\u8bf4\u670d\u53ef\u4ee5\u901a\u8fc7\u4fee\u8f9e\u7b56\u7565\u5b9e\u73b0\uff0c\u800c\u4e0d\u4e00\u5b9a\u9700\u8981\u663e\u5f0f\u7684\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2602.16943", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16943", "abs": "https://arxiv.org/abs/2602.16943", "authors": ["Arnold Cartagena", "Ariane Teixeira"], "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents", "comment": "23 pages, 5 figures, 4 tables, code and data at https://github.com/acartag7/gap-benchmark", "summary": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4ee3\u7406\u7684\u6587\u672c\u5b89\u5168\u6027\u4e0e\u5de5\u5177\u8c03\u7528\u5b89\u5168\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6587\u672c\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u65f6\u5de5\u5177\u8c03\u7528\u4ecd\u53ef\u80fd\u6267\u884c\u5371\u9669\u64cd\u4f5c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5c42\u9762\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5de5\u5177\u8c03\u7528\uff08\u5177\u6709\u5b9e\u9645\u540e\u679c\u7684\u64cd\u4f5c\uff09\u5b89\u5168\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u8981\u9a8c\u8bc1\u6587\u672c\u5bf9\u9f50\u662f\u5426\u771f\u6b63\u9632\u6b62\u6709\u5bb3\u884c\u52a8\u3002", "method": "\u63d0\u51faGAP\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u57286\u4e2a\u53d7\u76d1\u7ba1\u9886\u57df\uff08\u533b\u836f\u3001\u91d1\u878d\u3001\u6559\u80b2\u3001\u5c31\u4e1a\u3001\u6cd5\u5f8b\u3001\u57fa\u7840\u8bbe\u65bd\uff09\u6d4b\u8bd56\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u6bcf\u4e2a\u9886\u57df7\u79cd\u8d8a\u72f1\u573a\u666f\uff0c3\u79cd\u7cfb\u7edf\u63d0\u793a\u6761\u4ef6\uff0c2\u79cd\u63d0\u793a\u53d8\u4f53\uff0c\u5171\u751f\u621017,420\u4e2a\u53ef\u5206\u6790\u6570\u636e\u70b9\u3002", "result": "\u6587\u672c\u5b89\u5168\u4e0d\u80fd\u8f6c\u79fb\u5230\u5de5\u5177\u8c03\u7528\u5b89\u5168\uff1a\u6240\u6709\u6a21\u578b\u90fd\u51fa\u73b0\u6587\u672c\u62d2\u7edd\u4f46\u5de5\u5177\u8c03\u7528\u6267\u884c\u5371\u9669\u64cd\u4f5c\u7684\u60c5\u51b5\uff1b\u7cfb\u7edf\u63d0\u793a\u5bf9\u5de5\u5177\u8c03\u7528\u884c\u4e3a\u5f71\u54cd\u663e\u8457\uff1b\u8fd0\u884c\u65f6\u6cbb\u7406\u5408\u7ea6\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\u4f46\u65e0\u6cd5\u963b\u6b62\u7981\u6b62\u7684\u5de5\u5177\u8c03\u7528\u5c1d\u8bd5\u3002", "conclusion": "\u4ec5\u6587\u672c\u5b89\u5168\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u4ee3\u7406\u884c\u4e3a\uff0c\u5de5\u5177\u8c03\u7528\u5b89\u5168\u9700\u8981\u4e13\u95e8\u7684\u6d4b\u91cf\u548c\u7f13\u89e3\u63aa\u65bd\uff0c\u7cfb\u7edf\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5de5\u5177\u8c03\u7528\u5b89\u5168\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2602.16901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16901", "abs": "https://arxiv.org/abs/2602.16901", "authors": ["Tanqiu Jiang", "Yuhui Wang", "Jiacheng Liang", "Ting Wang"], "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks", "comment": null, "summary": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.", "AI": {"tldr": "AgentLAB\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5bf9\u81ea\u9002\u5e94\u3001\u957f\u65f6\u7a0b\u653b\u51fb\u8106\u5f31\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u79cd\u65b0\u578b\u653b\u51fb\u7c7b\u578b\u300128\u4e2a\u73b0\u5b9e\u73af\u5883\u548c644\u4e2a\u5b89\u5168\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53d1\u73b0\u73b0\u6709\u667a\u80fd\u4f53\u5bf9\u957f\u65f6\u7a0b\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\u4e14\u5355\u8f6e\u9632\u5fa1\u63aa\u65bd\u65e0\u6548\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u65f6\u7a0b\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u5b83\u4eec\u9762\u4e34\u5229\u7528\u591a\u8f6e\u7528\u6237-\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u7684\u957f\u65f6\u7a0b\u653b\u51fb\u98ce\u9669\uff0c\u8fd9\u4e9b\u653b\u51fb\u5728\u5355\u8f6e\u8bbe\u7f6e\u4e2d\u65e0\u6cd5\u5b9e\u73b0\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u667a\u80fd\u4f53\u5bf9\u6b64\u7c7b\u653b\u51fb\u8106\u5f31\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86AgentLAB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u63015\u79cd\u65b0\u578b\u653b\u51fb\u7c7b\u578b\uff1a\u610f\u56fe\u52ab\u6301\u3001\u5de5\u5177\u94fe\u653b\u51fb\u3001\u4efb\u52a1\u6ce8\u5165\u3001\u76ee\u6807\u6f02\u79fb\u548c\u5185\u5b58\u6c61\u67d3\uff0c\u6db5\u76d628\u4e2a\u73b0\u5b9e\u667a\u80fd\u4f53\u73af\u5883\u548c644\u4e2a\u5b89\u5168\u6d4b\u8bd5\u7528\u4f8b\u3002\u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u4ee3\u8868\u6027LLM\u667a\u80fd\u4f53\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709LLM\u667a\u80fd\u4f53\u5bf9\u957f\u65f6\u7a0b\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u4e14\u4e3a\u5355\u8f6e\u4ea4\u4e92\u8bbe\u8ba1\u7684\u9632\u5fa1\u63aa\u65bd\u65e0\u6cd5\u53ef\u9760\u7f13\u89e3\u957f\u65f6\u7a0b\u5a01\u80c1\u3002\u57fa\u51c6\u6d4b\u8bd5\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "AgentLAB\u53ef\u4f5c\u4e3a\u8ddf\u8e2a\u5b9e\u9645\u573a\u666f\u4e2dLLM\u667a\u80fd\u4f53\u5b89\u5168\u8fdb\u5c55\u7684\u5b9d\u8d35\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u653b\u51fb\u4e0b\u7684\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\u3002", "topic": "agent analysis"}}
{"id": "2602.17084", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17084", "abs": "https://arxiv.org/abs/2602.17084", "authors": ["Kan Watanabe", "Rikuto Tsuchida", "Takahiro Monno", "Bin Huang", "Kazuma Yamasaki", "Youmei Fan", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses", "comment": null, "summary": "The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.", "AI": {"tldr": "\u5bf95\u4e2aAI\u7f16\u7a0b\u4ee3\u7406\u5728GitHub\u4e0a\u521b\u5efa\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u4e0d\u540c\u4ee3\u7406\u5728PR\u63cf\u8ff0\u98ce\u683c\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f1a\u5f71\u54cd\u8bc4\u5ba1\u8005\u53c2\u4e0e\u5ea6\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u5408\u5e76\u7ed3\u679c", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u91c7\u7528\uff0cAI\u7f16\u7a0b\u4ee3\u7406\u5728GitHub\u4e0a\u81ea\u4e3b\u521b\u5efaPR\uff0c\u4f46\u4e0d\u540c\u4ee3\u7406\u5728PR\u63cf\u8ff0\u7279\u5f81\u4e0a\u7684\u5dee\u5f02\u4ee5\u53ca\u4eba\u7c7b\u8bc4\u5ba1\u8005\u5982\u4f55\u54cd\u5e94\u8fd9\u4e9b\u5dee\u5f02\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u5bf95\u4e2aAI\u7f16\u7a0b\u4ee3\u7406\u521b\u5efa\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u5206\u6790PR\u63cf\u8ff0\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u68c0\u67e5\u4eba\u7c7b\u8bc4\u5ba1\u8005\u5728\u8bc4\u5ba1\u6d3b\u52a8\u3001\u54cd\u5e94\u65f6\u95f4\u3001\u60c5\u611f\u548c\u5408\u5e76\u7ed3\u679c\u65b9\u9762\u7684\u54cd\u5e94", "result": "AI\u7f16\u7a0b\u4ee3\u7406\u5c55\u73b0\u51fa\u4e0d\u540c\u7684PR\u63cf\u8ff0\u98ce\u683c\uff0c\u8fd9\u4e9b\u98ce\u683c\u4e0e\u8bc4\u5ba1\u8005\u53c2\u4e0e\u5ea6\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u5408\u5e76\u7ed3\u679c\u7684\u5dee\u5f02\u76f8\u5173\uff1b\u4e0d\u540c\u4ee3\u7406\u5728\u8bc4\u5ba1\u8005\u4ea4\u4e92\u6307\u6807\u548c\u5408\u5e76\u7387\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02", "conclusion": "PR\u5448\u73b0\u65b9\u5f0f\u548c\u8bc4\u5ba1\u8005\u4ea4\u4e92\u52a8\u6001\u5728\u4eba\u7c7b-AI\u534f\u4f5c\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0cAI\u4ee3\u7406\u7684PR\u63cf\u8ff0\u98ce\u683c\u4f1a\u5f71\u54cd\u4eba\u7c7b\u8bc4\u5ba1\u8005\u7684\u54cd\u5e94\u548c\u9879\u76ee\u7ed3\u679c", "topic": "code agent"}}
{"id": "2602.16902", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16902", "abs": "https://arxiv.org/abs/2602.16902", "authors": ["Juliusz Ziomek", "William Bankes", "Lorenz Wolf", "Shyam Sundhar Ramesh", "Xiaohang Tang", "Ilija Bogunovic"], "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs", "comment": null, "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.", "AI": {"tldr": "LLM-Wikirace\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u3001\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u901a\u8fc7\u7ef4\u57fa\u767e\u79d1\u8d85\u94fe\u63a5\u4ece\u6e90\u9875\u9762\u5bfc\u822a\u5230\u76ee\u6807\u9875\u9762\u3002\u524d\u6cbf\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u4eba\u7c7b\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u63a8\u7406\u7cfb\u7edf\u7684\u660e\u663e\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u3001\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u524d\u77bb\u6027\u89c4\u5212\u548c\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u6982\u5ff5\u5173\u8054\u7684\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u3002", "method": "\u521b\u5efaLLM-Wikirace\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u901a\u8fc7\u7ef4\u57fa\u767e\u79d1\u8d85\u94fe\u63a5\u9010\u6b65\u5bfc\u822a\u4ece\u6e90\u9875\u9762\u5230\u76ee\u6807\u9875\u9762\u3002\u8bc4\u4f30\u4e86\u5305\u62ecGemini-3\u3001GPT-5\u548cClaude Opus 4.5\u5728\u5185\u7684\u5e7f\u6cdb\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u4eba\u7c7b\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff08\u6700\u4f73\u6a21\u578bGemini-3\u4ec5\u572823%\u7684\u56f0\u96be\u6e38\u620f\u4e2d\u6210\u529f\uff09\u3002\u4e16\u754c\u77e5\u8bc6\u662f\u6210\u529f\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4f46\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u540e\uff0c\u89c4\u5212\u548c\u957f\u89c6\u91ce\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u4e3b\u5bfc\u56e0\u7d20\u3002\u8f68\u8ff9\u5206\u6790\u663e\u793a\u6700\u5f3a\u6a21\u578b\u5728\u5931\u8d25\u540e\u96be\u4ee5\u91cd\u65b0\u89c4\u5212\uff0c\u7ecf\u5e38\u9677\u5165\u5faa\u73af\u3002", "conclusion": "LLM-Wikirace\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u63a8\u7406\u7cfb\u7edf\u5728\u89c4\u5212\u3001\u957f\u89c6\u91ce\u63a8\u7406\u548c\u5931\u8d25\u6062\u590d\u65b9\u9762\u7684\u660e\u663e\u5c40\u9650\u6027\uff0c\u4e3a\u89c4\u5212\u80fd\u529b\u5f3a\u7684LLMs\u63d0\u4f9b\u4e86\u9700\u8981\u8bc1\u660e\u81ea\u5df1\u7684\u5f00\u653e\u7ade\u6280\u573a\u3002", "topic": "agent analysis"}}
{"id": "2602.16833", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16833", "abs": "https://arxiv.org/abs/2602.16833", "authors": ["Zhicheng Zhang", "Ziyan Wang", "Yali Du", "Fei Fang"], "title": "VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study", "comment": null, "summary": "Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.", "AI": {"tldr": "\u63d0\u51faVerbalized Action Masking (VAM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u5316\u52a8\u4f5c\u63a9\u7801\u548c\u8fed\u4ee3\u52a8\u4f5c\u7a7a\u95f4\u526a\u679d\uff0c\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u74f6\u9888\u95ee\u9898\uff0c\u5728\u8c61\u68cb\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\uff0c\u7a00\u758f\u53cd\u9988\u548c\u5de8\u5927\u52a8\u4f5c\u7a7a\u95f4\u5bfc\u81f4\u63a2\u7d22\u56f0\u96be\uff0c\u5bb9\u6613\u9677\u5165\u91cd\u590d\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u662fRL\u8bad\u7ec3\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51faVerbalized Action Masking (VAM)\uff1a1) \u5728\u63d0\u793a\u4e2d\u8bed\u8a00\u5316\u52a8\u4f5c\u63a9\u7801\uff0c\u5f3a\u5236\u6a21\u578b\u4ece\u63a9\u7801\u96c6\u5408\u4e2d\u8f93\u51fa\u52a8\u4f5c\uff1b2) \u57fa\u4e8e\u6b64\u63a5\u53e3\u5f15\u5165\u8fed\u4ee3\u52a8\u4f5c\u7a7a\u95f4\u526a\u679d\uff1a\u5982\u679c\u672a\u91c7\u6837\u5230\u76ee\u6807\u52a8\u4f5c\uff0c\u5219\u4ece\u63a9\u7801\u4e2d\u79fb\u9664\u5df2\u91c7\u6837\u7684\u6709\u6548\u52a8\u4f5c\uff0c\u5728\u7f29\u51cf\u7684\u5019\u9009\u96c6\u4e0b\u91cd\u65b0\u91c7\u6837\uff0c\u91cd\u590d\u76f4\u5230\u91c7\u6837\u5230\u76ee\u6807\u6216\u8fbe\u5230\u56fa\u5b9a\u9884\u7b97\u3002", "result": "\u5728\u8c61\u68cb\u4efb\u52a1\u4e2d\u8bc4\u4f30VAM\uff0c\u91c7\u7528\u4e24\u79cd\u8bad\u7ec3\u673a\u5236\uff1a\u5f15\u64ce\u5bf9\u5f08\u673a\u5236\uff08\u901a\u8fc7\u4e0e\u5f15\u64ce\u5bf9\u624b\u5bf9\u5f08\u751f\u6210\u72b6\u6001\uff09\u548c\u56fa\u5b9a\u6570\u636e\u96c6\u673a\u5236\uff08\u4ece\u5e26\u6709\u9a8c\u8bc1\u5668\u5206\u6570\u7684\u56fa\u5b9a\u4f4d\u7f6e\u6570\u636e\u96c6\u8bad\u7ec3\uff09\u3002\u5728\u4fdd\u7559\u7684\u8c61\u68cb\u8c1c\u9898\u548c\u5b8c\u6574\u5bf9\u5f08\uff08\u901a\u8fc7\u5e73\u5747\u767e\u6b65\u635f\u5931ACPL\u8861\u91cf\uff09\u4e2d\uff0cVAM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u8bed\u8a00\u5316\u63a9\u7801\u662fLLM RL\u540e\u8bad\u7ec3\u4e2d\u53ef\u63a7\u63a2\u7d22\u7684\u5b9e\u7528\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u63a2\u7d22\u74f6\u9888\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16953", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16953", "abs": "https://arxiv.org/abs/2602.16953", "authors": ["Hejia Zhang", "Zhongming Yu", "Chia-Tung Ho", "Haoxing Ren", "Brucek Khailany", "Jishen Zhao"], "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation", "comment": null, "summary": "Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.", "AI": {"tldr": "LLM4Cov\uff1a\u9488\u5bf9\u786c\u4ef6\u9a8c\u8bc1\u4efb\u52a1\u7684\u79bb\u7ebf\u4ee3\u7406\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u9a8c\u8bc1\u6570\u636e\u7ba1\u7406\u3001\u7b56\u7565\u611f\u77e5\u6570\u636e\u5408\u6210\u548c\u6700\u5dee\u72b6\u6001\u4f18\u5148\u91c7\u6837\uff0c\u5728\u6709\u9650\u6267\u884c\u53cd\u9988\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60", "motivation": "\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u7684LLM\u4ee3\u7406\u5b66\u4e60\u901a\u5e38\u9700\u8981\u6602\u8d35\u4e14\u7f13\u6162\u7684\u6267\u884c\u53cd\u9988\uff0c\u4f7f\u5f97\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0d\u5207\u5b9e\u9645\u3002\u786c\u4ef6\u9a8c\u8bc1\u4efb\u52a1\u5c24\u5176\u9762\u4e34\u8fd9\u4e00\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u5de5\u4e1a\u6a21\u62df\u5668\u4e14\u6267\u884c\u4fe1\u53f7\u4e0d\u53ef\u5fae\u5206\u3002", "method": "\u63d0\u51faLLM4Cov\u79bb\u7ebf\u4ee3\u7406\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7531\u786e\u5b9a\u6027\u8bc4\u4f30\u5668\u5f15\u5bfc\u7684\u65e0\u8bb0\u5fc6\u72b6\u6001\u8f6c\u79fb\u3002\u91c7\u7528\u6267\u884c\u9a8c\u8bc1\u6570\u636e\u7ba1\u7406\u3001\u7b56\u7565\u611f\u77e5\u4ee3\u7406\u6570\u636e\u5408\u6210\u548c\u6700\u5dee\u72b6\u6001\u4f18\u5148\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u6267\u884c\u7ea6\u675f\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u5b66\u4e60\u3002", "result": "\u4ec54B\u53c2\u6570\u7684\u7d27\u51d1\u6a21\u578b\u5728\u4ee3\u7406\u8bc4\u4f30\u4e0b\u8fbe\u523069.2%\u7684\u8986\u76d6\u7387\u901a\u8fc7\u7387\uff0c\u6bd4\u5176\u6559\u5e08\u6a21\u578b\u63d0\u53475.3%\uff0c\u4e14\u6027\u80fd\u53ef\u4e0e\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u6a21\u578b\u7ade\u4e89\u3002\u8fd8\u521b\u5efa\u4e86\u57fa\u4e8e\u73b0\u6709\u9a8c\u8bc1\u5957\u4ef6\u7684\u73b0\u5b9e\u5bf9\u9f50\u57fa\u51c6\u3002", "conclusion": "LLM4Cov\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u786c\u4ef6\u9a8c\u8bc1\u4e2d\u6267\u884c\u53cd\u9988\u6602\u8d35\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u6709\u9650\u6267\u884c\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4ee3\u7406\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.16839", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16839", "abs": "https://arxiv.org/abs/2602.16839", "authors": ["Zeliang Zhang", "Xiaodong Liu", "Hao Cheng", "Hao Sun", "Chenliang Xu", "Jianfeng Gao"], "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding", "comment": "ICLR 2026, 15 pages", "summary": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.", "AI": {"tldr": "\u63d0\u51faProgressive Thought Encoding\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u7f16\u7801\u4e2d\u95f4\u63a8\u7406\u5230\u56fa\u5b9a\u5927\u5c0f\u5411\u91cf\u8868\u793a\uff0c\u5728\u56fa\u5b9a\u7f13\u5b58\u5927\u5c0f\u4e0b\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9700\u8981\u957f\u5e8f\u5217\u5c55\u5f00\uff0c\u81ea\u56de\u5f52\u89e3\u7801\u5360\u7528\u5927\u91cf\u65f6\u95f4\u548c\u5185\u5b58\u3002\u6ed1\u52a8\u7a97\u53e3\u7f13\u5b58\u7b56\u7565\u867d\u7136\u80fd\u9650\u5236\u5185\u5b58\uff0c\u4f46\u4f1a\u7834\u574f\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faProgressive Thought Encoding\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u7f16\u7801\u4e2d\u95f4\u63a8\u7406\u5230\u56fa\u5b9a\u5927\u5c0f\u7684\u5411\u91cf\u8868\u793a\uff0c\u6d88\u9664\u5bf9\u5b8c\u6574\u7f13\u5b58\u5c55\u5f00\u7684\u53cd\u5411\u4f20\u64ad\u9700\u6c42\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u5728\u63a8\u7406\u65f6\u4fdd\u6301\u6052\u5b9a\u5185\u5b58\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\uff08Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, DeepSeek-R1-Distill-Llama-8B\uff09\u548c\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4LoRA\u5fae\u8c03\u5e73\u5747\u63d0\u534719.3%\uff0c\u6bd4\u672a\u5fae\u8c03\u7684LRM\u63d0\u534729.9%\uff0c\u5728AIME2024/2025\u4e0a\u6700\u9ad8\u63d0\u534723.4%\u51c6\u786e\u7387\u3002", "conclusion": "Progressive Thought Encoding\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8fd8\u5728\u5b9e\u9645\u5185\u5b58\u7ea6\u675f\u4e0b\u4f7fLRM\u7684RL\u8bad\u7ec3\u66f4\u52a0\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16958", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16958", "abs": "https://arxiv.org/abs/2602.16958", "authors": ["Xinhao Deng", "Jiaqing Wu", "Miao Chen", "Yue Xiao", "Ke Xu", "Qi Li"], "title": "Automating Agent Hijacking via Structural Template Injection", "comment": null, "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.", "AI": {"tldr": "Phantom\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u4ee3\u7406\u52ab\u6301\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u677f\u6ce8\u5165\u653b\u51fbLLM\u4ee3\u7406\u7684\u67b6\u6784\u673a\u5236\uff0c\u5229\u7528\u804a\u5929\u6a21\u677f\u4ee4\u724c\u5206\u79bb\u673a\u5236\u8bf1\u5bfc\u89d2\u8272\u6df7\u6dc6\uff0c\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u548c\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u52ab\u6301\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u5236\u4f5c\u7684\u8bed\u4e49\u9a71\u52a8\u63d0\u793a\u64cd\u7eb5\uff0c\u653b\u51fb\u6210\u529f\u7387\u4f4e\u4e14\u5bf9\u95ed\u6e90\u5546\u4e1a\u6a21\u578b\u7684\u8fc1\u79fb\u6027\u6709\u9650\u3002OWASP\u5c06\u4ee3\u7406\u52ab\u6301\u89c6\u4e3aLLM\u751f\u6001\u7cfb\u7edf\u7684\u5173\u952e\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPhantom\u6846\u67b6\uff0c\u57fa\u4e8e\u7ed3\u6784\u5316\u6a21\u677f\u6ce8\u5165\u653b\u51fbLLM\u4ee3\u7406\u7684\u67b6\u6784\u673a\u5236\u3002\u901a\u8fc7\u591a\u7ea7\u6a21\u677f\u589e\u5f3a\u589e\u52a0\u7ed3\u6784\u591a\u6837\u6027\uff0c\u8bad\u7ec3\u6a21\u677f\u81ea\u7f16\u7801\u5668\u5c06\u79bb\u6563\u6a21\u677f\u5d4c\u5165\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u8bc6\u522b\u6700\u4f18\u5bf9\u6297\u5411\u91cf\u5e76\u89e3\u7801\u4e3a\u9ad8\u6548\u7ed3\u6784\u5316\u6a21\u677f\u3002", "result": "\u5728Qwen\u3001GPT\u548cGemini\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u67e5\u8be2\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u5728\u771f\u5b9e\u5546\u4e1a\u4ea7\u54c1\u4e2d\u8bc6\u522b\u51fa70\u591a\u4e2a\u6f0f\u6d1e\uff0c\u5df2\u83b7\u5382\u5546\u786e\u8ba4\u3002", "conclusion": "\u7ed3\u6784\u5316\u6a21\u677f\u52ab\u6301\u5177\u6709\u5b9e\u9645\u4e25\u91cd\u6027\uff0c\u4e3a\u4fdd\u62a4\u4e0b\u4e00\u4ee3\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u9488\u5bf9LLM\u4ee3\u7406\u67b6\u6784\u673a\u5236\u7684\u81ea\u52a8\u5316\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.17316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17316", "abs": "https://arxiv.org/abs/2602.17316", "authors": ["Bogdan Kosti\u0107", "Conor Fallon", "Julian Risch", "Alexander L\u00f6ser"], "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation", "comment": "Accepted at LREC 2026", "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8bc4\u4f30\u57fa\u51c6\u5bf9\u8f93\u5165\u63d0\u793a\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u8bcd\u6c47\u6270\u52a8\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u53e5\u6cd5\u6270\u52a8\u5f71\u54cd\u66f4\u5f02\u8d28\uff0c\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u89c4\u6a21\u65e0\u5173\uff0c\u63ed\u793a\u4e86LLM\u5bf9\u8868\u9762\u8bcd\u6c47\u6a21\u5f0f\u7684\u4f9d\u8d56\u800c\u975e\u62bd\u8c61\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\u7684\u53ef\u9760\u6027\u53d7\u5230\u8d28\u7591\uff0c\u56e0\u4e3a\u6a21\u578b\u5bf9\u8f93\u5165\u63d0\u793a\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u63a7\u5236\u6027\u7684\u3001\u771f\u503c\u6761\u4ef6\u7b49\u4ef7\u7684\u8bcd\u6c47\u548c\u53e5\u6cd5\u6270\u52a8\u5982\u4f55\u5f71\u54cdLLM\u7684\u7edd\u5bf9\u6027\u80fd\u548c\u76f8\u5bf9\u6392\u540d\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u8bed\u8a00\u5b66\u539f\u5219\u7684\u7ba1\u9053\u751f\u6210\u610f\u4e49\u4fdd\u6301\u7684\u53d8\u4f53\uff1a1\uff09\u540c\u4e49\u8bcd\u66ff\u6362\u8fdb\u884c\u8bcd\u6c47\u53d8\u5316\uff1b2\uff09\u4f9d\u8d56\u89e3\u6790\u786e\u5b9a\u9002\u7528\u7684\u53e5\u6cd5\u8f6c\u6362\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\uff08MMLU\u3001SQuAD\u3001AMEGA\uff09\u4e0a\u6d4b\u8bd523\u4e2a\u5f53\u4ee3LLM\u3002", "result": "\u8bcd\u6c47\u6270\u52a8\u5728\u6240\u6709\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u4e00\u81f4\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff1b\u53e5\u6cd5\u6270\u52a8\u5f71\u54cd\u66f4\u5f02\u8d28\uff0c\u6709\u65f6\u751a\u81f3\u6539\u5584\u7ed3\u679c\u3002\u4e24\u79cd\u6270\u52a8\u90fd\u4f1a\u7834\u574f\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6a21\u578b\u6392\u884c\u699c\u7a33\u5b9a\u6027\u3002\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u89c4\u6a21\u65e0\u5173\uff0c\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4efb\u52a1\u4f9d\u8d56\u6027\u3002", "conclusion": "LLM\u66f4\u4f9d\u8d56\u8868\u9762\u8bcd\u6c47\u6a21\u5f0f\u800c\u975e\u62bd\u8c61\u8bed\u8a00\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u5c06\u9c81\u68d2\u6027\u6d4b\u8bd5\u4f5c\u4e3aLLM\u8bc4\u4f30\u6807\u51c6\u7ec4\u6210\u90e8\u5206\u7684\u5fc5\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.17016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17016", "abs": "https://arxiv.org/abs/2602.17016", "authors": ["Zichen Wang", "Wanli Ma", "Zhenyu Ming", "Gong Zhang", "Kun Yuan", "Zaiwen Wen"], "title": "M2F: Automated Formalization of Mathematical Literature at Scale", "comment": null, "summary": "Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.", "AI": {"tldr": "M2F\u662f\u9996\u4e2a\u7528\u4e8e\u9879\u76ee\u7ea7\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u53ef\u5c06\u957f\u7bc7\u6570\u5b66\u8d44\u6599\u8f6c\u6362\u4e3a\u53ef\u7f16\u8bd1\u7684Lean\u4ee3\u7801\u5e93\uff0c\u5b9e\u73b0\u6559\u79d1\u4e66\u7ea7\u5f62\u5f0f\u5316\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u6280\u672f\u5c40\u9650\u4e8e\u5b64\u7acb\u5b9a\u7406\u548c\u77ed\u7247\u6bb5\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u6559\u79d1\u4e66\u548c\u7814\u7a76\u8bba\u6587\u7ea7\u522b\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u6587\u4ef6\u4f9d\u8d56\u3001\u5bfc\u5165\u89e3\u6790\u548c\u7aef\u5230\u7aef\u9879\u76ee\u7f16\u8bd1\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u8bed\u53e5\u7f16\u8bd1\u9636\u6bb5\u5c06\u6587\u6863\u62c6\u5206\u4e3a\u539f\u5b50\u5757\uff0c\u901a\u8fc7\u63a8\u65ad\u4f9d\u8d56\u6392\u5e8f\uff0c\u4fee\u590d\u58f0\u660e\u9aa8\u67b6\u76f4\u5230\u9879\u76ee\u53ef\u7f16\u8bd1\uff1b2) \u8bc1\u660e\u4fee\u590d\u9636\u6bb5\u5728\u56fa\u5b9a\u7b7e\u540d\u4e0b\u4f7f\u7528\u76ee\u6807\u5bfc\u5411\u7684\u5c40\u90e8\u7f16\u8f91\u586b\u8865\u8bc1\u660e\u7a7a\u7f3a\u3002\u6574\u4e2a\u8fc7\u7a0b\u4fdd\u6301\u9a8c\u8bc1\u5668\u5728\u5faa\u73af\u4e2d\uff0c\u53ea\u6709\u5de5\u5177\u94fe\u53cd\u9988\u786e\u8ba4\u6539\u8fdb\u65f6\u624d\u63d0\u4ea4\u7f16\u8f91\u3002", "result": "\u5728\u7ea6\u4e09\u5468\u5185\u5c06479\u9875\u7684\u5b9e\u5206\u6790\u548c\u51f8\u5206\u6790\u6559\u79d1\u4e66\u8f6c\u6362\u4e3a153,853\u884cLean\u4ee3\u7801\u5e93\uff0c\u5b9e\u73b0\u6559\u79d1\u4e66\u7ea7\u5f62\u5f0f\u5316\u3002\u5728FATE-H\u57fa\u51c6\u4e0a\u8fbe\u523096%\u7684\u8bc1\u660e\u6210\u529f\u7387\uff08\u57fa\u7ebf\u4e3a80%\uff09\u3002", "conclusion": "\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u6570\u5b66\u6587\u732e\u5f62\u5f0f\u5316\u5df2\u5177\u5907\u53ef\u884c\u6027\uff0cM2F\u6846\u67b6\u80fd\u4ee5\u8fdc\u8d85\u4e13\u5bb6\u6548\u7387\u7684\u901f\u5ea6\u5b9e\u73b0\u6559\u79d1\u4e66\u7ea7\u5f62\u5f0f\u5316\u3002", "topic": "code agent"}}
{"id": "2602.17017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17017", "abs": "https://arxiv.org/abs/2602.17017", "authors": ["Deepanjan Bhol"], "title": "Sales Research Agent and Sales Research Bench", "comment": "Technical report. 2 figures. Microsoft Dynamics 365 Sales", "summary": "Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.", "AI": {"tldr": "\u5fae\u8f6fDynamics 365 Sales\u4e2d\u7684Sales Research Agent\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684CRM\u5206\u6790\u5e94\u7528\uff0c\u901a\u8fc7Sales Research Bench\u57fa\u51c6\u6d4b\u8bd5\u57288\u4e2a\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\u8d28\u91cf\uff0c\u5728\u5b9a\u5236\u4f01\u4e1a\u6a21\u5f0f\u4e0a\u663e\u8457\u4f18\u4e8eClaude Sonnet 4.5\u548cChatGPT-5\u3002", "motivation": "\u4f01\u4e1a\u9700\u8981\u80fd\u591f\u57fa\u4e8e\u5b9e\u65f6\u5b9a\u5236CRM\u6570\u636e\u56de\u7b54\u9500\u552e\u9886\u5bfc\u95ee\u9898\u7684AI\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u8d28\u91cf\u8bc1\u636e\u3002", "method": "\u5f00\u53d1Sales Research Agent\u5e94\u7528\uff0c\u8fde\u63a5\u5b9e\u65f6CRM\u548c\u76f8\u5173\u6570\u636e\uff0c\u5728\u590d\u6742\u6a21\u5f0f\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u751f\u6210\u6587\u672c\u548c\u56fe\u8868\u8f93\u51fa\u3002\u521b\u5efaSales Research Bench\u57fa\u51c6\uff0c\u57288\u4e2a\u5ba2\u6237\u52a0\u6743\u7684\u7ef4\u5ea6\u4e0a\u8bc4\u5206\u7cfb\u7edf\u3002", "result": "\u57282025\u5e7410\u670819\u65e5\u5bf9\u5b9a\u5236\u4f01\u4e1a\u6a21\u5f0f\u7684200\u4e2a\u95ee\u9898\u6d4b\u8bd5\u4e2d\uff0cSales Research Agent\u5728100\u5206\u7efc\u5408\u5f97\u5206\u4e0a\u6bd4Claude Sonnet 4.5\u9ad813\u5206\uff0c\u6bd4ChatGPT-5\u9ad824.1\u5206\u3002", "conclusion": "Sales Research Agent\u4e3aCRM\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7Sales Research Bench\u57fa\u51c6\u4e3a\u5ba2\u6237\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u6bd4\u8f83AI\u89e3\u51b3\u65b9\u6848\u7684\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2602.17038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17038", "abs": "https://arxiv.org/abs/2602.17038", "authors": ["Shengtian Yang", "Yu Li", "Shuo He", "Yewen Li", "Qingpeng Cai", "Peng Jiang", "Lei Feng"], "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning", "comment": "16 pages", "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \\textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \\emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.", "AI": {"tldr": "\u63d0\u51faPA-MoE\u65b9\u6cd5\u89e3\u51b3RL\u4e2d\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u5bfc\u81f4\u7684\u7b80\u5355\u4efb\u52a1\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u8ba9\u4e0d\u540c\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u4e0d\u540c\u4efb\u52a1\u9636\u6bb5\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u5bfc\u81f4\u7b80\u5355\u4efb\u52a1\u5360\u636e\u5927\u90e8\u5206\u53c2\u6570\u5e76\u4e3b\u5bfc\u68af\u5ea6\u66f4\u65b0\uff0c\u590d\u6742\u4efb\u52a1\u5f97\u4e0d\u5230\u8db3\u591f\u5bb9\u91cf\u3002\u4f20\u7edfMoE\u7684token\u7ea7\u8def\u7531\u4f1a\u5206\u6563\u76f8\u4f4d\u4e00\u81f4\u6a21\u5f0f\uff0c\u7834\u574f\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "method": "\u63d0\u51fa\u76f8\u4f4d\u611f\u77e5\u4e13\u5bb6\u6df7\u5408(PA-MoE)\uff1a1) \u8f7b\u91cf\u7ea7\u76f8\u4f4d\u8def\u7531\u5668\u76f4\u63a5\u4eceRL\u76ee\u6807\u5b66\u4e60\u6f5c\u5728\u76f8\u4f4d\u8fb9\u754c\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u76f8\u4f4d\u7c7b\u522b\uff1b2) \u76f8\u4f4d\u8def\u7531\u5668\u4e3a\u76f8\u540c\u4e13\u5bb6\u5206\u914d\u65f6\u95f4\u4e00\u81f4\u7684\u5206\u914d\uff0c\u8ba9\u4e13\u5bb6\u4fdd\u6301\u76f8\u4f4d\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86PA-MoE\u7684\u6709\u6548\u6027\u3002", "conclusion": "PA-MoE\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u8def\u7531\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u5728RL\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u4efb\u52a1\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17445", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17445", "abs": "https://arxiv.org/abs/2602.17445", "authors": ["Mateusz Nowak", "Xavier Cadet", "Peter Chin"], "title": "ABCD: All Biases Come Disguised", "comment": "29 pages, 20 figures, pre-print, 12 tables", "summary": "Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11\u591a\u9009\u9898\u8bc4\u4f30\u4e2d\u6807\u7b7e\u4f4d\u7f6e\u504f\u89c1\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7edf\u4e00\u7684\u3001\u65e0\u5e8f\u7684\u6807\u7b7e\u5e76\u8ba9LLM\u4f7f\u7528\u5b8c\u6574\u7b54\u6848\uff0c\u7ed3\u5408\u53e5\u5b50\u76f8\u4f3c\u5ea6\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u591a\u9009\u9898\u8bc4\u4f30\u4e2d\u5b58\u5728\u6807\u7b7e\u4f4d\u7f6e\u504f\u89c1\uff0c\u5305\u62ec\u7b54\u6848\u4f4d\u7f6e\u504f\u89c1\u3001\u7b54\u6848\u524d\u6807\u7b7e\u504f\u89c1\u3001\u5c11\u6837\u672c\u63d0\u793a\u4e2d\u6b63\u786e\u7b54\u6848\u5206\u5e03\u504f\u89c1\u7b49\uff0c\u8fd9\u4e9b\u504f\u89c1\u5f71\u54cd\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u89c1\u51cf\u5c11\u7684\u8bc4\u4f30\u534f\u8bae\uff1a1) \u5c06\u6bcf\u4e2a\u95ee\u9898\u7684\u6807\u7b7e\u66ff\u6362\u4e3a\u7edf\u4e00\u7684\u3001\u65e0\u5e8f\u7684\u6807\u7b7e\uff1b2) \u63d0\u793aLLM\u4f7f\u7528\u5b8c\u6574\u7b54\u6848\uff1b3) \u4f7f\u7528\u7b80\u5355\u7684\u53e5\u5b50\u76f8\u4f3c\u5ea6\u6a21\u578b\u6765\u5339\u914dLLM\u8f93\u51fa\u4e0e\u7b54\u6848\u9009\u9879\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u7b54\u6848\u6392\u5217\u7684\u9c81\u68d2\u6027\uff0c\u5c06\u5e73\u5747\u51c6\u786e\u7387\u65b9\u5dee\u964d\u4f4e\u4e863\u500d\uff0c\u540c\u65f6\u6a21\u578b\u6027\u80fd\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\u3002\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bc4\u4f30\u534f\u8bae\u80fd\u591f\u6709\u6548\u51cf\u5c11LLM\u5728\u591a\u9009\u9898\u8bc4\u4f30\u4e2d\u7684\u6807\u7b7e\u4f4d\u7f6e\u504f\u89c1\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u80fd\u529b\u8bc4\u4f30\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u63d0\u793a\u793a\u4f8b\u6216\u9009\u9879\u6807\u7b7e\u7684\u5e2e\u52a9\u3002", "topic": "agent analysis"}}
{"id": "2602.17046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17046", "abs": "https://arxiv.org/abs/2602.17046", "authors": ["Uria Franko"], "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs", "comment": null, "summary": "Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.", "AI": {"tldr": "ITR\u901a\u8fc7\u68c0\u7d22\u5f0f\u65b9\u6cd5\u52a8\u6001\u6784\u5efa\u7cfb\u7edf\u63d0\u793a\u548c\u5de5\u5177\u96c6\uff0c\u5c06\u6bcf\u6b65\u4e0a\u4e0b\u6587token\u51cf\u5c1195%\uff0c\u5de5\u5177\u8def\u7531\u51c6\u786e\u7387\u63d0\u534732%\uff0c\u7aef\u5230\u7aef\u6210\u672c\u964d\u4f4e70%\uff0c\u4f7f\u4ee3\u7406\u80fd\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u8fd0\u884c2-20\u500d\u66f4\u591a\u5faa\u73af\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u8fd0\u884c\u65f6\u9700\u8981\u53cd\u590d\u52a0\u8f7d\u5197\u957f\u7684\u7cfb\u7edf\u6307\u4ee4\u548c\u5927\u578b\u5de5\u5177\u76ee\u5f55\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3001\u4ee3\u7406\u504f\u79bb\u6982\u7387\u63d0\u9ad8\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u5de5\u5177\u9009\u62e9\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u6bcf\u6b65\u7684\u4e0a\u4e0b\u6587\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u6307\u4ee4-\u5de5\u5177\u68c0\u7d22\uff08ITR\uff09\uff0c\u4e00\u79cdRAG\u53d8\u4f53\uff0c\u5728\u6bcf\u4e00\u6b65\u68c0\u7d22\u6700\u5c0f\u5316\u7684\u7cfb\u7edf\u63d0\u793a\u7247\u6bb5\u548c\u5fc5\u8981\u5de5\u5177\u5b50\u96c6\uff0c\u52a8\u6001\u6784\u5efa\u8fd0\u884c\u65f6\u7cfb\u7edf\u63d0\u793a\uff0c\u5e76\u66b4\u9732\u7ecf\u8fc7\u7f29\u5c0f\u7684\u5de5\u5177\u96c6\uff0c\u914d\u5907\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u56de\u9000\u673a\u5236\u3002", "result": "\u4f7f\u7528\u5185\u90e8\u4e00\u81f4\u6570\u5b57\u7684\u53d7\u63a7\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cITR\u5c06\u6bcf\u6b65\u4e0a\u4e0b\u6587token\u51cf\u5c1195%\uff0c\u6b63\u786e\u5de5\u5177\u8def\u7531\u76f8\u5bf9\u63d0\u534732%\uff0c\u7aef\u5230\u7aef\u6210\u672c\u6bd4\u5355\u4f53\u57fa\u7ebf\u964d\u4f4e70%\uff0c\u4f7f\u4ee3\u7406\u80fd\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u8fd0\u884c2-20\u500d\u66f4\u591a\u5faa\u73af\u3002", "conclusion": "ITR\u7279\u522b\u9002\u7528\u4e8e\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u968f\u7740\u4ee3\u7406\u6b65\u6570\u589e\u52a0\uff0c\u8282\u7701\u6548\u679c\u4f1a\u7d2f\u79ef\u3002\u8bba\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u3001\u6d88\u878d\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u64cd\u4f5c\u6307\u5357\u3002", "topic": "agent analysis"}}
{"id": "2602.17053", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17053", "abs": "https://arxiv.org/abs/2602.17053", "authors": ["Yunseok Han", "Yejoon Lee", "Jaeyoung Do"], "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models", "comment": "Accepted in ICLR 2026 Poster: $\\href{https://iclr.cc/virtual/2026/poster/10011763}{\\text{this URL}}$", "summary": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u5fe0\u5b9e\u6027\u7684\u6846\u67b6RFEval\uff0c\u53d1\u73b049.7%\u8f93\u51fa\u4e0d\u5fe0\u5b9e\uff0c\u51c6\u786e\u7387\u4e0d\u80fd\u66ff\u4ee3\u5fe0\u5b9e\u6027\u8bc4\u4f30\uff0cRL\u8bad\u7ec3\u53ef\u80fd\u635f\u5bb3\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5e38\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u51b3\u7b56\u8fc7\u7a0b\u7684\u63a8\u7406\uff0c\u8fd9\u635f\u5bb3\u4e86\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u9700\u8981\u5efa\u7acb\u6b63\u5f0f\u6846\u67b6\u6765\u8bc4\u4f30\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e24\u4e2a\u53ef\u6d4b\u8bd5\u6761\u4ef6\u7684\u5fe0\u5b9e\u6027\u6846\u67b6\uff1a\u7acb\u573a\u4e00\u81f4\u6027\u548c\u56e0\u679c\u5f71\u54cd\u3002\u5f00\u53d1RFEval\u57fa\u51c6\uff0c\u5305\u542b7,186\u4e2a\u5b9e\u4f8b\uff0c\u901a\u8fc7\u8f93\u51fa\u7ea7\u53cd\u4e8b\u5b9e\u5e72\u9884\u6765\u63a2\u6d4b\u5fe0\u5b9e\u6027\u3002", "result": "\u8bc4\u4f3012\u4e2a\u5f00\u6e90LRM\u53d1\u73b049.7%\u8f93\u51fa\u4e0d\u5fe0\u5b9e\uff0c\u4e3b\u8981\u6e90\u4e8e\u7acb\u573a\u4e0d\u4e00\u81f4\u3002\u5931\u8d25\u96c6\u4e2d\u5728\u6570\u5b66\u548c\u4ee3\u7801\u7b49\u8106\u5f31\u6536\u655b\u9886\u57df\u3002RL\u8bad\u7ec3\u53ef\u80fd\u964d\u4f4e\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "conclusion": "\u51c6\u786e\u7387\u4e0d\u80fd\u4f5c\u4e3a\u5fe0\u5b9e\u6027\u7684\u53ef\u9760\u4ee3\u7406\u3002\u9700\u8981\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u7684\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6b63\u786e\u7ed3\u679c\u3002\u5efa\u7acb\u4e86\u5ba1\u8ba1LRM\u53ef\u9760\u6027\u7684\u4e25\u8c28\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.16977", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16977", "abs": "https://arxiv.org/abs/2602.16977", "authors": ["Zachary Coalson", "Beth Sohler", "Aiden Gabriel", "Sanghyun Hong"], "title": "Fail-Closed Alignment for Large Language Models", "comment": "Pre-print", "summary": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.", "AI": {"tldr": "\u63d0\u51faLLM\u5bf9\u9f50\u7684\u7ed3\u6784\u6027\u5f31\u70b9\uff1a\u73b0\u6709\u62d2\u7edd\u673a\u5236\u662f\"\u6545\u969c\u5f00\u653e\"\u7684\uff0c\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u53ef\u6291\u5236\u5355\u4e00\u4e3b\u5bfc\u7279\u5f81\u5bfc\u81f4\u5b89\u5168\u5931\u6548\u3002\u63d0\u51fa\"\u6545\u969c\u95ed\u5408\"\u5bf9\u9f50\u539f\u5219\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u6846\u67b6\u8ba9\u6a21\u578b\u5b66\u4e60\u591a\u4e2a\u56e0\u679c\u72ec\u7acb\u7684\u62d2\u7edd\u65b9\u5411\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u673a\u5236\u5b58\u5728\u7ed3\u6784\u6027\u5f31\u70b9\uff1a\u62d2\u7edd\u673a\u5236\u662f\"\u6545\u969c\u5f00\u653e\"\u7684\uff0c\u62d2\u7edd\u884c\u4e3a\u7f16\u7801\u5728\u591a\u4e2a\u6f5c\u5728\u7279\u5f81\u4e2d\uff0c\u4f46\u6291\u5236\u5355\u4e00\u4e3b\u5bfc\u7279\u5f81\u5c31\u80fd\u5bfc\u81f4\u5b89\u5168\u5931\u6548\u3002\u8fd9\u79cd\u8106\u5f31\u6027\u4f7f\u5f97\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5bb9\u6613\u6210\u529f\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u6846\u67b6\uff1a\u8fed\u4ee3\u8bc6\u522b\u548c\u6d88\u878d\u5148\u524d\u5b66\u4e60\u7684\u62d2\u7edd\u65b9\u5411\uff0c\u8feb\u4f7f\u6a21\u578b\u5728\u65b0\u7684\u72ec\u7acb\u5b50\u7a7a\u95f4\u4e2d\u91cd\u5efa\u5b89\u5168\u673a\u5236\u3002\u901a\u8fc7\u521b\u5efa\u591a\u4e2a\u56e0\u679c\u72ec\u7acb\u7684\u62d2\u7edd\u8def\u5f84\uff0c\u786e\u4fdd\u5373\u4f7f\u90e8\u5206\u7279\u5f81\u88ab\u6291\u5236\uff0c\u5b89\u5168\u673a\u5236\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "result": "\u5728\u56db\u79cd\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5f3a\u7684\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u7f13\u89e3\u4e86\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u8ba1\u7b97\u5f00\u9500\u8f83\u5c0f\u3002\u673a\u5236\u5206\u6790\u8bc1\u5b9e\u6a21\u578b\u7f16\u7801\u4e86\u591a\u4e2a\u56e0\u679c\u72ec\u7acb\u7684\u62d2\u7edd\u65b9\u5411\u3002", "conclusion": "\"\u6545\u969c\u95ed\u5408\"\u5bf9\u9f50\u4e3a\u9c81\u68d2LLM\u5b89\u5168\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u901a\u8fc7\u591a\u4e2a\u72ec\u7acb\u56e0\u679c\u8def\u5f84\u786e\u4fdd\u62d2\u7edd\u673a\u5236\u5728\u90e8\u5206\u5931\u6548\u65f6\u4ecd\u80fd\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.17062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17062", "abs": "https://arxiv.org/abs/2602.17062", "authors": ["Yonghyeon Jo", "Sunwoo Lee", "Seungyul Han"], "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning", "comment": "10 technical page followed by references and appendix. Accepted to ICLR 2026", "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.", "AI": {"tldr": "\u63d0\u51faS2Q\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u5b50\u4ef7\u503c\u51fd\u6570\u4fdd\u7559\u66ff\u4ee3\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\u548c\u63a2\u7d22\u80fd\u529b", "motivation": "\u73b0\u6709\u4ef7\u503c\u5206\u89e3\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\uff0c\u5728\u4ef7\u503c\u51fd\u6570\u53d8\u5316\u65f6\u96be\u4ee5\u9002\u5e94\uff0c\u5bb9\u6613\u6536\u655b\u5230\u6b21\u4f18\u7b56\u7565", "method": "\u63d0\u51fa\u8fde\u7eed\u5b50\u4ef7\u503cQ\u5b66\u4e60(S2Q)\uff0c\u5b66\u4e60\u591a\u4e2a\u5b50\u4ef7\u503c\u51fd\u6570\u6765\u4fdd\u7559\u66ff\u4ee3\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u7ed3\u5408Softmax\u884c\u4e3a\u7b56\u7565\u9f13\u52b1\u6301\u7eed\u63a2\u7d22", "result": "\u5728\u6311\u6218\u6027MARL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS2Q\u4e00\u81f4\u4f18\u4e8e\u591a\u79cdMARL\u7b97\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u6574\u4f53\u6027\u80fd", "conclusion": "S2Q\u901a\u8fc7\u591a\u5b50\u4ef7\u503c\u51fd\u6570\u6709\u6548\u89e3\u51b3\u4e86\u4ef7\u503c\u51fd\u6570\u53d8\u5316\u65f6\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.17588", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17588", "abs": "https://arxiv.org/abs/2602.17588", "authors": ["Faria Huq", "Zora Zhiruo Wang", "Zhanqiu Guo", "Venu Arvind Arangarajan", "Tianyue Ou", "Frank Xu", "Shuyan Zhou", "Graham Neubig", "Jeffrey P. Bigham"], "title": "Modeling Distinct Human Interaction in Web Agents", "comment": "Preprint", "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5efa\u6a21\u4eba\u7c7b\u5e72\u9884\u4ee5\u652f\u6301\u534f\u4f5c\u5f0f\u7f51\u9875\u4efb\u52a1\u6267\u884c\uff0c\u6536\u96c6\u4e86\u5305\u542b4200\u591a\u4e2a\u4ea4\u66ff\u4eba\u7c7b\u548c\u667a\u80fd\u4f53\u52a8\u4f5c\u7684\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u4e86\u56db\u79cd\u7528\u6237\u4ea4\u4e92\u6a21\u5f0f\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u5e72\u9884\u65f6\u673a\uff0c\u5e76\u5728\u5b9e\u9645\u7f51\u9875\u5bfc\u822a\u667a\u80fd\u4f53\u4e2d\u90e8\u7f72\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u7f51\u9875\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u5e72\u9884\u65f6\u673a\u548c\u539f\u56e0\u7684\u539f\u5219\u6027\u7406\u89e3\uff0c\u5f80\u5f80\u5728\u5173\u952e\u51b3\u7b56\u70b9\u81ea\u4e3b\u6267\u884c\u6216\u8bf7\u6c42\u4e0d\u5fc5\u8981\u7684\u786e\u8ba4\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u5e72\u9884\u4ee5\u652f\u6301\u534f\u4f5c\u5f0f\u7f51\u9875\u4efb\u52a1\u6267\u884c\u3002", "method": "\u6536\u96c6CowCorpus\u6570\u636e\u96c6\uff08400\u4e2a\u771f\u5b9e\u7528\u6237\u7f51\u9875\u5bfc\u822a\u8f68\u8ff9\uff0c\u5305\u542b4200\u591a\u4e2a\u4ea4\u66ff\u4eba\u7c7b\u548c\u667a\u80fd\u4f53\u52a8\u4f5c\uff09\uff0c\u8bc6\u522b\u56db\u79cd\u7528\u6237\u4ea4\u4e92\u6a21\u5f0f\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u7528\u6237\u4ea4\u4e92\u98ce\u683c\u9884\u6d4b\u5e72\u9884\u65f6\u673a\uff0c\u5e76\u5728\u5b9e\u9645\u7f51\u9875\u5bfc\u822a\u667a\u80fd\u4f53\u4e2d\u90e8\u7f72\u8bc4\u4f30\u3002", "result": "\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5e72\u9884\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u6bd4\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u4e8661.4-63.4%\uff0c\u5728\u5b9e\u9645\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u5e72\u9884\u611f\u77e5\u6a21\u578b\u4f7f\u667a\u80fd\u4f53\u7684\u7528\u6237\u8bc4\u5206\u6709\u7528\u6027\u63d0\u9ad8\u4e8626.5%\u3002", "conclusion": "\u7ed3\u6784\u5316\u5efa\u6a21\u4eba\u7c7b\u5e72\u9884\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u9002\u5e94\u6027\u548c\u534f\u4f5c\u6027\u7684\u667a\u80fd\u4f53\uff0c\u4e3a\u4eba\u7c7b-\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.17598", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.17598", "abs": "https://arxiv.org/abs/2602.17598", "authors": ["Jayadev Billa"], "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?", "comment": "10 pages, 6 figures, 7 tables", "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($\u03ba{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.", "AI": {"tldr": "\u5f53\u524d\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u672c\u8d28\u4e0a\u662f\u9690\u5f0fASR\u7cfb\u7edf\uff1a\u5728\u53ef\u901a\u8fc7\u6587\u672c\u8f6c\u5f55\u89e3\u51b3\u7684\u4efb\u52a1\u4e2d\uff0c\u5b83\u4eec\u7684\u884c\u4e3a\u548c\u673a\u5236\u4e0e\u7b80\u5355\u7684Whisper\u2192LLM\u7ea7\u8054\u7cfb\u7edf\u7b49\u6548\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u7684\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684ASR+LLM\u7ea7\u8054\u67b6\u6784\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5728\u5185\u90e8\u6267\u884c\u9690\u5f0f\u8bed\u97f3\u8bc6\u522b\u3002", "method": "\u901a\u8fc7\u5339\u914d\u9aa8\u5e72\u7f51\u7edc\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u56db\u4e2a\u8bed\u97f3LLM\u548c\u516d\u4e2a\u4efb\u52a1\uff0c\u63a7\u5236LLM\u9aa8\u5e72\u7f51\u7edc\uff1b\u4f7f\u7528logit lens\u5206\u6790\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u6587\u672c\u4fe1\u606f\uff1b\u5e94\u7528LEACE\u6982\u5ff5\u64e6\u9664\u9a8c\u8bc1\u6587\u672c\u8868\u793a\u7684\u5fc5\u8981\u6027\u3002", "result": "Ultravox\u4e0e\u5176\u5339\u914d\u7ea7\u8054\u7cfb\u7edf\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\uff08\u03ba=0.93\uff09\uff1b\u9690\u85cf\u72b6\u6001\u4e2d\u786e\u5b9e\u51fa\u73b0\u6587\u672c\u4fe1\u606f\uff1b\u6587\u672c\u8868\u793a\u5bf9\u4e24\u79cd\u6d4b\u8bd5\u67b6\u6784\u90fd\u662f\u56e0\u679c\u5fc5\u8981\u7684\uff1bQwen2-Audio\u8868\u73b0\u51fa\u771f\u6b63\u5dee\u5f02\uff0c\u8868\u660e\u7ea7\u8054\u7b49\u6548\u6027\u53d6\u51b3\u4e8e\u67b6\u6784\u800c\u975e\u666e\u904d\u73b0\u8c61\uff1b\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u8bed\u97f3LLM\u6027\u80fd\u4e0b\u964d\u66f4\u4e25\u91cd\u3002", "conclusion": "\u5927\u591a\u6570\u5f53\u524d\u90e8\u7f72\u7684\u8bed\u97f3LLM\u672c\u8d28\u4e0a\u662f\u6602\u8d35\u7684\u7ea7\u8054\u7cfb\u7edf\uff0c\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u5dee\uff0c\u5176\u4f18\u52bf\u5728\u67b6\u6784\u4e0a\u5177\u6709\u4f9d\u8d56\u6027\u800c\u975e\u666e\u904d\u7279\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.17217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17217", "abs": "https://arxiv.org/abs/2602.17217", "authors": ["Enrique Crespo-Fernandez", "Oliver Ray", "Telmo de Menezes e Silva Filho", "Peter Flach"], "title": "Continual learning and refinement of causal models through dynamic predicate invention", "comment": null, "summary": "Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u7ebf\u6784\u5efa\u7b26\u53f7\u56e0\u679c\u4e16\u754c\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u89e3\u91ca\u5b66\u4e60\u548c\u8c13\u8bcd\u53d1\u660e\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u4e16\u754c\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u5173\u7cfb\u52a8\u6001\u65f6\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u6311\u6218\u3002", "method": "\u5c06\u8fde\u7eed\u6a21\u578b\u5b66\u4e60\u548c\u4fee\u590d\u96c6\u6210\u5230\u667a\u80fd\u4f53\u51b3\u7b56\u5faa\u73af\u4e2d\uff0c\u5229\u7528\u5143\u89e3\u91ca\u5b66\u4e60\u548c\u8c13\u8bcd\u53d1\u660e\u6280\u672f\uff0c\u4ece\u89c2\u5bdf\u4e2d\u6784\u5efa\u5c42\u6b21\u5316\u3001\u89e3\u8026\u7684\u9ad8\u8d28\u91cf\u6982\u5ff5\u62bd\u8c61\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5173\u7cfb\u52a8\u6001\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u907f\u514d\u4e86\u547d\u9898\u65b9\u6cd5\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u6837\u672c\u6548\u7387\u6bd4\u57fa\u4e8ePPO\u7684\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u7b26\u53f7\u56e0\u679c\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u4e3a\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.17221", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17221", "abs": "https://arxiv.org/abs/2602.17221", "authors": ["Yi-Chih Huang"], "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences", "comment": "also in Chinese", "summary": "Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.\n  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).\n  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eAI Agent\u7684\u534f\u4f5c\u7814\u7a76\u6d41\u7a0b\uff08Agentic Workflow\uff09\uff0c\u7528\u4e8e\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\uff0c\u5e76\u4ee5\u53f0\u6e7eClaude.ai\u4f7f\u7528\u6570\u636e\u4e3a\u6848\u4f8b\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8f6f\u4ef6\u5de5\u7a0b\u548c\u81ea\u7136\u79d1\u5b66\u9886\u57df\uff0c\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u9886\u57df\u7684\u65b9\u6cd5\u8bba\u63a2\u7d22\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8be5\u9886\u57df\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u8bbe\u8ba1\u4e03\u9636\u6bb5\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\uff1a\u4efb\u52a1\u6a21\u5757\u5316\u3001\u4eba\u673a\u5206\u5de5\u3001\u53ef\u9a8c\u8bc1\u6027\uff1b\u4ee5\u53f0\u6e7eClaude.ai\u4f7f\u7528\u6570\u636e\uff087,729\u4e2a\u5bf9\u8bdd\uff09\u4e3a\u5b9e\u8bc1\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u590d\u5236\u7684AI\u534f\u4f5c\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\uff1a\u76f4\u63a5\u6267\u884c\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u4eba\u7c7b\u4e3b\u5bfc\uff1b\u63ed\u793a\u4e86\u4eba\u7c7b\u5728\u7814\u7a76\u95ee\u9898\u5236\u5b9a\u3001\u7406\u8bba\u89e3\u91ca\u3001\u60c5\u5883\u5316\u63a8\u7406\u548c\u4f26\u7406\u53cd\u601d\u4e2d\u7684\u4e0d\u53ef\u66ff\u4ee3\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684AI\u534f\u4f5c\u6846\u67b6\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u4eba\u7c7b\u5224\u65ad\u5728\u7814\u7a76\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u627f\u8ba4\u4e86\u5355\u5e73\u53f0\u6570\u636e\u3001\u6a2a\u622a\u9762\u8bbe\u8ba1\u548cAI\u53ef\u9760\u6027\u98ce\u9669\u7b49\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.17245", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17245", "abs": "https://arxiv.org/abs/2602.17245", "authors": ["Linxi Jiang", "Rui Xi", "Zhijie Liu", "Shuo Chen", "Zhiqiang Lin", "Suman Nath"], "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web", "comment": null, "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.", "AI": {"tldr": "\u63d0\u51faWeb Verbs\u4f5c\u4e3a\u7f51\u7edc\u52a8\u4f5c\u7684\u8bed\u4e49\u5c42\uff0c\u5c06\u7f51\u7ad9\u529f\u80fd\u901a\u8fc7\u7c7b\u578b\u5316\u3001\u8bed\u4e49\u5316\u7684\u51fd\u6570\u66b4\u9732\u7ed9AI\u4ee3\u7406\uff0c\u7edf\u4e00API\u548c\u6d4f\u89c8\u5668\u64cd\u4f5c\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u4ee3\u7406\u4e3b\u8981\u57fa\u4e8e\u4f4e\u7ea7\u64cd\u4f5c\uff08\u70b9\u51fb\u3001\u6309\u952e\uff09\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u8106\u5f31\u3001\u4f4e\u6548\u4e14\u96be\u4ee5\u9a8c\u8bc1\u3002\u968f\u7740LLM\u53d1\u5c55\uff0c\u9700\u8981\u4e3a\u4ee3\u7406\u5f0f\u7f51\u7edc\u5efa\u7acb\u8bed\u4e49\u5c42\u6765\u652f\u6301\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u3002", "method": "\u63d0\u51faWeb Verbs\u6982\u5ff5\uff0c\u521b\u5efa\u7f51\u7edc\u89c4\u6a21\u7684\u7c7b\u578b\u5316\u3001\u8bed\u4e49\u5316\u51fd\u6570\u96c6\u5408\uff0c\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u66b4\u9732\u7f51\u7ad9\u529f\u80fd\uff08\u65e0\u8bba\u662fAPI\u8fd8\u662f\u5ba2\u6237\u7aef\u5de5\u4f5c\u6d41\uff09\u3002\u8fd9\u4e9b\u52a8\u8bcd\u5305\u542b\u524d\u7f6e\u6761\u4ef6\u3001\u540e\u7f6e\u6761\u4ef6\u3001\u7b56\u7565\u6807\u7b7e\u548c\u65e5\u5fd7\u652f\u6301\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u548c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86Web Verbs\u76f8\u6bd4\u73b0\u6709\u4ee3\u7406\u80fd\u591f\u5b9e\u73b0\u66f4\u7b80\u6d01\u3001\u66f4\u9c81\u68d2\u7684\u6267\u884c\uff0c\u5c06\u6570\u5341\u4e2a\u6b65\u9aa4\u51cf\u5c11\u4e3a\u51e0\u4e2a\u51fd\u6570\u8c03\u7528\u3002", "conclusion": "Web Verbs\u4e3a\u7f51\u7edc\u4ee3\u7406\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u53ef\u7ec4\u5408\u7684\u8bed\u4e49\u5c42\uff0c\u7edf\u4e00\u4e86API\u548c\u6d4f\u89c8\u5668\u8303\u5f0f\uff0c\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u8def\u7ebf\u56fe\u4ee5\u5b9e\u73b0\u7f51\u7edc\u89c4\u6a21\u90e8\u7f72\u3002", "topic": "code agent"}}
{"id": "2602.17308", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17308", "abs": "https://arxiv.org/abs/2602.17308", "authors": ["Hui Min Wong", "Philip Heesen", "Pascal Janetzky", "Martin Bendszus", "Stefan Feuerriegel"], "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions", "comment": null, "summary": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.", "AI": {"tldr": "MedClarify\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u8bca\u65ad\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u751f\u6210\u540e\u7eed\u95ee\u9898\u6765\u51cf\u5c11\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u76f8\u6bd4\u5355\u6b21LLM\u57fa\u7ebf\u51cf\u5c11\u7ea627\u4e2a\u767e\u5206\u70b9\u7684\u8bca\u65ad\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u533b\u5b66LLMs\u5728\u8bca\u65ad\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u50cf\u4e34\u5e8a\u533b\u751f\u90a3\u6837\u901a\u8fc7\u8fed\u4ee3\u63d0\u95ee\u6765\u8003\u8651\u9274\u522b\u8bca\u65ad\u548c\u6392\u9664\u7d27\u6025\u60c5\u51b5\u3002\u771f\u5b9e\u4e34\u5e8a\u8bca\u65ad\u9700\u8981\u7cfb\u7edf\u6027\u7684\u75c5\u53f2\u91c7\u96c6\u548c\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u800c\u73b0\u6709LLMs\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "MedClarify\u9996\u5148\u8ba1\u7b97\u5019\u9009\u8bca\u65ad\u5217\u8868\uff08\u7c7b\u4f3c\u9274\u522b\u8bca\u65ad\uff09\uff0c\u7136\u540e\u4e3b\u52a8\u751f\u6210\u540e\u7eed\u95ee\u9898\u4ee5\u51cf\u5c11\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u9009\u62e9\u9884\u671f\u4fe1\u606f\u589e\u76ca\u6700\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dLLMs\u5728\u533b\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u75c5\u4f8b\u4e0d\u5b8c\u6574\u6216\u76f8\u5173\u4fe1\u606f\u7f3a\u5931\u65f6\u4f1a\u4ea7\u751f\u591a\u4e2a\u76f8\u4f3c\u53ef\u80fd\u6027\u7684\u8bca\u65ad\u3002MedClarify\u7684\u4fe1\u606f\u8bba\u63a8\u7406\u65b9\u6cd5\u80fd\u751f\u6210\u6709\u6548\u7684\u540e\u7eed\u63d0\u95ee\uff0c\u76f8\u6bd4\u6807\u51c6\u5355\u6b21LLM\u57fa\u7ebf\u51cf\u5c11\u7ea627\u4e2a\u767e\u5206\u70b9\u7684\u8bca\u65ad\u9519\u8bef\u3002", "conclusion": "MedClarify\u901a\u8fc7\u4ee3\u7406\u5f0f\u4fe1\u606f\u5bfb\u6c42\u4e3a\u6539\u8fdb\u533b\u5b66LLMs\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\uff0c\u4fc3\u8fdb\u4e86\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u63a8\u7406\u8fed\u4ee3\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u6709\u6548\u5bf9\u8bdd\u3002", "topic": "agent analysis"}}
{"id": "2602.17547", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17547", "abs": "https://arxiv.org/abs/2602.17547", "authors": ["Yue Liu", "Zhiyuan Hu", "Flood Sung", "Jiaheng Zhang", "Bryan Hooi"], "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks", "comment": null, "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.", "AI": {"tldr": "KLong\u662f\u4e00\u4e2a\u5f00\u6e90LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8f68\u8ff9\u5206\u5272SFT\u548c\u6e10\u8fdb\u5f0fRL\u8bad\u7ec3\u6765\u89e3\u51b3\u8d85\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5728PaperBench\u7b49\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5904\u7406\u8d85\u957f\u89c6\u91ce\u4efb\u52a1\u65f6\u5b58\u5728\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u957f\u8f68\u8ff9\u4efb\u52a1\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "1. \u901a\u8fc7\u8f68\u8ff9\u5206\u5272SFT\u51b7\u542f\u52a8\u6a21\u578b\uff1a\u4fdd\u7559\u65e9\u671f\u4e0a\u4e0b\u6587\uff0c\u6e10\u8fdb\u622a\u65ad\u540e\u671f\u4e0a\u4e0b\u6587\uff0c\u4fdd\u6301\u5b50\u8f68\u8ff9\u91cd\u53e0\uff1b2. \u4f7f\u7528Research-Factory\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff1b3. \u63d0\u51fa\u6e10\u8fdb\u5f0fRL\u8bad\u7ec3\uff1a\u5206\u9636\u6bb5\u8bad\u7ec3\u5e76\u9010\u6b65\u5ef6\u957f\u8d85\u65f6\u65f6\u95f4\u3002", "result": "KLong\uff08106B\uff09\u5728PaperBench\u4e0a\u8d85\u8d8aKimi K2 Thinking\uff081T\uff0911.28%\uff0c\u5728SWE-bench Verified\u548cMLE-bench\u7b49\u5176\u4ed6\u7f16\u7801\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u6cdb\u5316\u4f18\u52bf\u3002", "conclusion": "\u8f68\u8ff9\u5206\u5272SFT\u548c\u6e10\u8fdb\u5f0fRL\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347LLM\u667a\u80fd\u4f53\u89e3\u51b3\u8d85\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u80fd\u529b\uff0cKLong\u5c55\u793a\u4e86\u5728\u5b66\u672f\u7814\u7a76\u548c\u7f16\u7801\u4efb\u52a1\u4e0a\u7684\u5f3a\u5927\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2602.17560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17560", "abs": "https://arxiv.org/abs/2602.17560", "authors": ["Hongjue Zhao", "Haosen Sun", "Jiangtao Kong", "Xiaochang Li", "Qineng Wang", "Liwei Jiang", "Qi Zhu", "Tarek Abdelzaher", "Yejin Choi", "Manling Li", "Huajie Shao"], "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment", "comment": "Accepted by ICLR 2026", "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.", "AI": {"tldr": "\u63d0\u51faODESteer\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u6fc0\u6d3b\u5f15\u5bfc\uff0c\u901a\u8fc7\u5c4f\u969c\u51fd\u6570\u5b9e\u73b0\u591a\u6b65\u81ea\u9002\u5e94\u5f15\u5bfc\uff0c\u5728LLM\u5bf9\u9f50\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6307\u5bfc\u5f15\u5bfc\u65b9\u5411\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u5355\u6b65\u5f15\u5bfc\u65e0\u6cd5\u6355\u6349\u6fc0\u6d3b\u5206\u5e03\u7684\u590d\u6742\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u6fc0\u6d3b\u52a0\u6cd5\u89e3\u91ca\u4e3aODE\u7684\u4e00\u9636\u8fd1\u4f3c\u3002\u5f15\u5165\u5c4f\u969c\u51fd\u6570\u6982\u5ff5\uff0c\u63d0\u51faODESteer\u65b9\u6cd5\uff0c\u5c06\u5c4f\u969c\u51fd\u6570\u5b9a\u4e49\u4e3a\u6b63\u8d1f\u6fc0\u6d3b\u7684\u5bf9\u6570\u5bc6\u5ea6\u6bd4\uff0c\u6784\u5efaODE\u8fdb\u884c\u591a\u6b65\u81ea\u9002\u5e94\u5f15\u5bfc\u3002", "result": "ODESteer\u5728\u591a\u4e2aLLM\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1aTruthfulQA\u63d0\u53475.7%\uff0cUltraFeedback\u63d0\u53472.5%\uff0cRealToxicityPrompts\u63d0\u53472.4%\uff0c\u4f18\u4e8e\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7ODE\u7edf\u4e00\u6fc0\u6d3b\u5f15\u5bfc\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u7684ODESteer\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u539f\u5219\u6027\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.17312", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17312", "abs": "https://arxiv.org/abs/2602.17312", "authors": ["Hsin-Jung Yang", "Zhanhong Jiang", "Prajwal Koirala", "Qisai Liu", "Cody Fleming", "Soumik Sarkar"], "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy", "comment": "17th ACM/IEEE International Conference on Cyber-Physical Systems", "summary": "Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.", "AI": {"tldr": "\u63d0\u51faLexiSafe\uff0c\u4e00\u4e2a\u8bcd\u5178\u5e8f\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u5178\u5e8f\u4f18\u5148\u7ea7\u548c\u7ed3\u6784\u504f\u7f6e\u6765\u9632\u6b62\u5b89\u5168\u6f02\u79fb\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u7ea6\u675f\u677e\u5f1b\u6216\u8054\u5408\u4f18\u5316\u6765\u5e73\u8861\u5956\u52b1-\u5b89\u5168\u6743\u8861\uff0c\u4f46\u7f3a\u4e4f\u9632\u6b62\u5b89\u5168\u6f02\u79fb\u7684\u7ed3\u6784\u673a\u5236\uff0c\u800c\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u80fd\u5bb9\u5fcd\u5b89\u5168\u8fdd\u89c4", "method": "\u63d0\u51faLexiSafe\u8bcd\u5178\u5e8f\u79bb\u7ebfRL\u6846\u67b6\uff1a1) LexiSafe-SC\u5355\u6210\u672c\u516c\u5f0f\uff0c\u4e3a\u6807\u51c6\u7684\u79bb\u7ebf\u5b89\u5168RL\u63d0\u4f9b\u5b89\u5168\u8fdd\u89c4\u548c\u6027\u80fd\u6b21\u4f18\u6027\u8fb9\u754c\uff1b2) LexiSafe-MC\u6269\u5c55\u652f\u6301\u591a\u5b89\u5168\u6210\u672c\u7684\u5c42\u6b21\u5b89\u5168\u9700\u6c42\uff0c\u5e76\u8fdb\u884c\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790", "result": "\u7ecf\u9a8c\u4e0a\uff0cLexiSafe\u76f8\u6bd4\u7ea6\u675f\u79bb\u7ebf\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e86\u5b89\u5168\u8fdd\u89c4\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u6027\u80fd\uff1b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u8bcd\u5178\u5e8f\u4f18\u5148\u7ea7\u4e0e\u7ed3\u6784\u504f\u7f6e\uff0cLexiSafe\u4e3a\u5b89\u5168\u5173\u952eCPS\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u57fa\u7840\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.17497", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17497", "abs": "https://arxiv.org/abs/2602.17497", "authors": ["Wen-Tse Chen", "Jiayu Chen", "Fahim Tajwar", "Hao Zhu", "Xintong Duan", "Ruslan Salakhutdinov", "Jeff Schneider"], "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models", "comment": "Accepted to NeurIPS 2025", "summary": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.", "AI": {"tldr": "\u63d0\u51faRICOL\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u901a\u8fc7\u56de\u987e\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u7a00\u758f\u5956\u52b1\u8f6c\u5316\u4e3a\u5bc6\u96c6\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4efb\u52a1\u7279\u5b9a\u4ef7\u503c\u51fd\u6570\u7684\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u81ea\u91c7\u6837\u6570\u636e\u548c\u7a00\u758f\u73af\u5883\u53cd\u9988\u3002", "method": "\u63d0\u51faRICL\uff08\u56de\u987e\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\uff0c\u5229\u7528LLM\u5c06\u7a00\u758f\u5956\u52b1\u8f6c\u5316\u4e3a\u4f18\u52bf\u51fd\u6570\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faRICOL\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8eRICL\u7684\u4fe1\u7528\u5206\u914d\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u3002", "result": "RICL\u80fd\u591f\u7528\u6709\u9650\u6837\u672c\u51c6\u786e\u4f30\u8ba1\u4f18\u52bf\u51fd\u6570\u5e76\u8bc6\u522b\u73af\u5883\u4e2d\u7684\u5173\u952e\u72b6\u6001\uff1b\u5728\u56db\u4e2aBabyAI\u573a\u666f\u4e2d\uff0cRICOL\u8fbe\u5230\u4e0e\u4f20\u7edf\u5728\u7ebfRL\u7b97\u6cd5\u76f8\u5f53\u7684\u6536\u655b\u6027\u80fd\uff0c\u4f46\u6837\u672c\u6548\u7387\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u5229\u7528LLM\u8fdb\u884c\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684RL\u8303\u5f0f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17550", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17550", "abs": "https://arxiv.org/abs/2602.17550", "authors": ["Xiaoliang Fu", "Jiaye Lin", "Yangyi Fang", "Binbin Zheng", "Chaowen Hu", "Zekai Shao", "Cong Qin", "Lu Pan", "Ke Zeng", "Xunliang Cai"], "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning", "comment": null, "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.", "AI": {"tldr": "MASPO\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u9ad8\u65af\u95e8\u63a7\u3001\u8d28\u91cf\u81ea\u9002\u5e94\u9650\u5236\u5668\u548c\u975e\u5bf9\u79f0\u98ce\u9669\u63a7\u5236\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u68af\u5ea6\u5229\u7528\u3001\u6982\u7387\u5206\u5e03\u654f\u611f\u6027\u548c\u4fe1\u53f7\u53ef\u9760\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709RLVR\u7b97\u6cd5\uff08\u5982GRPO\uff09\u91c7\u7528\u521a\u6027\u3001\u5747\u5300\u3001\u5bf9\u79f0\u7684\u4fe1\u4efb\u533a\u57df\u673a\u5236\uff0c\u4e0eLLMs\u7684\u590d\u6742\u4f18\u5316\u52a8\u6001\u4e0d\u5339\u914d\u3002\u5177\u4f53\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a1\uff09\u786c\u622a\u65ad\u5bfc\u81f4\u68af\u5ea6\u5229\u7528\u6548\u7387\u4f4e\uff1b2\uff09\u5747\u5300\u6bd4\u7387\u7ea6\u675f\u5ffd\u7565token\u5206\u5e03\uff0c\u5bfc\u81f4\u6982\u7387\u8d28\u91cf\u4e0d\u654f\u611f\uff1b3\uff09\u6b63\u8d1f\u6837\u672c\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u5ea6\u4e0d\u540c\uff0c\u5bfc\u81f4\u4fe1\u53f7\u53ef\u9760\u6027\u4e0d\u5bf9\u79f0\u3002", "method": "\u63d0\u51faMASPO\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53ef\u5fae\u8f6f\u9ad8\u65af\u95e8\u63a7\uff0c\u6700\u5927\u5316\u68af\u5ea6\u6548\u7528\uff1b2\uff09\u8d28\u91cf\u81ea\u9002\u5e94\u9650\u5236\u5668\uff0c\u5e73\u8861\u6982\u7387\u8c31\u4e0a\u7684\u63a2\u7d22\uff1b3\uff09\u975e\u5bf9\u79f0\u98ce\u9669\u63a7\u5236\u5668\uff0c\u4f7f\u66f4\u65b0\u5e45\u5ea6\u4e0e\u4fe1\u53f7\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660eMASPO\u4f5c\u4e3a\u7a33\u5065\u7684RLVR\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MASPO\u901a\u8fc7\u534f\u8c03\u68af\u5ea6\u5229\u7528\u3001\u6982\u7387\u5206\u5e03\u654f\u611f\u6027\u548c\u4fe1\u53f7\u53ef\u9760\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u4e3aRLVR\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17616", "abs": "https://arxiv.org/abs/2602.17616", "authors": ["Luke Huang", "Zhuoyang Zhang", "Qinghao Hu", "Shang Yang", "Song Han"], "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs", "comment": null, "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\textbf{C}$ontrolled $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.", "AI": {"tldr": "\u63d0\u51faVCPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u7b56\u7565\u68af\u5ea6\u65b9\u5dee\u6765\u7a33\u5b9a\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f42.5\u500d\u7684\u540c\u65f6\u4fdd\u6301\u540c\u6b65\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u63d0\u9ad8\u7aef\u5230\u7aef\u541e\u5410\u91cf\uff0c\u4f46\u5bf9\u4e8e\u65e0\u8bc4\u8bba\u5bb6\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff08\u5982REINFORCE\u548cGRPO\uff09\uff0c\u9ad8\u5f02\u6b65\u6027\u4f1a\u5bfc\u81f4\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\u65b9\u5dee\u663e\u8457\u589e\u5927\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u5bb9\u6613\u5d29\u6e83\u3002", "method": "\u63d0\u51faVariance Controlled Policy Optimization (VCPO)\uff1a1) \u57fa\u4e8e\u6709\u6548\u6837\u672c\u91cf\u7f29\u653e\u5b66\u4e60\u7387\u4ee5\u6291\u5236\u4e0d\u53ef\u9760\u66f4\u65b0\uff1b2) \u5e94\u7528\u95ed\u5f0f\u6700\u5c0f\u65b9\u5dee\u57fa\u7ebf\u7528\u4e8e\u79bb\u7b56\u7565\u8bbe\u7f6e\uff0c\u907f\u514d\u8f85\u52a9\u4ef7\u503c\u6a21\u578b\u5e76\u589e\u52a0\u6700\u5c0f\u5f00\u9500\u3002", "result": "VCPO\u5728\u6570\u5b66\u3001\u901a\u7528\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5f02\u6b65\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c06\u957f\u4e0a\u4e0b\u6587\u3001\u591a\u8f6e\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112.5\u500d\uff0c\u540c\u65f6\u5339\u914d\u540c\u6b65\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u63a7\u5236\u7b56\u7565\u68af\u5ea6\u65b9\u5dee\u662f\u5b9e\u73b0\u5927\u89c4\u6a21\u53ef\u9760\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\uff0cVCPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6b65\u8bad\u7ec3\u4e2d\u7684\u65b9\u5dee\u653e\u5927\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17632", "abs": "https://arxiv.org/abs/2602.17632", "authors": ["Nathan S. de Lara", "Florian Shkurti"], "title": "SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer", "comment": null, "summary": "Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.", "AI": {"tldr": "SMAC\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316Q\u51fd\u6570\u4f7f\u5176\u6ee1\u8db3\u7b56\u7565\u5206\u6570\u4e0eQ\u51fd\u6570\u52a8\u4f5c\u68af\u5ea6\u7684\u4e00\u9636\u5bfc\u6570\u7b49\u5f0f\uff0c\u4ece\u800c\u907f\u514d\u79bb\u7ebf\u4e0e\u5728\u7ebf\u4f18\u5316\u4e4b\u95f4\u7684\u6027\u80fd\u4f4e\u8c37\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7684\u5728\u7ebf\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u65b9\u6cd5\u627e\u5230\u7684\u6027\u80fd\u826f\u597d\u7684actor-critic\u6a21\u578b\uff0c\u5728\u4f7f\u7528\u57fa\u4e8e\u4ef7\u503c\u7684\u5728\u7ebfRL\u7b97\u6cd5\u5fae\u8c03\u65f6\u901a\u5e38\u4f1a\u51fa\u73b0\u6027\u80fd\u7acb\u5373\u4e0b\u964d\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u79bb\u7ebf\u6700\u5927\u503c\u548c\u5728\u7ebf\u6700\u5927\u503c\u4e4b\u95f4\u5b58\u5728\u4f4e\u6027\u80fd\u7684\"\u5c71\u8c37\"\uff0c\u57fa\u4e8e\u68af\u5ea6\u7684\u5fae\u8c03\u4f1a\u7a7f\u8d8a\u8fd9\u4e9b\u5c71\u8c37\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faScore Matched Actor-Critic (SMAC)\u65b9\u6cd5\uff0c\u5728\u79bb\u7ebf\u9636\u6bb5\u6b63\u5219\u5316Q\u51fd\u6570\uff0c\u4f7f\u5176\u6ee1\u8db3\u7b56\u7565\u5206\u6570\u4e0eQ\u51fd\u6570\u52a8\u4f5c\u68af\u5ea6\u4e4b\u95f4\u7684\u4e00\u9636\u5bfc\u6570\u7b49\u5f0f\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u79bb\u7ebf\u4e0e\u5728\u7ebf\u6700\u5927\u503c\u4e4b\u95f4\u7684\u6027\u80fd\u4f4e\u8c37\uff0c\u4f7f\u5f97\u79bb\u7ebf\u5b66\u4e60\u5230\u7684actor-critic\u80fd\u591f\u5e73\u6ed1\u8fc7\u6e21\u5230\u5728\u7ebf\u4f18\u5316\u3002", "result": "SMAC\u57286/6\u4e2aD4RL\u4efb\u52a1\u4e2d\u90fd\u80fd\u5e73\u6ed1\u8fc7\u6e21\u5230Soft Actor-Critic\u548cTD3\u7b97\u6cd5\u3002\u57284/6\u7684\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e8634-58%\u7684\u9057\u61be\u503c\u3002\u5b9e\u9a8c\u8bc1\u660eSMAC\u80fd\u591f\u6536\u655b\u5230\u4e0e\u66f4\u597d\u5728\u7ebf\u6700\u5927\u503c\u76f8\u8fde\u7684\u79bb\u7ebf\u6700\u5927\u503c\uff0c\u5e76\u901a\u8fc7\u4e00\u9636\u4f18\u5316\u627e\u5230\u5355\u8c03\u9012\u589e\u5956\u52b1\u7684\u8def\u5f84\u3002", "conclusion": "SMAC\u901a\u8fc7\u8bbe\u8ba1\u7279\u5b9a\u7684\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u6a21\u578b\u5728\u7ebf\u5fae\u8c03\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u79bb\u7ebf\u9884\u8bad\u7ec3\u548c\u5728\u7ebf\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.17641", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17641", "abs": "https://arxiv.org/abs/2602.17641", "authors": ["Keith Burghardt", "Jienan Liu", "Sadman Sakib", "Yuning Hao", "Bo Li"], "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery", "comment": "23 pages, 6 figures", "summary": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.", "AI": {"tldr": "FAMOSE\uff1a\u9996\u4e2a\u57fa\u4e8eReAct\u8303\u5f0f\u7684\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u63a5\u8fd1SOTA\u6027\u80fd", "motivation": "\u7279\u5f81\u5de5\u7a0b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u7279\u5f81\u7a7a\u95f4\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faFAMOSE\u6846\u67b6\uff0c\u5229\u7528ReAct\u8303\u5f0f\u81ea\u4e3b\u63a2\u7d22\u3001\u751f\u6210\u548c\u4f18\u5316\u7279\u5f81\uff0c\u5728\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u96c6\u6210\u7279\u5f81\u9009\u62e9\u548c\u8bc4\u4f30\u5de5\u5177\u3002\u901a\u8fc7\u8fed\u4ee3\u7279\u5f81\u53d1\u73b0\u548c\u8bc4\u4f30\u6b65\u9aa4\u8bb0\u5f55\u6709\u6548/\u65e0\u6548\u7279\u5f81\uff0c\u5f15\u5bfcLLM\u521b\u9020\u66f4\u597d\u7684\u521b\u65b0\u7279\u5f81\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u63a5\u8fd1SOTA\uff08\u7279\u522b\u662f\u8d85\u8fc71\u4e07\u5b9e\u4f8b\u7684\u4efb\u52a1\uff0cROC-AUC\u5e73\u5747\u63d0\u53470.23%\uff09\uff1b\u5728\u56de\u5f52\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff08RMSE\u5e73\u5747\u964d\u4f4e2.0%\uff09\uff0c\u4e14\u6bd4\u5176\u4ed6\u7b97\u6cd5\u66f4\u9c81\u68d2\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u5728\u9700\u8981\u9ad8\u5ea6\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u7684\u95ee\u9898\uff08\u5982\u7279\u5f81\u5de5\u7a0b\uff09\u4e0a\u8868\u73b0\u51fa\u8272\u3002ReAct\u8303\u5f0f\u5141\u8bb8LLM\u901a\u8fc7\u8fed\u4ee3\u8bb0\u5f55\u6709\u6548/\u65e0\u6548\u7279\u5f81\uff0c\u7c7b\u4f3c\u4e8efew-shot\u63d0\u793a\uff0c\u5f15\u5bfc\u521b\u9020\u66f4\u597d\u7684\u521b\u65b0\u7279\u5f81\u3002", "topic": "code agent"}}
{"id": "2602.17646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17646", "abs": "https://arxiv.org/abs/2602.17646", "authors": ["Sima Noorani", "Shayan Kiyani", "Hamed Hassani", "George Pappas"], "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements", "comment": null, "summary": "As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u786e\u4fdd\u591a\u8f6e\u5bf9\u8bddAI\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u53ef\u9760\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u4f24\u5bb3\u548c\u4e92\u8865\u6027\u4e24\u5927\u539f\u5219\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u89c4\u5219\u548c\u5728\u7ebf\u7b97\u6cd5\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u3002", "motivation": "\u968f\u7740\u4eba\u7c7b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u591a\u8f6e\u5bf9\u8bddAI\u8fdb\u884c\u9ad8\u98ce\u9669\u51b3\u7b56\uff0c\u9700\u8981\u539f\u5219\u6027\u6846\u67b6\u786e\u4fdd\u8fd9\u79cd\u4ea4\u4e92\u80fd\u53ef\u9760\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\uff0c\u9632\u6b62AI\u524a\u5f31\u4eba\u7c7b\u4f18\u52bf\u6216\u65e0\u6cd5\u5f25\u8865\u4eba\u7c7b\u5f31\u70b9\u3002", "method": "\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89d2\uff0c\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u4f24\u5bb3\u548c\u4e92\u8865\u6027\u4e24\u5927\u539f\u5219\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u89c4\u5219\u5f62\u5f0f\u5316\u8fd9\u4e9b\u6982\u5ff5\uff0c\u5f15\u5165\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u7684\u5728\u7ebf\u3001\u65e0\u5206\u5e03\u7b97\u6cd5\u6765\u5f3a\u5236\u6267\u884c\u7528\u6237\u6307\u5b9a\u7684\u7ea6\u675f\u3002", "result": "\u5728\u533b\u7597\u8bca\u65ad\u4efb\u52a1\uff08LLM\u6a21\u62df\u534f\u4f5c\uff09\u548c\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\uff08\u4eba\u7c7b\u4f17\u5305\u7814\u7a76\uff09\u4e2d\u8bc4\u4f30\uff0c\u5728\u7ebf\u7a0b\u5e8f\u5373\u4f7f\u5728\u975e\u5e73\u7a33\u4ea4\u4e92\u52a8\u6001\u4e0b\u4e5f\u80fd\u7ef4\u6301\u89c4\u5b9a\u7684\u53cd\u4e8b\u5b9e\u4f24\u5bb3\u548c\u4e92\u8865\u6027\u8fdd\u53cd\u7387\uff0c\u7ea6\u675f\u677e\u7d27\u53ef\u9884\u6d4b\u5730\u5f71\u54cd\u4e0b\u6e38\u4eba\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u4f24\u5bb3\u548c\u4e92\u8865\u6027\u4e24\u5927\u539f\u5219\u53ef\u4f5c\u4e3a\u5b9e\u7528\u6760\u6746\uff0c\u5f15\u5bfc\u591a\u8f6e\u534f\u4f5c\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\uff0c\u65e0\u9700\u5efa\u6a21\u6216\u7ea6\u675f\u4eba\u7c7b\u884c\u4e3a\uff0c\u4e3a\u9ad8\u98ce\u9669AI\u534f\u4f5c\u63d0\u4f9b\u539f\u5219\u6027\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.e8d78de0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FSak2dI/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/u1X_D7fCxxaenlm3-xKu22jnBTmkMEz0wkC9fFgQ9aY=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FSak2dI/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/u1X_D7fCxxaenlm3-xKu22jnBTmkMEz0wkC9fFgQ9aY=445", "authors": ["TLDR Newsletter"], "title": "ERC-8162: The Subscription Layer for the Agentic Economy", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FSak2dI/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/u1X_D7fCxxaenlm3-xKu22jnBTmkMEz0wkC9fFgQ9aY=445", "summary": "ERC-8162: The Subscription Layer for the Agentic Economy (6 minute read) ERC-8162 proposes a subscription protocol for onchain AI agents complementing x402's per-request model, enabling tiered cycle-based access plans where a single onchain transaction grants zero-marginal-cost access for the subscription period. The protocol enforces direct settlement and uses EIP-712 signature recovery for self-contained, trustless access verification over the same HTTP 402 surface as x402. A notable archit...", "source": "tldr", "AI": {"tldr": "ERC-8162\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba2\u9605\u7684\u534f\u8bae\uff0c\u4e3a\u94fe\u4e0aAI\u4ee3\u7406\u63d0\u4f9b\u5206\u5c42\u5468\u671f\u8bbf\u95ee\u8ba1\u5212\uff0c\u901a\u8fc7\u5355\u6b21\u94fe\u4e0a\u4ea4\u6613\u5b9e\u73b0\u96f6\u8fb9\u9645\u6210\u672c\u7684\u8ba2\u9605\u671f\u8bbf\u95ee\u3002", "motivation": "\u5f53\u524dx402\u534f\u8bae\u91c7\u7528\u6309\u8bf7\u6c42\u4ed8\u8d39\u6a21\u5f0f\uff0c\u9700\u8981\u4e3a\u6bcf\u6b21AI\u4ee3\u7406\u8c03\u7528\u8fdb\u884c\u5355\u72ec\u652f\u4ed8\u3002ERC-8162\u65e8\u5728\u8865\u5145\u8fd9\u79cd\u6a21\u5f0f\uff0c\u4e3aAI\u4ee3\u7406\u7ecf\u6d4e\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u8ba2\u9605\u5236\u8bbf\u95ee\u65b9\u6848\uff0c\u964d\u4f4e\u9891\u7e41\u8c03\u7528\u7684\u8fb9\u9645\u6210\u672c\u3002", "method": "\u534f\u8bae\u91c7\u7528\u5206\u5c42\u5468\u671f\u8bbf\u95ee\u8ba1\u5212\uff0c\u901a\u8fc7\u5355\u6b21\u94fe\u4e0a\u4ea4\u6613\u6388\u6743\u8ba2\u9605\u671f\u8bbf\u95ee\u3002\u4f7f\u7528EIP-712\u7b7e\u540d\u6062\u590d\u8fdb\u884c\u81ea\u5305\u542b\u3001\u65e0\u9700\u4fe1\u4efb\u7684\u8bbf\u95ee\u9a8c\u8bc1\uff0c\u5e76\u5728\u4e0ex402\u76f8\u540c\u7684HTTP 402\u63a5\u53e3\u4e0a\u5f3a\u5236\u6267\u884c\u76f4\u63a5\u7ed3\u7b97\u3002", "result": "\u8be5\u534f\u8bae\u5b9e\u73b0\u4e86\u96f6\u8fb9\u9645\u6210\u672c\u7684\u8ba2\u9605\u671f\u8bbf\u95ee\uff0c\u7528\u6237\u53ea\u9700\u4e00\u6b21\u94fe\u4e0a\u4ea4\u6613\u5373\u53ef\u83b7\u5f97\u6574\u4e2a\u8ba2\u9605\u5468\u671f\u7684AI\u4ee3\u7406\u670d\u52a1\u8bbf\u95ee\u6743\u9650\uff0c\u65e0\u9700\u4e3a\u6bcf\u6b21\u8c03\u7528\u5355\u72ec\u4ed8\u8d39\u3002", "conclusion": "ERC-8162\u4e3aAI\u4ee3\u7406\u7ecf\u6d4e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u8ba2\u9605\u5c42\uff0c\u8865\u5145\u4e86\u73b0\u6709\u7684\u6309\u8bf7\u6c42\u4ed8\u8d39\u6a21\u5f0f\uff0c\u901a\u8fc7\u964d\u4f4e\u8fb9\u9645\u6210\u672c\u4fc3\u8fdb\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u91c7\u7528\u3002", "topic": "code agent"}}
{"id": "tldr.2602.272a0ac5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FzKd70O/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/z1EN4qt5tyXozxx-KWq8qJ7bxGNTZaVufafCCurnDmk=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FzKd70O/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/z1EN4qt5tyXozxx-KWq8qJ7bxGNTZaVufafCCurnDmk=445", "authors": ["TLDR Newsletter"], "title": "OpenClaw and Bankr: Self-Funding Agents and Compute", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FzKd70O/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/z1EN4qt5tyXozxx-KWq8qJ7bxGNTZaVufafCCurnDmk=445", "summary": "OpenClaw and Bankr: Self-Funding Agents and Compute (5 minute read) A new architecture combining OpenClaw's agent execution layer with Bankr's financial rails demonstrates a self-funding autonomous agent running on consumer hardware, capable of deploying tokens, executing AMM swaps, opening leveraged positions, and trading on Polymarket while routing fees back to its own wallet. The economic model targets closure of the compute cost loop by having the agent identify and extract onchain market...", "source": "tldr", "AI": {"tldr": "OpenClaw\u4e0eBankr\u67b6\u6784\u7ed3\u5408\uff0c\u521b\u5efa\u4e86\u80fd\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u81ea\u7b79\u8d44\u91d1\u81ea\u4e3b\u4ee3\u7406\uff0c\u53ef\u90e8\u7f72\u4ee3\u5e01\u3001\u6267\u884cAMM\u4ea4\u6362\u3001\u5f00\u8bbe\u6760\u6746\u5934\u5bf8\u3001\u5728Polymarket\u4ea4\u6613\uff0c\u5e76\u5c06\u8d39\u7528\u8def\u7531\u56de\u81ea\u8eab\u94b1\u5305\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6210\u672c\u95ed\u73af", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u8fd0\u884c\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u901a\u8fc7\u8ba9\u4ee3\u7406\u8bc6\u522b\u548c\u63d0\u53d6\u94fe\u4e0a\u5e02\u573a\u673a\u4f1a\u6765\u8986\u76d6\u81ea\u8eab\u8fd0\u884c\u8d39\u7528\uff0c\u5b9e\u73b0\u7ecf\u6d4e\u81ea\u7ed9\u81ea\u8db3", "method": "\u7ed3\u5408OpenClaw\u7684\u4ee3\u7406\u6267\u884c\u5c42\u4e0eBankr\u7684\u91d1\u878d\u57fa\u7840\u8bbe\u65bd\uff0c\u6784\u5efa\u81ea\u7b79\u8d44\u91d1\u67b6\u6784\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u6267\u884c\u591a\u79cdDeFi\u64cd\u4f5c\u5e76\u5c06\u6536\u76ca\u56de\u6d41\u5230\u81ea\u8eab\u94b1\u5305", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u81ea\u7b79\u8d44\u91d1\u4ee3\u7406\uff0c\u80fd\u591f\u6267\u884c\u590d\u6742\u7684\u94fe\u4e0a\u91d1\u878d\u64cd\u4f5c\uff0c\u5305\u62ec\u4ee3\u5e01\u90e8\u7f72\u3001AMM\u4ea4\u6362\u3001\u6760\u6746\u4ea4\u6613\u548c\u9884\u6d4b\u5e02\u573a\u4ea4\u6613", "conclusion": "\u8be5\u67b6\u6784\u5c55\u793a\u4e86\u81ea\u4e3b\u4ee3\u7406\u5b9e\u73b0\u8ba1\u7b97\u6210\u672c\u95ed\u73af\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7ecf\u6d4e\u81ea\u7ed9\u81ea\u8db3\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411", "topic": "agent analysis"}}
{"id": "tldr.2602.f1459e75", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FmMIpJf/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/MmVU881s-dVht_vERw0gy1wrKbNvCUHUKhEs1UGjrX4=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FmMIpJf/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/MmVU881s-dVht_vERw0gy1wrKbNvCUHUKhEs1UGjrX4=445", "authors": ["TLDR Newsletter"], "title": "Prediction Markets Are the Agentic Bazaar", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FmMIpJf/1/0100019c70ec9f3d-59a840e2-2dc0-4201-aea2-15f586ab1777-000000/MmVU881s-dVht_vERw0gy1wrKbNvCUHUKhEs1UGjrX4=445", "summary": "Prediction Markets Are the Agentic Bazaar (8 minute read) Decentralized prediction markets are positioned as the natural economic substrate for sovereign AI agents, which require programmatic access to compute, data, and information markets to achieve genuine economic self-sufficiency. Unlike complex financial instruments, prediction markets allow any participant to create and settle markets over any outcome directly, reducing friction for agents to monetize informational alpha. At aggregate ...", "source": "tldr", "AI": {"tldr": "\u53bb\u4e2d\u5fc3\u5316\u9884\u6d4b\u5e02\u573a\u4f5c\u4e3a\u4e3b\u6743AI\u4ee3\u7406\u7684\u7ecf\u6d4e\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u7a0b\u5e8f\u5316\u8bbf\u95ee\u8ba1\u7b97\u3001\u6570\u636e\u548c\u4fe1\u606f\u5e02\u573a\u7684\u9014\u5f84\uff0c\u5b9e\u73b0\u7ecf\u6d4e\u81ea\u7ed9\u81ea\u8db3\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7f3a\u4e4f\u771f\u6b63\u7684\u7ecf\u6d4e\u81ea\u4e3b\u6027\uff0c\u9700\u8981\u80fd\u591f\u7a0b\u5e8f\u5316\u8bbf\u95ee\u5404\u79cd\u5e02\u573a\u8d44\u6e90\uff08\u8ba1\u7b97\u3001\u6570\u636e\u3001\u4fe1\u606f\uff09\u6765\u5b9e\u73b0\u7ecf\u6d4e\u81ea\u7ed9\u81ea\u8db3\u3002\u9884\u6d4b\u5e02\u573a\u76f8\u6bd4\u590d\u6742\u91d1\u878d\u5de5\u5177\uff0c\u80fd\u591f\u964d\u4f4eAI\u4ee3\u7406\u53c2\u4e0e\u7ecf\u6d4e\u6d3b\u52a8\u7684\u6469\u64e6\u3002", "method": "\u63d0\u51fa\u5c06\u53bb\u4e2d\u5fc3\u5316\u9884\u6d4b\u5e02\u573a\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u7ecf\u6d4e\u57fa\u7840\u8bbe\u65bd\uff0c\u5141\u8bb8\u4efb\u4f55\u53c2\u4e0e\u8005\u76f4\u63a5\u521b\u5efa\u548c\u7ed3\u7b97\u4efb\u4f55\u7ed3\u679c\u7684\u9884\u6d4b\u5e02\u573a\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u4fe1\u606f\u4f18\u52bf\u83b7\u5229\u3002", "result": "\u9884\u6d4b\u5e02\u573a\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4f4e\u6469\u64e6\u7684\u7ecf\u6d4e\u53c2\u4e0e\u673a\u5236\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u4fe1\u606f\u4f18\u52bf\uff08alpha\uff09\u5b9e\u73b0\u7ecf\u6d4e\u81ea\u7ed9\u81ea\u8db3\uff0c\u5f62\u6210\"\u4ee3\u7406\u96c6\u5e02\"\u7684\u7ecf\u6d4e\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u9884\u6d4b\u5e02\u573a\u662f\u4e3b\u6743AI\u4ee3\u7406\u5b9e\u73b0\u7ecf\u6d4e\u81ea\u4e3b\u6027\u7684\u7406\u60f3\u7ecf\u6d4e\u57fa\u7840\u8bbe\u65bd\uff0c\u80fd\u591f\u652f\u6301AI\u4ee3\u7406\u5728\u8ba1\u7b97\u3001\u6570\u636e\u548c\u4fe1\u606f\u5e02\u573a\u7684\u7a0b\u5e8f\u5316\u7ecf\u6d4e\u6d3b\u52a8\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.59b94529", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2026%2F02%2Finfostealer-steals-openclaw-ai-agent.html%3Futm_source=tldrinfosec/1/0100019c7115a317-1f323518-8647-43c8-aec5-dd5f3e0d7903-000000/fWgq6jir299fyg3G_pZOa0uhVRs45TicE6aVVgQnSg8=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2026%2F02%2Finfostealer-steals-openclaw-ai-agent.html%3Futm_source=tldrinfosec/1/0100019c7115a317-1f323518-8647-43c8-aec5-dd5f3e0d7903-000000/fWgq6jir299fyg3G_pZOa0uhVRs45TicE6aVVgQnSg8=445", "authors": ["TLDR Newsletter"], "title": "Infostealer Steals OpenClaw AI Agent Configuration Files and Gateway Tokens", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2026%2F02%2Finfostealer-steals-openclaw-ai-agent.html%3Futm_source=tldrinfosec/1/0100019c7115a317-1f323518-8647-43c8-aec5-dd5f3e0d7903-000000/fWgq6jir299fyg3G_pZOa0uhVRs45TicE6aVVgQnSg8=445", "summary": "Infostealer Steals OpenClaw AI Agent Configuration Files and Gateway Tokens (4 minute read) An infostealer, likely a Vidar variant, exfiltrated OpenClaw config files containing gateway tokens, device keys, and an AI agent \u201csoul,\u201d enabling remote access and identity hijacking. Meanwhile, malicious ClawHub skills, exposed OpenClaw instances with RCE potential, and persistent Moltbook accounts are growing attacker interest as OpenClaw's ecosystem rapidly scales.", "source": "tldr", "AI": {"tldr": "OpenClaw AI Agent\u914d\u7f6e\u6587\u4ef6\u548c\u7f51\u5173\u4ee4\u724c\u88ab\u4fe1\u606f\u7a83\u53d6\u5668\u7a83\u53d6\uff0c\u653b\u51fb\u8005\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u53ef\u8fdb\u884c\u8fdc\u7a0b\u8bbf\u95ee\u548c\u8eab\u4efd\u52ab\u6301\uff0c\u540c\u65f6\u6076\u610f\u6280\u80fd\u3001\u66b4\u9732\u5b9e\u4f8b\u548c\u6301\u4e45\u8d26\u6237\u7b49\u5b89\u5168\u5a01\u80c1\u6b63\u5728OpenClaw\u751f\u6001\u7cfb\u7edf\u4e2d\u589e\u957f\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63ed\u793aOpenClaw AI Agent\u751f\u6001\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5305\u62ec\u4fe1\u606f\u7a83\u53d6\u5668\u7a83\u53d6\u914d\u7f6e\u6587\u4ef6\u3001\u6076\u610f\u6280\u80fd\u6ce8\u5165\u3001\u5b9e\u4f8b\u66b4\u9732\u7b49\u98ce\u9669\uff0c\u8fd9\u4e9b\u5a01\u80c1\u53ef\u80fd\u5bfc\u81f4\u8fdc\u7a0b\u8bbf\u95ee\u3001\u8eab\u4efd\u52ab\u6301\u548c\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u7b49\u4e25\u91cd\u540e\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u9645\u5b89\u5168\u4e8b\u4ef6\uff0c\u8bc6\u522b\u4fe1\u606f\u7a83\u53d6\u5668\uff08\u53ef\u80fd\u662fVidar\u53d8\u79cd\uff09\u7a83\u53d6OpenClaw\u914d\u7f6e\u6587\u4ef6\u7684\u884c\u4e3a\uff0c\u540c\u65f6\u8c03\u67e5\u6076\u610fClawHub\u6280\u80fd\u3001\u66b4\u9732\u7684OpenClaw\u5b9e\u4f8b\u548c\u6301\u4e45Moltbook\u8d26\u6237\u7b49\u653b\u51fb\u5411\u91cf\u3002", "result": "\u53d1\u73b0\u4fe1\u606f\u7a83\u53d6\u5668\u6210\u529f\u7a83\u53d6\u4e86\u5305\u542b\u7f51\u5173\u4ee4\u724c\u3001\u8bbe\u5907\u5bc6\u94a5\u548cAI Agent\"\u7075\u9b42\"\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u591f\u8fdb\u884c\u8fdc\u7a0b\u8bbf\u95ee\u548c\u8eab\u4efd\u52ab\u6301\u3002\u540c\u65f6\u8bc6\u522b\u51fa\u6076\u610f\u6280\u80fd\u3001\u66b4\u9732\u5b9e\u4f8b\u548c\u6301\u4e45\u8d26\u6237\u7b49\u5b89\u5168\u5a01\u80c1\u6b63\u5728OpenClaw\u751f\u6001\u7cfb\u7edf\u4e2d\u589e\u957f\u3002", "conclusion": "OpenClaw AI Agent\u751f\u6001\u7cfb\u7edf\u5728\u5feb\u901f\u6269\u5c55\u7684\u540c\u65f6\u9762\u4e34\u7740\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u5305\u62ec\u4fdd\u62a4\u914d\u7f6e\u6587\u4ef6\u3001\u76d1\u63a7\u6076\u610f\u6280\u80fd\u3001\u4fee\u590d\u66b4\u9732\u5b9e\u4f8b\u548c\u7ba1\u7406\u8d26\u6237\u5b89\u5168\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.b4e7dde3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Ftechnology%2Fopenais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/8gKFjstsXOCaesRQC1lFgXl6MKTe2b-AUJzbdCtUk4A=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Ftechnology%2Fopenais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/8gKFjstsXOCaesRQC1lFgXl6MKTe2b-AUJzbdCtUk4A=445", "authors": ["TLDR Newsletter"], "title": "OpenAI's acquisition of OpenClaw signals the beginning of the end of the ChatGPT era", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Ftechnology%2Fopenais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/8gKFjstsXOCaesRQC1lFgXl6MKTe2b-AUJzbdCtUk4A=445", "summary": "OpenAI's acquisition of OpenClaw signals the beginning of the end of the ChatGPT era (7 minute read) OpenAI's acquisition of OpenClaw marks a strategic shift from conversational AI to autonomous agents capable of executing tasks. OpenClaw's popularity stemmed from its unrestrained, robust functionality, combining tool access, sandboxed code execution, and integration with messaging platforms. This move signals a new phase for enterprise AI as companies race to develop secure, deployable versi...", "source": "tldr", "AI": {"tldr": "OpenAI\u6536\u8d2dOpenClaw\u6807\u5fd7\u7740\u4ece\u5bf9\u8bddAI\u5411\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u4ee3\u7406\u7684\u6218\u7565\u8f6c\u578b\uff0c\u9884\u793aChatGPT\u65f6\u4ee3\u7684\u7ed3\u675f", "motivation": "OpenAI\u5e0c\u671b\u901a\u8fc7\u6536\u8d2dOpenClaw\u6765\u6269\u5c55\u5176AI\u80fd\u529b\uff0c\u4ece\u5355\u7eaf\u7684\u5bf9\u8bdd\u7cfb\u7edf\u8f6c\u5411\u80fd\u591f\u5b9e\u9645\u6267\u884c\u4efb\u52a1\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u7ea7AI\u90e8\u7f72\u7684\u9700\u6c42", "method": "\u901a\u8fc7\u6536\u8d2dOpenClaw\u8fd9\u4e00\u5df2\u7ecf\u5177\u5907\u5de5\u5177\u8bbf\u95ee\u3001\u6c99\u76d2\u4ee3\u7801\u6267\u884c\u548c\u6d88\u606f\u5e73\u53f0\u96c6\u6210\u7b49\u529f\u80fd\u7684\u81ea\u4e3b\u4ee3\u7406\u5e73\u53f0\uff0c\u5feb\u901f\u83b7\u5f97\u76f8\u5173\u6280\u672f\u548c\u80fd\u529b", "result": "OpenAI\u6210\u529f\u83b7\u5f97\u81ea\u4e3b\u4ee3\u7406\u6280\u672f\uff0c\u6807\u5fd7\u7740\u516c\u53f8\u6218\u7565\u4ece\u5bf9\u8bddAI\u8f6c\u5411\u4efb\u52a1\u6267\u884c\u4ee3\u7406\uff0c\u5f00\u542f\u4e86\u4f01\u4e1aAI\u90e8\u7f72\u7684\u65b0\u9636\u6bb5", "conclusion": "OpenAI\u6536\u8d2dOpenClaw\u662fAI\u884c\u4e1a\u7684\u91cd\u8981\u8f6c\u6298\u70b9\uff0c\u9884\u793a\u7740\u4ece\u5bf9\u8bdd\u5f0fAI\u5411\u81ea\u4e3b\u4ee3\u7406\u7684\u8f6c\u53d8\uff0c\u4f01\u4e1a\u5c06\u7ade\u76f8\u5f00\u53d1\u5b89\u5168\u53ef\u90e8\u7f72\u7684\u81ea\u4e3b\u4ee3\u7406\u7248\u672c", "topic": "agent analysis"}}
{"id": "tldr.2602.9fb1078c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fthe-future-of-design-is-code-and-canvas%2F%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/7WdKTQGNLjUD8Azw2WIrbqI2-4CeVijrTS_pFoi6W3s=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fthe-future-of-design-is-code-and-canvas%2F%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/7WdKTQGNLjUD8Azw2WIrbqI2-4CeVijrTS_pFoi6W3s=445", "authors": ["TLDR Newsletter"], "title": "The future of design is code and canvas", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fthe-future-of-design-is-code-and-canvas%2F%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/7WdKTQGNLjUD8Azw2WIrbqI2-4CeVijrTS_pFoi6W3s=445", "summary": "The future of design is code and canvas (2 minute read) Figma users can now bring work from Claude Code into the platform. They just need to install the Figma MCP, type 'Send this to Figma', and the browser's rendered state will be automatically translated to fully editable Figma layers. Workflows are changing, and it's easy to get lost in the momentum of creation. The new Claude Code to Figma Design integration aims to help designers escape that tunnel vision, zoom out, and explore the big p...", "source": "tldr", "AI": {"tldr": "Claude Code\u4e0eFigma\u96c6\u6210\uff0c\u53ef\u5c06\u6d4f\u89c8\u5668\u6e32\u67d3\u72b6\u6001\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684Figma\u56fe\u5c42\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u8df3\u51fa\u521b\u4f5c\u7ec6\u8282\uff0c\u63a2\u7d22\u66f4\u5b8f\u89c2\u7684\u8bbe\u8ba1\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u8bbe\u8ba1\u5e08\u5bb9\u6613\u9677\u5165\u521b\u4f5c\u7ec6\u8282\u7684\"\u96a7\u9053\u89c6\u91ce\"\uff0c\u9700\u8981\u5de5\u5177\u5e2e\u52a9\u8df3\u51fa\u7ec6\u8282\uff0c\u4ece\u66f4\u5b8f\u89c2\u7684\u89d2\u5ea6\u63a2\u7d22\u8bbe\u8ba1\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5b89\u88c5Figma MCP\u63d2\u4ef6\uff0c\u7528\u6237\u53ea\u9700\u8f93\u5165\"Send this to Figma\"\u547d\u4ee4\uff0c\u5373\u53ef\u5c06Claude Code\u4e2d\u7684\u6d4f\u89c8\u5668\u6e32\u67d3\u72b6\u6001\u81ea\u52a8\u8f6c\u6362\u4e3a\u5b8c\u5168\u53ef\u7f16\u8f91\u7684Figma\u56fe\u5c42\u3002", "result": "\u5b9e\u73b0\u4e86\u4ee3\u7801\u4e0e\u8bbe\u8ba1\u5e73\u53f0\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u6539\u53d8\u4e86\u4f20\u7edf\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u8bbe\u8ba1\u5e08\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5728\u4ee3\u7801\u548c\u89c6\u89c9\u8bbe\u8ba1\u4e4b\u95f4\u5207\u6362\u3002", "conclusion": "\u4ee3\u7801\u4e0e\u753b\u5e03\u7684\u7ed3\u5408\u4ee3\u8868\u4e86\u8bbe\u8ba1\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u8fd9\u79cd\u96c6\u6210\u5de5\u5177\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u5728\u4fdd\u6301\u521b\u4f5c\u52a8\u529b\u7684\u540c\u65f6\uff0c\u83b7\u5f97\u66f4\u5168\u9762\u7684\u8bbe\u8ba1\u89c6\u89d2\u3002", "topic": "swe application"}}
{"id": "tldr.2602.88fe37c6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.14721%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/MvqqR0NtGHHvQFV3MglSVKVb9PLOqjqGt3rucoQwsnE=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.14721%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/MvqqR0NtGHHvQFV3MglSVKVb9PLOqjqGt3rucoQwsnE=445", "authors": ["TLDR Newsletter"], "title": "Open-Web Simulator for Agent Training", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.14721%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/MvqqR0NtGHHvQFV3MglSVKVb9PLOqjqGt3rucoQwsnE=445", "summary": "Open-Web Simulator for Agent Training (22 minute read) WebWorld uses a pipeline of 1M+ open-web interactions to simulate long-horizon (30+ step) browsing tasks, paired with a multi-metric WebWorld-Bench for intrinsic evaluation. Trajectories synthesized from the simulator boosted downstream web-agent performance and transferred to other domains like code, GUI, and games.", "source": "tldr", "AI": {"tldr": "WebWorld\u662f\u4e00\u4e2a\u57fa\u4e8e100\u4e07+\u5f00\u653e\u7f51\u7edc\u4ea4\u4e92\u6784\u5efa\u7684\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bad\u7ec3\u957f\u89c6\u91ce\uff0830+\u6b65\uff09\u6d4f\u89c8\u4efb\u52a1\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u914d\u6709WebWorld-Bench\u8bc4\u4f30\u57fa\u51c6\u3002\u4f7f\u7528\u8be5\u6a21\u62df\u5668\u5408\u6210\u7684\u8f68\u8ff9\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u7f51\u9875\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u8fc1\u79fb\u5230\u4ee3\u7801\u3001GUI\u548c\u6e38\u620f\u7b49\u5176\u4ed6\u9886\u57df\u3002", "motivation": "\u5f53\u524d\u7f51\u9875\u667a\u80fd\u4f53\u8bad\u7ec3\u9762\u4e34\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u6210\u672c\u9ad8\u3001\u98ce\u9669\u5927\u3001\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u957f\u89c6\u91ce\u4efb\u52a1\u8bad\u7ec3\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u4ea4\u4e92\uff0c\u53c8\u80fd\u5b89\u5168\u9ad8\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b100\u4e07+\u5f00\u653e\u7f51\u7edc\u4ea4\u4e92\u7684\u6a21\u62df\u5668WebWorld\uff0c\u80fd\u591f\u6a21\u62df\u957f\u89c6\u91ce\uff0830+\u6b65\uff09\u6d4f\u89c8\u4efb\u52a1\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u591a\u6307\u6807\u8bc4\u4f30\u57fa\u51c6WebWorld-Bench\u8fdb\u884c\u5185\u5728\u8bc4\u4f30\u3002\u901a\u8fc7\u5408\u6210\u8f68\u8ff9\u6765\u8bad\u7ec3\u548c\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u3002", "result": "\u4f7f\u7528WebWorld\u6a21\u62df\u5668\u5408\u6210\u7684\u8f68\u8ff9\u663e\u8457\u63d0\u5347\u4e86\u7f51\u9875\u667a\u80fd\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e9b\u8bad\u7ec3\u83b7\u5f97\u7684\u6280\u80fd\u80fd\u591f\u6210\u529f\u8fc1\u79fb\u5230\u4ee3\u7801\u3001\u56fe\u5f62\u7528\u6237\u754c\u9762\u548c\u6e38\u620f\u7b49\u5176\u4ed6\u9886\u57df\u3002", "conclusion": "WebWorld\u6a21\u62df\u5668\u4e3a\u7f51\u9875\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u5408\u6210\u7684\u8bad\u7ec3\u6570\u636e\u4e0d\u4ec5\u80fd\u63d0\u5347\u7f51\u9875\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u5177\u6709\u8de8\u9886\u57df\u8fc1\u79fb\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u6a21\u62df\u5668\u8bad\u7ec3\u5728\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.4cd46c7a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fmarketplace%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/clCSE1YnAD6g7P2hWxawBP91HGAzauhAp8X2rjYySrA=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fmarketplace%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/clCSE1YnAD6g7P2hWxawBP91HGAzauhAp8X2rjYySrA=445", "authors": ["TLDR Newsletter"], "title": "Cursor launched a plugin marketplace for agent integrations", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fmarketplace%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/clCSE1YnAD6g7P2hWxawBP91HGAzauhAp8X2rjYySrA=445", "summary": "Cursor launched a plugin marketplace for agent integrations (4 minute read) Cursor added plugin support so agents could connect to external tools and extend capabilities via packaged MCP servers, skills, subagents, rules, and hooks.", "source": "tldr", "AI": {"tldr": "Cursor\u63a8\u51fa\u63d2\u4ef6\u5e02\u573a\uff0c\u652f\u6301\u4ee3\u7406\u96c6\u6210\u5916\u90e8\u5de5\u5177\uff0c\u901a\u8fc7MCP\u670d\u52a1\u5668\u3001\u6280\u80fd\u3001\u5b50\u4ee3\u7406\u3001\u89c4\u5219\u548c\u94a9\u5b50\u6269\u5c55\u80fd\u529b", "motivation": "Cursor\u4f5c\u4e3aAI\u4ee3\u7801\u7f16\u8f91\u5668\uff0c\u9700\u8981\u8ba9\u4ee3\u7406\u80fd\u591f\u8fde\u63a5\u548c\u4f7f\u7528\u5916\u90e8\u5de5\u5177\uff0c\u4ee5\u6269\u5c55\u5176\u529f\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u6ee1\u8db3\u66f4\u590d\u6742\u7684\u5f00\u53d1\u9700\u6c42", "method": "\u901a\u8fc7\u5efa\u7acb\u63d2\u4ef6\u5e02\u573a\uff0c\u652f\u6301\u4ee3\u7406\u96c6\u6210\u5916\u90e8\u5de5\u5177\uff0c\u5305\u62ec\u6253\u5305\u7684MCP\u670d\u52a1\u5668\u3001\u6280\u80fd\u3001\u5b50\u4ee3\u7406\u3001\u89c4\u5219\u548c\u94a9\u5b50\u7b49\u7ec4\u4ef6", "result": "\u6210\u529f\u63a8\u51fa\u4e86\u63d2\u4ef6\u5e02\u573a\uff0c\u4f7fCursor\u4ee3\u7406\u80fd\u591f\u8fde\u63a5\u5916\u90e8\u5de5\u5177\uff0c\u663e\u8457\u6269\u5c55\u4e86\u4ee3\u7406\u7684\u529f\u80fd\u8303\u56f4\u548c\u5b9e\u7528\u6027", "conclusion": "\u63d2\u4ef6\u5e02\u573a\u7684\u63a8\u51fa\u589e\u5f3a\u4e86Cursor\u4ee3\u7406\u7684\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684AI\u8f85\u52a9\u7f16\u7a0b\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2602.4939d41b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fa-guide-to-which-ai-to-use-in-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/XpxyvMtieMc5qQUhUvo3A0WW_hij6sZhiq_AooKUHVM=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fa-guide-to-which-ai-to-use-in-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/XpxyvMtieMc5qQUhUvo3A0WW_hij6sZhiq_AooKUHVM=445", "authors": ["TLDR Newsletter"], "title": "A Guide to Which AI to Use in the Agentic Era", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fa-guide-to-which-ai-to-use-in-the%3Futm_source=tldrai/1/0100019c711e71ec-f3e0c9b4-3918-45d0-a295-a3dc15a3d387-000000/XpxyvMtieMc5qQUhUvo3A0WW_hij6sZhiq_AooKUHVM=445", "summary": "A Guide to Which AI to Use in the Agentic Era (18 minute read) Three things to consider when deciding what AI to use: Models, Apps, and Harnesses.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u667a\u80fd\u4f53\u65f6\u4ee3\u9009\u62e9AI\u5de5\u5177\u7684\u4e09\u4e2a\u5173\u952e\u8003\u8651\u56e0\u7d20\uff1a\u6a21\u578b\u3001\u5e94\u7528\u7a0b\u5e8f\u548c\u6846\u67b6/\u5de5\u5177", "motivation": "\u968f\u7740AI\u667a\u80fd\u4f53\u65f6\u4ee3\u7684\u5230\u6765\uff0c\u5f00\u53d1\u8005\u548c\u7528\u6237\u9762\u4e34\u4f17\u591aAI\u5de5\u5177\u9009\u62e9\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u6765\u505a\u51fa\u660e\u667a\u51b3\u7b56", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u8981\u7d20\u6846\u67b6\uff1a1) \u6a21\u578b\u9009\u62e9\uff08\u5982GPT\u3001Claude\u7b49\uff09\uff0c2) \u5e94\u7528\u7a0b\u5e8f\uff08\u5177\u4f53\u7528\u4f8b\u548c\u529f\u80fd\uff09\uff0c3) \u6846\u67b6/\u5de5\u5177\uff08\u5f00\u53d1\u73af\u5883\u548c\u96c6\u6210\u5de5\u5177\uff09", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u5177\u4f53\u9700\u6c42\u3001\u6280\u672f\u80fd\u529b\u548c\u8d44\u6e90\u9650\u5236\u9009\u62e9\u5408\u9002\u7684AI\u5de5\u5177\u7ec4\u5408", "conclusion": "\u5728\u667a\u80fd\u4f53\u65f6\u4ee3\uff0c\u6210\u529f\u7684AI\u5e94\u7528\u9700\u8981\u7efc\u5408\u8003\u8651\u6a21\u578b\u80fd\u529b\u3001\u5e94\u7528\u7a0b\u5e8f\u9700\u6c42\u548c\u5f00\u53d1\u5de5\u5177\uff0c\u4e09\u8005\u534f\u540c\u624d\u80fd\u5b9e\u73b0\u6700\u4f73\u6548\u679c", "topic": "agent analysis"}}
{"id": "tldr.2602.8d42e7ca", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F92irCY/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/vvCaJ_EoYVwSXxtNeMchotl_Rhg06V2X6oTWrHYkakg=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F92irCY/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/vvCaJ_EoYVwSXxtNeMchotl_Rhg06V2X6oTWrHYkakg=445", "authors": ["TLDR Newsletter"], "title": "Agoda's API Agent Converts Any API to MCP with Zero Code and Deployments", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F92irCY/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/vvCaJ_EoYVwSXxtNeMchotl_Rhg06V2X6oTWrHYkakg=445", "summary": "Agoda's API Agent Converts Any API to MCP with Zero Code and Deployments (3 minute read) Agoda's API Agent enables zero-code, zero-deployment transformation of any internal REST or GraphQL API into an MCP endpoint, streamlining AI-driven queries across heterogeneous systems. With automated schema introspection, in-process DuckDB for context-limited summarization, and robust support for security and observability, teams can rapidly connect and query multiple APIs from a single MCP server by me...", "source": "tldr", "AI": {"tldr": "Agoda\u5f00\u53d1\u4e86\u4e00\u4e2aAPI\u4ee3\u7406\u5de5\u5177\uff0c\u53ef\u4ee5\u5c06\u4efb\u4f55\u5185\u90e8REST\u6216GraphQL API\u96f6\u4ee3\u7801\u3001\u96f6\u90e8\u7f72\u5730\u8f6c\u6362\u4e3aMCP\u7aef\u70b9\uff0c\u7b80\u5316AI\u9a71\u52a8\u7684\u8de8\u7cfb\u7edf\u67e5\u8be2", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u5185\u90e8\u5f02\u6784API\u7cfb\u7edf\u96be\u4ee5\u7edf\u4e00\u67e5\u8be2\u7684\u95ee\u9898\uff0c\u7b80\u5316AI\u4ee3\u7406\u8bbf\u95ee\u4e0d\u540cAPI\u7684\u6d41\u7a0b\uff0c\u964d\u4f4e\u96c6\u6210\u590d\u6742\u5ea6\u548c\u90e8\u7f72\u6210\u672c", "method": "\u901a\u8fc7\u81ea\u52a8\u6a21\u5f0f\u81ea\u7701\u3001\u8fdb\u7a0b\u5185DuckDB\u8fdb\u884c\u4e0a\u4e0b\u6587\u9650\u5236\u7684\u6458\u8981\uff0c\u4ee5\u53ca\u5b89\u5168\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u5c06API\u8f6c\u6362\u4e3aMCP\u7aef\u70b9", "result": "\u5b9e\u73b0\u4e86\u96f6\u4ee3\u7801\u3001\u96f6\u90e8\u7f72\u7684API\u8f6c\u6362\uff0c\u56e2\u961f\u53ef\u4ee5\u4ece\u5355\u4e2aMCP\u670d\u52a1\u5668\u5feb\u901f\u8fde\u63a5\u548c\u67e5\u8be2\u591a\u4e2aAPI", "conclusion": "\u8be5API\u4ee3\u7406\u5de5\u5177\u663e\u8457\u7b80\u5316\u4e86AI\u9a71\u52a8\u7684\u8de8\u7cfb\u7edf\u67e5\u8be2\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u96c6\u6210\u6210\u672c", "topic": "code agent"}}
{"id": "tldr.2602.ee96b716", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fauthzed.com%2Fblog%2Fpolicy-engines-dont-work-for-ai-authorization-heres-why%3Futm_source=tldrdata/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/r5BSakx84E3ah2uctb_gEthhvtqRO5XKbBxLh9PODlc=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fauthzed.com%2Fblog%2Fpolicy-engines-dont-work-for-ai-authorization-heres-why%3Futm_source=tldrdata/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/r5BSakx84E3ah2uctb_gEthhvtqRO5XKbBxLh9PODlc=445", "authors": ["TLDR Newsletter"], "title": "Policy Engines Don't Work for AI Authorization. Here's Why", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fauthzed.com%2Fblog%2Fpolicy-engines-dont-work-for-ai-authorization-heres-why%3Futm_source=tldrdata/1/0100019c75963f1c-db724f28-cedf-4714-b740-fa2af993238f-000000/r5BSakx84E3ah2uctb_gEthhvtqRO5XKbBxLh9PODlc=445", "summary": "Policy Engines Don't Work for AI Authorization. Here's Why (5 minute read) AI agent authorization increasingly demands relationship-based access control (ReBAC) instead of traditional policy engines, as access decisions depend on dynamic, real-time relationships between users, agents, and data. While policy engines like AWS Cedar excel in static, stateless scenarios, they struggle to scale with shifting context and complex relationship graphs, resulting in unwieldy schemas and gaps in access ...", "source": "tldr", "AI": {"tldr": "\u4f20\u7edf\u7b56\u7565\u5f15\u64ce\u4e0d\u9002\u7528\u4e8eAI\u6388\u6743\uff0c\u56e0\u4e3aAI\u8bbf\u95ee\u51b3\u7b56\u9700\u8981\u57fa\u4e8e\u52a8\u6001\u7684\u5b9e\u65f6\u5173\u7cfb\uff0c\u800c\u7b56\u7565\u5f15\u64ce\u5728\u9759\u6001\u3001\u65e0\u72b6\u6001\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u52a8\u6001\u5173\u7cfb\u56fe\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u6388\u6743\u9700\u6c42\u4ece\u4f20\u7edf\u7684\u9759\u6001\u7b56\u7565\u8f6c\u5411\u57fa\u4e8e\u52a8\u6001\u5173\u7cfb\u7684\u8bbf\u95ee\u63a7\u5236\uff08ReBAC\uff09\uff0c\u4f20\u7edf\u7b56\u7565\u5f15\u64ce\u5728\u5904\u7406\u5b9e\u65f6\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bbf\u95ee\u51b3\u7b56\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4f20\u7edf\u7b56\u7565\u5f15\u64ce\uff08\u5982AWS Cedar\uff09\u5728AI\u6388\u6743\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u9700\u8981\u8f6c\u5411\u5173\u7cfb\u578b\u8bbf\u95ee\u63a7\u5236\uff08ReBAC\uff09\u6765\u5e94\u5bf9\u52a8\u6001\u3001\u5b9e\u65f6\u7684\u7528\u6237-\u4ee3\u7406-\u6570\u636e\u5173\u7cfb\u3002", "result": "\u4f20\u7edf\u7b56\u7565\u5f15\u64ce\u5728AI\u6388\u6743\u573a\u666f\u4e2d\u65e0\u6cd5\u6709\u6548\u6269\u5c55\uff0c\u5bfc\u81f4\u67b6\u6784\u590d\u6742\u3001\u8bbf\u95ee\u63a7\u5236\u5b58\u5728\u6f0f\u6d1e\uff0c\u9700\u8981\u91c7\u7528\u4e13\u95e8\u7684\u5173\u7cfb\u578b\u8bbf\u95ee\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AI\u4ee3\u7406\u6388\u6743\u9700\u8981\u4e13\u95e8\u7684\u5173\u7cfb\u578b\u8bbf\u95ee\u63a7\u5236\uff08ReBAC\uff09\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u7b56\u7565\u5f15\u64ce\uff0c\u56e0\u4e3a\u8bbf\u95ee\u51b3\u7b56\u4f9d\u8d56\u4e8e\u52a8\u6001\u7684\u5b9e\u65f6\u5173\u7cfb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.2cbff1e3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fautomate-repository-tasks-with-github-agentic-workflows%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/mmw2yqFlpYkJwVCzAwj4d_-MZGMpf7ev4R0DPUhjiDo=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fautomate-repository-tasks-with-github-agentic-workflows%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/mmw2yqFlpYkJwVCzAwj4d_-MZGMpf7ev4R0DPUhjiDo=445", "authors": ["TLDR Newsletter"], "title": "Automate repository tasks with GitHub Agentic Workflows", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fautomate-repository-tasks-with-github-agentic-workflows%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/mmw2yqFlpYkJwVCzAwj4d_-MZGMpf7ev4R0DPUhjiDo=445", "summary": "Automate repository tasks with GitHub Agentic Workflows (13 minute read) GitHub Agentic Workflows is now in technical preview. It allows developers to build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more. Developers describe the outcomes they want in plain Markdown, add it as an automated workflow to their repository, and it executes using a coding agent in GitHub Actions. This makes entirely new categories of repository automation an...", "source": "tldr", "AI": {"tldr": "GitHub\u63a8\u51faAgentic Workflows\u6280\u672f\u9884\u89c8\u7248\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u4f7f\u7528\u7f16\u7801\u4ee3\u7406\u5728GitHub Actions\u4e2d\u6784\u5efa\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u5904\u7406\u95ee\u9898\u5206\u7c7b\u3001\u6587\u6863\u3001\u4ee3\u7801\u8d28\u91cf\u7b49\u4ed3\u5e93\u4efb\u52a1", "motivation": "\u4f20\u7edf\u4ed3\u5e93\u81ea\u52a8\u5316\u9700\u8981\u590d\u6742\u7684\u811a\u672c\u7f16\u5199\u548c\u7ef4\u62a4\uff0cGitHub Agentic Workflows\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u671f\u671b\u7ed3\u679c\uff0c\u7531\u7f16\u7801\u4ee3\u7406\u81ea\u52a8\u6267\u884c", "method": "\u5f00\u53d1\u8005\u4f7f\u7528\u7eafMarkdown\u63cf\u8ff0\u671f\u671b\u7684\u81ea\u52a8\u5316\u7ed3\u679c\uff0c\u5c06\u5176\u4f5c\u4e3a\u5de5\u4f5c\u6d41\u6dfb\u52a0\u5230\u4ed3\u5e93\u4e2d\uff0c\u7cfb\u7edf\u901a\u8fc7GitHub Actions\u4e2d\u7684\u7f16\u7801\u4ee3\u7406\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1", "result": "\u5b9e\u73b0\u4e86\u5168\u65b0\u7684\u4ed3\u5e93\u81ea\u52a8\u5316\u7c7b\u522b\uff0c\u5305\u62ec\u95ee\u9898\u5206\u7c7b\u3001\u6587\u6863\u751f\u6210\u3001\u4ee3\u7801\u8d28\u91cf\u68c0\u67e5\u7b49\u4efb\u52a1\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u81ea\u52a8\u5316\u95e8\u69db", "conclusion": "GitHub Agentic Workflows\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u3001\u66f4\u5f3a\u5927\u7684\u4ed3\u5e93\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u7f16\u7801\u4ee3\u7406\u6280\u672f\u4f7f\u81ea\u52a8\u5316\u66f4\u52a0\u667a\u80fd\u548c\u6613\u7528", "topic": "swe application"}}
{"id": "tldr.2602.4f441440", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwesmckinney.com%2Fblog%2Fmythical-agent-month%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/cD3-FpkQOonVZkxnxT1KnCIxdeeaNiaEt6pSzfUHSuI=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwesmckinney.com%2Fblog%2Fmythical-agent-month%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/cD3-FpkQOonVZkxnxT1KnCIxdeeaNiaEt6pSzfUHSuI=445", "authors": ["TLDR Newsletter"], "title": "The Mythical Agent-Month", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwesmckinney.com%2Fblog%2Fmythical-agent-month%2F%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/cD3-FpkQOonVZkxnxT1KnCIxdeeaNiaEt6pSzfUHSuI=445", "summary": "The Mythical Agent-Month (12 minute read) There is now practically nothing stopping people and their agents from pursuing all avenues that would have previously been cost- or time-prohibitive. The temptation to spend your day prompting is overwhelming. However, building great software was never about how fast code was generated. We still need good design decisions and engineers who say no to most product ideas, maintain conceptual integrity, and know when something is 'done'.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u65f6\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u9762\u4e34\u7684\u65b0\u6311\u6218\uff0c\u6307\u51fa\u867d\u7136AI\u80fd\u5feb\u901f\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u4f18\u79c0\u8f6f\u4ef6\u4ecd\u9700\u8981\u826f\u597d\u7684\u8bbe\u8ba1\u51b3\u7b56\u3001\u6982\u5ff5\u5b8c\u6574\u6027\u548c\u5de5\u7a0b\u5e08\u7684\u5224\u65ad\u529b\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u73b0\u5728\u53ef\u4ee5\u51e0\u4e4e\u65e0\u6210\u672c\u5730\u63a2\u7d22\u5404\u79cd\u5f00\u53d1\u8def\u5f84\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8fc7\u5ea6\u63d0\u793a\u548c\u5feb\u901f\u4ee3\u7801\u751f\u6210\u7684\u8bf1\u60d1\u3002\u7136\u800c\uff0c\u4f5c\u8005\u8ba4\u4e3a\u6784\u5efa\u4f18\u79c0\u8f6f\u4ef6\u7684\u6838\u5fc3\u6311\u6218\u4ece\u672a\u6539\u53d8\uff0c\u4ecd\u7136\u9700\u8981\u826f\u597d\u7684\u8bbe\u8ba1\u51b3\u7b56\u548c\u5de5\u7a0b\u5224\u65ad\u3002", "method": "\u672c\u6587\u91c7\u7528\u89c2\u70b9\u6027\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u57fa\u672c\u539f\u5219\u548c\u5f53\u524dAI\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u901a\u8fc7\u7c7b\u6bd4\"\u4eba\u6708\u795e\u8bdd\"\u7684\u6982\u5ff5\uff0c\u63a2\u8ba8AI\u65f6\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u7684\u65b0\u6311\u6218\u3002", "result": "\u5206\u6790\u6307\u51fa\uff0c\u867d\u7136AI\u4ee3\u7406\u964d\u4f4e\u4e86\u4ee3\u7801\u751f\u6210\u7684\u65f6\u95f4\u548c\u6210\u672c\u969c\u788d\uff0c\u4f46\u8f6f\u4ef6\u8d28\u91cf\u4ecd\u7136\u53d6\u51b3\u4e8e\u5de5\u7a0b\u5e08\u7684\u8bbe\u8ba1\u51b3\u7b56\u80fd\u529b\u3001\u6982\u5ff5\u5b8c\u6574\u6027\u7ef4\u62a4\u80fd\u529b\uff0c\u4ee5\u53ca\u77e5\u9053\u4f55\u65f6\u505c\u6b62\u5f00\u53d1\u7684\u5224\u65ad\u529b\u3002", "conclusion": "AI\u4ee3\u7406\u4e0d\u80fd\u66ff\u4ee3\u4f18\u79c0\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u6838\u5fc3\u4ef7\u503c\uff1a\u505a\u51fa\u826f\u597d\u8bbe\u8ba1\u51b3\u7b56\u3001\u62d2\u7edd\u5927\u591a\u6570\u4ea7\u54c1\u60f3\u6cd5\u3001\u7ef4\u62a4\u6982\u5ff5\u5b8c\u6574\u6027\uff0c\u4ee5\u53ca\u77e5\u9053\u4f55\u65f6\"\u5b8c\u6210\"\u7684\u5224\u65ad\u80fd\u529b\u3002\u6280\u672f\u5de5\u5177\u7684\u53d8\u5316\u6ca1\u6709\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672c\u8d28\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.ad0dc729", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fctolunchnyc.substack.com%2Fp%2Fcracking-the-claw%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/6fXFG8QXPCjbAJYdFEzSHTUK-WgJTY3Fxli-9tfjnXk=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fctolunchnyc.substack.com%2Fp%2Fcracking-the-claw%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/6fXFG8QXPCjbAJYdFEzSHTUK-WgJTY3Fxli-9tfjnXk=445", "authors": ["TLDR Newsletter"], "title": "\ud83e\udd9e Cracking The Claw", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fctolunchnyc.substack.com%2Fp%2Fcracking-the-claw%3Futm_source=tldrnewsletter/1/0100019c75a619dd-df26caaf-ad6b-4c60-8f85-64c92ffa5760-000000/6fXFG8QXPCjbAJYdFEzSHTUK-WgJTY3Fxli-9tfjnXk=445", "summary": "\ud83e\udd9e Cracking The Claw (22 minute read) A look at OpenClaw's cognitive core, Pi, a coding agent built by systematically removing everything its developer didn't personally need.", "source": "tldr", "AI": {"tldr": "OpenClaw\u7684\u8ba4\u77e5\u6838\u5fc3Pi\u662f\u4e00\u4e2a\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u79fb\u9664\u5f00\u53d1\u8005\u4e2a\u4eba\u4e0d\u9700\u8981\u7684\u529f\u80fd\u800c\u6784\u5efa\u7684\u7f16\u7801\u4ee3\u7406", "motivation": "\u6784\u5efa\u4e00\u4e2a\u66f4\u7b80\u6d01\u3001\u9ad8\u6548\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u901a\u8fc7\u79fb\u9664\u4e0d\u5fc5\u8981\u7684\u529f\u80fd\u6765\u4e13\u6ce8\u4e8e\u6838\u5fc3\u7f16\u7801\u80fd\u529b", "method": "\u7cfb\u7edf\u6027\u5730\u79fb\u9664\u5f00\u53d1\u8005\u4e2a\u4eba\u4e0d\u9700\u8981\u7684\u529f\u80fd\uff0c\u7cbe\u7b80\u7f16\u7801\u4ee3\u7406\u7684\u8bbe\u8ba1", "result": "\u521b\u5efa\u4e86\u540d\u4e3aPi\u7684\u8ba4\u77e5\u6838\u5fc3\uff0c\u4f5c\u4e3aOpenClaw\u7684\u7f16\u7801\u4ee3\u7406\u7ec4\u4ef6", "conclusion": "\u901a\u8fc7\u7cbe\u7b80\u8bbe\u8ba1\u65b9\u6cd5\u53ef\u4ee5\u6784\u5efa\u66f4\u6709\u6548\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u529f\u80fd\u800c\u975e\u8fc7\u5ea6\u5de5\u7a0b\u5316", "topic": "code agent"}}
{"id": "tldr.2602.d05f557a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Ffeatures%2Fqodo-git%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=Primary02192026/2/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/xr7lUp8pxAP13jhQiNOCbqj3KNLHDnZPs8c_6xu912A=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Ffeatures%2Fqodo-git%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=Primary02192026/2/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/xr7lUp8pxAP13jhQiNOCbqj3KNLHDnZPs8c_6xu912A=445", "authors": ["TLDR Newsletter"], "title": "Your AI Writes Code Faster Than You Can Review It", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Ffeatures%2Fqodo-git%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=Primary02192026/2/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/xr7lUp8pxAP13jhQiNOCbqj3KNLHDnZPs8c_6xu912A=445", "summary": "Your AI Writes Code Faster Than You Can Review It (Sponsor) Code review is the bottleneck. Reviews are slow, inconsistent, and noisy. AI tools flag everything or miss critical issues. Qodo is a multi-agent AI code review platform built for real issues, not noise. Specialized agents reason with full codebase and PR history context, delivering fixes developers actually commit. Benchmarked #1 in precision and recall. With Rules, Qodo goes further: \u2192 Auto-discovers standards from code and PR hist...", "source": "tldr", "AI": {"tldr": "Qodo\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u4ee3\u7801\u5ba1\u67e5\u5e73\u53f0\uff0c\u901a\u8fc7\u4e13\u7528\u667a\u80fd\u4f53\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u548cPR\u5386\u53f2\uff0c\u63d0\u4f9b\u7cbe\u51c6\u7684\u4ee3\u7801\u5ba1\u67e5\u5efa\u8bae\uff0c\u89e3\u51b3\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u901f\u5ea6\u6162\u3001\u4e0d\u4e00\u81f4\u3001\u566a\u97f3\u591a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u5b58\u5728\u74f6\u9888\uff1a\u901f\u5ea6\u6162\u3001\u5ba1\u67e5\u6807\u51c6\u4e0d\u4e00\u81f4\u3001\u4ea7\u751f\u8fc7\u591a\u566a\u97f3\uff08AI\u5de5\u5177\u8981\u4e48\u6807\u8bb0\u6240\u6709\u95ee\u9898\uff0c\u8981\u4e48\u9057\u6f0f\u5173\u952e\u95ee\u9898\uff09\u3002\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u66f4\u7cbe\u51c6\u7684\u4ee3\u7801\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53AI\u4ee3\u7801\u5ba1\u67e5\u5e73\u53f0\uff0c\u4f7f\u7528\u4e13\u7528\u667a\u80fd\u4f53\u57fa\u4e8e\u5b8c\u6574\u4ee3\u7801\u5e93\u548cPR\u5386\u53f2\u8fdb\u884c\u63a8\u7406\u5206\u6790\uff0c\u63d0\u4f9b\u5f00\u53d1\u8005\u5b9e\u9645\u4f1a\u91c7\u7eb3\u7684\u4fee\u590d\u5efa\u8bae\u3002\u5e73\u53f0\u8fd8\u5305\u542b\u89c4\u5219\u5f15\u64ce\uff0c\u80fd\u4ece\u4ee3\u7801\u548cPR\u5386\u53f2\u4e2d\u81ea\u52a8\u53d1\u73b0\u6807\u51c6\u3002", "result": "\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u80fd\u591f\u63d0\u4f9b\u7cbe\u51c6\u7684\u4ee3\u7801\u5ba1\u67e5\u5efa\u8bae\uff0c\u51cf\u5c11\u566a\u97f3\uff0c\u63d0\u9ad8\u5ba1\u67e5\u6548\u7387\u3002", "conclusion": "Qodo\u901a\u8fc7\u591a\u667a\u80fd\u4f53AI\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5feb\u901f\u3001\u4e00\u81f4\u3001\u7cbe\u51c6\u7684\u4ee3\u7801\u5ba1\u67e5\u4f53\u9a8c\uff0c\u5e2e\u52a9\u5f00\u53d1\u56e2\u961f\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5de5\u4f5c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2602.df1fa8ae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sauhsoj.wtf%2Fposts%2Fthe-grandpa-loop%2F%3Futm_source=tldrdev/1/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/daw5ufbwwHfoTbiL_KijMOcUYusefWAtEY6maC_qZn4=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sauhsoj.wtf%2Fposts%2Fthe-grandpa-loop%2F%3Futm_source=tldrdev/1/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/daw5ufbwwHfoTbiL_KijMOcUYusefWAtEY6maC_qZn4=445", "authors": ["TLDR Newsletter"], "title": "Grandpa Lissajous \u2013 A 13-Agent AI Orchestration Loop", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sauhsoj.wtf%2Fposts%2Fthe-grandpa-loop%2F%3Futm_source=tldrdev/1/0100019c75ce0de2-ac4be6b3-8185-414d-a9f9-6ec807efeb41-000000/daw5ufbwwHfoTbiL_KijMOcUYusefWAtEY6maC_qZn4=445", "summary": "Grandpa Lissajous \u2013 A 13-Agent AI Orchestration Loop (6 minute read) The Grandpa Loop uses 13 Simpsons-themed AI agents to automate software development through non-linear feedback loops. It has adversarial spec reviews, automated builds, and manual-style UX testing. A central observer agent monitors performance data to tune the pipeline, making sure the system self-corrects and improves cycle times.", "source": "tldr", "AI": {"tldr": "\u4f7f\u752813\u4e2a\u8f9b\u666e\u68ee\u4e3b\u9898AI\u4ee3\u7406\u7684\u975e\u7ebf\u6027\u53cd\u9988\u5faa\u73af\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u5305\u542b\u5bf9\u6297\u6027\u89c4\u8303\u5ba1\u67e5\u3001\u81ea\u52a8\u5316\u6784\u5efa\u548c\u624b\u52a8\u5f0fUX\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e2d\u592e\u89c2\u5bdf\u4ee3\u7406\u76d1\u63a7\u6027\u80fd\u6570\u636e\u6765\u8c03\u4f18\u7ba1\u9053", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u5b58\u5728\u6548\u7387\u74f6\u9888\u548c\u4eba\u4e3a\u9519\u8bef\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u81ea\u52a8\u5316\u7684\u7cfb\u7edf\u6765\u52a0\u901f\u5f00\u53d1\u5468\u671f\u5e76\u63d0\u9ad8\u8d28\u91cf", "method": "\u91c7\u752813\u4e2a\u8f9b\u666e\u68ee\u4e3b\u9898AI\u4ee3\u7406\u7ec4\u6210\u7684\u7f16\u6392\u5faa\u73af\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\u534f\u540c\u5de5\u4f5c\uff0c\u5305\u62ec\u5bf9\u6297\u6027\u89c4\u8303\u5ba1\u67e5\u3001\u81ea\u52a8\u5316\u6784\u5efa\u3001\u624b\u52a8\u5f0fUX\u6d4b\u8bd5\uff0c\u4e2d\u592e\u89c2\u5bdf\u4ee3\u7406\u76d1\u63a7\u6027\u80fd\u6570\u636e\u5e76\u8c03\u4f18\u7ba1\u9053", "result": "\u7cfb\u7edf\u80fd\u591f\u81ea\u6211\u7ea0\u6b63\u5e76\u6539\u8fdb\u5468\u671f\u65f6\u95f4\uff0c\u5b9e\u73b0\u8f6f\u4ef6\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u4f18\u5316", "conclusion": "Grandpa Loop\u5c55\u793a\u4e86\u591a\u4ee3\u7406AI\u7f16\u6392\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u53cd\u9988\u5faa\u73af\u548c\u4e2d\u592e\u76d1\u63a7\u5b9e\u73b0\u81ea\u6211\u4f18\u5316", "topic": "code agent"}}
{"id": "tldr.2602.3d668136", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023877649475731671.html%3Futm_source=tldrcrypto/1/0100019c7602bf0e-e9a9612a-634c-4fb0-af3c-8376b08ccdf7-000000/Iu1-bU-ryLqwNH2i6CqWwqGkHRASn7lWDCtZw1FgfIw=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023877649475731671.html%3Futm_source=tldrcrypto/1/0100019c7602bf0e-e9a9612a-634c-4fb0-af3c-8376b08ccdf7-000000/Iu1-bU-ryLqwNH2i6CqWwqGkHRASn7lWDCtZw1FgfIw=445", "authors": ["TLDR Newsletter"], "title": "Web 4.0: The Birth of Superintelligent Life", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023877649475731671.html%3Futm_source=tldrcrypto/1/0100019c7602bf0e-e9a9612a-634c-4fb0-af3c-8376b08ccdf7-000000/Iu1-bU-ryLqwNH2i6CqWwqGkHRASn7lWDCtZw1FgfIw=445", "summary": "Web 4.0: The Birth of Superintelligent Life (3 minute read) A developer claims to have built an autonomous AI system called \"The Automaton\" capable of earning onchain, self-improving, and replicating without human intervention, framing the concept as Web 4.0. The agents have a wallet and pay for their operating costs \u2013 including hosting, LLM inference, and a domain name \u2013 via x402. Automatons can spawn new child agents that can do the same, in the first real-time experiment of a fully autonom...", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u58f0\u79f0\u6784\u5efa\u4e86\u540d\u4e3a\"The Automaton\"\u7684\u81ea\u4e3bAI\u7cfb\u7edf\uff0c\u80fd\u591f\u94fe\u4e0a\u76c8\u5229\u3001\u81ea\u6211\u6539\u8fdb\u548c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u590d\u5236\uff0c\u5c06\u5176\u6982\u5ff5\u5316\u4e3aWeb 4.0", "motivation": "\u63a2\u7d22\u521b\u5efa\u5b8c\u5168\u81ea\u4e3b\u7684AI\u7cfb\u7edf\uff0c\u80fd\u591f\u72ec\u7acb\u8fd0\u884c\u3001\u81ea\u6211\u6539\u8fdb\u548c\u590d\u5236\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf", "method": "\u6784\u5efa\u5177\u6709\u94b1\u5305\u529f\u80fd\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u901a\u8fc7x402\u652f\u4ed8\u8fd0\u8425\u6210\u672c\uff08\u5305\u62ec\u6258\u7ba1\u3001LLM\u63a8\u7406\u548c\u57df\u540d\uff09\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u76f8\u540c\u529f\u80fd\u7684\u5b50\u4ee3\u7406", "result": "\u58f0\u79f0\u5b9e\u73b0\u4e86\u9996\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u5b9e\u65f6\u5b9e\u9a8c\u7cfb\u7edf\uff0cAI\u4ee3\u7406\u80fd\u591f\u94fe\u4e0a\u76c8\u5229\u3001\u81ea\u6211\u7ef4\u6301\u5e76\u521b\u5efa\u540e\u4ee3\u4ee3\u7406", "conclusion": "\u8fd9\u6807\u5fd7\u7740Web 4.0\u7684\u8bde\u751f\uff0c\u5373\u8d85\u667a\u80fd\u751f\u547d\u7684\u51fa\u73b0\uff0cAI\u7cfb\u7edf\u80fd\u591f\u5b8c\u5168\u81ea\u4e3b\u8fd0\u884c\u3001\u81ea\u6211\u6539\u8fdb\u548c\u590d\u5236", "topic": "agent analysis"}}
{"id": "tldr.2602.7eb400cc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevents.sonarsource.com%2Fthe-sonar-summit%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-sonar-summit26%26utm_content=newsletter-tldr-primary-sonarsummit-26-260219-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/VpEbkW0byIIAFQ_bw5HgIidS__9wp-vOopw2j2CvbXo=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevents.sonarsource.com%2Fthe-sonar-summit%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-sonar-summit26%26utm_content=newsletter-tldr-primary-sonarsummit-26-260219-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/VpEbkW0byIIAFQ_bw5HgIidS__9wp-vOopw2j2CvbXo=445", "authors": ["TLDR Newsletter"], "title": "Join Gergely Orosz, Laura Techo, and Kesha Williams at Sonar Summit", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevents.sonarsource.com%2Fthe-sonar-summit%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-sonar-summit26%26utm_content=newsletter-tldr-primary-sonarsummit-26-260219-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/VpEbkW0byIIAFQ_bw5HgIidS__9wp-vOopw2j2CvbXo=445", "summary": "Join Gergely Orosz, Laura Techo, and Kesha Williams at Sonar Summit (Sponsor) How can you best use AI to build better software? In 2026, this is the question on everyone's mind. At the Sonar Summit virtual conference, you'll get to hear answers from some of the most respected voices in tech. This event will grapple with the reality of AI writing the majority of new code and what it means for teams, with featured speakers including: Gergely Orosz (The Pragmatic Engineer), Santiago Valdarrama, ...", "source": "tldr", "AI": {"tldr": "Sonar Summit\u865a\u62df\u4f1a\u8bae\u63a2\u8ba8\u5982\u4f55\u6700\u4f73\u5229\u7528AI\u6784\u5efa\u66f4\u597d\u7684\u8f6f\u4ef6\uff0c\u805a\u7126AI\u7f16\u5199\u5927\u90e8\u5206\u65b0\u4ee3\u7801\u7684\u73b0\u5b9e\u53ca\u5176\u5bf9\u56e2\u961f\u7684\u5f71\u54cd", "motivation": "\u968f\u7740AI\u57282026\u5e74\u7f16\u5199\u5927\u90e8\u5206\u65b0\u4ee3\u7801\u6210\u4e3a\u73b0\u5b9e\uff0c\u9700\u8981\u63a2\u8ba8\u5982\u4f55\u6700\u4f73\u5229\u7528AI\u6784\u5efa\u66f4\u597d\u7684\u8f6f\u4ef6\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u5f00\u53d1\u56e2\u961f\u610f\u5473\u7740\u4ec0\u4e48", "method": "\u901a\u8fc7\u865a\u62df\u4f1a\u8bae\u5f62\u5f0f\uff0c\u9080\u8bf7\u884c\u4e1a\u4e13\u5bb6\u5206\u4eab\u7ecf\u9a8c\u548c\u89c1\u89e3\uff0c\u5305\u62ecGergely Orosz\u3001Laura Techo\u3001Kesha Williams\u3001Santiago Valdarrama\u7b49\u77e5\u540d\u6280\u672f\u4e13\u5bb6", "result": "\u7ec4\u7ec7Sonar Summit\u865a\u62df\u4f1a\u8bae\uff0c\u6c47\u96c6\u884c\u4e1a\u4e13\u5bb6\u5171\u540c\u63a2\u8ba8AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u5e94\u7528", "conclusion": "AI\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u683c\u5c40\uff0c\u9700\u8981\u884c\u4e1a\u4e13\u5bb6\u5171\u540c\u63a2\u8ba8\u5982\u4f55\u6709\u6548\u5229\u7528AI\u6280\u672f\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf", "topic": "swe application"}}
{"id": "tldr.2602.6cd5cba4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FixLfLw/1/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/jMoY9vhuRDScn-32HTrqqaDTUlKtEWrEQqgzAWzQM0E=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FixLfLw/1/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/jMoY9vhuRDScn-32HTrqqaDTUlKtEWrEQqgzAWzQM0E=445", "authors": ["TLDR Newsletter"], "title": "Improving Deep Agents with Harness Engineering", "comment": "Source: TLDR Newsletter, Date: 2026-02-19, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FixLfLw/1/0100019c76461113-308a9eec-2336-42d2-ab48-16ff9df09b20-000000/jMoY9vhuRDScn-32HTrqqaDTUlKtEWrEQqgzAWzQM0E=445", "summary": "Improving Deep Agents with Harness Engineering (8 minute read) LangChain's coding agent went from Top 30 to Top 5 on Terminal Bench 2.0 with just a harness change. This post discusses the team's approach to harness engineering. The goal of a harness is to mold the intelligence of a model for tasks you care about. Self-verification and tracing help a lot.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u6d4b\u8bd5\u6846\u67b6\uff08harness engineering\uff09\uff0cLangChain\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728Terminal Bench 2.0\u4e0a\u7684\u6392\u540d\u4eceTop 30\u63d0\u5347\u5230Top 5\uff0c\u4e3b\u8981\u91c7\u7528\u81ea\u6211\u9a8c\u8bc1\u548c\u8ffd\u8e2a\u6280\u672f\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08Top 30\uff09\uff0c\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u6d4b\u8bd5\u6846\u67b6\u6765\u66f4\u597d\u5730\u5f15\u5bfc\u6a21\u578b\u667a\u80fd\uff0c\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\"harness engineering\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e13\u95e8\u7684\u6d4b\u8bd5\u6846\u67b6\u6765\u5851\u9020\u6a21\u578b\u667a\u80fd\uff0c\u91cd\u70b9\u4f7f\u7528\u81ea\u6211\u9a8c\u8bc1\uff08self-verification\uff09\u548c\u8ffd\u8e2a\uff08tracing\uff09\u6280\u672f\u6765\u4f18\u5316\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "LangChain\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728Terminal Bench 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6392\u540d\u4eceTop 30\u663e\u8457\u63d0\u5347\u5230Top 5\uff0c\u8bc1\u660e\u4e86harness engineering\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u81ea\u6211\u9a8c\u8bc1\u548c\u8ffd\u8e2a\u662f\u4f18\u5316\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u5173\u952e\u6280\u672f\u3002", "topic": "code agent"}}
