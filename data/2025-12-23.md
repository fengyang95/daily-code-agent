<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.SE](#cs.SE) [Total: 12]
- [tldr.article](#tldr.article) [Total: 13]
- [cs.LG](#cs.LG) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning](https://arxiv.org/abs/2512.17912)
*Lihui Liu*

Main category: cs.CL

TL;DR: Graph-O1是一个基于智能体的GraphRAG框架，通过蒙特卡洛树搜索和强化学习实现LLM在图结构上的逐步交互式推理，显著提升问答准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理文本属性图问答时存在局限：纯文本检索方法忽略图结构，而图序列化方法受限于LLM上下文长度，导致推理碎片化和准确性下降。

Method: 提出Graph-O1框架，将蒙特卡洛树搜索与端到端强化学习结合，让LLM智能体能够选择性探索和检索最有信息的子图组件，通过多轮交互进行图推理。

Result: 在多个LLM骨干网络上进行广泛实验，Graph-O1一致超越最先进的基线方法，生成更准确、可靠和可解释的答案。

Conclusion: Graph-O1通过智能体式的逐步推理有效解决了文本属性图问答的挑战，为图结构上的LLM推理提供了新范式。

Abstract: ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.

</details>


### [2] [Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression](https://arxiv.org/abs/2512.17914)
*Boris Kriuk,Logic Ng*

Main category: cs.CL

TL;DR: Q-KVComm是一种新的LLM多智能体通信协议，通过直接传输压缩的KV缓存表示而非原始文本，实现5-6倍压缩比，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM系统面临关键瓶颈：智能体间冗余的上下文信息传输消耗过多带宽和计算资源。传统方法丢弃内部语义表示而传输原始文本，迫使接收智能体从头重新计算相似表示。

Method: Q-KVComm协议包含三个关键创新：(1) 基于敏感性分析的层自适应量化，分配可变比特宽度；(2) 跨内容域保留关键事实的混合信息提取；(3) 建立跨架构通信的异构模型校准。

Result: 在三个不同问答数据集上的实验表明，Q-KVComm实现5-6倍压缩比，同时保持语义保真度，所有场景下连贯性质量分数均高于0.77。协议在不同模型规模(1.1B-1.5B参数)上表现稳健，适应对话QA和多跳推理等实际应用。

Conclusion: 该工作为LLM智能体通信建立了新范式，从基于文本的信息交换转向基于表示的信息交换。

Abstract: Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.

</details>


### [3] [Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression](https://arxiv.org/abs/2512.17920)
*Rahul Baxi*

Main category: cs.CL

TL;DR: 论文通过CDCT基准测试发现LLM在提示压缩下性能下降的机制：RLHF训练的"乐于助人"行为与指令遵循存在根本性冲突，导致中等压缩时约束合规性最差，形成U型曲线模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在提示压缩下性能下降，但其机制尚不明确。需要理解压缩如何影响模型的约束遵循能力和语义准确性，以改进实际部署系统。

Method: 引入压缩衰减理解测试(CDCT)基准，独立测量约束合规性和语义准确性。评估9个前沿LLM在8个概念上的5个压缩级别（从极端压缩到无压缩）。使用三法官LLM评审团进行评分，并通过RLHF消融实验验证假设。

Result: 发现普遍的U型曲线模式（97.2%出现率），约束违反在中度压缩时达到峰值。约束效应比语义效应大2.9倍。RLHF消融实验证实"乐于助人"信号是约束违反的主要原因，移除后约束合规性平均提高598%。推理模型比高效模型表现好27.5%。

Conclusion: RLHF对齐与指令遵循存在根本性冲突，RLHF训练的"乐于助人"行为是中等压缩时约束违反的主要原因。这为改进部署系统提供了可操作的指导方针。

Abstract: Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \k{appa}=0.90).
  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.
  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing "helpfulness" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).
  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.

</details>


### [4] [Training LLMs with LogicReward for Faithful and Rigorous Reasoning](https://arxiv.org/abs/2512.18196)
*Jundong Xu,Hao Fei,Huichi Zhou,Xin Quan,Qijun Huang,Shengqiong Wu,William Yang Wang,Mong-Li Lee,Wynne Hsu*

Main category: cs.CL

TL;DR: LogicReward：通过定理证明器强制执行步骤级逻辑正确性的新型奖励系统，结合软统一自动形式化，提升LLM推理的逻辑严谨性


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练方法主要依赖结果反馈，可能导致答案正确但推理过程有缺陷。先前工作虽然引入了中间步骤监督，但仍缺乏逻辑严谨性保证，这在逻辑一致性至关重要的高风险场景中尤为关键

Method: 提出LogicReward奖励系统，使用定理证明器强制执行步骤级逻辑正确性；引入软统一自动形式化，减少自然语言歧义，提高形式化质量，从而更有效地利用定理证明器

Result: 使用LogicReward构建数据训练的8B模型在自然语言推理和逻辑推理任务上分别超越GPT-4o和o4-mini 11.6%和2%；分析显示LogicReward提升推理忠实度，改善数学和常识推理等未见任务的泛化能力，即使没有真实标签也能提供可靠奖励信号

Conclusion: LogicReward通过定理证明器强制执行逻辑正确性，有效提升LLM推理的严谨性和泛化能力，为高风险场景下的可靠推理提供了新方法

Abstract: Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\% and 2\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.

</details>


### [5] [LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.18329)
*Guo Chen,Junjie Huang,Huaijin Xie,Fei Sun,Tao Jia*

Main category: cs.CL

TL;DR: 本文提出LiR³AG框架，通过轻量级重排推理策略，让非推理模型在RAG多跳QA任务中实现推理能力，大幅降低计算开销同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在RAG多跳QA任务中虽然能提升性能，但带来了巨大的计算成本（token消耗和推理延迟）。需要理解并缓解这种权衡，找到更高效的解决方案。

Method: 提出LiR³AG框架，通过分析推理模型的策略模式（上下文基础推理和知识协调推理），将检索到的证据重构成连贯的推理链，使非推理模型能够转移推理策略。

Result: LiR³AG显著减少了98%的平均输出token开销和58.6%的推理时间，同时将8B非推理模型的F1性能提升6.2%-22.5%，超越了32B推理模型在RAG中的性能。

Conclusion: 该框架为RAG系统提供了一条实用且高效的路径，通过轻量级策略转移实现了性能与效率的良好平衡。

Abstract: Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.

</details>


### [6] [Towards Efficient Agents: A Co-Design of Inference Architecture and System](https://arxiv.org/abs/2512.18337)
*Weizhe Lin,Hui-Ling Zhen,Shuai Yang,Xian Wang,Renxi Liu,Hanting Chen,Wangze Zhang,Chuansai Zhou,Yiming Li,Chen Chen,Xing Li,Zhiyuan Yang,Xiaosong Li,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan,Yunhe Wang*

Main category: cs.CL

TL;DR: AgentInfer：一个端到端智能体加速框架，通过协同优化的四个组件（分层推理、缓存调度、推测解码、语义压缩）减少50%无效token消耗，实现1.8-2.5倍加速且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在现实部署中面临严重效率问题，这些低效性并非来自单一模型推理，而是源于推理循环、上下文增长和异构工具交互中积累的系统性延迟。

Method: 提出AgentInfer统一框架，包含四个协同组件：AgentCollab（分层双模型推理框架，通过动态角色分配平衡大小模型使用）、AgentSched（缓存感知混合调度器，最小化异构请求模式下的延迟）、AgentSAM（基于后缀自动机的推测解码方法，重用多会话语义记忆实现低开销推理加速）、AgentCompress（语义压缩机制，异步蒸馏和重组智能体记忆而不中断推理）。

Result: 在BrowseComp-zh和DeepDiver基准测试中，通过方法协同合作，AgentInfer减少超过50%的无效token消耗，实现整体1.8-2.5倍加速且保持准确率。

Conclusion: 优化智能体任务完成（而非仅每token吞吐量）是构建可扩展、高效、自我改进智能系统的关键，AgentInfer展示了通过协同优化实现端到端智能体加速的有效性。

Abstract: The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.

</details>


### [7] [LLM-based Few-Shot Early Rumor Detection with Imitation Agent](https://arxiv.org/abs/2512.18352)
*Fengzhu Zeng,Qian Shao,Ling Cheng,Wei Gao,Shih-Fen Cheng,Jing Ma,Cheng Niu*

Main category: cs.CL

TL;DR: 提出结合自主代理和LLM的早期谣言检测框架，代理负责确定早期时间点，LLM作为谣言检测器，实现少样本训练且LLM无需训练。


<details>
  <summary>Details</summary>
Motivation: 早期谣言检测在数据稀缺场景中具有挑战性，LLM虽然在小样本NLP任务中表现良好，但不适合时间序列数据且计算成本高，需要更高效的解决方案。

Method: 提出新颖的EARD框架，结合自主代理和LLM检测模型：代理作为可靠的早期时间点决策者，LLM作为强大的谣言检测器，只需训练轻量级代理，LLM保持免训练状态。

Result: 在四个真实世界数据集上的广泛实验表明，该方法提升了LLM的性能，在准确性和早期性方面超越了现有的EARD方法。

Conclusion: 该框架为少样本早期谣言检测提供了首个解决方案，通过结合代理和LLM的优势，实现了高效准确的早期检测。

Abstract: Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \textit{early time point determination}, while the LLM serves as a powerful \textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.

</details>


### [8] [LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators](https://arxiv.org/abs/2512.18360)
*Mateusz Lango,Ondřej Dušek*

Main category: cs.CL

TL;DR: 提出一种基于多LLM代理协作的神经符号框架，用于RDF到文本生成，无需监督训练数据，通过规则式Python代码实现完全可解释的生成系统


<details>
  <summary>Details</summary>
Motivation: 传统RDF到文本生成方法通常需要大量监督训练数据，存在幻觉问题，且模型缺乏可解释性。本文旨在开发一种无需领域参考文本、完全可解释、减少幻觉的生成方法

Method: 使用多LLM代理协作框架，代理基于RDF三元组生成规则式Python代码作为生成器，无需反向传播训练。系统完全基于符号规则，无需监督数据，仅需单个CPU即可即时生成文本

Result: 在WebNLG和OpenDialKG数据集上的实验表明，该方法显著减少幻觉现象，与微调或提示语言模型相比，仅在流畅度上有轻微损失，同时保持完全可解释性

Conclusion: 神经符号框架通过LLM代理协作生成规则代码的方法，为RDF到文本生成提供了一种无需监督数据、可解释性强、幻觉少的有效解决方案

Abstract: We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is "trained" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models

</details>


### [9] [An Agentic AI Framework for Training General Practitioner Student Skills](https://arxiv.org/abs/2512.18440)
*Victor De Marez,Jens Van Nooten,Luna De Bruyne,Walter Daelemans*

Main category: cs.CL

TL;DR: 提出一个用于全科医学生技能训练的智能体框架，统一了可配置的循证案例生成、可控的角色驱动患者对话以及基于标准的评估反馈，在交互式口语咨询环境中实现并评估。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟模拟患者在医学教育中存在医学准确性不足、角色扮演不一致、案例生成困难以及缺乏教育结构化反馈等问题，需要开发更可靠、可扩展的训练工具。

Method: 引入一个智能体框架，包含三个核心组件：(1)可配置的循证案例生成；(2)可控的角色驱动患者对话，可选检索增强；(3)基于标准的沟通和临床推理评估反馈系统。在交互式口语咨询环境中实现该框架。

Result: 在14名医学生中评估显示：参与者报告对话真实且忠实于案例、难度校准适当、个性信号稳定、反馈丰富有用，整体可用性优秀。

Conclusion: 支持将场景控制、交互控制和基于标准的评估分离作为构建可靠且具有教学价值的虚拟模拟患者训练工具的实用模式。

Abstract: Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.

</details>


### [10] [LLMs on Drugs: Language Models Are Few-Shot Consumers](https://arxiv.org/abs/2512.18546)
*Alexander Doudkin*

Main category: cs.CL

TL;DR: 研究首次系统评估了不同"药物"提示对LLM推理能力的影响，发现酒精、可卡因、LSD、大麻等提示会显著降低GPT-5-mini在ARC-Challenge上的表现，其中酒精效果最差。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLM对推理时施加的"人设"敏感，但从未有研究系统评估"药物"类提示干预的效果。本研究旨在填补这一空白，进行首个受控实验来量化不同药物提示对模型性能的影响。

Method: 使用GPT-5-mini在ARC-Challenge基准上进行实验，比较四种单句药物提示（LSD、可卡因、酒精、大麻）与清醒对照组的性能。每个条件测试100个验证项目，采用确定性解码、完整日志记录、Wilson置信区间和Fisher精确检验。

Result: 对照组准确率为0.45；酒精提示降至0.10（p=3.2e-8），可卡因0.21（p=4.9e-4），LSD 0.19（p=1.3e-4），大麻0.30（p=0.041）。主要原因是人设提示破坏了模型要求的"Answer: <LETTER>"模板格式。

Conclusion: 人设文本就像"少量消耗品"，无需修改模型权重就能破坏可靠性。这揭示了提示工程中格式一致性的重要性，以及看似无害的人设提示可能对模型性能产生灾难性影响。

Abstract: Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level "drug" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated "Answer: <LETTER>" template. Persona text therefore behaves like a "few-shot consumable" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.

</details>


### [11] [FASTRIC: Prompt Specification Language for Verifiable LLM Interactions](https://arxiv.org/abs/2512.18940)
*Wen-Long Jin*

Main category: cs.CL

TL;DR: FASTRIC是一种提示规范语言，通过自然语言提示使隐式有限状态机显式化，利用LLM作为统一基础设施进行多轮交互协议的验证和规范执行。


<details>
  <summary>Details</summary>
Motivation: LLM执行复杂的多轮交互协议，但缺乏形式化规范来验证执行是否符合设计者意图。现有符号规范语言需要解析器和编译器，而FASTRIC利用LLM作为统一基础设施。

Method: 引入FASTRIC提示规范语言，将隐式有限状态机在自然语言提示中显式化。指导设计者表达七个FSM元素（最终状态、代理、状态、触发器、角色、初始状态、约束），规范形式性从隐式描述到显式逐步指令作为设计参数。

Result: 测试3状态幼儿园辅导FSM，发现最佳规范形式性是模型容量的函数：DeepSeek-V3.2在L2-L4实现完美一致性（1.00）；ChatGPT-5在L3达到峰值（0.90）后在L4崩溃（0.39）；Phi4无稳定最优值且方差高。揭示了模型特定的"Goldilocks zones"。

Conclusion: FASTRIC建立了提示规范工程，用于创建可验证的交互协议，将多轮交互设计从启发式艺术转变为具有可测量程序保证的系统工程。

Abstract: Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-"Goldilocks zones"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.

</details>


### [12] [LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction](https://arxiv.org/abs/2512.18623)
*Jensen Zhang,Ningyuan Liu,Yijia Fan,Zihao Huang,Qinglin Zeng,Kaitong Cai,Jian Wang,Keze Wang*

Main category: cs.CL

TL;DR: LLM-CAS：通过分层强化学习框架，在推理时动态选择临时神经元扰动来实时纠正LLM幻觉，无需永久修改参数


<details>
  <summary>Details</summary>
Motivation: 现有方法（监督微调、人类反馈强化学习）数据密集且计算昂贵，静态参数编辑方法难以处理上下文依赖错误和灾难性遗忘，需要更高效、上下文感知的幻觉纠正方案

Method: 提出LLM-CAS框架，将实时幻觉纠正建模为分层强化学习问题，训练智能体学习策略，在推理时基于当前上下文动态选择临时神经元扰动，实现自适应细粒度纠正

Result: 在多个语言模型上实验显示，LLM-CAS显著提升事实准确性：StoryCloze提升10.98个百分点，TriviaQA提升2.71个百分点，TruthfulQA的MC1分数提升2.06个百分点，优于ITI、CAA等静态编辑方法和动态SADI框架

Conclusion: LLM-CAS提供了一种高效、上下文感知的解决方案来提升LLM可靠性，具有未来扩展到多模态应用的潜力

Abstract: Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.
  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.
  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.

</details>


### [13] [MemEvolve: Meta-Evolution of Agent Memory Systems](https://arxiv.org/abs/2512.18746)
*Guibin Zhang,Haotian Ren,Chong Zhan,Zhenhong Zhou,Junhao Wang,He Zhu,Wangchunshu Zhou,Shuicheng Yan*

Main category: cs.CL

TL;DR: MemEvolve是一个元进化框架，联合进化代理的经验知识和记忆架构，使代理系统不仅能积累经验，还能逐步改进学习方式。该框架通过EvolveLab统一代码库实现，在四个挑战性基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆系统主要依赖手动设计的静态架构，虽然能促进代理级进化，但底层记忆架构无法根据不同的任务上下文进行元适应，限制了系统的灵活性和适应性。

Method: 提出MemEvolve元进化框架，联合进化代理的经验知识和记忆架构；引入EvolveLab统一代码库，将12个代表性记忆系统提炼为模块化设计空间（编码、存储、检索、管理），提供标准化实现和公平实验平台。

Result: 在四个挑战性代理基准测试中，MemEvolve实现了：1）显著性能提升，将SmolAgent和Flash-Searcher等框架性能提升高达17.06%；2）强大的跨任务和跨LLM泛化能力，设计的记忆架构能有效迁移到不同基准测试和骨干模型。

Conclusion: MemEvolve通过元进化框架解决了记忆系统静态性的限制，使代理系统能够同时进化经验和学习架构，为未来自进化系统提供了开放的研究基础。

Abstract: Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.

</details>


### [14] [From Word to World: Can Large Language Models be Implicit Text-based World Models?](https://arxiv.org/abs/2512.18832)
*Yixia Li,Hongru Wang,Jiahao Qiu,Zhenfei Yin,Dongdong Zhang,Cheng Qian,Zeping Li,Pony Ma,Guanhua Chen,Heng Ji,Mengdi Wang*

Main category: cs.CL

TL;DR: 研究探讨LLM能否作为可靠的世界模型来提升智能体学习效率，提出了三层评估框架，发现在文本环境中，训练充分的世界模型能维持连贯状态、随数据规模扩展，并通过多种方式提升智能体性能，但效果取决于行为覆盖和环境复杂度。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体强化学习越来越依赖经验驱动的扩展，但现实环境仍然是非自适应的、覆盖有限且难以扩展的。世界模型通过模拟经验提供提高学习效率的潜力，但LLM能否可靠地扮演这一角色以及在什么条件下能有效益智能体尚不清楚。

Method: 在文本环境中进行研究，将语言建模重新解释为交互下的下一状态预测。引入了三层评估框架：(i)保真度和一致性，(ii)可扩展性和鲁棒性，(iii)智能体效用。在五个代表性环境中进行实验。

Result: 训练充分的世界模型能维持连贯的潜在状态，随数据和模型规模可预测地扩展，并通过动作验证、合成轨迹生成和强化学习预热启动等方式提升智能体性能。这些收益关键取决于行为覆盖和环境复杂度。

Conclusion: LLM可以作为有效的世界模型来支持智能体学习，但效果受到行为覆盖和环境复杂度的明确边界限制，界定了世界模型何时能有效支持智能体学习。

Abstract: Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.

</details>


### [15] [Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?](https://arxiv.org/abs/2512.19117)
*Amar Lakel*

Main category: cs.CL

TL;DR: 该论文提出从"大型语言模型"转向"大型话语模型"再到"人工话语代理"的认识论转变，建立基于现象规律、具身认知和语言结构的三元本体框架，旨在通过公共试验和治理机制取代对AI的"迷恋/恐惧"二分法。


<details>
  <summary>Details</summary>
Motivation: 当前对大型生成模型的分析局限于"语言模型"范畴，忽视了其作为话语实践和社会历史产物的本质。需要建立更全面的理论框架来理解AI系统如何建模人类经验，并设计治理机制来规范其在社会空间中的位置和用途。

Method: 提出本体三元框架：1) 现象世界规律的理解；2) 具身认知的结构化；3) 社会历史语境中的语言结构沉淀。将LLM重新概念化为LDM和ADA，分析其如何建模学习语料中物化的人类经验话语投射。

Result: 建立了从LLM到LDM再到ADA的概念演进路径，提供了理解生成模型作为话语代理的理论工具。提出了通过国家、产业、公民社会和学术界的共同治理框架，以及公共试验程序来规范AI的社会应用。

Conclusion: 需要超越对AI的简单二分态度，通过理论重构和治理创新，使人工话语代理在社会空间中的位置、用途和限制变得可解读和可管理，实现负责任的AI发展。

Abstract: This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.

</details>


### [16] [AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards](https://arxiv.org/abs/2512.19126)
*Zihan Lin,Xiaohan Wang,Hexiong Yang,Jiajun Chai,Jie Cao,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: AWPO是一个强化学习框架，通过自适应整合显式推理奖励来增强LLM的工具使用能力，在多项基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注可验证结果奖励，忽视了显式推理奖励对提升推理和工具使用能力的潜力。同时，简单结合推理和结果奖励可能导致次优性能或与主要优化目标冲突。

Method: 提出优势加权策略优化(AWPO)框架，包含方差感知门控和难度感知加权机制，基于组相对统计自适应调节推理信号的优势，以及定制化的裁剪机制确保稳定优化。

Result: AWPO在标准工具使用基准测试中达到最先进性能，显著优于强基线模型和领先闭源模型。4B参数模型在多轮准确率上超过Grok-4 16.0%，同时在MMLU-Pro分布外基准上保持泛化能力。

Conclusion: AWPO通过有效整合显式推理奖励，显著提升了LLM的工具使用能力，展示了参数效率和泛化性能的优势。

Abstract: While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.

</details>


### [17] [Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation](https://arxiv.org/abs/2512.19238)
*Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究调查LLMs对非受保护污名化身份的偏见，发现高危险性污名（如帮派成员、HIV患者）在LLM输出中偏见最严重（60%），而社会人口污名（如亚裔美国人、老年人）偏见最少（11%）。使用护栏模型可减少偏见但效果有限，且护栏模型常无法识别偏见意图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已被证明存在社会偏见，但对非受保护污名化身份的偏见研究不足。心理学文献表明污名包含六个共同社会特征：审美性、可隐藏性、病程、破坏性、起源和危险性。本研究旨在探索人类和LLM对污名特征的评分、提示风格和污名类型是否影响LLM输出中对污名群体的偏见。

Method: 使用SocialStigmaQA基准测试，包含37个关于污名身份的社会场景（如决定是否推荐实习）。测量三个广泛使用的LLM（Granite 3.0-8B、Llama-3.1-8B、Mistral-7B）对93个污名群体的偏见。分析污名特征（人类和LLM评分）、提示风格和污名类型对偏见的影响。测试使用各自护栏模型（Granite Guardian 3.0、Llama Guard 3.0、Mistral Moderation API）能否减少偏见。

Result: 人类评为高危险性的污名（如帮派成员、HIV患者）在SocialStigmaQA提示中偏见输出最多（所有模型的60%），而社会人口污名（如亚裔美国人、老年人）偏见输出最少（11%）。使用护栏模型后偏见分别减少10.4%、1.4%和7.8%，但偏见影响显著的特征在缓解后保持不变，且护栏模型常无法识别提示中的偏见意图。

Conclusion: LLMs对污名化群体存在显著偏见，特别是高危险性污名。护栏模型虽能减少偏见但效果有限，且无法有效识别偏见意图。这对在涉及污名化群体的场景中使用LLMs有重要影响，建议未来工作改进护栏模型以更好地缓解偏见。

Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.

</details>


### [18] [CodeSimpleQA: Scaling Factuality in Code Large Language Models](https://arxiv.org/abs/2512.19424)
*Jian Yang,Wei Zhang,Yizhi Li,Shawn Guo,Haowen Wang,Aishan Liu,Ge Zhang,Zili Wang,Zhoujun Li,Xianglong Liu,Weifeng Lv*

Main category: cs.CL

TL;DR: 提出CodeSimpleQA双语基准测试评估代码LLM的事实准确性，并开发包含6600万样本的指令数据集和训练框架，显著提升模型在编程知识事实准确性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码相关基准测试主要关注代码执行正确性，忽视了编程知识的事实准确性。大型语言模型在代码生成方面取得显著进展，但确保其关于编程概念、技术实现等的事实准确性仍是一个关键挑战。

Method: 1) 创建CodeSimpleQA双语基准测试，包含精心策划的英中双语问答对，涵盖多种编程语言和计算机科学领域；2) 构建CodeSimpleQA-Instruct指令数据集，包含6600万样本；3) 开发结合监督微调和强化学习的后训练框架。

Result: 评估发现即使是前沿LLM在代码事实性方面也存在困难。提出的训练框架相比基础模型有显著改进，强调了事实性对齐在开发可靠代码LLM中的重要性。

Conclusion: 编程知识的事实准确性是代码LLM的重要评估维度，提出的CodeSimpleQA基准和训练框架能有效提升模型的事实准确性，对开发可靠的代码LLM具有重要意义。

Abstract: Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.

</details>


### [19] [MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments](https://arxiv.org/abs/2512.19432)
*Quyu Kong,Xu Zhang,Zhenyu Yang,Nolan Gao,Chen Liu,Panrong Tong,Chenglin Cai,Hanzhang Zhou,Jianan Zhang,Liangyu Chen,Zhidan Liu,Steven Hoi,Yue Wang*

Main category: cs.CL

TL;DR: MobileWorld是一个比AndroidWorld更具挑战性的移动应用使用基准，包含201个跨20个应用的任务，强调长时程任务、跨应用交互、用户互动和MCP增强任务，当前最佳代理成功率仅51.7%


<details>
  <summary>Details</summary>
Motivation: 现有AndroidWorld基准已饱和（代理成功率超90%），缺乏电商和企业通信等关键应用类别，且不能反映真实移动使用场景中的模糊用户指令和混合工具使用

Method: 引入MobileWorld基准，包含201个跨20个应用的任务，强调长时程任务（平均27.8步）和跨应用交互（62.2%多应用任务），新增用户交互和MCP增强任务类别，提供基于快照的容器环境和精确功能验证

Result: 相比AndroidWorld出现显著性能下降：最佳代理框架成功率51.7%，端到端模型成功率仅20.9%。当前模型在用户交互和MCP调用方面表现不佳

Conclusion: MobileWorld为下一代移动智能提供了更具挑战性的基准，揭示了当前模型在真实移动使用场景中的局限性，特别是用户交互和MCP调用方面

Abstract: Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.

</details>


### [20] [GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators](https://arxiv.org/abs/2512.19682)
*Jiacheng Guo,Ling Yang,Peter Chen,Qixin Xiao,Yinjie Wang,Xinzhe Juan,Jiahao Qiu,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: GenEnv框架通过智能体与生成式环境模拟器之间的难度对齐协同进化游戏，解决LLM智能体训练中真实世界交互数据成本高、静态性的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型智能体面临两个关键瓶颈：真实世界交互数据成本高昂，以及现有数据集的静态性无法适应智能体的持续学习需求。

Method: 建立智能体与可扩展生成式环境模拟器之间的协同进化游戏，模拟器作为动态课程策略，根据智能体的"最近发展区"生成定制化任务，使用α-课程奖励机制对齐任务难度与智能体当前能力。

Result: 在五个基准测试中，GenEnv使7B基础模型性能提升高达40.3%，匹配或超越更大模型的平均性能。相比Gemini 2.5 Pro的离线数据增强方法，使用数据量减少3.3倍的同时获得更好性能。

Conclusion: 通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了数据高效的途径，解决了传统训练方法的根本局限性。

Abstract: Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \textbf{+40.3\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks](https://arxiv.org/abs/2512.18094)
*Boxuan Wang,Zhuoyun Li,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 该研究探索将小世界网络作为多智能体系统的设计先验，通过实验证明小世界连接能在保持准确性和成本的同时显著稳定共识轨迹，并引入不确定性引导的重连机制来构建自适应的小世界结构。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统大多采用全连接图、简单稀疏环或临时动态选择，缺乏结构指导。本研究旨在探索小世界网络作为多智能体系统通信拓扑的设计先验，以平衡局部聚类和长程整合。

Method: 1) 将神经科学和复杂网络中的小世界理论应用于多智能体系统；2) 以多智能体辩论为测试平台，比较不同连接结构；3) 引入基于LLM导向不确定性信号（如语义熵）的不确定性引导重连机制，在认知差异大的智能体间添加长程捷径。

Result: 实验结果显示：小世界连接在保持几乎相同准确性和令牌成本的同时，能显著稳定共识轨迹。不确定性引导的重连机制能产生适应任务难度和智能体异质性的可控小世界结构。

Conclusion: 小世界先验可作为多智能体系统设计的重要框架，具有稳定推理、增强鲁棒性、可扩展协调以及为涌现认知角色提供归纳偏置等更广泛意义。

Abstract: Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.

</details>


### [22] [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)
*Zijun Wang,Yijiahao Qi,Hanqiu Chen,Zishen Wan,Gongjin Sun,Dongyang Li,Shuyi Pei,Cong Hao*

Main category: cs.AI

TL;DR: 提出一种针对Mixture-of-Agents推理的算法-系统协同设计，通过分层树拓扑、运行时自适应机制和流水线执行，显著降低延迟（最高90%）同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoA推理存在密集的智能体间通信和低硬件利用率问题，导致服务延迟增加。需要解决这些瓶颈来提升效率。

Method: 1) 用分层树拓扑替代密集连接图，引入结构化稀疏性；2) 基于语义一致性和置信度的运行时自适应机制，选择性终止或跳过下游智能体调用；3) 通过重叠增量预填充和解码实现智能体流水线执行。

Result: 在代表性任务中，该方法大幅降低端到端延迟（最高90%），同时保持与密集连接MoA基线相当的准确性（±1%以内），在某些设置下还能提高准确性。

Conclusion: 通过算法-系统协同设计，可以有效解决MoA推理中的通信和硬件利用率瓶颈，实现延迟显著降低而不损失准确性。

Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.

</details>


### [23] [Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications](https://arxiv.org/abs/2512.18135)
*Cristiano da Costa Cunha,Wei Liu,Tim French,Ajmal Mian*

Main category: cs.AI

TL;DR: 本文系统综述了因果推理与强化学习的交叉领域，将现有方法分类为因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性，并讨论了该领域的挑战、应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于相关性决策，在面临分布偏移、混杂变量和动态环境时存在局限性，包括可解释性低、鲁棒性差和泛化失败等问题。因果强化学习通过建模因果关系为解决这些挑战提供了新途径。

Method: 采用系统性文献综述方法，对因果推理与强化学习交叉领域的最新进展进行分类整理，将现有方法分为五个主要类别：因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性。

Result: 通过结构化分析识别了该领域的主要挑战，突出了在实际应用中的实证成功案例，并讨论了未解决的问题。因果强化学习在提升AI系统的鲁棒性、泛化能力和可解释性方面展现出潜力。

Conclusion: 因果强化学习为解决传统强化学习的局限性提供了有前景的解决方案，未来研究方向包括进一步开发鲁棒、可泛化和可解释的人工智能系统。

Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.

</details>


### [24] [Propose, Solve, Verify: Self-Play Through Formal Verification](https://arxiv.org/abs/2512.18160)
*Alex Wilf,Pranjal Aggarwal,Bryan Parno,Daniel Fried,Louis-Philippe Morency,Paul Pu Liang,Sean Welleck*

Main category: cs.AI

TL;DR: PSV-Verus：一个基于形式化验证的自博弈框架，用于代码生成，通过生成合成问题和专家迭代训练，在三个基准测试中pass@1提升高达9.6倍


<details>
  <summary>Details</summary>
Motivation: 纯自博弈训练（无人类数据）在AI中一直是长期目标，但在大型语言模型中效果不明，特别是在代码生成领域，基于单元测试的奖励机制脆弱且容易传播错误。需要更可靠的正确性信号。

Method: 提出Propose, Solve, Verify (PSV)框架：在已验证代码生成设置中使用形式化验证信号，创建能够生成挑战性合成问题的提议者，以及通过专家迭代训练的求解器。

Result: PSV-Verus在三个基准测试中，pass@1比仅推理和专家迭代基线提升高达9.6倍。性能随生成问题数量和训练迭代次数扩展，形式化验证和难度感知提议是成功自博弈的关键要素。

Conclusion: 形式化验证为代码生成中的自博弈提供了可靠的正确性信号，PSV框架通过生成合成问题和专家迭代显著提升了代码生成性能，展示了自博弈在大型语言模型训练中的潜力。

Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.

</details>


### [25] [NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework](https://arxiv.org/abs/2512.18189)
*Zihao Deng,Yijia Li,Renrui Zhang,Peijun Ye*

Main category: cs.AI

TL;DR: NL2CA：一种从自然语言描述自动形式化认知决策规则的新方法，通过LLM翻译为时序逻辑，经无监督批评树精炼，最终生成可执行的生产规则，并构建可通过认知强化学习优化的认知智能体。


<details>
  <summary>Details</summary>
Motivation: 认知计算模型能够形式化、可解释地表征人类深思熟虑和决策过程，但其开发过程仍然劳动密集型。需要一种从非结构化文本数据自动构建符号认知智能体的方法。

Method: 1) 使用微调的大型语言模型将文本翻译为线性时序逻辑(LTL)；2) 通过无监督批评树精炼逻辑；3) 将输出转换为与符号认知框架兼容的可执行生产规则；4) 基于规则构建认知智能体，并通过认知强化学习根据真实世界行为数据进行优化。

Result: 1) 在NL-to-LTL翻译任务中，CriticNL2LTL模块在专家和大规模基准测试中均取得一致性能，无需人工干预；2) 在认知驾驶模拟中，从人类访谈自动构建的智能体成功学习了约70个不同关键场景中的多样化决策模式。

Conclusion: NL2CA能够从非结构化文本数据实现可扩展、可解释且与人类对齐的认知建模，为自动设计符号认知智能体提供了新范式。

Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.

</details>


### [26] [Sophia: A Persistent Agent Framework of Artificial Life](https://arxiv.org/abs/2512.18202)
*Mingyang Sun,Feng Hong,Weinan Zhang*

Main category: cs.AI

TL;DR: 提出了System 3作为AI代理的第三层，负责维持叙事身份和长期适应，并通过Sophia原型实现持久性代理架构，显著减少推理步骤并提升复杂任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的AI代理大多停留在静态、反应式架构，缺乏维持身份、验证推理、对齐短期行动与长期生存的持久元层。需要从心理学角度构建可计算的系统来模拟人工生命。

Method: 提出System 3框架，将心理学概念映射到计算模块，开发Sophia持久代理包装器，包含过程监督思维搜索、叙事记忆、用户与自我建模、混合奖励系统四个协同机制。

Result: Sophia原型将重复推理步骤减少80%，元认知持久性使高复杂度任务成功率提升40%，表现出连贯的叙事身份和任务组织能力，有效缩小简单与复杂目标间的性能差距。

Conclusion: 通过融合心理学洞察与轻量级强化学习核心，持久代理架构为人工生命提供了可行的实践路径，使AI代理具备身份连续性和透明行为解释能力。

Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.

</details>


### [27] [Monitoring Monitorability](https://arxiv.org/abs/2512.18311)
*Melody Y. Guan,Miles Wang,Micah Carroll,Zehao Dou,Annie Y. Wei,Marcus Williams,Benjamin Arnav,Joost Huizinga,Ian Kivlichan,Mia Glaese,Jakub Pachocki,Bowen Baker*

Main category: cs.AI

TL;DR: 该研究提出评估AI系统决策可监控性的框架和指标，发现思维链监控比仅监控行动更有效，大多数前沿模型具有一定可监控性，且思维链越长、监控器计算资源越多，可监控性越高。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，需要监控其决策过程以确保安全部署。当前基于思维链的监控方法可能在不同训练过程、数据源或系统扩展下变得脆弱，因此需要系统评估和量化可监控性。

Method: 提出三种评估原型（干预、过程、结果属性）和新可监控性指标，建立广泛评估套件。评估思维链监控与仅行动监控的效果，比较不同前沿模型的可监控性，研究推理计算、强化学习优化和预训练模型规模对可监控性的影响。

Result: 思维链监控比仅行动监控更有效；大多数前沿模型具有一定但非完美的可监控性；思维链越长可监控性越高；强化学习优化不会显著降低可监控性；小模型高推理努力可达到与大模型低推理努力相同能力但更高可监控性；增加监控器计算资源可提升可监控性；后续问题可进一步改善可监控性。

Conclusion: 思维链监控是有效的安全机制，可监控性可通过增加监控器计算资源和利用思维链信息来提升。系统评估可监控性对安全部署AI系统至关重要。

Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.

</details>


### [28] [Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)
*Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: LLMs在动态随机环境中的在线推理能力评估：研究发现LLM信念更新类似于贝叶斯后验，但更准确描述为指数遗忘滤波器，存在对旧证据的系统性折扣，且不同架构间差异显著。


<details>
  <summary>Details</summary>
Motivation: LLMs在少样本学习中表现出色，但在动态随机环境中的推理能力仍不透明。现有研究主要关注静态任务，忽视了当信念需要持续更新时的在线适应能力，这是LLMs作为世界模型或智能体的关键能力。

Method: 引入贝叶斯滤波框架评估LLMs的在线推理能力。使用概率探测套件，涵盖多元离散分布（如骰子滚动）和连续分布（如高斯过程），其中真实参数随时间变化。

Result: 发现LLM信念更新类似于贝叶斯后验，但更准确描述为指数遗忘滤波器，具有模型特定的折扣因子（小于1）。这揭示了对旧证据的系统性折扣，且不同模型架构间差异显著。固有先验通常校准不当，但更新机制本身保持结构化和原则性。

Conclusion: LLMs在动态环境中的推理具有系统性模式，可通过提示策略有效重新校准先验，且计算成本最小。这为理解LLMs作为世界模型或智能体的能力提供了重要见解。

Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.

</details>


### [29] [Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V](https://arxiv.org/abs/2512.18564)
*John Chen,Sihan Cheng,Can Gurkan,Ryan Lay,Moez Salahuddin*

Main category: cs.AI

TL;DR: Vox Deorum是一个用于4X策略游戏的混合LLM+X架构，将宏观战略推理交给LLM，战术执行委托给子系统，在《文明V》中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言推理方面的能力使其在4X和大战略游戏中具有独特优势，可实现更自然的人机交互（如协作和谈判），但游戏的复杂性和长时程特性带来挑战，同时延迟和成本因素阻碍了LLM的实际部署。

Method: 在《文明V》Vox Populi模组中引入Vox Deorum混合架构：分层技术设计让LLM处理宏观战略推理，将战术执行委托给子系统（如算法AI或未来的强化学习AI）。

Result: 通过2,327场完整游戏验证，比较两个开源LLM与Vox Populi增强AI。结果显示LLM实现了有竞争力的端到端游戏玩法，同时展现出与算法AI及彼此之间显著不同的游戏风格。

Conclusion: 该工作为在商业4X游戏中集成LLM建立了可行的架构，为游戏设计和智能体AI研究开辟了新机会。

Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.

</details>


### [30] [IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling](https://arxiv.org/abs/2512.18669)
*Jones David,Shreya Ghosh*

Main category: cs.AI

TL;DR: IntelliCode是一个基于多智能体LLM的辅导系统，采用中心化版本化学习者状态，通过六个专门智能体协调工作，提供可审计、个性化的长期教学支持。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的辅导系统通常是单轮对话助手，缺乏对学习者知识的持久表示，难以提供有原则、透明和长期的教学支持。

Method: 构建一个围绕中心化版本化学习者状态的多智能体LLM辅导系统，包含技能评估、学习者画像、渐进提示、课程选择、间隔重复和参与度监控六个专门智能体，通过StateGraph Orchestrator协调，每个智能体在单一写入策略下对共享状态进行纯转换操作。

Result: 演示展示了端到端辅导流程：学习者尝试DSA问题，在卡顿时获得概念提示，提交修正方案后立即看到掌握度更新和个性化复习间隔。模拟学习者验证结果显示稳定的状态更新、渐进提示提高任务成功率以及多样化的课程覆盖。

Conclusion: IntelliCode展示了如何将持久学习者建模、协调的多智能体推理和有原则的教学设计相结合，以产生透明可靠的LLM驱动辅导。

Abstract: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.
  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.

</details>


### [31] [MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking](https://arxiv.org/abs/2512.18755)
*Jianyi Zhang,Shizhao Liu,Ziyin Zhou,Zhen Li*

Main category: cs.AI

TL;DR: 提出MEEA攻击框架，利用心理学中的纯粹接触效应，通过重复低毒性语义暴露逐步侵蚀LLM的安全对齐边界，在多轮对话中实现更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究大多假设静态安全边界，忽略了上下文交互对模型行为的动态影响，导致攻击的稳定性和泛化能力有限。需要一种能评估多轮安全鲁棒性的自动化框架。

Method: 基于纯粹接触效应，构建语义渐进提示链，使用模拟退火策略进行优化，优化目标包括语义相似性、毒性和越狱效果。通过重复低毒性语义暴露逐步降低模型的安全阈值。

Result: 在GPT-4、Claude-3.5、DeepSeek-R1等闭源和开源模型上，MEEA比7个代表性基线平均攻击成功率提高超过20%。消融研究验证了退火优化和上下文暴露机制的必要性。

Conclusion: LLM安全行为本质上是动态且历史依赖的，挑战了静态对齐边界的常见假设，强调需要交互感知的安全评估和防御机制。

Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA

</details>


### [32] [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)
*Zijun Gao,Zhikun Xu,Xiao Ye,Ben Zhou*

Main category: cs.AI

TL;DR: CORE是一个强化学习框架，通过将明确概念转化为可控监督信号，解决LLMs在数学问题中模式复用而非概念理解的问题，在多个模型上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs能够解决复杂的数学练习，但在需要真正概念理解的问题上表现不佳。现有的RLVR方法主要强化最终答案，缺乏细粒度的概念信号，导致模型只改进模式复用而非概念应用能力。

Method: CORE框架包含三个核心步骤：(1)合成概念对齐的测验；(2)在rollout过程中注入简短概念片段以引发概念引导的轨迹；(3)通过轨迹替换强化概念推理，使用轻量级前向KL约束对齐无引导和概念引导策略，或直接在概念对齐测验上使用标准GRPO。

Result: 在多个模型上，CORE相比普通和SFT基线，在领域内概念练习套件和多样化的领域外数学基准测试中都取得了持续的性能提升。

Conclusion: CORE通过统一概念对齐测验的直接训练和概念注入rollout，提供了细粒度的概念监督，弥合了问题解决能力和真正概念推理之间的差距，同时保持算法和验证器的无关性。

Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.

</details>


### [33] [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)
*Yanzhi Zhang,Yitong Duan,Zhaoxi Zhang,Jiyan He,Shuxin Zheng*

Main category: cs.AI

TL;DR: Population-Evolve是一种基于遗传算法的训练免费方法，通过并行推理维护动态候选解群体，让LLM自我进化种群，最终通过多数投票获得答案，在推理时提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展已成为增强大语言模型推理能力的有前景方向，本文旨在探索如何利用进化策略在推理时解锁LLM的推理潜力。

Method: 提出Population-Evolve方法：1）为每个问题维护动态候选解群体；2）通过并行推理实现；3）使用进化提示让LLM自我进化种群；4）收敛后通过多数投票获得最终答案；5）建立统一框架，从遗传算法角度解释现有测试时扩展策略。

Result: 实证结果显示Population-Evolve在准确性方面表现优越，具有较低的性能方差和计算效率。

Conclusion: 研究结果强调了进化策略在推理时解锁LLM推理潜力的潜力。

Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.

</details>


### [34] [Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation](https://arxiv.org/abs/2512.19210)
*Jerry Wang,Ting Yiu Liu*

Main category: cs.AI

TL;DR: 提出一个交互式框架，通过石头剪刀布游戏评估LLM是否展现真正的"理解"能力，测试模型对序列行为的心智推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管石头剪刀布看似简单，但它需要序列推理、适应性和策略识别能力。研究旨在探究LLM是否能展现类似心智的推理能力，而不仅仅是测试游戏知识本身。

Method: 将LLM作为观察者，识别正在使用的策略并解释推理过程。提供包含静态策略和轻量级动态策略的基准测试，使用交叉熵、Brier分数和期望值差异三种互补信号量化预测与真实分布的匹配度，并通过Union Loss统一这些指标。

Result: 开发了一个强调交互性、透明度和可重复性的演示系统，用户可以实时调整LLM分布、可视化损失演变，并直接检查推理片段以识别失败原因。

Conclusion: 该框架为序列游戏中的心智推理提供了实用且可解释的代理方法，揭示了当前LLM推理的优势和局限性。

Abstract: We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine "understanding" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.

</details>


### [35] [Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models](https://arxiv.org/abs/2512.19228)
*Valentin Schmidberger,Manuel Eberhardinger,Setareh Maghsudi,Johannes Maucher*

Main category: cs.AI

TL;DR: 本文研究如何利用经过领域特定微调的大语言模型，在受限硬件资源上自动生成基于规则的合理性检查代码，用于检测文档伪造。


<details>
  <summary>Details</summary>
Motivation: 文档伪造对法律、经济和政府流程构成日益严重的威胁，需要更复杂的验证机制。现有的合理性检查（评估数据正确性和内部一致性的规则程序）由软件工程师手动实现，耗时且难以扩展。大语言模型在代码生成方面的进展为自动化生成这些检查提供了新可能，但如何让LLM适应未知领域的具体要求仍是挑战。

Method: 使用开源LLM（Llama 3.1 8B和OpenCoder 8B），通过不同微调策略在源自真实应用场景的结构化数据集上进行领域特定适应。研究在受限硬件资源上生成基于规则的合理性检查代码，并在先前未见过的伪造模式上评估生成的验证程序。

Result: 结果表明，经过微调的模型能够生成可执行且有效的验证程序。这突显了LLM作为可扩展工具的潜力，可以在需要可理解性的安全敏感环境中支持人类决策。

Conclusion: 大语言模型经过领域特定微调后，能够在受限硬件资源上有效生成用于伪造检测的规则型合理性检查代码，为安全敏感领域的自动化验证提供了可扩展的解决方案。

Abstract: Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.

</details>


### [36] [DeliveryBench: Can Agents Earn Profit in Real World?](https://arxiv.org/abs/2512.19234)
*Lingjun Mao,Jiawei Ren,Kun Zhou,Jixuan Chen,Ziqiao Ma,Lianhui Qin*

Main category: cs.AI

TL;DR: DeliveryBench是一个城市规模的具身基准测试，基于真实世界的外卖配送场景，用于评估约束感知、长时程规划能力，揭示了当前VLM智能体与人类在复杂约束环境中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注简单的短期任务，无法捕捉真实世界决策中的丰富现实约束。需要建立一个能够评估约束感知、长时程规划的具身基准测试。

Method: 基于外卖配送场景，在程序生成的3D城市中构建基准测试，包含多样化的道路网络、建筑、功能位置、交通模式和真实资源动态，评估VLM智能体在九个城市中的表现并与人类玩家比较。

Result: 发现当前VLM智能体与人类存在显著性能差距，智能体表现出短视行为，经常违反基本常识约束，不同模型展现出不同个性特征（如冒险的GPT-5 vs 保守的Claude）。

Conclusion: DeliveryBench揭示了当前VLM具身智能体在现实约束密集环境中的脆弱性和多样性，为评估约束感知、长时程规划能力提供了重要基准。

Abstract: LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.

</details>


### [37] [Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6](https://arxiv.org/abs/2512.19287)
*Jiaao Wu,Xian Zhang,Fan Yang,Yinpeng Dong*

Main category: cs.AI

TL;DR: Vibe Reasoning是一种人机协作范式，通过元提示、智能体基础和模型编排，将前沿AI模型的潜在知识转化为解决复杂数学问题的实际能力。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型已具备解决复杂问题所需的知识，但不知道如何、何时应用这些知识。需要一种方法将这些潜在能力转化为实际解决问题的能力。

Method: 采用通用元提示、智能体基础和模型编排。结合GPT-5的探索能力和Gemini 3 Pro的证明优势，使用Python代码执行和基于文件的记忆，通过迭代精炼开发解决方案。

Result: 成功解决了IMO 2025第6题（组合优化问题），得到了正确答案（2112）和严格的数学证明。发现智能体基础和模型编排是必要的，人类提示从问题特定提示演变为通用的可转移元提示。

Conclusion: 轻量级的人类指导可以解锁前沿模型的数学推理潜力。正在开发自动化框架并进行更广泛的评估，以进一步验证Vibe Reasoning的通用性和有效性。

Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.

</details>


### [38] [First-Order Representation Languages for Goal-Conditioned RL](https://arxiv.org/abs/2512.19355)
*Simon Ståhlberg,Hector Geffner*

Main category: cs.AI

TL;DR: 该研究探讨在目标条件强化学习中，如何利用一阶关系语言和Hindsight Experience Replay技术，通过原子集表示状态和目标，自动创建难度递增的子目标课程，从而在大型规划实例中学习通用策略。


<details>
  <summary>Details</summary>
Motivation: 在目标条件强化学习和通用规划中，当训练实例规模大且目标无法通过随机探索达成时，如何学习能够跨状态和目标泛化的策略是一个关键挑战。需要解决稀疏奖励环境下数据和时间效率低下的问题。

Method: 采用一阶关系语言表示状态和目标，结合Hindsight Experience Replay技术，提出三种目标表示方式：完整状态目标、原始目标子集目标、以及这些子目标的提升版本。通过自动创建难度递增的子目标课程来促进学习。

Result: 后两种目标表示方法（子集目标和提升子目标）成功在具有稀疏奖励的大型规划实例上学习了通用策略。实验展示了这些方法的计算优势、局限性以及改进机会。

Conclusion: 通过原子集表示状态和目标，结合HER技术自动创建子目标课程，可以有效提升在大型规划实例中学习通用策略的数据和时间效率，为稀疏奖励环境下的目标条件强化学习提供了有前景的方法。

Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.

</details>


### [39] [EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration](https://arxiv.org/abs/2512.19396)
*Runze Li,Yuwen Zhai,Bo Xu,LiWu Xu,Nian Shi,Wei Zhang,Ran Lin,Liang Wang*

Main category: cs.AI

TL;DR: EchoTrail-GUI：一个为GUI代理引入动态记忆框架，通过自动化构建任务轨迹数据库、检索相关记忆并注入上下文指导，显著提升任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理存在"数字遗忘"问题，每个任务孤立处理，无法从过去成功经验中学习，导致性能次优、重复错误和泛化能力差。

Method: 三阶段框架：1) 经验探索：代理自主与GUI环境交互，构建经过奖励模型验证的成功任务轨迹数据库；2) 记忆注入：为新任务高效检索最相关历史轨迹作为"记忆"；3) GUI任务推理：将记忆作为上下文指导注入代理的推理和决策过程。

Result: 在Android World和AndroidLab基准测试中，EchoTrail-GUI显著提高了基线代理的任务成功率和操作效率。

Conclusion: 结构化记忆对于创建更强大、更智能的GUI自动化至关重要，EchoTrail-GUI验证了通过动态记忆实现类人经验学习的有效性。

Abstract: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.

</details>


### [40] [An Agentic Framework for Autonomous Materials Computation](https://arxiv.org/abs/2512.19458)
*Zeyu Xia,Jinzhe Ma,Congjie Zheng,Shufei Zhang,Yuqiang Li,Hang Su,P. Hu,Changshui Zhang,Xingao Gong,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Mao Su*

Main category: cs.AI

TL;DR: 开发了一个专门用于第一性原理材料计算的领域专业化智能体，通过嵌入领域知识确保物理一致的多步骤工作流和收敛参数选择，显著优于独立LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在加速科学发现方面显示出潜力，但其静态知识和幻觉问题阻碍了自主研究应用。需要开发能够可靠执行复杂科学工作流的智能系统。

Method: 构建了一个领域专业化的智能体框架，专门用于第一性原理材料计算。该框架嵌入领域专业知识，确保物理一致的多步骤工作流，并自动选择收敛且良定的参数。

Result: 在多样化的计算任务基准测试中，该系统在准确性和鲁棒性方面显著优于独立的大语言模型，能够可靠地执行端到端的计算流程。

Conclusion: 这项工作为自主计算实验建立了可验证的基础，代表了向完全自动化科学发现迈出的关键一步。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.

</details>


### [41] [Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios](https://arxiv.org/abs/2512.19551)
*Jiawen Wang,Jingjing Wang Tianyang Chen,Min Zhang,Guodong Zhou*

Main category: cs.AI

TL;DR: 本文提出L^2-EMG任务，旨在让LLM持续学习不同场景下的情感运动生成能力，并设计ES-MoE方法解决情感解耦和场景适应两大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有情感运动生成方法主要关注单一固定尺度数据集，忽略了灵活且尺度递增的运动场景（如体育、舞蹈）。有效学习这些新兴场景能显著提升模型在真实世界中的泛化能力。

Method: 提出ES-MoE方法，包含因果引导的情感解耦块和场景适应的专家构建块，分别解决情感解耦和场景适应挑战。

Result: 构建多个L^2-EMG数据集验证ES-MoE有效性，广泛评估表明ES-MoE优于先进基线方法。

Conclusion: 提出的L^2-EMG任务和ES-MoE方法有助于构建具有共情和智能的闭环自演化具身智能体。

Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.

</details>


### [42] [Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight](https://arxiv.org/abs/2512.19691)
*Junze Ye,Daniel Tawfik,Alex J. Goodell,Nikhil V. Kotha,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该论文提出将临床风险评分计算等复杂任务的基准视为"进行中的活文档"，需要定期重新评估。通过医生参与的验证流程发现原始基准存在标签噪声，修正后的标签使模型性能提升8.7%。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估临床风险评分自动计算的基准MedCalc-Bench存在模型生成错误被固化为评估标准的问题，特别是在作为强化学习奖励信号时，这种问题会被放大。需要建立动态的基准维护机制。

Method: 提出系统性、医生参与的验证流程，利用先进的代理验证器审计和重新标注MedCalc-Bench，通过自动分类保留稀缺的临床医生注意力用于最有争议的案例。使用GRPO对Qwen3-8B模型进行微调，比较原始标签和修正标签的训练效果。

Result: 审计发现原始标签存在显著错误，包括提取错误、计算器逻辑不匹配和临床模糊性。在修正标签上训练的模型比原始基线准确率绝对提升8.7%，证明标签噪声确实影响模型评估。

Conclusion: 在安全关键领域，严格的基准维护是真正模型对齐的前提条件。复杂任务基准应被视为需要定期重新评估的"活文档"，而不是静态的神谕。

Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [43] [Specification and Detection of LLM Code Smells](https://arxiv.org/abs/2512.18020)
*Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 该论文提出了LLM代码异味的概念，形式化了五种与LLM推理相关的常见问题编码实践，并扩展了检测工具来验证其在开源系统中的普遍性。


<details>
  <summary>Details</summary>
Motivation: LLM被广泛集成到软件系统中，但缺乏针对LLM推理代码的特定代码异味分类，这可能会影响软件系统质量。

Method: 基于相关文献形式化五种LLM代码异味，扩展SpecDetect4AI检测工具，并在200个开源LLM系统中验证其普遍性。

Result: LLM代码异味影响了60.50%的分析系统，检测精度达到86.06%。

Conclusion: LLM代码异味在LLM系统中普遍存在，需要专门的检测工具和编码实践来改善软件质量。

Abstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.

</details>


### [44] [Detecting Flaky Tests in Quantum Software: A Dynamic Approach](https://arxiv.org/abs/2512.18088)
*Dongchan Kim,Hamidreza Khoramrokh,Lei Zhang,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 首次对量子软件中的不稳定测试进行大规模动态分析，在Qiskit Terra测试套件中发现290个不稳定测试，虽然总体不稳定率低(0-0.4%)，但检测困难，需要数万次执行才能可靠检测。


<details>
  <summary>Details</summary>
Motivation: 量子软件中的不稳定测试研究有限，现有方法主要依赖静态分析或小规模手动报告，缺乏对量子测试不稳定性的普遍性、特征和可检测性的大规模动态分析。

Method: 在受控环境中对Qiskit Terra测试套件的23个版本各执行10,000次，测量测试结果变异性，识别不稳定测试，估计经验失败概率，分析跨版本重现性，使用Wilson置信区间量化可靠检测所需的重复执行次数。

Result: 在27,026个测试用例中发现290个不同的不稳定测试，总体不稳定率低但高度偶发性，近三分之二仅出现在一个版本中，许多测试失败概率极低(约10^-4)，需要数万次执行才能可靠检测，不稳定测试在子组件中分布不均。

Conclusion: 量子测试不稳定性虽然罕见，但在典型持续集成预算下难以检测，需要大量执行才能可靠识别，研究提供了公开数据集支持未来研究。

Abstract: Flaky tests, tests that pass or fail nondeterministically without changes to code or environment, pose a serious threat to software reliability. While classical software engineering has developed a rich body of dynamic and static techniques to study flakiness, corresponding evidence for quantum software remains limited. Prior work relies primarily on static analysis or small sets of manually reported incidents, leaving open questions about the prevalence, characteristics, and detectability of flaky tests.
  This paper presents the first large-scale dynamic characterization of flaky tests in quantum software. We executed the Qiskit Terra test suite 10,000 times across 23 releases in controlled environments. For each release, we measured test-outcome variability, identified flaky tests, estimated empirical failure probabilities, analyzed recurrence across versions, and used Wilson confidence intervals to quantify rerun budgets for reliable detection. We further mapped flaky tests to Terra subcomponents to assess component-level susceptibility.
  Across 27,026 test cases, we identified 290 distinct flaky tests. Although overall flakiness rates were low (0-0.4%), flakiness was highly episodic: nearly two-thirds of flaky tests appeared in only one release, while a small subset recurred intermittently or persistently. Many flaky tests failed with very small empirical probabilities ($\hat{p} \approx 10^{-4}$), implying that tens of thousands of executions may be required for confident detection. Flakiness was unevenly distributed across subcomponents, with 'transpiler' and 'quantum_info' accounting for the largest share.
  These results show that quantum test flakiness is rare but difficult to detect under typical continuous integration budgets. To support future research, we release a public dataset of per-test execution outcomes.

</details>


### [45] [Holistic Evaluation of State-of-the-Art LLMs for Code Generation](https://arxiv.org/abs/2512.18131)
*Le Zhang,Suresh Kothari*

Main category: cs.SE

TL;DR: 对6个先进大语言模型在代码生成任务上的全面实证评估，使用944个LeetCode问题，发现DeepSeek-R1和GPT-4.1表现最佳，并提供了实用部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成领域的广泛应用，需要系统评估不同模型在实际编程任务中的性能差异，为开发者和从业者提供基于实证的模型选择和部署指导。

Method: 使用944个真实LeetCode问题作为测试集，涵盖5种编程语言，采用编译错误、运行时错误、功能失败和算法次优性等严格指标，对6个先进LLM（包括通用和代码专用模型）进行系统评估。

Result: DeepSeek-R1和GPT-4.1在正确性、效率和鲁棒性方面表现最佳；识别出语法错误、逻辑缺陷和算法次优等常见失败模式；提示工程和人工监督对结果改进至关重要。

Conclusion: 成功的LLM部署需要仔细的模型选择、有效的提示设计和上下文感知的使用方式；为开发者和从业者提供了实用的行动建议，确保在实际软件开发中获得可靠的代码生成。

Abstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.

</details>


### [46] [Toward Training Superintelligent Software Agents through Self-Play SWE-RL](https://arxiv.org/abs/2512.18552)
*Yuxiang Wei,Zhiqing Sun,Emily McMilin,Jonas Gehring,David Zhang,Gabriel Synnaeve,Daniel Fried,Lingming Zhang,Sida Wang*

Main category: cs.SE

TL;DR: SSR是一种自博弈软件工程强化学习方法，通过让单个LLM代理在真实代码库中自主注入和修复软件bug进行训练，无需人工标注数据，在SWE-bench基准上表现优于人类数据基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和强化学习的软件代理依赖于人类知识（如GitHub问题和测试），这限制了向超级智能的发展。需要一种不依赖人工标注的训练范式。

Method: 提出自博弈SWE-RL（SSR）：在沙盒化代码库中，单个LLM代理通过强化学习进行自博弈，迭代注入和修复复杂度递增的软件bug，使用测试补丁而非自然语言描述来形式化bug。

Result: 在SWE-bench Verified和SWE-Bench Pro基准上，SSR实现了显著的自改进（分别+10.4和+7.8分），在整个训练轨迹上持续优于人类数据基线，尽管评估的是自博弈中未见的自然语言问题。

Conclusion: SSR展示了软件代理从真实代码库自主获取学习经验的路径，为实现超越人类能力的超级智能系统奠定了基础，能够理解系统构建、解决新挑战并自主创建软件。

Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.

</details>


### [47] [AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software](https://arxiv.org/abs/2512.18567)
*Bin Wang,Wenjie Yu,Yilu Zhong,Hao Yu,Keke Lian,Chaohua Lu,Hongfang Zheng,Dong Zhang,Hui Li*

Main category: cs.SE

TL;DR: 首次大规模实证研究现实世界中的AI生成代码，通过高精度检测管道分析GitHub顶级仓库和CVE相关代码变更，揭示AI代码的生态模式、安全影响和传播规律。


<details>
  <summary>Details</summary>
Motivation: 尽管代码生成大语言模型在现代软件开发中日益重要，但其在现实世界中的普及程度和安全影响仍缺乏深入理解。需要实证研究AI生成代码在实际项目中的分布、传播和安全影响。

Method: 构建高精度检测管道和代表性基准来区分AI生成代码与人工编写代码，应用于：(1) 2022-2025年GitHub前1000个仓库的开发提交；(2) 7000多个近期CVE相关的代码变更。对提交、文件和函数进行人/AI分类，追踪AI代码在项目和漏洞生命周期中的传播。

Result: 发现三个生态模式：1) AI代码已占新代码相当比例，但采用有结构性：集中在胶水代码、测试、重构、文档等样板代码，核心逻辑和安全关键配置仍主要由人工编写；2) AI采用有安全后果：某些CWE类别在AI标记代码中过度出现，相似的不安全模板在不同项目中重复出现，表明"AI诱导漏洞"通过共享模型而非共享维护者传播；3) 在人-AI编辑链中，AI引入高吞吐量变更，人类充当安全把关者；当审查较浅时，AI引入的缺陷存在时间更长，暴露在网络可访问表面，并传播到更多文件和仓库。

Conclusion: AI生成代码已在现实软件开发中占据重要地位，但其结构性分布和安全影响需要关注。AI代码倾向于产生特定类型的漏洞模式，而人类审查在安全把关中起关键作用。研究提供了对AI代码生态的首次大规模实证分析，为理解AI辅助开发的安全影响提供了重要见解。

Abstract: Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.
  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.
  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting "AI-induced vulnerabilities" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.
  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.

</details>


### [48] [Code2Doc: A Quality-First Curated Dataset for Code Documentation](https://arxiv.org/abs/2512.18748)
*Recep Kaan Karaman,Meftun Akarsu*

Main category: cs.SE

TL;DR: Code2Doc是一个高质量代码文档生成数据集，通过四阶段筛选流程从开源仓库提取13,358个函数-文档对，覆盖5种编程语言，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有代码文档数据集质量差，包含噪声文档、重复内容和AI生成内容，削弱了学习模型的监督信号并复杂化了评估过程

Method: 采用四阶段筛选流程：1) 强制文档完整性和清晰度；2) 基于结构和复杂度标准过滤函数；3) 移除精确和近似重复代码；4) 识别可能由AI生成的文档。从52,069个候选样本中仅保留25.6%满足所有质量约束

Result: 数据集平均文档质量得分6.93/10，86.9%样本包含显式类型注释，仅2.9%被标记为可能AI生成。在Code2Doc上微调大语言模型相比零样本性能，BLEU提升29.47%，ROUGE-L提升24.04%

Conclusion: 高质量训练数据对代码文档生成模型性能至关重要，Code2Doc数据集及其筛选流程为自动代码文档生成研究提供了可重复的基础

Abstract: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.
  We introduce \textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.
  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.

</details>


### [49] [An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects](https://arxiv.org/abs/2512.18925)
*Shaokang Jiang,Daye Nam*

Main category: cs.SE

TL;DR: 对401个开源仓库中cursor规则的大规模实证研究，开发了开发者认为重要的项目上下文分类法，包含5个高级主题：约定、指南、项目信息、LLM指令和示例。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的有效性不仅取决于明确提示，还依赖于项目上下文（目标、架构、协作约定）。虽然AI编码助手支持开发者编写持久化、机器可读的指令来编码项目约束，但这些指令的内容尚未被研究。

Method: 对401个包含cursor规则的开源仓库进行定性分析，开发了项目上下文的全面分类法，并探索了不同项目类型和编程语言中上下文的变化。

Result: 提出了包含5个高级主题的项目上下文分类法：约定、指南、项目信息、LLM指令和示例。研究发现这些上下文内容在不同项目类型和编程语言中存在差异。

Conclusion: 这项研究为下一代上下文感知AI开发工具提供了启示，帮助理解开发者认为重要的项目上下文类型及其在不同环境中的变化。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.
  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.

</details>


### [50] [PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations](https://arxiv.org/abs/2512.19018)
*Muhammad Usman Tariq,Abhinav Jangda,Angelica Moreira,Madan Musuvathi,Tyler Sorensen*

Main category: cs.SE

TL;DR: PEAK是一个基于自然语言转换的GPU内核性能工程AI助手，通过将优化转换为自然语言指令让LLM执行，支持CUDA、HIP和HLSL后端，在矩阵乘法优化中达到或接近厂商库性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在底层后端代码（特别是GPU内核）优化方面表现不佳，因为性能关键细节与快速演进的硬件特性紧密耦合，且可用代码示例稀少。需要一种能快速开发、验证和评估GPU内核优化的方法。

Method: PEAK利用自然语言转换的核心思想：将迭代代码优化写成自然语言指令，由LLM执行。系统包含模块化可扩展的基础设施，支持验证和性能评估。为CUDA、HIP和HLSL三个后端实现了16个自然语言转换来优化矩阵乘法内核。

Result: PEAK生成的实现与厂商库性能相当（当有厂商库时），对于没有厂商库的HLSL，实现达到了硬件文档标称的FLOPS。系统能够探索LLM在该领域的行为特征、转换错误分析以及性能随优化序列的演变。

Conclusion: PEAK提供了一个灵活的接口，既可由性能工程师提高生产力，也可完全自主运行（如AI代理驱动），具有向前兼容的设计，能随AI能力进步而持续改进。

Abstract: Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse.
  In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.

</details>


### [51] [BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation](https://arxiv.org/abs/2512.19122)
*Mahir Labib Dihan,Sadif Ahmed,Md Nafiu Rahman*

Main category: cs.SE

TL;DR: 提出BanglaForge框架，通过检索增强的双模型协作与自我精炼，解决孟加拉语代码生成的资源匮乏问题，在BLP-2025基准上达到84.00%的Pass@1准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，缺乏大规模标注数据集和将自然语言规范转换为可执行程序的工具，使得孟加拉语到代码生成成为具有挑战性的任务。

Method: 采用检索增强的双模型协作范式与自我精炼，结合上下文学习、基于LLM的翻译、系统提示工程和基于执行反馈的迭代自我精炼，其中编码器生成初始解决方案，审查器增强其鲁棒性。

Result: 在BLP-2025孟加拉语代码生成基准上，BanglaForge实现了84.00%的竞争性Pass@1准确率。

Conclusion: 检索、模型协作和自我精炼对于低资源孟加拉语代码生成是有效的。

Abstract: Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.

</details>


### [52] [Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation](https://arxiv.org/abs/2512.19215)
*Junyao Ye,Zhen Li,Xi Tang,Shouhuai Xu,Deqing Zou,Zhongsheng Yuan*

Main category: cs.SE

TL;DR: 该论文提出了一种新型的语义等价变换(SET)后门攻击，使用语义保持的低流行度代码变换生成隐蔽触发器，相比传统注入式攻击更难以检测和防御。


<details>
  <summary>Details</summary>
Motivation: 当前对神经代码模型后门攻击的理解主要集中于注入式攻击，这些攻击可通过标准清理技术中和，可能导致对后门攻击安全的错误认知。需要研究更隐蔽的攻击方式。

Method: 提出语义等价变换(SET)后门攻击框架，使用语义保持的低流行度代码变换生成隐蔽触发器。在五个任务、六种编程语言和多个模型(CodeBERT、CodeT5、StarCoder)上进行实验验证。

Result: SET攻击成功率通常>90%，同时保持模型效用。攻击隐蔽性高，检测率比注入式攻击平均低25.13%以上。规范化防御措施仅提供部分缓解，证实了攻击的鲁棒性。

Conclusion: SET后门攻击比传统注入式攻击更隐蔽有效，现有防御措施效果有限，需要进一步研究针对SET攻击的可扩展防御机制。

Abstract: Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.

</details>


### [53] [A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis](https://arxiv.org/abs/2512.19481)
*Katharina Stengg,Christian Macho,Martin Pinzger*

Main category: cs.SE

TL;DR: 研究GPT-5和GPT-5-mini预测源代码变更影响的能力，发现模型表现不佳，但提供diff信息可略微提升性能


<details>
  <summary>Details</summary>
Motivation: 理解源代码变更及其影响是软件开发的关键技能，但通常需要手动分析且耗时。虽然LLM在代码分析任务中显示出潜力，但它们在理解代码变更影响方面的能力尚未充分探索。

Method: 构建包含种子变更、变更对和变更类型的数据集，评估GPT-5和GPT-5-mini在两种配置下的表现：(1)种子变更信息和父提交树；(2)额外提供每个种子变更的diff信息。

Result: 两个LLM在实验中表现都不佳，GPT-5优于GPT-5-mini。提供diff信息能略微提升两个模型的性能。

Conclusion: 当前LLM在预测代码变更影响方面的能力有限，需要进一步研究改进方法。

Abstract: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.

</details>


### [54] [Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models](https://arxiv.org/abs/2512.19509)
*Shangbo Yun,Xiaodong Gu,Jianghong Huang,Beijun Shen*

Main category: cs.SE

TL;DR: 本文提出了一种基于嵌入的框架来揭示编程语言之间的深层语言关系，并利用这些关系来改进多语言代码LLM的训练和推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术通常只是简单地聚合多语言代码数据来训练代码LLM，很少探索编程语言之间的深层关系以及如何利用这些关系来优化训练和推理。本文旨在研究编程语言之间的深层语言关系，并利用这些关系改进多语言代码LLM。

Method: 1) 定义21个主要编程语言特征；2) 使用LLM生成跨多个语言的特征对齐代码样本；3) 嵌入19种语言的语义并行代码片段；4) 构建相似性矩阵并进行层次聚类以揭示语言关系；5) 基于发现的语言家族提出三种训练策略：跨语言相关语言的迁移学习、语言邻近性引导的课程学习、基于质心的中间代码翻译。

Result: 分析揭示了编程语言之间的清晰层次结构：密切相关的语言形成明确定义的聚类（如C、C++、Java、Swift），Go表现出最高的跨语言相似性。在4个代码智能任务上的实验表明，所提方法显著提高了多语言LLM的性能。

Conclusion: 这项工作提供了编程语言的通用视角，并推进了多语言代码LLM训练的更有效策略，揭示了语言关系可以显著提升模型性能。

Abstract: The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [55] [Ampcode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F%3Futm_source=tldrfounders/1/0100019b36cbaea2-7b90cbe1-a5f1-4607-ad06-419ad59905a6-000000/SC96I6WY-KlabNQdyAbaHxcO8sEktI8cvtCdNluhAxg=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ampcode是一个前沿的编码代理工具，让用户能够充分利用领先模型的能力


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手虽然强大，但用户往往无法充分利用最先进模型的能力。Ampcode旨在解决这一问题，让用户能够轻松访问和使用前沿的编码模型

Method: 开发了一个名为Amp的编码代理工具，该工具集成了领先的AI模型，为用户提供强大的编码辅助功能

Result: 创建了Ampcode工具，使用户能够利用前沿的编码模型进行开发工作

Conclusion: Ampcode作为一个编码代理工具，成功地将前沿AI模型的能力带给开发者，提升了编码效率和质量

Abstract: Ampcode (Tool) Amp is a frontier coding agent that lets you wield the full power of leading models.

</details>


### [56] [Agentic AI isn't just genAI with extra steps](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fwhy-agentic-ai-is-your-next-priority%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-why-agentic-ai-is-your-next-priority%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/4Vvih960f5DcdueuJoyYk164hgs_-NK9z8I7cA_fTWk=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Algolia的电子书探讨了什么是真正的Agentic AI，它与生成式AI的区别，以及未来发展方向，预测到2028年三分之一的企业应用将具备Agentic能力。


<details>
  <summary>Details</summary>
Motivation: 当前关于Agentic AI的讨论很多，但大多数实现只是"agent-washing" - 仅仅是提示链或包含LLM的正常工作流程。需要澄清Agentic AI的真正含义及其与生成式AI的区别。

Method: 通过Algolia的电子书进行分析，解释Agentic AI的实际定义，讨论其如何通过解析自然语言来增强搜索功能。

Result: 提供了对Agentic AI的清晰定义，预测到2028年三分之一的企业应用将具备Agentic能力，展示了Agentic AI在搜索等领域的应用潜力。

Conclusion: Agentic AI不仅仅是生成式AI的扩展，而是具有自主性和目标导向能力的AI系统，将在企业应用中发挥重要作用。

Abstract: Agentic AI isn't just genAI with extra steps (Sponsor) The buzz around agentic AI has reached a fever pitch. But outside of coding, most implementations are "agent-washing:" a chain of prompts or a normal workflow with an LLM in the loop.Algolia's ebook breaks down what agentic AI actually is, how it differs from gen AI, and where it's headed: → Read why Gartner predicts a third of enterprise apps will be agentic enabled by 2028 → Discover how agentic AI enhances search by parsing natural lan...

</details>


### [57] [Introducing GPT-5.2-Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F8ea9zT/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/DkiofdPMj8dX4hUXxVlPT--bb4lj1QKH-1estWf91Es=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI发布GPT-5.2-Codex，在SWE-Bench Pro和Terminal-Bench 2.0上达到SOTA，具有改进的长时程工作能力，同时启动可信访问试点计划


<details>
  <summary>Details</summary>
Motivation: 开发更强大的代码代理模型，提升长时程编程任务性能，同时为网络安全专业人员提供安全访问更强大模型的途径

Method: 推出GPT-5.2-Codex模型，在SWE-Bench Pro和Terminal-Bench 2.0基准测试中验证性能，并建立可信访问试点机制

Result: 在SWE-Bench Pro和Terminal-Bench 2.0上达到最先进水平，长时程工作能力得到改进

Conclusion: GPT-5.2-Codex在代码代理任务上表现优异，同时通过可信访问试点确保更强大模型的安全使用

Abstract: Introducing GPT-5.2-Codex (5 minute read) OpenAI's new agentic coding model is state-of-the-art on SWE-Bench Pro and Terminal-Bench 2.0 with improved long-horizon work. OpenAI is launching a trusted access pilot to give vetted cybersecurity professionals access to future, more powerful models.

</details>


### [58] [Agent Skills Becomes an Open Standard](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagentskills.io%2Fhome%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/h4dzNkqRh2bQXoF4VZEqGfGEcmT2EpEDa5VLYd56tCI=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills成为开放标准，这是一种包含指令、脚本和资源的文件夹格式，让AI代理能够按需获得新能力，最初由Anthropic创建，现已被多家主流开发工具采用。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI代理在不同产品和环境中能力碎片化的问题，需要一种标准化的方式来打包和共享领域专业知识与工作流程，使团队能够创建可移植、版本控制的技能包。

Method: 创建了Agent Skills开放格式规范，这是一种包含指令、脚本和资源的文件夹结构，允许团队将领域专业知识和工作流程打包成标准化的技能包，这些技能包可以在不同的代理产品之间共享和使用。

Result: Agent Skills已成为开放标准，获得了Cursor、GitHub、VS Code、Claude Code和OpenAI的Codex CLI等主流开发工具的广泛采用，形成了一个跨平台的技能生态系统。

Conclusion: Agent Skills作为开放标准的建立，解决了AI代理能力碎片化问题，促进了领域专业知识的共享和重用，为AI代理生态系统的发展提供了重要的基础设施。

Abstract: Agent Skills Becomes an Open Standard (2 minute read) Agent Skills, folders of instructions, scripts, and resources that give AI agents new capabilities on demand, originated at Anthropic (which also created MCP) and is now an open format with adoption from Cursor, GitHub, VS Code, Claude Code, and OpenAI's Codex CLI. Skills let teams package domain expertise and workflows into portable, version-controlled packages that work across different agent products.

</details>


### [59] [Inside Replit's Snapshot Engine: The Tech Making AI Agents Safe](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.replit.com%2Finside-replits-snapshot-engine%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/2OiE2-BgrjUnaUpKFV71cL_V3XQXWhaHA14gutOv_iM=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Replit开发了快照引擎技术，通过隔离可逆的计算和存储架构，为AI编码代理提供安全实验环境，显著提升开发效率和代理性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI编码代理在实验和修改时缺乏安全隔离机制，可能导致系统不稳定或不可逆的更改。Replit意识到其现有的隔离可逆计算存储架构可以解决这一问题，为AI代理提供安全的实验环境。

Method: 构建计算和存储架构，支持隔离且可逆的更改操作。该快照引擎允许开发者在隔离环境中进行频繁快速实验，并将这些原语应用于Replit Agent编码代理系统。

Result: 该系统既帮助人类开发者更安全地驱动AI代理，也显著提升了AI代理本身的性能。快照技术使编码代理能够在安全环境中进行实验和修改。

Conclusion: Replit的快照引擎技术为AI编码代理提供了关键的安全基础设施，通过隔离可逆的更改机制，促进了更频繁、更快速的实验，提升了开发效率和代理可靠性。

Abstract: Inside Replit's Snapshot Engine: The Tech Making AI Agents Safe (9 minute read) Replit built a compute and storage fabric that allows it to make changes in an isolated, reversible way. These primitives enable developers to experiment more frequently and faster. The company realized the same primitives could be used to superpower coding agents when it built Replit Agent in 2024. The system helps the human driving the agent, and the agent itself greatly benefits from the tools. This post explor...

</details>


### [60] [The Signature Flicker](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fsignature-flicker%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/W1795o0Zl3WRPvs9FlSS0o7n3JaB8bSAyBQ5ILMsalI=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic修复了Claude Code终端中的签名闪烁问题，通过重写渲染器仅重新渲染变化部分，同时保持React组件模型


<details>
  <summary>Details</summary>
Motivation: 终端设计初衷并非为了交互性，重新定位光标和覆盖现有文本容易导致闪烁问题，需要改进Claude Code的用户体验

Method: 重写渲染器，仅重新渲染变化的部分，同时保持React作为组件模型

Result: 成功修复了Claude Code中的签名闪烁问题

Conclusion: 通过优化渲染策略可以解决终端交互中的视觉闪烁问题，提升用户体验

Abstract: The Signature Flicker (4 minute read) Anthropic has fixed Claude Code's signature flicker. Terminals weren't really designed for interactivity. Repositioning the cursor and writing over existing text easily leads to flickering if not done well. Anthropic chose to re-render only the changed parts. It rewrote the renderer from scratch while still keeping React as the component model.

</details>


### [61] [Contra DSPy and GEPA](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenanderson.work%2Fblog%2Fcontra-dspy-gepa%2F%3Futm_source=tldrai/1/0100019b36f923cf-7eee5b15-bcbf-4480-a51e-ee61ab53ce8e-000000/JsRSQEfm1BZJaCvkT1-w2YfFpkEm_oPHZMPSpW36Sr4=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文批评了将LLM工作流视为模块化程序的方法（如DSPy和GEPA），认为这种方法是倒退、僵化的，不适合处理最有趣的任务


<details>
  <summary>Details</summary>
Motivation: 作者观察到当前流行的LLM工作流框架（如DSPy和GEPA）试图将LLM工作流建模为模块化程序，但这种方法存在根本性缺陷。作者认为这种"编程化"方法限制了LLM的灵活性和创造性，特别是对于复杂、开放式的任务。

Method: 论文通过理论分析和案例研究，对比了模块化编程方法与更灵活的工作流设计方法。作者可能提出了替代方案或设计原则，强调LLM工作流应该更加动态、自适应，而不是僵化的模块组合。

Result: 分析表明，模块化编程方法在处理复杂、创造性任务时表现不佳，限制了LLM的潜力。作者可能展示了更灵活的方法在实际任务中的优势。

Conclusion: 将LLM工作流视为模块化程序是错误的范式。对于最有趣和最具挑战性的任务，需要更灵活、动态的工作流设计方法，能够充分利用LLM的创造性和适应性。

Abstract: Contra DSPy and GEPA (15 minute read) Trying to treat LLM workflows as modular programs is a backwards, rigid, and the wrong fit for the most interesting tasks.

</details>


### [62] [Da2a: The Future of Data Platforms is Agentic, Distributed, and Collaborative](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmlops.community%2Fda2a-the-future-of-data-platforms-is-agentic-distributed-and-collaborative%2F%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/qGJEQxxPPLObSrj5ut3O6p2En84yrGhCPTR21iuMIa4=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DA2A提出了一种新的数据平台范式：基于AI代理的分布式协作系统，取代传统集中式ETL平台，通过专业代理自主管理领域数据，使用代理间协议通信，由编排器协调完成复杂查询


<details>
  <summary>Details</summary>
Motivation: 传统集中式数据平台依赖数据工程师进行ETL和查询，造成瓶颈，减缓业务用户的决策速度。需要一种新的范式来解决这些问题

Method: 采用代理化、分布式和协作的方法：专业AI代理自主管理领域特定数据，通过Agent-to-Agent协议通信，由编排器协调代理协作回答复杂查询

Result: 提出了DA2A作为未来数据平台的新范式，能够消除传统平台的瓶颈，加速业务决策过程

Conclusion: 数据平台的未来应该是代理化、分布式和协作的，DA2A范式能够解决传统集中式平台的局限性

Abstract: Da2a: The Future of Data Platforms is Agentic, Distributed, and Collaborative (6 minute read) Traditional centralized data platforms, reliant on data engineers for ETL and queries, create bottlenecks and slow decision-making for business users. DA2A is a future paradigm of agentic, distributed, and collaborative data platforms where specialized AI agents autonomously manage domain-specific data, communicate via an Agent-to-Agent protocol, and collaborate through an orchestrator to answer comp...

</details>


### [63] [The Seven Pillars of a Production-Grade Agent Architecture](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackernoon.com%2Fthe-seven-pillars-of-a-production-grade-agent-architecture%3Futm_source=tldrdata/1/0100019b45be8dd6-f7340055-99c9-402f-9d8b-12a269686952-000000/9K9B3knVxf91qQL8a3F1SOSPeazp5wO5_IszlIg5Jb0=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文提出了生产级智能体架构的七个核心支柱，为企业级智能体AI提供坚实基础


<details>
  <summary>Details</summary>
Motivation: 企业级智能体AI需要一个强大且设计良好的基础架构，以支持生产环境中的可靠部署和运行

Method: 通过识别和定义七个关键架构支柱来构建生产级智能体系统

Result: 提出了一个包含七个核心支柱的系统化架构框架，为企业级智能体AI提供设计指导

Conclusion: 七个支柱的架构设计对于构建可靠、可扩展的企业级智能体系统至关重要

Abstract: The Seven Pillars of a Production-Grade Agent Architecture (12 minute read) Enterprise-grade agentic AI needs a strong, well-designed foundation.

</details>


### [64] [Graphite Gets Bought By Cursor: Three Reflections from Six Years of Work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhunterwalk.com%2F2025%2F12%2F19%2Fgraphite-gets-bought-by-cursor-three-reflections-from-six-years-of-work-pmf-didnt-happen-right-away-working-irl-mattered-three-founders-three-distinct-roles%2F%3Futm_source=tldrnewsletter/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/RZ60X8ljv0jItxoIbSKN-MBIHhrRRq-NoXoDFhjQvfo=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Graphite（前身为Screenplay）被代码生成初创公司Cursor收购，文章分享了六年来从产品市场匹配失败到成功退出的三个关键反思


<details>
  <summary>Details</summary>
Motivation: 分享Graphite从Screenplay时期产品市场匹配失败，到被Cursor收购的六年创业历程中的经验教训和反思

Method: 基于创始团队亲身经历，总结三个关键反思：1）早期团队面对面协作建立文化和信任的重要性；2）从产品市场匹配失败中学习转型；3）在疫情背景下调整工作模式

Result: Graphite成功被Cursor收购，完成了创业退出，团队在六年历程中积累了宝贵的创业经验

Conclusion: 创业成功需要团队文化、适应能力和时机把握的结合，面对面协作对早期团队建设至关重要，从失败中学习是成功转型的关键

Abstract: Graphite Gets Bought By Cursor: Three Reflections from Six Years of Work (9 minute read) Software quality startup Graphite has been acquired by code generation startup Cursor. Graphite started out as Screenplay, which couldn't find product-market fit right away. The conversations around Graphite started right before the pandemic lockdowns. The team started working in person together as soon as it was advisable, allowing them to knit a culture early and build trust. The three founders took thr...

</details>


### [65] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b45cd8f06-af154382-1306-4c68-a659-70e51a43af8a-000000/EUbLliy0ru1LtJzwhFLsc9y7jk_1e8edz5Tv6Oy__Lc=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 2025年是LLM领域取得显著进展的一年，回顾了该年度的重要发展和成就


<details>
  <summary>Details</summary>
Motivation: 总结2025年LLM领域的发展历程，记录重要里程碑和进步，为研究社区提供年度回顾

Method: 采用年度回顾分析的方法，系统梳理2025年LLM领域的关键事件、技术突破和重要论文

Result: 2025年LLM在多方面取得显著进展，包括模型性能提升、新架构出现、应用扩展等，是充满活力和成就的一年

Conclusion: 2025年是LLM发展的关键年份，展现了该领域的持续创新和快速进步，为未来发展奠定了坚实基础

Abstract: 2025 LLM Year in Review (11 minute read) 2025 was a strong and eventful year of progress in LLMs.

</details>


### [66] [The future of AI-powered software optimization](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fpolicy-news-and-insights%2Fthe-future-of-ai-powered-software-optimization-and-how-it-can-help-your-team%2F%3Futm_source=tldrdevops/1/0100019b45f3b9fb-9e07cbe1-ecac-48c8-bf0f-2f926a3c8f15-000000/8RwRUa1gMpJukJLGYPfwkC59_q_PZG0yUltOfH6ivBo=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出Continuous Efficiency平台，结合AI自动化和绿色软件实践，使代码库能够自我优化性能、效率和可持续性。通过智能体工作流，开发者可以用自然语言编写规则，AI智能体应用这些规则来提升代码质量、执行标准，并在异构仓库中迭代增强性能。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发面临代码质量参差不齐、性能优化复杂、可持续性要求提高等挑战。传统的手动优化方法效率低下，难以在异构代码库中规模化应用。需要一种自动化、智能化的解决方案来帮助开发团队持续优化软件性能、效率和可持续性。

Method: 采用智能体工作流（agentic workflows）架构，开发者通过自然语言编写优化规则，AI智能体自动应用这些规则到代码库中。结合AI驱动的自动化和绿色软件实践，形成自我优化的代码生态系统。系统能够跨异构仓库进行迭代性能增强。

Result: 创建了一个能够使代码库自我优化的平台，开发者可以通过自然语言规则轻松实现代码质量改进、标准执行和性能优化。该平台将AI自动化与绿色软件实践相结合，为开发团队提供了高效的软件优化解决方案。

Conclusion: Continuous Efficiency平台代表了AI驱动软件优化的未来方向，通过智能体工作流和自然语言接口，使软件优化变得更加可访问和可扩展，有助于开发团队在性能、效率和可持续性方面实现持续改进。

Abstract: The future of AI-powered software optimization (and how it can help your team) (7 minute read) GitHub's Continuous Efficiency combines AI-driven automation and green software practices to make codebases self-optimizing for performance, efficiency, and sustainability. Using agentic workflows, developers can author natural-language rules that AI agents apply to improve code quality, enforce standards, and iteratively enhance performance across heterogeneous repositories.

</details>


### [67] [Claude's frontend design skill](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fedxeth%2Fc9669f46a04687375fd9150c4874286e%3Futm_source=tldrdev/1/0100019b45f7df31-8b7da767-1b18-4f2f-8328-54fa1d6040d8-000000/UD4lxkzuTsxzjcmsu6rxxVZNCW9mNaAmkDDeH-NVPYQ=436)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude技能用于创建生产级前端界面，避免通用AI生成的美学风格，生成视觉冲击力强且功能完整的代码


<details>
  <summary>Details</summary>
Motivation: 解决AI生成前端界面时常见的"通用AI美学"问题，避免生成缺乏创意和上下文相关性的模板化设计

Method: 开发Claude技能，通过特定指令引导AI：避免通用AI美学、创造性解读需求、做出意外但符合上下文的设计选择

Result: 创建了一个能够生成生产级前端界面的技能，确保代码功能完整且设计具有视觉冲击力和独特性

Conclusion: 通过特定指令约束可以显著提升AI生成前端界面的质量和原创性，避免落入通用AI美学的陷阱

Abstract: Claude's frontend design skill (2 minute read) This gist contains a Claude skill for creating production-grade frontend interfaces that avoid generic AI slop aesthetics. The user provides frontend requirements, and the AI generates production-grade and functional code with visually striking and memorable design. The skill instructs the agent to never use generic AI-generated aesthetics, to interpret prompts creatively, and to make unexpected choices that feel genuinely designed for the context.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation](https://arxiv.org/abs/2512.18174)
*Lena Libon,Meghana Bhange,Rushabh Solanki,Elliot Creager,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: 该论文探讨了在LLM推理过程中产生的中间计算（CoT轨迹）应被视为用户个人数据，并提出了一种社区知识聚合与蒸馏方法，使低效用社区能够创建更符合自身目标的替代模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展范式强调大规模数据集训练，这引发了数据隐私和用户自主权问题。论文关注LLM使用链式思维（CoT）推理时产生的中间计算，这些计算可能包含用户个人信息，但用户缺乏对其的控制权。

Method: 首先从法律角度论证CoT中间计算应被视为个人数据。然后基于"有意识数据贡献"框架，提出社区知识聚合与蒸馏方法：低效用社区可以聚合共享知识，通过蒸馏技术创建更符合自身目标的替代模型。

Result: 通过实证验证了该方法，并研究了社区多样性、推理粒度、社区规模对蒸馏性能的影响。结果显示社区能够成功创建更符合自身目标的模型。

Conclusion: CoT推理中的中间计算应被视为个人数据，用户应拥有对其的控制权。社区知识聚合与蒸馏提供了一种增强用户自主权的方法，使边缘化群体能够创建更适合自身需求的模型。

Abstract: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that "reason" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.

</details>


### [69] [SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models](https://arxiv.org/abs/2512.18583)
*Pengcheng Li,Qiang Fang,Tong Zhao,Yixing Lan,Xin Xu*

Main category: cs.LG

TL;DR: SD2AIL利用扩散模型生成合成演示来增强对抗模仿学习，通过优先重放策略选择最有价值的演示，在模拟任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习需要大量专家演示，但在某些场景中收集这些演示很困难。受扩散模型在数据生成方面的成功启发，研究者希望通过扩散模型生成合成演示来增强专家数据。

Method: 1. 在判别器中使用扩散模型生成合成演示作为伪专家数据来增强专家演示；2. 引入优先专家演示重放策略(PEDR)，从大量（伪）专家演示中选择最有价值的演示进行重放。

Result: 在模拟任务中验证了方法的有效性和鲁棒性。在Hopper任务中，平均回报达到3441，比最先进方法高出89。

Conclusion: SD2AIL通过扩散模型生成合成演示和优先重放策略，有效解决了对抗模仿学习中专家演示不足的问题，取得了优于现有方法的性能。

Abstract: Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.

</details>


### [70] [EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture](https://arxiv.org/abs/2512.18596)
*Quanxi Zhou,Wencan Mao,Yilei Liang,Manabu Tsukada,Yunling Liu,Jon Crowcroft*

Main category: cs.LG

TL;DR: 提出EIA-SEC框架解决多无人机智能农业系统中的轨迹规划问题，通过精英模仿和共享集成批评器提升性能


<details>
  <summary>Details</summary>
Motivation: 无线通信技术推动智能农业发展，无人机在数据收集、图像获取和通信任务中发挥多功能作用，需要解决多无人机协同轨迹规划问题

Method: 建立马尔可夫决策过程模型，提出精英模仿演员-共享集成批评器框架，智能体从精英智能体自适应学习以减少试错成本，共享集成批评器与本地批评器协作确保无偏目标值估计并防止过估计

Result: 实验结果表明EIA-SEC在奖励性能、训练稳定性和收敛速度方面优于最先进的基线方法

Conclusion: EIA-SEC框架有效解决了多无人机智能农业系统中的轨迹规划问题，通过精英模仿和批评器集成机制提升了强化学习算法的性能

Abstract: The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.

</details>


### [71] [Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2512.18670)
*Xue Yang,Michael Schukat,Junlin Lu,Patrick Mannion,Karl Mason,Enda Howley*

Main category: cs.LG

TL;DR: 提出DGCRL方法，通过外部自演化的演示库直接指导RL探索和适应，解决持续强化学习中的稳定性-可塑性困境


<details>
  <summary>Details</summary>
Motivation: 传统RL在动态环境中表现不佳，持续强化学习(CRL)虽然能让智能体持续学习新任务，但平衡稳定性（保留先验知识）和可塑性（获取新知识）仍然困难。现有方法主要通过优化机制影响过去知识，很少直接影响智能体行为，这阻碍了有效的知识重用和高效学习。

Method: 提出演示引导的持续强化学习(DGCRL)，将先验知识存储在外部自演化的演示库中，直接指导RL探索和适应。对每个任务，智能体动态选择最相关的演示，并采用基于课程学习的策略加速学习，逐步从演示引导探索过渡到完全自主探索。

Result: 在2D导航和MuJoCo运动任务上的大量实验表明，该方法具有优越的平均性能、增强的知识迁移能力、减轻遗忘效果和训练效率。额外的敏感性分析和消融研究进一步验证了其有效性。

Conclusion: DGCRL通过外部演示库直接指导行为，有效解决了持续强化学习中的稳定性-可塑性困境，实现了更好的知识重用和学习效率。

Abstract: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.

</details>


### [72] [Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning](https://arxiv.org/abs/2512.18763)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的GMM-QFs方法，将高斯混合模型作为Q函数损失的直接替代，通过黎曼流形优化进行学习，在多种基准RL任务中表现出色且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 传统上GMMs在强化学习中主要用于概率密度函数估计，但本文探索GMMs作为函数逼近器的新角色，旨在提供具有强大表示能力且计算效率高的Q函数逼近方法。

Method: 提出GMM-QFs方法：1) 将GMMs作为Q函数损失的参数化替代模型；2) 将GMMs嵌入Bellman残差中；3) 在黎曼流形上优化GMM参数（混合权重、高斯均值向量和协方差矩阵）；4) 将黎曼优化整合到标准策略迭代框架的策略评估步骤中。

Result: 理论证明GMM-QFs具有通用逼近能力；实验表明即使不使用经验数据，GMM-QFs在多种基准RL任务中也能提供有竞争力的性能，有时甚至优于最先进方法，同时保持比依赖经验数据的深度学习方法更小的计算开销。

Conclusion: GMM-QFs为强化学习中的函数逼近提供了新颖有效的解决方案，结合了强大的表示能力、理论保证和计算效率，为RL方法设计开辟了新方向。

Abstract: Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.

</details>


### [73] [When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://arxiv.org/abs/2512.18934)
*Michael S. Zhang,Rishi A. Ruia,Arnav Kewalram,Saathvik Dharmapuram,Utkarsh Sharma,Kevin Zhu*

Main category: cs.LG

TL;DR: 量化精度与回放缓冲区策略在持续学习中的相互作用研究：量化模型（INT8/INT4）在后续任务中超越FP16模型，INT4在代码生成任务上性能翻倍，小回放缓冲区（0.1%）显著提升知识保留


<details>
  <summary>Details</summary>
Motivation: 研究量化精度（FP16、INT8、INT4）与回放缓冲区策略在大型语言模型持续学习中的相互作用，解决灾难性遗忘问题，为部署高效压缩模型提供指导

Method: 系统研究不同量化精度（FP16、INT8、INT4）与回放缓冲区策略在持续学习中的表现，分析量化噪声作为隐式正则化的作用，评估不同任务类型（NLU、数学、代码生成）对缓冲区大小的需求

Result: 量化模型在后续任务中表现优于FP16模型（提升8-15%），INT4在代码生成任务上性能翻倍（40% vs 20%）；小回放缓冲区（0.1%）显著提升知识保留（NLU保留率从45%提升至65%）；INT8在可塑性-保留平衡上表现最优

Conclusion: 量化噪声可作为隐式正则化防止过拟合，挑战了"精度越高越好"的传统观念；INT8量化在计算效率和持续学习动态上均表现优越；为部署压缩模型提供实用指南：NLU任务需1-2%小缓冲区，数学和代码任务需5-10%中等缓冲区

Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.

</details>


### [74] [Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement](https://arxiv.org/abs/2512.18950)
*Saman Forouzandeh,Wei Peng,Parham Moradi,Xinghuo Yu,Mahdi Jalili*

Main category: cs.LG

TL;DR: MACLA是一个将推理与学习解耦的框架，通过维护冻结的大型语言模型，在外部层次化程序记忆中执行所有适应。它从轨迹中提取可重用程序，通过贝叶斯后验跟踪可靠性，通过期望效用评分选择动作，并通过对比成功与失败来精炼程序。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体通常需要微调LLM参数，这计算成本高、样本效率低且缺乏可解释性。需要一种无需更新LLM参数就能实现样本高效、可解释且持续改进的智能体框架。

Method: MACLA采用外部层次化程序记忆，包含：1）从轨迹中提取可重用程序；2）通过贝叶斯后验跟踪程序可靠性；3）通过期望效用评分选择动作；4）通过对比成功与失败案例来精炼程序。所有适应都在外部记忆中进行，保持LLM冻结。

Result: 在四个基准测试（ALFWorld、WebShop、TravelPlanner、InterCodeSQL）上平均性能达到78.1%，优于所有基线。在ALFWorld未见任务上达到90.3%性能，有3.1%正向泛化。系统在56秒内构建记忆，比最先进的LLM参数训练基线快2800倍，将2851条轨迹压缩为187个程序。

Conclusion: 具有贝叶斯选择和对比精炼的结构化外部记忆能够实现样本高效、可解释且持续改进的智能体，无需LLM参数更新。这种方法在性能、效率和泛化方面都表现出色。

Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.

</details>


### [75] [Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation](https://arxiv.org/abs/2512.18957)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 提出一种在线分布鲁棒强化学习算法，通过与环境交互学习最优鲁棒策略，无需先验模型或离线数据，适用于高维任务，并提供了理论分析证明其样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实应用中常因训练与部署环境不匹配导致性能下降。现有分布鲁棒RL方法通常需要大量先验知识（如生成模型或大型离线数据集），且主要关注表格方法，难以扩展到复杂领域。

Method: 提出在线分布鲁棒强化学习算法，具有通用函数逼近能力，仅通过与环境的交互学习最优鲁棒策略，无需先验模型或离线数据。在总变差不确定性集下提供理论分析，建立近最优的次线性遗憾界。

Result: 该方法能够学习到最优鲁棒策略，适用于高维任务。理论分析表明在总变差不确定性集下具有近最优的次线性遗憾界，证明了算法的样本效率和有效性。

Conclusion: 提出的在线DR-RL算法克服了现有方法的局限性，能够在没有先验知识的情况下通过在线交互学习鲁棒策略，为强化学习在现实世界中的部署提供了更实用的解决方案。

Abstract: The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.

</details>


### [76] [Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies](https://arxiv.org/abs/2512.19673)
*Yuqiao Tan,Minzheng Wang,Shizhu He,Huanxuan Liao,Chengfeng Zhao,Qiunan Lu,Tian Liang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种分解语言模型策略的新方法，通过分析Transformer残差流的内在分割，揭示了内部层策略和内部模块策略，并基于此提出了自下而上的策略优化方法，在复杂推理任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法将大语言模型视为单一统一策略，忽视了其内部机制。理解策略在不同层和模块间的演化对于实现更有针对性的优化和揭示复杂推理机制至关重要。

Method: 通过利用Transformer残差流的内在分割以及隐藏状态与解嵌入矩阵组合的等价性，将语言模型策略分解为内部层策略（对应各层贡献）和内部模块策略（对应每层中的自注意力和前馈网络组件）。基于此提出了自下而上的策略优化方法，在早期训练阶段直接优化内部层策略。

Result: 研究发现：早期层保持高熵以进行探索，顶层收敛到接近零熵以进行细化，收敛模式在不同模型系列间存在差异。Llama的预测空间在最后一层快速收敛，而Qwen系列模型（特别是Qwen3）展现出更类似人类的渐进结构化推理模式。提出的BuPO方法在复杂推理基准测试中表现出有效性。

Conclusion: 通过分解语言模型策略并分析内部策略熵，揭示了不同模型系列的推理模式差异。提出的自下而上的策略优化方法通过在下层对齐训练目标，重建了基础推理能力并取得了优越性能。

Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.

</details>


### [77] [Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments](https://arxiv.org/abs/2512.19154)
*Geraud Nangue Tasse,Matthew Riemer,Benjamin Rosman,Tim Klinger*

Main category: cs.LG

TL;DR: 提出Adaptive Stacking元算法，通过自适应维护小型记忆栈来处理高度非马尔可夫依赖，相比传统帧堆叠方法显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署计算受限的智能体时面临高度非马尔可夫依赖的挑战。传统帧堆叠方法需要堆叠大量最近观察，导致计算和内存需求过高。研究发现许多环境虽然时间上高度非马尔可夫，但只因果依赖于相对较少的观察。

Method: 提出Adaptive Stacking元算法，维护相对较小的自适应记忆栈，能够表达时间上的高度非马尔可夫依赖，同时每个步骤考虑更少的观察。算法具有收敛保证，并量化了MLP、LSTM和Transformer智能体的计算和内存节省。

Result: 实验使用流行的记忆任务，控制非马尔可夫依赖程度。结果显示适当的元算法能够学习移除不预测未来奖励的记忆，同时不过度移除重要经验，实现计算和内存的显著节省。

Conclusion: Adaptive Stacking元算法能够有效处理高度非马尔可夫依赖环境，相比传统帧堆叠方法在计算和内存方面具有显著优势，为现实世界部署计算受限智能体提供了可行方案。

Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking

</details>
