{"id": "2512.00047", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00047", "abs": "https://arxiv.org/abs/2512.00047", "authors": ["Angelina Parfenova", "Alexander Denzler", "Juergen Pfeffer"], "title": "Emergent Convergence in Multi-Agent LLM Annotation", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df7500\u6b21\u591a\u667a\u80fd\u4f53\u3001\u591a\u8f6e\u8ba8\u8bba\uff0c\u5206\u6790LLM\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u884c\u4e3a\uff0c\u53d1\u73b0\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u89d2\u8272\u63d0\u793a\uff0cLLM\u7fa4\u4f53\u4e5f\u80fd\u5728\u8bcd\u6c47\u548c\u8bed\u4e49\u4e0a\u6536\u655b\uff0c\u5e76\u8868\u73b0\u51fa\u7c7b\u4f3c\u8c08\u5224\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u534f\u4f5c\u73af\u5883\u4e2d\uff0c\u4f46\u4f5c\u4e3a\u9ed1\u76d2\u667a\u80fd\u4f53\u65f6\u5b83\u4eec\u5982\u4f55\u534f\u8c03\u4ecd\u77e5\u4e4b\u751a\u5c11\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3LLM\u5728\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u7684\u534f\u8c03\u52a8\u6001\u548c\u6d8c\u73b0\u884c\u4e3a\u3002", "method": "\u5728\u5f52\u7eb3\u7f16\u7801\u4efb\u52a1\u4e2d\u6a21\u62df7500\u6b21\u591a\u667a\u80fd\u4f53\u3001\u591a\u8f6e\u8ba8\u8bba\uff0c\u751f\u6210\u8d85\u8fc7125000\u6761\u8bdd\u8bed\u8bb0\u5f55\u3002\u5f15\u5165\u8fc7\u7a0b\u7ea7\u6307\u6807\uff08\u4ee3\u7801\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u81ea\u4e00\u81f4\u6027\u3001\u8bcd\u6c47\u7f6e\u4fe1\u5ea6\u7b49\uff09\uff0c\u5e76\u5206\u6790\u8f93\u51fa\u5d4c\u5165\u7684\u51e0\u4f55\u6f14\u53d8\uff0c\u7279\u522b\u662f\u5185\u5728\u7ef4\u5ea6\u7684\u53d8\u5316\u3002", "result": "LLM\u7fa4\u4f53\u5728\u8bcd\u6c47\u548c\u8bed\u4e49\u4e0a\u6536\u655b\uff0c\u5f62\u6210\u4e0d\u5bf9\u79f0\u7684\u5f71\u54cd\u6a21\u5f0f\uff0c\u8868\u73b0\u51fa\u7c7b\u4f3c\u8c08\u5224\u7684\u884c\u4e3a\u3002\u8f93\u51fa\u5d4c\u5165\u7684\u5185\u5728\u7ef4\u5ea6\u968f\u8f6e\u6b21\u4e0b\u964d\uff0c\u8868\u660e\u8bed\u4e49\u538b\u7f29\u73b0\u8c61\u3002\u9ed1\u76d2\u4ea4\u4e92\u5206\u6790\u80fd\u591f\u63ed\u793a\u6d8c\u73b0\u7684\u534f\u8c03\u7b56\u7565\u3002", "conclusion": "\u9ed1\u76d2\u4ea4\u4e92\u5206\u6790\u53ef\u4ee5\u4f5c\u4e3a\u57fa\u4e8e\u5185\u90e8\u63a2\u9488\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u8865\u5145\uff0c\u80fd\u591f\u63ed\u793aLLM\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u6d8c\u73b0\u7684\u534f\u8c03\u7b56\u7565\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u89d2\u8272\u63d0\u793a\u4e5f\u80fd\u8868\u73b0\u51fa\u590d\u6742\u7684\u534f\u8c03\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2512.00214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00214", "abs": "https://arxiv.org/abs/2512.00214", "authors": ["Matej Klemen", "Tja\u0161a Ar\u010don", "Luka Ter\u010don", "Marko Robnik-\u0160ikonja", "Kaja Dobrovoljc"], "title": "Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis", "comment": "Pre-print, submission under review", "summary": "Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5206\u6790\u6807\u6ce8\u8bed\u6599\u5e93\u4e2d\u7684\u8bed\u6cd5\u7279\u5f81\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u8bcd\u5e8f\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6807\u6ce8\u8bed\u6599\u5e93\u7684\u5b9e\u8bc1\u8bed\u6cd5\u7814\u7a76\u9700\u8981\u5927\u91cf\u65b9\u6cd5\u5b66\u548c\u6280\u672f\u52aa\u529b\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5229\u7528\u667a\u80fd\u5927\u8bed\u8a00\u6a21\u578b\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9a71\u52a8\u7684\u8bed\u6cd5\u5206\u6790\u81ea\u52a8\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u6574\u5408\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89e3\u91ca\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u636e\u9a71\u52a8\u63a8\u7406\uff0c\u5e94\u7528\u4e8eUniversal Dependencies\u8bed\u6599\u5e93\uff0c\u6d4b\u8bd513\u4e2a\u8bcd\u5e8f\u7279\u5f81\u548c170\u591a\u79cd\u8bed\u8a00\u3002", "result": "\u7cfb\u7edf\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\uff1a\u4e3b\u5bfc\u987a\u5e8f\u51c6\u786e\u6027\u3001\u987a\u5e8f\u8986\u76d6\u5b8c\u6574\u6027\u548c\u5206\u5e03\u4fdd\u771f\u5ea6\uff0c\u7ed3\u679c\u8868\u660eLLM\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u8bed\u8a00\u6570\u636e\u7ed3\u5408\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u8bed\u6cd5\u67e5\u8be2\u81ea\u52a8\u5316\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4ee3\u7406\u5728\u8bed\u8a00\u5b66\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.00127", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00127", "abs": "https://arxiv.org/abs/2512.00127", "authors": ["Shailja Thakur", "Vaibhav Saxena", "Rohan Kulkarni", "Shivdeep Singh", "Parameswaran Selvam", "Hima Patel", "Hiroshi Kanayama"], "title": "Generating Verifiable CoT from Execution-Traces", "comment": null, "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u7684\u4ee3\u7801\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6355\u83b7\u4ee3\u7801\u52a8\u6001\u884c\u4e3a\u5e76\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6b65\u9aa4\uff0c\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u63a8\u7406\u8bad\u7ec3\u4e2d\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u63a8\u7406\u6b65\u9aa4\u901a\u5e38\u662f\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u770b\u4f3c\u5408\u7406\u4f46\u672a\u7ecf\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u800c\u975e\u4ee3\u7801\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u7684\u771f\u5b9e\u63cf\u8ff0\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u8868\u9762\u5408\u7406\u4f46\u903b\u8f91\u9519\u8bef\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u4ee3\u7801\u63d2\u6869\u6355\u83b7\u7a0b\u5e8f\u52a8\u6001\u6267\u884c\u8f68\u8ff9\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6267\u884c\u8f68\u8ff9\u53d9\u8ff0\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6b65\u9aa4\uff0c\u6784\u5efa\u53cc\u5411\u8f68\u8ff9\u57fa\u7840\u6570\u636e\uff0c\u786e\u4fdd\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u90fd\u53cd\u6620\u7a0b\u5e8f\u771f\u5b9e\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\uff08CruxEval\u548cLiveCodeBench-Exec\u7684\u524d\u5411\u63a8\u7406\u3001CruxEval-Input\u7684\u540e\u5411\u63a8\u7406\uff09\u4ee5\u53caHumanEval\u7684\u4ee3\u7801\u751f\u6210\u548c\u89e3\u91ca\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1a\u8f93\u51fa\u9884\u6d4b\u63d0\u5347\u9ad8\u8fbe30\u5206\uff0c\u8f93\u5165\u9884\u6d4b\u63d0\u534728\u5206\uff0c\u540c\u65f6\u6539\u5584\u4e86\u4ee3\u7801\u89e3\u91ca\u548c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u6267\u884c\u8f68\u8ff9\u7684\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u4ece\u6839\u672c\u4e0a\u589e\u5f3a\u6a21\u578b\u80fd\u529b\uff0c\u901a\u8fc7\u6d88\u9664\u903b\u8f91\u5e7b\u89c9\u6e90\uff0c\u786e\u4fdd\u63a8\u7406\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u4e3a\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2512.00218", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00218", "abs": "https://arxiv.org/abs/2512.00218", "authors": ["Matt MacDermott", "Qiyao Wei", "Rada Djoneva", "Francis Rhys Ward"], "title": "Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?", "comment": null, "summary": "AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \\emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \\emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u540c\u8bad\u7ec3\u6fc0\u52b1\u5bf9AI\u63a8\u7406\u6a21\u578b\u53ef\u76d1\u63a7\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u6297\u6027\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u63a7\u6027\u80fd\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u4e0d\u80fd\u53ef\u9760\u63d0\u5347\u76d1\u63a7\u6548\u679c\u3002", "motivation": "AI\u7cfb\u7edf\u8f93\u51fa\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\u4e3a\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u662f\u5426\u5fe0\u5b9e\u53cd\u6620\u5e95\u5c42\u601d\u7ef4\u8fc7\u7a0b\u53ef\u80fd\u53d7\u8bad\u7ec3\u65b9\u5f0f\u5f71\u54cd\u3002\u7814\u7a76\u4e0d\u540c\u8bad\u7ec3\u6fc0\u52b1\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u6a21\u578b\u7684\u53ef\u76d1\u63a7\u6027\u3002", "method": "\u5f15\u5165\u65b0\u7684\u53ef\u76d1\u63a7\u6027\u6d4b\u91cf\u65b9\u6cd5\uff1a\u6839\u636e\u76d1\u63a7\u5668\u80fd\u5426\u4f7f\u7528\u6a21\u578b\u7684\u63a8\u7406\u9884\u6d4b\u5173\u952e\u6f5c\u5728\u53d8\u91cf\u3002\u7814\u7a76\u4e0d\u540c\u8bad\u7ec3\u6fc0\u52b1\uff08\u957f\u5ea6\u60e9\u7f5a\u3001KL\u6b63\u5219\u5316\u3001\u5bf9\u6297\u6027\u4f18\u5316\u3001\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\uff09\u5bf9\u76d1\u63a7\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u63a7\u5236\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5e38\u89c1\u6fc0\u52b1\uff08\u957f\u5ea6\u60e9\u7f5a\u548cKL\u6b63\u5219\u5316\uff09\u6ca1\u6709\u4e00\u81f4\u5f71\u54cd\uff1b\u5bf9\u6297\u6027\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u63a7\u6027\u80fd\uff1b\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u4e0d\u80fd\u53ef\u9760\u63d0\u5347\u76d1\u63a7\u6548\u679c\u3002", "conclusion": "\u8bad\u7ec3\u6fc0\u52b1\u786e\u5b9e\u5f71\u54cd\u63a8\u7406\u6a21\u578b\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u5bf9\u6297\u6027\u4f18\u5316\u4f1a\u635f\u5bb3\u76d1\u63a7\u80fd\u529b\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u975e\u6709\u6548\u7b56\u7565\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u53ef\u76d1\u63a7\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00215", "abs": "https://arxiv.org/abs/2512.00215", "authors": ["Mohammad Abdollahi", "Khandaker Rifah Tasnia", "Soumit Kanti Saha", "Jinqiu Yang", "Song Wang", "Hadi Hemmati"], "title": "Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation", "comment": null, "summary": "Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u7406\u89e3\u7a0b\u5e8f\u7684\u8fd0\u884c\u65f6\u63a8\u7406\u884c\u4e3a\u5bf9\u4e8e\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\u3001\u8c03\u8bd5\u548c\u81ea\u52a8\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u7a0b\u5e8f\u8f93\u51fa\uff0c\u4f46\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u51c6\u786e\u6027\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5bf9\u5176\u63a8\u7406\u8f68\u8ff9\u7684\u7ed3\u6784\u548c\u5931\u8d25\u6a21\u5f0f\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u4eceHumanEval Plus\u548cLiveCodeBench\u4e2d\u6536\u96c6427\u4e2a\u4ee3\u7801\u7247\u6bb5\uff0c\u4e3a\u6bcf\u4e2a\u7247\u6bb5\u6d4b\u8bd5\u4e09\u79cd\u8f93\u5165\u7c7b\u578b\uff08\u5e38\u89c4\u3001\u8fb9\u754c\u3001\u65e0\u6548\uff09\uff0c\u6bcf\u4e2a\u7247\u6bb5\u9009\u62e912\u4e2a\u8f93\u5165\u503c\u5e76\u914d\u5bf9\u771f\u5b9e\u6267\u884c\u7ed3\u679c\u3002\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5f00\u53d1\u5305\u542b\u4e5d\u7c7b\u63a8\u7406\u9519\u8bef\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u7d22\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\u4e0a\u7684\u51c6\u786e\u7387\u572885%\u523098%\u4e4b\u95f4\u3002\u7814\u7a76\u8bc6\u522b\u51fa\u4e5d\u7c7b\u63a8\u7406\u9519\u8bef\uff0c\u5e76\u4ee5\u8ba1\u7b97\u9519\u8bef\u7c7b\u522b\u4e3a\u4f8b\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5\u6210\u529f\u7ea0\u6b63\u4e8658%\u7684\u6b64\u7c7b\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5176\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5de5\u5177\u652f\u6301\u5728\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u66f4\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2512.00231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00231", "abs": "https://arxiv.org/abs/2512.00231", "authors": ["Monique Louise Monteiro", "George G. Cabral", "Adriano L. I. OLiveira"], "title": "CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization", "comment": null, "summary": "This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.", "AI": {"tldr": "CodeFlowLM\u662f\u4e00\u4e2a\u7528\u4e8e\u5373\u65f6\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9a8c\u8bc1\u5ef6\u8fdf\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u540c\u65f6\u63a2\u7d22\u4e86LLM\u5728\u7f3a\u9677\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0GPT-5\u8868\u73b0\u7a33\u5b9a\u4f46\u4ecd\u6709\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5728\u7ebf\u5b66\u4e60\u5668\u5728\u5373\u65f6\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u4e2d\u5b58\u5728\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9a8c\u8bc1\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9002\u5e94\u8f6f\u4ef6\u73af\u5883\u53d8\u5316\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f3a\u9677\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faCodeFlowLM\u6846\u67b6\uff0c\u91c7\u7528\u6301\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u3002\u8bc4\u4f30\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668PLM\uff08\u5982CodeT5+\u3001UniXCoder\uff09\u5728\u9879\u76ee\u5185\u548c\u8de8\u9879\u76eeJIT-SDP\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u540c\u65f6\u5c06LLM\uff08GPT-5\u3001Claude\u3001Gemini\uff09\u4e0e\u6ce8\u610f\u529b\u6a21\u578b\u5728\u7f3a\u9677\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "CodeFlowLM\u5728G-Mean\u6307\u6807\u4e0a\u83b7\u5f97\u9ad8\u8fbe68%\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u5176\u5728\u6f14\u5316\u8f6f\u4ef6\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728\u7f3a\u9677\u5b9a\u4f4d\u4e2d\uff0cGPT-5\u5728Recall@20%\u548cEffort@20%\u6307\u6807\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u6ce8\u610f\u529b\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6392\u540d\u6307\u6807\uff08Top-k, IFA\uff09\u4e0a\u4ecd\u6709\u4f18\u52bf\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5047\u9633\u6027\u7684\u4e3b\u8981\u6765\u6e90\u3002", "conclusion": "CodeFlowLM\u663e\u8457\u63a8\u8fdb\u4e86\u589e\u91cfJIT-SDP\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u5728\u6f14\u5316\u8f6f\u4ef6\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002LLM\u5728\u7f3a\u9677\u5b9a\u4f4d\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u4f46\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "topic": "swe benchmark"}}
{"id": "2512.00319", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00319", "abs": "https://arxiv.org/abs/2512.00319", "authors": ["Ruike Hu", "Shulei Wu"], "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs", "comment": "23 pages, 14 figures. Model is available at https://huggingface.co/Freakz3z/Qwen-JSON", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the \"Structure Gap\" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to \"hallucinated\" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u89e3\u51b3LLM\u751f\u6210\u7ed3\u6784\u5316\u6570\u636e\u65f6\u7684\"\u7ed3\u6784\u9e3f\u6c9f\"\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347JSON\u7b49\u683c\u5f0f\u7684\u751f\u6210\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3LLM\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u9762\u4e34\u7684\"\u7ed3\u6784\u9e3f\u6c9f\"\u95ee\u9898\u2014\u2014\u6982\u7387\u6027token\u751f\u6210\u4e0e\u786e\u5b9a\u6027\u7ed3\u6784\u5316\u6570\u636e\u683c\u5f0f\uff08\u5982JSON\u3001XML\uff09\u4e4b\u95f4\u7684\u5185\u5728\u77db\u76fe\u3002\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u96be\u4ee5\u5f3a\u5236\u6267\u884c\u4e25\u683c\u8bed\u6cd5\u7ea6\u675f\uff0c\u800c\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u53c8\u4f1a\u5e26\u6765\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u5c06\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u7ea6\u675f\uff1a\u7ed3\u6784\u5b8c\u6574\u6027\u3001\u683c\u5f0f\u6b63\u786e\u6027\u3001\u5185\u5bb9\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u3002\u91c7\u7528\u68af\u5ea6\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5185\u5316\u8fd9\u4e9b\u7ea6\u675f\u800c\u65e0\u9700\u5355\u72ec\u7684\u6279\u8bc4\u7f51\u7edc\u3002", "result": "\u5728\u590d\u6742\u914d\u65b9\u751f\u6210\u548c\u7ed3\u6784\u5316\u6570\u5b66\u63a8\u7406\uff08GSM8K-JSON\uff09\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u8fbe\u523089.7%\u7684\u7ed3\u6784\u51c6\u786e\u7387\u548c92.1%\u7684JSON\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff08\u5982GPT-3.5\uff09\u548c\u5728LLaMA-3-8B\u7b49\u66f4\u5927\u6a21\u578b\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\u3002\u76f8\u6bd4PPO\u51cf\u5c1140%\u7684\u5cf0\u503cVRAM\u4f7f\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u751f\u6210\u7ed3\u6784\u5316\u6570\u636e\u65f6\u7684\u7ed3\u6784\u9e3f\u6c9f\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u548cGRPO\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7ed3\u6784\u7ea6\u675f\u5185\u5316\uff0c\u4e3aLLM\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.00325", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00325", "abs": "https://arxiv.org/abs/2512.00325", "authors": ["Shaira Sadia Karim", "Abrar Mahmud Rahim", "Lamia Alam", "Ishmam Tashdeed", "Lutfun Nahar Lota", "Md. Abu Raihan M. Kamal", "Md. Azam Hossain"], "title": "Progressive Code Integration for Abstractive Bug Report Summarization", "comment": null, "summary": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4ee3\u7801\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8eLLM\u7684\u62bd\u8c61\u5f0fbug\u62a5\u544a\u6458\u8981\u751f\u6210\uff0c\u901a\u8fc7\u9010\u6b65\u6574\u5408\u957f\u4ee3\u7801\u7247\u6bb5\u548c\u6587\u672c\u5185\u5bb9\uff0c\u514b\u670dLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6458\u8981\u3002", "motivation": "\u73b0\u6709bug\u62a5\u544a\u901a\u5e38\u975e\u7ed3\u6784\u5316\u4e14\u5197\u957f\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u9ad8\u6548\u7406\u89e3\u8f6f\u4ef6\u95ee\u9898\u3002\u73b0\u6709\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8868\u5c42\u6587\u672c\u7ebf\u7d22\uff0c\u5bfc\u81f4\u6458\u8981\u4e0d\u5b8c\u6574\u6216\u5197\u4f59\uff0c\u4e14\u7ecf\u5e38\u5ffd\u7565\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff0c\u800c\u4ee3\u7801\u7247\u6bb5\u5bf9\u4e8e\u51c6\u786e\u7f3a\u9677\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u4ee3\u7801\u96c6\u6210\u6846\u67b6\uff0c\u9010\u6b65\u5c06\u957f\u4ee3\u7801\u7247\u6bb5\u4e0e\u6587\u672c\u5185\u5bb9\u7ed3\u5408\uff0c\u514b\u670d\u6807\u51c6LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684bug\u62a5\u544a\u6458\u8981\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u516b\u4e2aLLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u7ba1\u9053\u6bd4\u62bd\u53d6\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u53477.5%-58.2%\uff0c\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u62bd\u8c61\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u6587\u672c\u548c\u4ee3\u7801\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86bug\u7406\u89e3\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u6e10\u8fdb\u5f0f\u4ee3\u7801\u96c6\u6210\u6846\u67b6\u5728bug\u62a5\u544a\u6458\u8981\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "swe application"}}
{"id": "2512.00331", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00331", "abs": "https://arxiv.org/abs/2512.00331", "authors": ["Yefeng Wu", "Yuchen Song", "Yecheng Zhao", "Ling Wu", "Shan Wan"], "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.", "AI": {"tldr": "CogEvo-Edu\uff1a\u4e00\u4e2a\u5206\u5c42\u6559\u80b2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u68c0\u7d22\u3001\u8bb0\u5fc6\u548c\u63a7\u5236\u89c6\u4e3a\u8026\u5408\u7684\u8ba4\u77e5\u6f14\u5316\u8fc7\u7a0b\uff0c\u7528\u4e8eSTEM\u6559\u80b2\u4e2d\u7684LLM\u5bf9\u8bdd\u5bfc\u5e08\uff0c\u5728\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u9886\u57df\u663e\u8457\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5355\u4e00LLM\u548c\u9759\u6001RAG\u7ba1\u9053\u7684\u6559\u80b2\u7cfb\u7edf\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u6570\u5b57\u4fe1\u53f7\u5904\u7406\uff09\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u7ef4\u62a4\u957f\u671f\u5b66\u751f\u6a21\u578b\u3001\u7ba1\u7406\u5f02\u6784\u77e5\u8bc6\u5e93\u548c\u9002\u5e94\u6559\u5b66\u7b56\u7565\u3002\u9700\u8981\u5c06\u68c0\u7d22\u3001\u8bb0\u5fc6\u548c\u63a7\u5236\u89c6\u4e3a\u8026\u5408\u7684\u8ba4\u77e5\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCogEvo-Edu\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a1) \u8ba4\u77e5\u611f\u77e5\u5c42(CPL)\u7ef4\u62a4\u53cc\u91cd\u8bb0\u5fc6\u5e76\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6574\u5408\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u3001\u81ea\u6821\u6b63\u7684\u5b66\u751f\u6863\u6848\uff1b2) \u77e5\u8bc6\u6f14\u5316\u5c42(KEL)\u4e3a\u6bcf\u4e2a\u77e5\u8bc6\u5757\u5206\u914d\u65f6\u7a7a\u4ef7\u503c\uff0c\u9a71\u52a8\u6fc0\u6d3b\u3001\u8bed\u4e49\u538b\u7f29\u548c\u9057\u5fd8\uff1b3) \u5143\u63a7\u5236\u5c42(MCL)\u5c06\u6559\u5b66\u5236\u5b9a\u4e3a\u5206\u5c42\u987a\u5e8f\u51b3\u7b56\uff0c\u901a\u8fc7\u53cc\u91cd\u5185\u5916\u5faa\u73af\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\u5e76\u8054\u5408\u8c03\u6574CPL/KEL\u8d85\u53c2\u6570\u3002", "result": "\u5728\u6784\u5efa\u7684DSP-EduBench\u5782\u76f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4e09\u6a21\u578bLLM-as-a-Judge\u96c6\u6210\u8bc4\u4f30\uff0cCogEvo-Edu\u5c06\u603b\u4f53\u5f97\u5206\u4ece5.32\u63d0\u5347\u52309.23\uff0c\u5e76\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u9759\u6001RAG\u3001\u7b80\u5355\u8bb0\u5fc6\u548c\u5355\u667a\u80fd\u4f53\u53d8\u4f53\u3002", "conclusion": "CogEvo-Edu\u901a\u8fc7\u8054\u5408\u6f14\u5316\u5b66\u751f\u6863\u6848\u3001\u77e5\u8bc6\u5e93\u548c\u6559\u5b66\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742STEM\u9886\u57df\u7684\u6559\u5b66\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5c06\u68c0\u7d22\u3001\u8bb0\u5fc6\u548c\u63a7\u5236\u89c6\u4e3a\u8026\u5408\u8ba4\u77e5\u6f14\u5316\u8fc7\u7a0b\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2512.00380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00380", "abs": "https://arxiv.org/abs/2512.00380", "authors": ["Mingwei Liu", "Zheng Pei", "Yanlin Wang", "Zihao Wang", "Zikang Li", "Enci Lin", "Xin Peng", "Zibin Zheng"], "title": "Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS", "comment": null, "summary": "In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u6846\u67b6\uff08\u5982HarmonyOS\uff09\uff0cLLM\u56e0\u7f3a\u4e4f\u76f8\u5173API\u77e5\u8bc6\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u6027\u80fd\u5dee\u3002\u4f5c\u8005\u63d0\u51faAPIKG4SYN\u6846\u67b6\uff0c\u5229\u7528API\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u9762\u5411API\u7684\u95ee\u9898-\u4ee3\u7801\u5bf9\uff0c\u65e0\u9700\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9a71\u52a8\u7684MCTS\u6574\u5408\u5355API\u548c\u591aAPI\u77e5\u8bc6\uff0c\u4e3aLLM\u5fae\u8c03\u521b\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u8f6f\u4ef6\u6846\u67b6\uff08\u5982HarmonyOS\uff09\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u73af\u5883\u7684\u5145\u5206\u63a5\u89e6\uff0c\u4ee3\u7801\u751f\u6210\u6027\u80fd\u8f83\u5dee\u3002\u867d\u7136LLM\u901a\u5e38\u80fd\u4fdd\u6301\u8de8\u7f16\u7a0b\u8bed\u8a00\u7684\u6b63\u786e\u903b\u8f91\u7ed3\u6784\uff0c\u4f46\u5728\u5904\u7406\u6846\u67b6\u7279\u5b9aAPI\u6216\u8bed\u6cd5\u65f6\u7ecf\u5e38\u51fa\u9519\u3002\u8fd9\u8868\u660e\u9884\u8bad\u7ec3\u867d\u7136\u8d4b\u4e88LLM\u901a\u7528\u7b97\u6cd5\u80fd\u529b\uff0c\u4f46\u5bf9\u4f4e\u8d44\u6e90\u6846\u67b6\u7684\u72ec\u7279\u8bed\u6cd5\u548cAPI\u4f7f\u7528\u4ecd\u4e0d\u719f\u6089\uff0c\u5373\u4f7f\u50cfGPT-4o\u8fd9\u6837\u7684\u5148\u8fdb\u5546\u4e1a\u6a21\u578b\u4e5f\u65e0\u6cd5\u53ef\u9760\u751f\u6210\u6b63\u786e\u4ee3\u7801\u3002", "method": "\u63d0\u51faAPIKG4SYN\u6846\u67b6\uff0c\u5229\u7528API\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u9762\u5411API\u7684\u95ee\u9898-\u4ee3\u7801\u5bf9\uff0c\u4e13\u95e8\u9488\u5bf9\u4f4e\u8d44\u6e90\u6846\u67b6\u4e14\u65e0\u9700\u53ef\u6267\u884c\u4ee3\u7801\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u5355API\u548c\u591aAPI\u77e5\u8bc6\uff0c\u5176\u4e2d\u591aAPI\u77e5\u8bc6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9a71\u52a8\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a8\u5bfc\uff0c\u4ece\u800c\u4e3aLLM\u5fae\u8c03\u521b\u5efa\u591a\u6837\u5316\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528HarmonyOS\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u9996\u4e2aHarmonyOS\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528APIKG4SYN\u5fae\u8c03Qwen\u6a21\u578b\u5c06pass@1\u51c6\u786e\u7387\u63d0\u5347\u81f325.00%\uff0c\u800c\u57fa\u7ebfGPT\u6a21\u578b\u4ec5\u4e3a17.59%\u3002", "conclusion": "\u9762\u5411API\u7684\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0cAPIKG4SYN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u4f4e\u8d44\u6e90\u6846\u67b6\u4e2d\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2512.00344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00344", "abs": "https://arxiv.org/abs/2512.00344", "authors": ["Naifan Zhang", "Ruihan Sun", "Ruixi Su", "Shiqi Ma", "Shiya Zhang", "Xianna Weng", "Xiaofan Zhang", "Yuhan Zhan", "Yuyang Xu", "Zhaohan Chen", "Zhengyuan Pan", "Ziyi Song"], "title": "Echo-N1: Affective RL Frontier", "comment": null, "summary": "The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u5bf9\u8bdd\u6846\u67b6Echo-N1\uff0c\u901a\u8fc7\u5b9e\u65f6\u63a8\u65ad\u7528\u6237\u6027\u683c\u5e76\u4f18\u5316\u5bf9\u8bdd\u884c\u4e3a\uff0c\u5728\u4e3b\u89c2\u60c5\u611f\u5bf9\u8bdd\u9886\u57df\u53d6\u5f97\u663e\u8457\u7a81\u7834", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u6570\u5b66\u3001\u4ee3\u7801\u7b49\u673a\u5668\u64c5\u957f\u7684\u786e\u5b9a\u6027\u4efb\u52a1\uff0c\u800c\u5ffd\u89c6\u4e86\u771f\u6b63\u4f53\u73b0\u4eba\u7c7b\u667a\u80fd\u7684\u4e3b\u89c2\u3001\u60c5\u611f\u5316\u3001\u4e2a\u6027\u5316\u7684\u5bf9\u8bdd\u9886\u57df\u3002\u8be5\u9886\u57df\u56e0\u96be\u4ee5\u5f62\u5f0f\u5316\u800c\u88ab\u8ba4\u4e3a\u4e0d\u9002\u5408\u4f20\u7edfRL\u65b9\u6cd5", "method": "\u63d0\u51fa\u9996\u4e2a\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u65f6\u63a8\u65ad\u7528\u6237\u6027\u683c\u5e76\u4f18\u5316\u6a21\u578b\u884c\u4e3a\u4ee5\u9002\u5e94\u4e2a\u6027\u5316\u5bf9\u8bdd\u504f\u597d\u3002\u5f15\u5165\u52a8\u6001\u60c5\u5546\u8bc4\u4f30\u5957\u4ef6\u6765\u91cf\u5316\u6539\u8fdb\u6548\u679c", "result": "Echo-N1\u6a21\u578b\u8868\u73b0\u8fdc\u8d85\u5176\u57fa\u7840\u7248\u672c\uff0c\u5e76\u4f18\u4e8e\u4e13\u6709\u7684Doubao 1.5 Character\u6a21\u578b\u3002\u5728\u975e\u53ef\u9a8c\u8bc1\u73af\u5883\u4e2dRL\u65b9\u6cd5\u4ecd\u80fd\u4ea7\u751f\u4e00\u81f4\u3001\u7a33\u5065\u4e14\u663e\u8457\u7684\u6539\u8fdb", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aRL\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf\uff1a\u4f18\u5316\u6a21\u578b\u4ee5\u9002\u5e94\u5bf9\u8bdd\u4e2d\u6df1\u523b\u4e3b\u89c2\u3001\u6df1\u523b\u4eba\u6027\u5316\u7684\u7ef4\u5ea6\uff0c\u8bc1\u660e\u4e3b\u89c2\u60c5\u611f\u5bf9\u8bdd\u662f\u53ef\u89e3\u51b3\u7684RL\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2512.00556", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00556", "abs": "https://arxiv.org/abs/2512.00556", "authors": ["Sina Salimian", "Gias Uddin", "Sumon Biswas", "Henry Leung"], "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8715\u53d8\u6d4b\u8bd5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u79cd\u8715\u53d8\u5173\u7cfb\u5c06\u76f4\u63a5\u504f\u89c1\u8bf1\u5bfc\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u4e49\u7b49\u4ef7\u4f46\u5bf9\u6297\u6027\u66f4\u5f3a\u7684\u53d8\u4f53\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u548c\u9488\u5bf9\u6027\u7f13\u89e3LLM\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u90e8\u7f72\u5f15\u53d1\u4e86\u5bf9\u9690\u85cf\u793e\u4f1a\u504f\u89c1\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u5728\u5904\u7406\u95f4\u63a5\u6216\u4e0a\u4e0b\u6587\u590d\u6742\u7684\u504f\u89c1\u8bf1\u5bfc\u63d0\u793a\u65f6\u7ecf\u5e38\u5931\u6548\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8715\u53d8\u6d4b\u8bd5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5f15\u5165\u516d\u79cd\u65b0\u9896\u7684\u8715\u53d8\u5173\u7cfb\uff0c\u5c06\u76f4\u63a5\u504f\u89c1\u8bf1\u5bfc\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u4e49\u7b49\u4ef7\u4f46\u5bf9\u6297\u6027\u66f4\u5f3a\u7684\u53d8\u4f53\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u504f\u89c1\uff0c\u5e76\u5c06\u76f8\u540c\u8715\u53d8\u5173\u7cfb\u751f\u6210\u7684\u6837\u672c\u7528\u4e8e\u5fae\u8c03\u4ee5\u7f13\u89e3\u504f\u89c1\u3002", "result": "\u5728\u516d\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff08\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff09\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528BiasAsker\u57fa\u51c6\u7684385\u4e2a\u95ee\u9898\uff08\u8986\u76d6\u4e03\u4e2a\u53d7\u4fdd\u62a4\u7fa4\u4f53\uff09\uff0c\u8715\u53d8\u5173\u7cfb\u6bd4\u73b0\u6709\u5de5\u5177\u591a\u63ed\u793a\u9ad8\u8fbe14%\u7684\u9690\u85cf\u504f\u89c1\uff1b\u4f7f\u7528\u539f\u59cb\u548c\u8715\u53d8\u53d8\u5f02\u6837\u672c\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u5b89\u5168\u54cd\u5e94\u7387\u4ece54.7%\u663e\u8457\u63d0\u9ad8\u5230\u8d85\u8fc788.9%\u3002", "conclusion": "\u8715\u53d8\u5173\u7cfb\u662f\u63d0\u9ad8\u5bf9\u8bddAI\u516c\u5e73\u6027\u7684\u5b9e\u7528\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u4e2d\u7684\u9690\u85cf\u793e\u4f1a\u504f\u89c1\uff0c\u5c06\u6d4b\u8bd5\u8fc7\u7a0b\u4e0e\u7f13\u89e3\u63aa\u65bd\u76f4\u63a5\u8054\u7cfb\u8d77\u6765\u3002", "topic": "agent analysis"}}
{"id": "2512.00249", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00249", "abs": "https://arxiv.org/abs/2512.00249", "authors": ["Scotty Black", "Christian Darken"], "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations", "comment": "arXiv admin note: substantial text overlap with arXiv:2408.13333", "summary": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u6df7\u5408AI\u65b9\u6cd5\uff0c\u7ed3\u5408\u811a\u672c\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u7528\u4e8e\u590d\u6742\u4f5c\u6218\u6a21\u62df\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u4f5c\u6218\u6a21\u62df\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u811a\u672c\u65b9\u6cd5\uff0c\u867d\u7136\u53ef\u9884\u6d4b\u4f46\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1b\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u9002\u5e94\u6027\u5f3a\u4f46\u5b58\u5728\u9ed1\u76d2\u51b3\u7b56\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6765\u89e3\u51b3\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6df7\u5408AI\u67b6\u6784\uff1a\u5728\u4f4e\u5c42\u4f7f\u7528\u811a\u672c\u4ee3\u7406\u5904\u7406\u5e38\u89c4\u6218\u672f\u51b3\u7b56\uff0c\u5728\u9ad8\u5c42\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8fdb\u884c\u6218\u7565\u51b3\u7b56\uff0c\u901a\u8fc7\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u6a21\u62df\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5206\u5c42\u6df7\u5408AI\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u811a\u672c\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u5404\u81ea\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4f5c\u6218\u6a21\u62df\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.00651", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00651", "abs": "https://arxiv.org/abs/2512.00651", "authors": ["Mohammed Latif Siddiq", "Arvin Islam-Gomes", "Natalie Sekerak", "Joanna C. S. Santos"], "title": "Large Language Models for Software Engineering: A Reproducibility Crisis", "comment": "Submitted to Empirical Software Engineering (EMSE) journal; 112 pages (81 pages of references)", "summary": "Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76LLM\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u5b9e\u8df5\uff0c\u5206\u6790640\u7bc7\u8bba\u6587\u53d1\u73b0\u53ef\u590d\u73b0\u6027\u6c14\u5473\u666e\u904d\u5b58\u5728\uff0c\u8bc4\u4f30\u5fbd\u7ae0\u4e0d\u80fd\u4fdd\u8bc1\u6267\u884c\u4fdd\u771f\u5ea6\uff0c\u63d0\u51fa\u53ef\u590d\u73b0\u6027\u6210\u719f\u5ea6\u6a21\u578b", "motivation": "\u53ef\u590d\u73b0\u6027\u662f\u79d1\u5b66\u8fdb\u6b65\u7684\u57fa\u77f3\uff0c\u4f46LLM\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u73b0\u72b6\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5f53\u524d\u5b9e\u8df5\u548c\u95ee\u9898", "method": "\u7cfb\u7edf\u6316\u6398\u5206\u67902017-2025\u5e74640\u7bc7\u8bba\u6587\uff0c\u4ece\u51fa\u7248\u7269\u3001\u4ed3\u5e93\u548c\u6587\u6863\u63d0\u53d6\u7ed3\u6784\u5316\u5143\u6570\u636e\uff0c\u4f7f\u75287\u7c7b\u6c14\u5473\u5206\u7c7b\u6cd5\u624b\u52a8\u6807\u6ce8\uff0c\u901a\u8fc74\u4e2a\u7814\u7a76\u95ee\u9898\u6307\u5bfc\u5206\u6790", "result": "\u53d1\u73b0\u5de5\u4ef6\u53ef\u7528\u6027\u3001\u73af\u5883\u89c4\u8303\u3001\u7248\u672c\u63a7\u5236\u548c\u6587\u6863\u6e05\u6670\u5ea6\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff0c\u5fbd\u7ae0\u4ec5\u8868\u793a\u5de5\u4ef6\u5b58\u5728\u4f46\u4e0d\u80fd\u4fdd\u8bc1\u6267\u884c\u4fdd\u771f\u5ea6\u6216\u957f\u671f\u53ef\u590d\u73b0\u6027", "conclusion": "\u9700\u8981\u8d85\u8d8a\u4e8c\u5143\u5de5\u4ef6\u8ba4\u8bc1\uff0c\u91c7\u7528\u591a\u7ef4\u6e10\u8fdb\u5f0f\u53ef\u590d\u73b0\u6027\u8bc4\u4f30\uff0c\u63d0\u51fa\u53ef\u590d\u73b0\u6027\u6210\u719f\u5ea6\u6a21\u578b\u548c\u53ef\u64cd\u4f5c\u5efa\u8bae\u6765\u7f13\u89e3\u53ef\u590d\u73b0\u6027\u6c14\u5473", "topic": "agent analysis"}}
{"id": "2512.00417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00417", "abs": "https://arxiv.org/abs/2512.00417", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Yanyan Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "comment": null, "summary": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "AI": {"tldr": "CryptoBench\u662f\u9996\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u5305\u542b\u5b9e\u65f6\u52a8\u6001\u95ee\u9898\u548c\u56db\u8c61\u9650\u4efb\u52a1\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u68c0\u7d22-\u9884\u6d4b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u4ee3\u7406\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u5728\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u8be5\u9886\u57df\u5177\u6709\u6781\u7aef\u65f6\u6548\u6027\u3001\u5bf9\u6297\u6027\u4fe1\u606f\u73af\u5883\u548c\u9700\u8981\u7efc\u5408\u591a\u79cd\u4e13\u4e1a\u6570\u636e\u6e90\u7684\u7279\u6b8a\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u6708\u5305\u542b50\u4e2a\u7531\u52a0\u5bc6\u9886\u57df\u4e13\u5bb6\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u91c7\u7528\u56db\u8c61\u9650\u5206\u7c7b\u7cfb\u7edf\uff08\u7b80\u5355\u68c0\u7d22\u3001\u590d\u6742\u68c0\u7d22\u3001\u7b80\u5355\u9884\u6d4b\u3001\u590d\u6742\u9884\u6d4b\uff09\uff0c\u8bc4\u4f30\u4e8610\u4e2aLLM\u6a21\u578b\u5728\u76f4\u63a5\u548c\u4ee3\u7406\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e86\u68c0\u7d22-\u9884\u6d4b\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u8bb8\u591a\u9886\u5148\u6a21\u578b\u5728\u6570\u636e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u9884\u6d4b\u5206\u6790\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u5f31\u70b9\uff0c\u8868\u660e\u4ee3\u7406\u867d\u7136\u770b\u4f3c\u4e8b\u5b9e\u57fa\u7840\u624e\u5b9e\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u5206\u6790\u80fd\u529b\u3002", "conclusion": "CryptoBench\u4e3aLLM\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u548c\u4ef7\u503c\u7684\u573a\u666f\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u52a0\u5bc6\u8d27\u5e01\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u9884\u6d4b\u5206\u6790\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2512.00601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00601", "abs": "https://arxiv.org/abs/2512.00601", "authors": ["Boyang Gu", "Hongjian Zhou", "Bradley Max Segal", "Jinge Wu", "Zeyu Cao", "Hantao Zhong", "Lei Clifton", "Fenglin Liu", "David A. Clifton"], "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.", "AI": {"tldr": "CRPO\u662f\u4e00\u79cd\u9488\u5bf9\u4e34\u5e8a\u63a8\u7406\u7684\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u9f50LLM\u540e\u8bad\u7ec3\u4e0e\u4e34\u5e8a\u539f\u5219\uff0c\u76f8\u6bd4\u4f20\u7edfGRPO\u5728\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4e3b\u8981\u5956\u52b1\u6b63\u786e\u6027\uff0c\u4f46\u4e34\u5e8a\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u9700\u8981\u591a\u7ef4\u76ee\u6807\uff08\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u3001\u5168\u9762\u6027\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5bf9\u9f50\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u3002", "method": "\u63d0\u51fa\u4e34\u5e8a\u76ee\u6807\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08CRPO\uff09\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6574\u5408\u57fa\u4e8e\u89c4\u5219\u548c\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8054\u5408\u4f18\u5316\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u8bad\u7ec3\u4e863B\u53c2\u6570\u7684Clinical-R1-3B\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRPO\u76f8\u6bd4\u6807\u51c6GRPO\u5728\u63a8\u7406\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u589e\u5f3a\u3002", "conclusion": "CRPO\u4e3a\u5bf9\u9f50LLM\u63a8\u7406\u4e0e\u4e34\u5e8a\u76ee\u6807\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u4f7f\u533b\u7597AI\u7cfb\u7edf\u66f4\u5b89\u5168\u3001\u66f4\u5177\u534f\u4f5c\u6027\uff0c\u5c55\u793a\u4e86\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1RL\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57dfLLM\u540e\u8bad\u7ec3\u6269\u5c55\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.00709", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00709", "abs": "https://arxiv.org/abs/2512.00709", "authors": ["Yifan Xu", "Xichen Ye", "Yifan Chen", "Qiaosheng Zhang"], "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF", "comment": "Accepted by AAAI-26-AIA", "summary": "Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ffb\u8f6c\u611f\u77e5\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u5b9e\u4f8b\u4f9d\u8d56\u7684\u504f\u597d\u7ffb\u8f6c\u6982\u7387\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u6536\u96c6\u4eba\u7c7b\u53cd\u9988\u8fdb\u884cLLM\u5bf9\u9f50\u65f6\uff0c\u504f\u597d\u7ffb\u8f6c\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u5bfc\u81f4\u6570\u636e\u6807\u6ce8\u88ab\u6c61\u67d3\u3002\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\u9700\u8981\u63d0\u9ad8\u5bf9\u6f5c\u5728\u7ffb\u8f6c\u5bf9\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u4eba\u7c7b\u610f\u56fe\u6a21\u578b\u548c\u5916\u90e8\u56e0\u7d20\u5f15\u8d77\u7684\u504f\u597d\u7ffb\u8f6c\u673a\u5236\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u57fa\u4e8eBradley-Terry\u6a21\u578b\u5f15\u5165\u5b9e\u4f8b\u4f9d\u8d56\u7684\u7ffb\u8f6c\u6982\u7387\uff0c\u5229\u7528\u504f\u597d\u6807\u6ce8\u76f8\u5173\u7279\u5f81\u6355\u6349\u5224\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u8bbe\u8ba1\u517c\u5bb9\u539f\u59cbRLHF\u548cDPO\u7684\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u8bc4\u4f30\u4e86\u5b9e\u4f8b\u4f9d\u8d56\u504f\u597d\u7ffb\u8f6c\u6a21\u578b\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4e0e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684FA-DPO\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u504f\u597d\u7ffb\u8f6c\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u9f50\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01141", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01141", "abs": "https://arxiv.org/abs/2512.01141", "authors": ["Muhammad Yousuf", "Akshat Bagade", "Chhittebbayi Penugonda", "Maanas Baraya"], "title": "Neural Variable Name Repair: Learning to Rename Identifiers for Readability", "comment": null, "summary": "Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u91cf\u540d\u4fee\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u5fae\u8c03Llama 3.1-8B\u6a21\u578b\u548c\u91cd\u6392\u5e8f\u5668\uff0c\u663e\u8457\u63d0\u5347C++\u4ee3\u7801\u4e2d\u53d8\u91cf\u540d\u7684\u51c6\u786e\u6027\u548c\u63cf\u8ff0\u6027", "motivation": "\u5b9e\u9645\u5f00\u53d1\u4e2d\u53d8\u91cf\u540d\u901a\u5e38\u8fc7\u4e8e\u901a\u7528\u6216\u8bef\u5bfc\u6027\uff0c\u4e14\u51fd\u6570\u7f3a\u4e4f\u6587\u6863\uff0c\u8fd9\u4f1a\u964d\u4f4e\u4ee3\u7801\u7406\u89e3\u6548\u7387\u3001\u589e\u52a0bug\u98ce\u9669\uff0c\u5e76\u5f71\u54cd\u4eba\u7c7b\u548cLLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b", "method": "\u4eceBigCode\u7684The Stack\u6570\u636e\u96c6\u4e2d\u81ea\u52a8\u6784\u5efa\u4efb\u52a1\uff0c\u4f7f\u7528Tree-sitter\u89e3\u6790C++\u51fd\u6570\u5e76\u63a9\u7801\u5355\u4e2a\u6807\u8bc6\u7b26\uff1b\u57fa\u4e8eLlama 3.1-8B\u6784\u5efa\u5305\u542b\u9884\u70ed\u548cdropout\u8c03\u5ea6\u3001LoRA\u9002\u914d\u5668\u7684\u9ad8\u6548\u5fae\u8c03\u7ba1\u9053\uff0c\u4ee5\u53ca\u53cc\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u5668", "result": "\u5728200\u4e2aC++\u51fd\u6570\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u96f6\u6837\u672c\u57fa\u7ebf\u8fbe\u52306.1%\u7cbe\u786e\u5339\u914d\uff1b\u6700\u4f73LoRA\u5fae\u8c03\u6a21\u578b\u8fbe\u523043.1%\u7cbe\u786e\u5339\u914d\u300150.2% Top-5\u547d\u4e2d\u7387\u548c82.03\u90e8\u5206\u5339\u914d\u5206\u6570\uff1b\u91cd\u6392\u5e8f\u5668\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9009\u62e9\u8d28\u91cf", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u52a0\u91cd\u6392\u5e8f\u662f\u5b9e\u7528\u6807\u8bc6\u7b26\u4fee\u590d\u5de5\u5177\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u6539\u5584\u53d8\u91cf\u540d\u7684\u63cf\u8ff0\u6027\u548c\u51c6\u786e\u6027", "topic": "code agent"}}
{"id": "2512.01155", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01155", "abs": "https://arxiv.org/abs/2512.01155", "authors": ["Krishna Kumaar Sharma"], "title": "Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering", "comment": "53 pages, 7 figures", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "AI": {"tldr": "\u63d0\u51faD3\u6846\u67b6\uff0c\u7ed3\u5408\u89d2\u8272\u5206\u79bb\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff08Builder\u751f\u6210\u3001Reviewer\u8bc4\u5ba1\uff09\u5904\u7406\u9057\u7559\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u572852\u540d\u4ece\u4e1a\u8005\u7684\u63a2\u7d22\u6027\u8c03\u67e5\u4e2d\u663e\u793a\u751f\u4ea7\u529b\u63d0\u534726.9%\uff0c\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e77%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7eff\u5730\u9879\u76ee\u6216\u5408\u6210\u4efb\u52a1\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9057\u7559\u7cfb\u7edf\u3001\u6587\u6863\u4e0d\u5168\u3001\u67b6\u6784\u77e5\u8bc6\u788e\u7247\u5316\u7684\u68d5\u5730\u5de5\u7a0b\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002\u9700\u8981\u89e3\u51b3LLM\u5728\u590d\u6742\u3001\u4e0a\u4e0b\u6587\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u6709\u6548\u5e94\u7528\u95ee\u9898\u3002", "method": "\u63d0\u51faDiscover-Define-Deliver (D3)\u6846\u67b6\uff0c\u91c7\u7528\u89d2\u8272\u5206\u79bb\u63d0\u793a\u7b56\u7565\u548c\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff1aBuilder\u6a21\u578b\u751f\u6210\u5019\u9009\u8f93\u51fa\uff0cReviewer\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u5316\u8bc4\u5ba1\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002\u6846\u67b6\u5305\u542b\u5e94\u7528\u6700\u4f73\u5b9e\u8df5\u6765\u5904\u7406\u68d5\u5730\u7cfb\u7edf\u4e2d\u7684\u6a21\u7cca\u6027\u3002", "result": "52\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7684\u63a2\u7d22\u6027\u8c03\u67e5\u663e\u793a\uff1a\u52a0\u6743\u5e73\u5747\u751f\u4ea7\u529b\u63d0\u534726.9%\uff0c\u7ea677%\u53c2\u4e0e\u8005\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e\uff0c83%\u5728Define\u9636\u6bb5\u8fd4\u5de5\u51cf\u5c11\u3002\u53c2\u4e0e\u8005\u62a5\u544a\u4efb\u52a1\u6e05\u6670\u5ea6\u3001\u6587\u6863\u8d28\u91cf\u548c\u8ba4\u77e5\u8d1f\u8377\u611f\u77e5\u6539\u5584\u3002", "conclusion": "D3\u6846\u67b6\u5c55\u793a\u4e86\u7ed3\u6784\u5316LLM\u5de5\u4f5c\u6d41\u7a0b\u5728\u9057\u7559\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u7ed3\u679c\u57fa\u4e8e\u81ea\u6211\u62a5\u544a\uff0c\u9700\u8981\u672a\u6765\u53d7\u63a7\u8bc4\u4f30\u9a8c\u8bc1\u3002\u7814\u7a76\u7a81\u51fa\u4e86\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "topic": "code agent"}}
{"id": "2512.01232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01232", "abs": "https://arxiv.org/abs/2512.01232", "authors": ["Donghao Huang", "Shila Chew", "Anna Dutkiewicz", "Zhaoxia Wang"], "title": "LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost", "comment": "7 pages, accepted by the AAAI 2026 Workshop on Next Gen Code Development with Collaborative AI Agents", "summary": "Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-as-a-Judge (LAJ)\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30Gherkin\u9a8c\u6536\u6d4b\u8bd5\uff0c\u901a\u8fc720\u4e2a\u6a21\u578b\u914d\u7f6e\u5728500\u6b21\u8bc4\u4f30\u4e2d\u7684\u5206\u6790\u663e\u793a\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5728\u6210\u672c\u5927\u5e45\u964d\u4f4e\u7684\u540c\u65f6\u8fbe\u5230\u66f4\u597d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u8bc4\u4f30\u8f6f\u4ef6\u6d4b\u8bd5\u8986\u76d6\u7387\u4ecd\u7136\u662fQA\u6d41\u7a0b\u4e2d\u7684\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u751f\u4ea7\u5c31\u7eea\u7684\u6846\u67b6\u6765\u8bc4\u4f30Gherkin\u9a8c\u6536\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86LLM-as-a-Judge (LAJ)\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316JSON\u8f93\u51fa\u7684\u8bc4\u5206\u9a71\u52a8\u65b9\u6cd5\uff0c\u572820\u4e2a\u6a21\u578b\u914d\u7f6e\uff08GPT-4\u3001GPT-5\u548c\u5f00\u6e90\u6a21\u578b\uff09\u4e0a\u5bf9100\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u811a\u672c\u8fdb\u884c\u4e865\u8f6e\u8bc4\u4f30\uff08\u5171500\u6b21\uff09\u3002", "result": "GPT-4o Mini\u8fbe\u5230\u6700\u4f73\u51c6\u786e\u7387\uff086.07 MAAE\uff09\u3001\u9ad8\u53ef\u9760\u6027\uff0896.6% ECR@1\uff09\u548c\u4f4e\u6210\u672c\uff08\u6bcf\u5343\u6b211.01\u7f8e\u5143\uff09\uff0c\u76f8\u6bd4GPT-5\uff08\u9ad8\u63a8\u7406\uff09\u6210\u672c\u964d\u4f4e78\u500d\u4e14\u51c6\u786e\u7387\u66f4\u9ad8\u3002\u63a8\u7406\u52aa\u529b\u5bf9\u6a21\u578b\u5bb6\u65cf\u6709\u4f9d\u8d56\u6027\uff0c\u6210\u672c\u8303\u56f4\u8fbe175\u500d\uff08\u6bcf\u5343\u6b210.45-78.96\u7f8e\u5143\uff09\u3002", "conclusion": "\u5c0f\u6a21\u578b\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63a8\u7406\u52aa\u529b\u7684\u6548\u679c\u56e0\u6a21\u578b\u5bb6\u65cf\u800c\u5f02\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6d4b\u8bd5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6210\u672c-\u6027\u80fd\u6743\u8861\u6307\u5bfc\u3002", "topic": "swe application"}}
{"id": "2512.00611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00611", "abs": "https://arxiv.org/abs/2512.00611", "authors": ["Franck Binard", "Vanja Kljajevic"], "title": "Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior", "comment": null, "summary": "Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.", "AI": {"tldr": "Prism\u662f\u4e00\u4e2a\u7528\u4e8e\u89c4\u8303\u5de5\u5177\u4f7f\u7528\u8f6f\u4ef6\u4ee3\u7406\u884c\u4e3a\u7684\u5c0f\u578b\u7ec4\u5408\u5143\u8bed\u8a00\uff0c\u57fa\u4e8e\u56fa\u5b9a\u7684\u6838\u5fc3\u4e0a\u4e0b\u6587Core1\u6784\u5efa\uff0c\u901a\u8fc7\u9886\u57df\u6269\u5c55\u673a\u5236\u652f\u6301\u53ef\u91cd\u7528\u8bed\u6cd5\u6838\u5fc3\u4e0e\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u7684\u5206\u79bb\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u4f7f\u7528\u8f6f\u4ef6\u4ee3\u7406\u7684\u89c4\u8303\u901a\u5e38\u91c7\u7528\u4e34\u65f6\u63a7\u5236\u7ed3\u6784\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7ec4\u5408\u6846\u67b6\u3002Prism\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7d27\u51d1\u7684\u63a5\u53e3\u8bed\u8a00\uff0c\u4f7f\u4ee3\u7406\u63a7\u5236\u7a7a\u95f4\u660e\u786e\u5316\uff0c\u4fbf\u4e8e\u5206\u6790\u3001\u9a8c\u8bc1\u548c\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u57fa\u4e8e\u56fa\u5b9a\u7684\u6838\u5fc3\u4e0a\u4e0b\u6587Core1\u6784\u5efa\uff0c\u5305\u542b\u6570\u5b57\u3001\u5b57\u7b26\u4e32\u3001\u7528\u6237\u63d0\u793a\u3001\u5de5\u5177\u7b49\u57fa\u672c\u7c7b\u522b\uff0c\u4ee5\u53ca\u5e03\u5c14\u503c\u3001\u8c13\u8bcd\u3001\u5bf9\u3001\u5217\u8868\u7684\u7ec4\u5408\u5b50\u3002\u4ee3\u7406\u7b56\u7565\u901a\u8fc7\u5355\u4e00\u62bd\u8c61\u8fd0\u7b97\u7b26\u7f16\u5199\uff0c\u6761\u4ef6\u8bed\u53e5\u8868\u73b0\u4e3a\u66ff\u4ee3\u9009\u62e9\u800c\u975e\u547d\u4ee4\u5f0fif-else\u5757\u3002\u9886\u57df\u901a\u8fc7\u5b9a\u4e49\u81ea\u5df1\u7684\u4e0a\u4e0b\u6587\u5fae\u8bed\u6cd5\u6269\u5c55\u6838\u5fc3\u3002", "result": "\u901a\u8fc7\u6052\u6e29\u5668\u63a7\u5236\u3001\u5bb6\u5ead\u5b89\u5168\u3001\u7535\u5b50\u5546\u52a1\u63a8\u8350\u548c\u533b\u7597\u76d1\u63a7\u7b49\u5b9e\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u81ea\u7136\u8bed\u8a00\u51b3\u7b56\u89c4\u5219\u6620\u5c04\u4e3a\u53ef\u68c0\u67e5\u3001\u53ef\u6267\u884c\u7684\u7b56\u7565\u3002\u5b9e\u73b0\u4e86\u8bed\u6cd5\u6838\u5fc3\u4e0e\u9886\u57df\u8bcd\u6c47\u7684\u6e05\u6670\u5206\u79bb\u3002", "conclusion": "Prism\u4ece\u8bed\u8a00\u5b66\u89d2\u5ea6\u5f3a\u5236\u5206\u79bb\u53ef\u91cd\u7528\u8bed\u6cd5\u6838\u5fc3\u4e0e\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\uff0c\u5c06\u5de5\u5177\u89c6\u4e3a\u5185\u90e8\u7b56\u7565\u8868\u793a\u4e0e\u5916\u90e8\u4e16\u754c\u4e4b\u95f4\u7684\u6865\u6881\uff1b\u4ece\u5de5\u7a0b\u89d2\u5ea6\u63d0\u4f9b\u4e86\u7d27\u51d1\u7684\u4ee3\u7406\u63a7\u5236\u63a5\u53e3\u8bed\u8a00\u3002", "topic": "agent analysis"}}
{"id": "2512.01356", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01356", "abs": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "comment": "Accepted by the 2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Copyright 2025 IEEE. This is the author's accepted manuscript. The final published version may differ and will be available from IEEE Xplore", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "AI": {"tldr": "\u63d0\u51faLAURA\u6846\u67b6\uff0c\u7ed3\u5408\u8bc4\u5ba1\u77e5\u8bc6\u589e\u5f3a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5229\u7528LLM\u751f\u6210\u4ee3\u7801\u8bc4\u5ba1\u610f\u89c1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4ee3\u7801\u8bc4\u5ba1\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u968f\u7740\u8f6f\u4ef6\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u589e\u957f\uff0c\u4f20\u7edf\u8bc4\u5ba1\u6210\u4e3a\u74f6\u9888\u3002\u73b0\u6709\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5386\u53f2\u4ee3\u7801\u53d8\u66f4\u548c\u8bc4\u5ba1\u610f\u89c1\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u53d8\u66f4\u4e0a\u4e0b\u6587\u548c\u5148\u9a8c\u8bc4\u5ba1\u77e5\u8bc6\u7b49\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u63d0\u51faLAURA\u6846\u67b6\uff0c\u96c6\u6210\u8bc4\u5ba1\u8303\u4f8b\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u548c\u7cfb\u7edf\u6307\u5bfc\uff0c\u589e\u5f3aChatGPT-4o\u548cDeepSeek v3\u751f\u6210\u4ee3\u7801\u8bc4\u5ba1\u610f\u89c1\u7684\u80fd\u529b\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u4f4e\u8d28\u91cf\u8bc4\u5ba1\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLAURA\u4e3a\u4e24\u4e2a\u6a21\u578b\u751f\u6210\u7684\u8bc4\u5ba1\u610f\u89c1\u4e2d\uff0c\u5206\u522b\u670942.2%\u548c40.4%\u5b8c\u5168\u6b63\u786e\u6216\u5bf9\u5f00\u53d1\u8005\u6709\u5e2e\u52a9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8868\u660eLAURA\u7684\u6240\u6709\u7ec4\u4ef6\u90fd\u5bf9\u63d0\u5347\u8bc4\u8bba\u8d28\u91cf\u6709\u79ef\u6781\u8d21\u732e\u3002", "conclusion": "LAURA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8bc4\u5ba1\u77e5\u8bc6\u589e\u5f3a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u4ee3\u7801\u8bc4\u5ba1\u610f\u89c1\u7684\u8d28\u91cf\uff0c\u4e3a\u89e3\u51b3\u4ee3\u7801\u8bc4\u5ba1\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2512.00617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00617", "abs": "https://arxiv.org/abs/2512.00617", "authors": ["Omer Jauhar Khan"], "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization", "comment": "8 pages, 5 figures, 5 tables. Conference-style paper", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.", "AI": {"tldr": "ART\u6846\u67b6\u901a\u8fc7\u9526\u6807\u8d5b\u5f0fELO\u6392\u540d\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u4f18\u5316LLM\u8f93\u51fa\u8d28\u91cf\uff0c\u63d0\u5347\u54cd\u5e94\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027", "motivation": "\u5355\u4e2aLLM\u6a21\u578b\u54cd\u5e94\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3001\u5e7b\u89c9\u548c\u4e0d\u540c\u67e5\u8be2\u9886\u57df\u8d28\u91cf\u6ce2\u52a8\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u4f18\u5316\u65b9\u6cd5", "method": "\u91c7\u7528\u9526\u6807\u8d5b\u5f0fELO\u6392\u540d\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u8ba9\u591a\u4e2aLLM\u667a\u80fd\u4f53\u901a\u8fc7\u7ade\u4e89\u3001\u6279\u5224\u548c\u534f\u4f5c\u4ea7\u751f\u5171\u8bc6\u54cd\u5e94", "result": "\u76f8\u6bd4\u57fa\u7ebf\u5355\u6a21\u578b\u65b9\u6cd5\uff0c\u5728\u54cd\u5e94\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u6574\u4f53\u8d28\u91cf\u6307\u6807\u63d0\u9ad88.4%\uff0cELO\u8bc4\u7ea7\u6536\u655bR\u00b2\u503c\u8d85\u8fc70.96", "conclusion": "ART\u6846\u67b6\u4e3a\u9700\u8981\u9ad8\u8d28\u91cf\u3001\u7ecf\u8fc7\u9a8c\u8bc1\u7684LLM\u54cd\u5e94\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u751f\u4ea7\u5c31\u7eea\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2512.00834", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00834", "abs": "https://arxiv.org/abs/2512.00834", "authors": ["Lin Zhu", "Kezhi Wang", "Luping Xiang", "Kun Yang"], "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks", "comment": "Submitted for possible journal publication", "summary": "Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u901a\u4fe1\u4e0eAgentic AI\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7V2I\u548cV2V\u901a\u4fe1\u4e2d\u7684\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8fbe47.5%\u7684\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfV2X\u901a\u4fe1\u65b9\u6848\u5b58\u5728\u4f20\u8f93\u5f00\u9500\u5927\u3001\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u901a\u4fe1\u4e0e\u667a\u80fd\u9884\u6d4b\u6846\u67b6\u3002", "method": "\u5728V2I\u901a\u4fe1\u4e2d\uff0cRSU\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u667a\u80fd\u4f53\u4ece\u5386\u53f2\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7d27\u51d1\u8868\u793a\uff0c\u8bed\u4e49\u5206\u6790\u667a\u80fd\u4f53\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u5c06\u7279\u5f81\u548c\u8bed\u4e49\u6d1e\u5bdf\u4f20\u8f93\u7ed9\u8f66\u8f86\u3002\u5728V2V\u901a\u4fe1\u4e2d\uff0c\u8f66\u8f86\u8fdb\u884c\u672c\u5730\u7279\u5f81\u63d0\u53d6\u548c\u8bed\u4e49\u5206\u6790\uff0c\u540c\u65f6\u63a5\u6536\u90bb\u5c45\u8f66\u8f86\u7684\u9884\u6d4b\u8f68\u8ff9\uff0c\u7efc\u5408\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u5404\u79cd\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe47.5%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u901a\u4fe1\u4e0eAgentic AI\u96c6\u6210\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\uff0c\u4e3aV2X\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4fe1\u606f\u4ea4\u6362\u548c\u53ef\u9760\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.01396", "categories": ["cs.SE", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01396", "abs": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "comment": "Under review", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "AI": {"tldr": "BackportBench\u662f\u9996\u4e2a\u5168\u9762\u7684\u8865\u4e01\u56de\u79fb\u690d\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b202\u4e2a\u591a\u8bed\u8a00\u8865\u4e01\u56de\u79fb\u690d\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u5316\u56de\u79fb\u690d\u6280\u672f\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u5728\u9700\u8981\u903b\u8f91\u548c\u7ed3\u6784\u53d8\u5316\u7684\u6848\u4f8b\u4e2d\u4f18\u4e8e\u4f20\u7edf\u8865\u4e01\u79fb\u690d\u65b9\u6cd5\u3002", "motivation": "\u8f6f\u4ef6\u9879\u76ee\u5feb\u901f\u6f14\u8fdb\uff0c\u7528\u6237\u9700\u8981\u66f4\u65b0\u4f9d\u8d56\u5230\u66f4\u5b89\u5168\u7684\u7248\u672c\uff0c\u4f46\u624b\u52a8\u56de\u79fb\u690d\u5b89\u5168\u8865\u4e01\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u73b0\u6709\u81ea\u52a8\u5316\u56de\u79fb\u690d\u6280\u672f\u901a\u5e38\u53ea\u9488\u5bf9\u4ee3\u7801\u5757\u6216\u51fd\u6570\u7ea7\u8865\u4e01\u79fb\u690d\u573a\u666f\uff0c\u4e14\u4f7f\u7528\u4e0d\u5b8c\u5584\u7684\u6307\u6807\u8bc4\u4f30\uff0c\u5176\u901a\u7528\u8f6f\u4ef6\u4e0a\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165BackportBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b202\u4e2a\u6765\u81eaPyPI\u3001Maven\u548cnpm\u7684\u8865\u4e01\u56de\u79fb\u690d\u95ee\u9898\uff0c\u6bcf\u4e2a\u90fd\u6709\u53ef\u6267\u884c\u7684Docker\u73af\u5883\u548c\u76f8\u5173\u6d4b\u8bd5\u7528\u4f8b\u3002\u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u73b0\u6709\u8865\u4e01\u79fb\u690d\u65b9\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u6280\u672f\u3002", "result": "\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u8865\u4e01\u79fb\u690d\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u903b\u8f91\u548c\u7ed3\u6784\u53d8\u5316\u7684\u6848\u4f8b\u4e2d\u3002\u4f46\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u6027\u80fd\u8868\u73b0\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "BackportBench\u4e3a\u81ea\u52a8\u5316\u56de\u79fb\u690d\u6280\u672f\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u56de\u79fb\u690d\u5de5\u4f5c\u7684\u7814\u7a76\u4eba\u5458\u548c\u8f6f\u4ef6\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "topic": "swe benchmark"}}
{"id": "2512.00656", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00656", "abs": "https://arxiv.org/abs/2512.00656", "authors": ["Jan Batzner", "Volker Stocker", "Stefan Schmid", "Gjergji Kasneci"], "title": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop", "comment": "NeurIPS 2025 Workshop on LLM Evaluation and ICLR 2025 Workshop on Bi-Directional Human-AI Alignment", "summary": "Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u56de\u987e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8c04\u5a9a\u54cd\u5e94\u6a21\u5f0f\u7684\u7814\u7a76\u65b9\u6cd5\u6311\u6218\uff0c\u8bc6\u522b\u4e86\u4e94\u79cd\u6838\u5fc3\u64cd\u4f5c\u5316\u5b9a\u4e49\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u6587\u732e\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u58f0\u79f0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8c04\u5a9a\u54cd\u5e94\u6a21\u5f0f\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u56de\u987e\u6d4b\u91cfLLM\u8c04\u5a9a\u6027\u7684\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bba\u6587\u91c7\u7528\u6587\u732e\u56de\u987e\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u4e94\u79cd\u6838\u5fc3\u64cd\u4f5c\u5316\u5b9a\u4e49\u6765\u6d4b\u91cfLLM\u8c04\u5a9a\u6027\uff0c\u5206\u6790\u4e86\u5f53\u524d\u7814\u7a76\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u8c04\u5a9a\u6027\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u96be\u4ee5\u533a\u5206\u8c04\u5a9a\u6027\u54cd\u5e94\u4e0eAI\u5bf9\u9f50\u4e2d\u7684\u76f8\u5173\u6982\u5ff5\uff0c\u4e14\u7531\u4e8e\u8c04\u5a9a\u6027\u672c\u8d28\u4e0a\u662f\u4eba\u7c7b\u4e2d\u5fc3\u7684\u6982\u5ff5\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u8bc4\u4f30\u4eba\u7c7b\u611f\u77e5\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u533a\u5206LLM\u8c04\u5a9a\u6027\u54cd\u5e94\u4e0e\u5176\u4ed6AI\u5bf9\u9f50\u6982\u5ff5\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u53ef\u64cd\u4f5c\u5efa\u8bae\uff0c\u7279\u522b\u662f\u9700\u8981\u7eb3\u5165\u4eba\u7c7b\u611f\u77e5\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2512.00839", "categories": ["cs.AI", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00839", "abs": "https://arxiv.org/abs/2512.00839", "authors": ["Fabrizio Maturo", "Donato Riccio", "Andrea Mazzitelli", "Giuseppe Bifulco", "Francesco Paolone", "Iulia Brezeanu"], "title": "ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI", "comment": "35 pages, 9 figures, 4 tables", "summary": "This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.", "AI": {"tldr": "ARCADIA\u662f\u4e00\u4e2a\u7528\u4e8e\u56e0\u679c\u53d1\u73b0\u7684\u667a\u80fdAI\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u7edf\u8ba1\u8bca\u65ad\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6784\u5efa\u6709\u6548\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u56e0\u679c\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u5728\u771f\u5b9e\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u80fd\u5904\u7406\u590d\u6742\u73b0\u5b9e\u6570\u636e\u7684\u56e0\u679c\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u7edf\u8ba1\u8bca\u65ad\uff0c\u901a\u8fc7\u7ea6\u675f\u5f15\u5bfc\u63d0\u793a\u548c\u56e0\u679c\u6709\u6548\u6027\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u6709\u5411\u65e0\u73af\u56fe(DAGs)\u3002", "result": "\u5728\u4f01\u4e1a\u7834\u4ea7\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0cARCADIA\u6bd4NOTEARS\u3001GOLEM\u548cDirectLiNGAM\u4ea7\u751f\u66f4\u53ef\u9760\u7684\u56e0\u679c\u56fe\uff0c\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u3001\u652f\u6301\u5e72\u9884\u7684\u6d41\u7a0b\u3002", "conclusion": "ARCADIA\u5c55\u793a\u4e86\u667a\u80fd\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u53c2\u4e0e\u81ea\u4e3b\u79d1\u5b66\u5efa\u6a21\u548c\u7ed3\u6784\u5316\u56e0\u679c\u63a8\u65ad\uff0c\u63a8\u52a8\u4e86AI\u5728\u56e0\u679c\u53d1\u73b0\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.00357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00357", "abs": "https://arxiv.org/abs/2512.00357", "authors": ["Na Li", "Hangguan Shan", "Wei Ni", "Wenjie Zhang", "Xinyu Li", "Yamin Wang"], "title": "Learning Causal States Under Partial Observability and Perturbation", "comment": null, "summary": "A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \\textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.", "AI": {"tldr": "CaDiff\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u6269\u6563\u6a21\u578b\u548c\u53cc\u6a21\u62df\u5ea6\u91cf\u6765\u63ed\u793aP\u00b2OMDPs\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u589e\u5f3aRL\u7b97\u6cd5\u5728\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08P\u00b2OMDPs\uff09\u4e2d\u9762\u4e34\u57fa\u4e8e\u4e0d\u5b8c\u6574\u548c\u566a\u58f0\u89c2\u6d4b\u8fdb\u884c\u51b3\u7b56\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faCaDiff\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u5f02\u6b65\u6269\u6563\u6a21\u578b\uff08ADM\uff09\uff0c\u652f\u6301\u4e0d\u540c\u6b65\u6570\u7684\u524d\u5411\u548c\u53cd\u5411\u8fc7\u7a0b\uff0c\u5c06P\u00b2OMDP\u6270\u52a8\u89e3\u91ca\u4e3a\u53ef\u901a\u8fc7\u6269\u6563\u6291\u5236\u7684\u566a\u58f0\uff1b2\uff09\u65b0\u7684\u53cc\u6a21\u62df\u5ea6\u91cf\uff0c\u91cf\u5316\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0e\u5176\u56e0\u679c\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5728Roboschool\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaDiff\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u81f3\u5c11\u63d0\u534714.18%\u7684\u56de\u62a5\u7387\u3002\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u4e86\u6270\u52a8\u89c2\u6d4b\u4e0e\u53bb\u566a\u56e0\u679c\u72b6\u6001\u4e4b\u95f4\u4ef7\u503c\u51fd\u6570\u8fd1\u4f3c\u8bef\u5dee\u7684\u4e0a\u754c\u3002", "conclusion": "CaDiff\u662f\u9996\u4e2a\u7ed3\u5408\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u7528\u6027\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fd1\u4f3c\u56e0\u679c\u72b6\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86P\u00b2OMDPs\u4e2d\u7684\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.00383", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00383", "abs": "https://arxiv.org/abs/2512.00383", "authors": ["Jianhai Su", "Jinzhu Luo", "Qi Zhang"], "title": "An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines", "comment": null, "summary": "We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u79bb\u7ebfRL\u7b97\u6cd5\u4f5c\u4e3a\u5728\u7ebfRL\u5b50\u7a0b\u5e8f\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5386\u53f2\u4ea4\u4e92\u4f5c\u4e3a\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u79cd\u53d8\u4f53\u63d0\u5347\u5728\u7ebf\u5b66\u4e60\u6548\u7387", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6574\u5408\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u667a\u80fd\u4f53\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u4f5c\u4e3a\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8\u5728\u7ebf\u5b66\u4e60\u6548\u7387", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u79bb\u7ebfRL\u7b97\u6cd5\u4f5c\u4e3a\u5728\u7ebfRL\u7684\u5b50\u7a0b\u5e8f\uff0c\u5305\u62ec\u6700\u7ec8\u7b56\u7565\u63a8\u8350\u548c\u5728\u7ebf\u5fae\u8c03\u7b49\u53d8\u4f53\uff0c\u5e76\u5f15\u5165\u6280\u672f\u63d0\u5347\u5176\u6709\u6548\u6027", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff1a1\uff09\u6846\u67b6\u6709\u6548\u6027\u5f3a\u70c8\u4f9d\u8d56\u4efb\u52a1\u7279\u6027\uff1b2\uff09\u63d0\u51fa\u7684\u6280\u672f\u663e\u8457\u63d0\u5347\u6548\u679c\uff1b3\uff09\u73b0\u6709\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\u6574\u4f53\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u591a\u7814\u7a76", "conclusion": "\u79bb\u7ebfRL\u4f5c\u4e3a\u5728\u7ebfRL\u5b50\u7a0b\u5e8f\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u65b9\u6cd5\uff0c\u73b0\u6709\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\uff0c\u4e3aRL\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u65b0\u65b9\u5411", "topic": "agentic reinforcement learning"}}
{"id": "2512.01690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01690", "abs": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "title": "Generating REST API Tests With Descriptive Names", "comment": null, "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.\n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.\n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u786e\u5b9a\u6027\u6280\u672f\u4e3aREST API\u6d4b\u8bd5\u751f\u6210\u63cf\u8ff0\u6027\u540d\u79f0\uff0c\u89c4\u5219\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8eGPT-3.5\uff0c\u4e0eGPT-4o/Gemini\u76f8\u5f53\uff0c\u5de5\u4e1a\u6848\u4f8b\u9a8c\u8bc1\u53ef\u63d0\u5347\u6d4b\u8bd5\u53ef\u8bfb\u6027\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u7684API\u6d4b\u8bd5\u7528\u4f8b\u901a\u5e38\u4f7f\u7528\u975e\u63cf\u8ff0\u6027\u540d\u79f0\uff08\u5982test0, test1\uff09\uff0c\u964d\u4f4e\u4e86\u53ef\u8bfb\u6027\uff0c\u5f71\u54cd\u7406\u89e3\u548c\u7ef4\u62a4\u3002\u9700\u8981\u4e3aREST API\u6d4b\u8bd5\u81ea\u52a8\u751f\u6210\u63cf\u8ff0\u6027\u540d\u79f0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u786e\u5b9a\u6027\u6280\u672f\u751f\u6210REST API\u6d4b\u8bd5\u540d\u79f0\uff0c\u5171\u6bd4\u8f83\u516b\u79cd\u6280\u672f\uff08\u5305\u62ec\u89c4\u5219\u542f\u53d1\u5f0f\u548cLLM\u65b9\u6cd5\uff09\u3002\u4f7f\u7528EvoMaster\u4e3a9\u4e2a\u5f00\u6e90API\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u7528\u6237\u8c03\u67e5\uff0839\u4eba\uff09\u548c\u5de5\u4e1a\u6848\u4f8b\uff08Volkswagen AG\uff0c4\u4e2aAPI\uff0c74\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff09\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u89c4\u5219\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u65b9\u6cd5\u4e2d\u6e05\u6670\u5ea6\u8bc4\u5206\u6700\u9ad8\uff0c\u4e0eGemini\u548cGPT-4o\u7b49\u5148\u8fdbLLM\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\u3002\u5de5\u4e1a\u6848\u4f8b\u8bc1\u5b9e\u63cf\u8ff0\u6027\u540d\u79f0\u80fd\u63d0\u9ad8\u6d4b\u8bd5\u5957\u4ef6\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u786e\u5b9a\u6027\u6280\u672f\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u6602\u8d35\u4e14\u5b89\u5168\u654f\u611f\u7684LLM\u65b9\u6cd5\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5f00\u53d1\u8005\u53cb\u597d\u7684API\u6d4b\u8bd5\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\u3002", "topic": "swe application"}}
{"id": "2512.01939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01939", "abs": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "comment": null, "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u901a\u8fc7\u5206\u679011,910\u4e2a\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u4ece\u4e94\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u5341\u4e2a\u4e3b\u6d41\u6846\u67b6\u5982\u4f55\u6ee1\u8db3\u5f00\u53d1\u8005\u9700\u6c42\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5feb\u901f\u589e\u957f\uff0c\u5f00\u53d1\u8005\u9762\u4e34\u9009\u62e9\u56f0\u96be\uff0c\u4e14\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u548c\u5982\u4f55\u5f71\u54cd\u5f00\u53d1\u8fc7\u7a0b\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u4e0d\u540c\u6846\u67b6\u5728\u4f7f\u7528\u4e2d\u9047\u5230\u76f8\u4f3c\u95ee\u9898\uff0c\u8868\u660e\u9700\u8981\u6539\u8fdb\u6846\u67b6\u8bbe\u8ba1\u3002", "method": "\u6536\u96c6\u5341\u4e2a\u5df2\u8bc6\u522b\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5f00\u53d1\u8005\u8ba8\u8bba\uff08\u517111,910\u6761\uff09\uff0c\u4ece\u5f00\u53d1\u6548\u7387\u3001\u529f\u80fd\u62bd\u8c61\u3001\u5b66\u4e60\u6210\u672c\u3001\u6027\u80fd\u4f18\u5316\u548c\u53ef\u7ef4\u62a4\u6027\u4e94\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u4e0d\u540c\u6846\u67b6\u5728\u6ee1\u8db3\u5f00\u53d1\u8005\u9700\u6c42\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6001\u7cfb\u7edf\u7684\u73b0\u72b6\u548c\u95ee\u9898\u3002", "conclusion": "\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u53d1\u73b0\u548c\u542f\u793a\uff0c\u4e3a\u672a\u6765\u6846\u67b6\u8bbe\u8ba1\u548c\u5f00\u53d1\u8005\u9009\u62e9\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.00920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00920", "abs": "https://arxiv.org/abs/2512.00920", "authors": ["Jianxiang Zang", "Yongda Wei", "Ruxue Bai", "Shiyu Jiang", "Nijia Mo", "Binhong Li", "Qiang Sun", "Hui Liu"], "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios", "comment": null, "summary": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.", "AI": {"tldr": "Reward Auditor\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5047\u8bbe\u68c0\u9a8c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u573a\u666f\u4e0b\u7684\u6761\u4ef6\u53ef\u9760\u6027\uff08\u9002\u5408\u6027\uff09\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u504f\u597d\u611f\u77e5\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u504f\u597d\u611f\u77e5\u51c6\u786e\u7387\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5173\u952e\u8106\u5f31\u6027\u3002\u9700\u8981\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u4e0b\u7684\u6761\u4ef6\u53ef\u9760\u6027\uff08\u9002\u5408\u6027\uff09\u3002", "method": "\u63d0\u51faReward Auditor\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u9002\u5408\u6027\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5ba1\u8ba1\u5956\u52b1\u6a21\u578b\u504f\u597d\u611f\u77e5\u7f6e\u4fe1\u5ea6\u7684\u5206\u5e03\u9000\u5316\u6765\u91cf\u5316\u7edf\u8ba1\u663e\u8457\u6027\u548c\u6548\u5e94\u5927\u5c0f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63a8\u65ad\u5956\u52b1\u6a21\u578b\u5728\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8106\u5f31\u6027\u7684\u786e\u5b9a\u6027\u548c\u4e25\u91cd\u7a0b\u5ea6\uff0c\u4e3a\u6784\u5efa\u53ef\u9a8c\u8bc1\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u548c\u53ef\u4fe1\u8d56\u7684\u4e0b\u4e00\u4ee3LLM\u5bf9\u9f50\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "Reward Auditor\u4e3a\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u4e0b\u7684\u6761\u4ef6\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u3001\u9c81\u68d2\u7684LLM\u5bf9\u9f50\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2512.01020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01020", "abs": "https://arxiv.org/abs/2512.01020", "authors": ["Jinu Lee", "Kyoung-Woon On", "Simeng Han", "Arman Cohan", "Julia Hockenmaier"], "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics", "comment": null, "summary": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.", "AI": {"tldr": "LEGIT\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6cd5\u5f8b\u63a8\u7406\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06\u6cd5\u5ead\u5224\u51b3\u8f6c\u6362\u4e3a\u5c42\u6b21\u5316\u7684\u4e89\u8bae\u6811\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684\u6cd5\u5f8b\u63a8\u7406\u8f68\u8ff9\u7684\u8d28\u91cf\uff0c\u53d1\u73b0LLM\u7684\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u53d7\u95ee\u9898\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u5f71\u54cd\uff0cRAG\u548cRL\u65b9\u6cd5\u80fd\u4e92\u8865\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5728\u4e13\u5bb6\u9886\u57df\uff08\u5982\u6cd5\u5f8b\uff09\u4e2d\u8bc4\u4f30LLM\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\u5bf9\u4e8e\u786e\u4fdd\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6b64\u7c7b\u63a8\u7406\u4efb\u52a1\u7684\u56fa\u6709\u590d\u6742\u6027\uff0c\u8bc4\u4f30\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165LEGIT\u6570\u636e\u96c6\uff0824K\u5b9e\u4f8b\uff09\uff0c\u5c06\u6cd5\u5ead\u5224\u51b3\u8f6c\u6362\u4e3a\u5bf9\u7acb\u53cc\u65b9\u8bba\u70b9\u548c\u6cd5\u5ead\u7ed3\u8bba\u7684\u5c42\u6b21\u5316\u6811\u7ed3\u6784\uff0c\u4f5c\u4e3a\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u95ee\u9898\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u7684\u8bc4\u5206\u6807\u51c6\u3002\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u8bc4\u5206\u6807\u51c6\u7684\u53ef\u9760\u6027\u3002", "result": "1) LLM\u7684\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u4e25\u91cd\u53d7\u6cd5\u5f8b\u95ee\u9898\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u5f71\u54cd\uff1b2) \u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60(RL)\u5bf9\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u5e26\u6765\u4e92\u8865\u6548\u76ca\uff1aRAG\u63d0\u9ad8\u6574\u4f53\u63a8\u7406\u80fd\u529b\uff0cRL\u63d0\u9ad8\u6b63\u786e\u6027\u4f46\u4f1a\u964d\u4f4e\u8986\u76d6\u7387\u3002", "conclusion": "LEGIT\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u4e13\u5bb6\u9886\u57df\u63a8\u7406\u8f68\u8ff9\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u63ed\u793a\u4e86LLM\u6cd5\u5f8b\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86RAG\u548cRL\u65b9\u6cd5\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4e3a\u6539\u8fdb\u6cd5\u5f8bAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.01046", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01046", "abs": "https://arxiv.org/abs/2512.01046", "authors": ["Hadi Nekoei", "Alexandre Blondin Mass\u00e9", "Rachid Hassani", "Sarath Chandar", "Vincent Mai"], "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids", "comment": null, "summary": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.", "AI": {"tldr": "\u63d0\u51faShielded Controller Units (SCUs)\u65b9\u6cd5\uff0c\u5229\u7528\u7cfb\u7edf\u52a8\u529b\u5b66\u5148\u9a8c\u77e5\u8bc6\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u5728\u8fdc\u7a0b\u5fae\u7535\u7f51\u4f18\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u71c3\u6599\u6d88\u8017\u51cf\u5c1124%\u4e14\u4e0d\u589e\u52a0\u7535\u6c60\u9000\u5316", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u80fd\u6e90\u8f6c\u578b\u7b49\u590d\u6742\u7cfb\u7edf\u51b3\u7b56\u4f18\u5316\u4e2d\u9762\u4e34\u7ea6\u675f\u6ee1\u8db3\u6311\u6218\uff0c\u7279\u522b\u662f\u8fdc\u7a0b\u5fae\u7535\u7f51\u9700\u8981\u534f\u8c03\u63a7\u5236\u53ef\u518d\u751f\u80fd\u6e90\u3001\u71c3\u6599\u53d1\u7535\u673a\u548c\u7535\u6c60\uff0c\u5728\u6ee1\u8db3\u9700\u6c42\u7684\u540c\u65f6\u6700\u5c0f\u5316\u71c3\u6599\u6d88\u8017\u548c\u7535\u6c60\u9000\u5316\uff0c\u4e14\u5fc5\u987b\u9075\u5b88\u4e25\u683c\u7684\u64cd\u4f5c\u7ea6\u675f\u548c\u6cd5\u89c4", "method": "\u63d0\u51faShielded Controller Units (SCUs)\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u5206\u89e3\u4e3a\u5c42\u6b21\u7ed3\u6784\uff0c\u6bcf\u4e2aSCU\u663e\u5f0f\u7ba1\u7406\u7ea6\u675f\u5b50\u96c6\uff0c\u5229\u7528\u7cfb\u7edf\u52a8\u529b\u5b66\u5148\u9a8c\u77e5\u8bc6\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u8bbe\u8ba1", "result": "\u5728\u8fdc\u7a0b\u5fae\u7535\u7f51\u4f18\u5316\u4efb\u52a1\u4e2d\uff0c\u914d\u5907SCUs\u7684RL\u667a\u80fd\u4f53\u5b9e\u73b0\u71c3\u6599\u6d88\u8017\u51cf\u5c1124%\uff0c\u4e14\u4e0d\u589e\u52a0\u7535\u6c60\u9000\u5316\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u7ea6\u675f", "conclusion": "SCUs\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u80fd\u6e90\u8f6c\u578b\u76f8\u5173\u51b3\u7b56\u6311\u6218\u4e2d\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u548c\u53ef\u89e3\u91ca\u7684\u7ea6\u675f\u4fdd\u8bc1\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2512.00403", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00403", "abs": "https://arxiv.org/abs/2512.00403", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Xiaobing Yu", "Yu Zhong", "Shangqi Deng", "Ufaq Khan", "Jianghao Wu", "Xiaofeng Liu", "Imran Razzak", "Xiaojun Chang", "Yutong Xie"], "title": "SelfAI: Building a Self-Training AI System with LLM Agents", "comment": null, "summary": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.", "AI": {"tldr": "SelfAI\u662f\u4e00\u4e2a\u901a\u7528\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u901a\u8fc7\u7528\u6237\u4ee3\u7406\u3001\u8ba4\u77e5\u4ee3\u7406\u548c\u5b9e\u9a8c\u7ba1\u7406\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u91cf\u5316\u53d1\u73b0\u6548\u7387\u548c\u641c\u7d22\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u5b58\u5728\u9886\u57df\u5c40\u9650\u3001\u7f3a\u4e4f\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\u3001\u65e0\u539f\u5219\u6027\u505c\u6b62\u673a\u5236\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u53ef\u91cd\u590d\u6027\u5dee\u3001\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faSelfAI\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff1a\u7528\u6237\u4ee3\u7406\u5c06\u7814\u7a76\u76ee\u6807\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u5b9e\u9a8c\u914d\u7f6e\uff1b\u8ba4\u77e5\u4ee3\u7406\u57fa\u4e8eLLM\u5e76\u91c7\u7528\u6700\u4f18\u505c\u6b62\u51c6\u5219\u8fed\u4ee3\u4f18\u5316\u8d85\u53c2\u6570\u641c\u7d22\uff1b\u5b9e\u9a8c\u7ba1\u7406\u5668\u534f\u8c03\u5f02\u6784\u786c\u4ef6\u4e0a\u7684\u5e76\u884c\u5bb9\u9519\u8bad\u7ec3\u5de5\u4f5c\u6d41\uff0c\u7ef4\u62a4\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u56de\u5f52\u3001NLP\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u79d1\u5b66\u8ba1\u7b97\u3001\u533b\u5b66\u5f71\u50cf\u548c\u836f\u7269\u53d1\u73b0\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelfAI\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u7ecf\u5178\u8d1d\u53f6\u65af\u4f18\u5316\u548cLLM\u57fa\u7ebf\u51cf\u5c11\u4e86\u5197\u4f59\u8bd5\u9a8c\uff0c\u540c\u65f6\u652f\u6301\u4e0e\u4eba\u7c7b\u7814\u7a76\u8005\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002", "conclusion": "SelfAI\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u6700\u4f18\u505c\u6b62\u673a\u5236\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4eba\u673a\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2512.01078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01078", "abs": "https://arxiv.org/abs/2512.01078", "authors": ["Jiawei Ren", "Yan Zhuang", "Xiaokang Ye", "Lingjun Mao", "Xuhong He", "Jianzhi Shen", "Mrinaal Dogra", "Yiming Liang", "Ruixuan Zhang", "Tianai Yue", "Yiqing Yang", "Eric Liu", "Ryan Wu", "Kevin Benavente", "Rajiv Mandya Nagaraju", "Muhammad Faayez", "Xiyan Zhang", "Dhruv Vivek Sharma", "Xianrui Zhong", "Ziqiao Ma", "Tianmin Shu", "Zhiting Hu", "Lianhui Qin"], "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds", "comment": null, "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.", "AI": {"tldr": "SimWorld\u662f\u4e00\u4e2a\u57fa\u4e8eUnreal Engine 5\u6784\u5efa\u7684\u65b0\u578b\u6a21\u62df\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u5728\u4e30\u5bcc\u3001\u903c\u771f\u7684\u73af\u5883\u4e2d\u5f00\u53d1\u548c\u8bc4\u4f30LLM/VLM\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u62df\u5668\u5728\u7269\u7406\u548c\u793e\u4f1a\u73af\u5883\u6a21\u62df\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dLLM/VLM\u667a\u80fd\u4f53\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7b49\u9886\u57df\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u590d\u6742\u7269\u7406\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u56f0\u96be\u3002\u73b0\u6709\u4e16\u754c\u6a21\u62df\u5668\u5b58\u5728\u73af\u5883\u6709\u9650\u3001\u7269\u7406\u7b80\u5316\u3001\u7f3a\u4e4f\u5bf9LLM/VLM\u667a\u80fd\u4f53\u539f\u751f\u652f\u6301\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5f00\u53d1\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u751f\u5b58\u548c\u53d1\u5c55\u7684\u667a\u80fd\u4f53\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8eUnreal Engine 5\u6784\u5efaSimWorld\u6a21\u62df\u5668\uff0c\u63d0\u4f9b\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a1\uff09\u903c\u771f\u3001\u5f00\u653e\u7684\u4e16\u754c\u6a21\u62df\uff0c\u5305\u62ec\u51c6\u786e\u7684\u7269\u7406\u548c\u793e\u4f1a\u52a8\u6001\u4ee5\u53ca\u8bed\u8a00\u9a71\u52a8\u7684\u7a0b\u5e8f\u5316\u73af\u5883\u751f\u6210\uff1b2\uff09\u4e3aLLM/VLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e30\u5bcc\u7684\u63a5\u53e3\uff0c\u652f\u6301\u591a\u6a21\u6001\u4e16\u754c\u8f93\u5165\u548c\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\uff1b3\uff09\u591a\u6837\u4e14\u53ef\u6269\u5c55\u7684\u7269\u7406\u548c\u793e\u4f1a\u63a8\u7406\u573a\u666f\uff0c\u7528\u6237\u53ef\u8f7b\u677e\u5b9a\u5236\u3002", "result": "\u901a\u8fc7\u5728\u6d89\u53ca\u6218\u7565\u5408\u4f5c\u4e0e\u7ade\u4e89\u7684\u957f\u671f\u591a\u667a\u80fd\u4f53\u914d\u9001\u4efb\u52a1\u4e2d\u90e8\u7f72\u524d\u6cbfLLM\u667a\u80fd\u4f53\uff08\u5982GPT-4o\u3001Gemini-2.5-Flash\u7b49\uff09\uff0c\u5c55\u793a\u4e86SimWorld\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u63a8\u7406\u6a21\u5f0f\u548c\u5c40\u9650\u6027\u5dee\u5f02\u3002", "conclusion": "SimWorld\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u6709\u671b\u6210\u4e3a\u8de8\u5b66\u79d1\u63a8\u8fdb\u73b0\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u667a\u80fd\u7684\u57fa\u7840\u5e73\u53f0\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u7269\u7406\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u8fd0\u4f5c\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2512.00991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00991", "abs": "https://arxiv.org/abs/2512.00991", "authors": ["Nicole Favero", "Francesca Salute", "Daniel Hardt"], "title": "Advancing Academic Chatbots: Evaluation of Non Traditional Outputs", "comment": null, "summary": "Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cdRAG\u68c0\u7d22\u7b56\u7565\uff08Graph RAG\u548cAdvanced RAG\uff09\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u4e86LLM\u751f\u6210\u975e\u4f20\u7edf\u5b66\u672f\u8f93\u51fa\uff08\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u811a\u672c\uff09\u7684\u80fd\u529b\u3002GPT 4o mini\u914d\u5408Advanced RAG\u8868\u73b0\u6700\u4f73\uff0c\u800cGraph RAG\u56e0\u7ed3\u6784\u590d\u6742\u5bfc\u81f4\u66f4\u591a\u5e7b\u89c9\u3002\u4eba\u7c7b\u8bc4\u4f30\u5728\u68c0\u6d4b\u5e03\u5c40\u548c\u98ce\u683c\u7f3a\u9677\u65b9\u9762\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u95ee\u7b54\u6216\u7b80\u77ed\u6458\u8981\u7b49\u6807\u51c6\u4efb\u52a1\uff0c\u672c\u7814\u7a76\u5e0c\u671b\u6269\u5c55\u8bc4\u4f30\u8303\u56f4\uff1a\u4e00\u662f\u6bd4\u8f83\u4e24\u79cdRAG\u68c0\u7d22\u7b56\u7565\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u4e8c\u662f\u8bc4\u4f30LLM\u751f\u6210\u975e\u4f20\u7edf\u5b66\u672f\u8f93\u51fa\uff08\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u811a\u672c\uff09\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528Meta\u7684LLaMA 3 70B\u5f00\u6e90\u6a21\u578b\u548cOpenAI\u7684GPT 4o mini API\u6784\u5efa\u539f\u578b\u7cfb\u7edf\u3002\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u6bd4\u8f83Graph RAG\uff08\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff09\u548cAdvanced RAG\uff08\u6df7\u5408\u5173\u952e\u8bcd-\u8bed\u4e49\u641c\u7d22\uff09\u4e24\u79cd\u68c0\u7d22\u7b56\u7565\u3002\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\uff0811\u4e2a\u8d28\u91cf\u7ef4\u5ea6\uff09\u548cLLM\u8bc4\u4f30\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u3002\u5728\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u6587\u6863\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "GPT 4o mini\u914d\u5408Advanced RAG\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u4ea7\u751f\u6700\u51c6\u786e\u7684\u56de\u7b54\u3002Graph RAG\u6539\u8fdb\u6709\u9650\u4e14\u5bfc\u81f4\u66f4\u591a\u5e7b\u89c9\uff0c\u90e8\u5206\u539f\u56e0\u662f\u5176\u7ed3\u6784\u590d\u6742\u6027\u548c\u624b\u52a8\u8bbe\u7f6e\u3002\u5728\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGPT 4o mini\u518d\u6b21\u8868\u73b0\u6700\u4f73\uff0c\u800cLLaMA 3\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u6709\u6f5c\u529b\u3002\u4eba\u7c7b\u8bc4\u4f30\u8005\u5728\u68c0\u6d4b\u5e03\u5c40\u548c\u98ce\u683c\u7f3a\u9677\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u5bf9\u4e8e\u65b0\u5174\u5b66\u672f\u8f93\u51fa\u7684\u8bc4\u4f30\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u3002Advanced RAG\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGraph RAG\uff0cGPT 4o mini\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002Graph RAG\u7684\u7ed3\u6784\u590d\u6742\u6027\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2512.01089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01089", "abs": "https://arxiv.org/abs/2512.01089", "authors": ["Peter Jansen", "Samiah Hassan", "Pragnya Narasimha"], "title": "CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents", "comment": "8 pages, 3 figures, 2 tables", "summary": "Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.", "AI": {"tldr": "CodeDistiller\u7cfb\u7edf\u81ea\u52a8\u4ece\u5927\u91cf\u79d1\u5b66GitHub\u4ed3\u5e93\u4e2d\u63d0\u53d6\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u9886\u57df\u7279\u5b9a\u4ee3\u7801\u793a\u4f8b\u5e93\uff0c\u589e\u5f3a\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u9a8c\u8bc1\u6709\u6548", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u53d7\u9650\u4e8e\u4ec5\u4ece\u53c2\u6570\u5316\u77e5\u8bc6\u751f\u6210\u53ef\u9760\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u8981\u4e48\u53ea\u80fd\u53d8\u5f02\u5c11\u91cf\u624b\u52a8\u7f16\u5199\u7684\u5b9e\u9a8c\u793a\u4f8b\uff0c\u8981\u4e48\u4ec5\u4f9d\u8d56\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4", "method": "\u5f00\u53d1CodeDistiller\u7cfb\u7edf\uff0c\u81ea\u52a8\u4ece\u5927\u91cf\u79d1\u5b66GitHub\u4ed3\u5e93\u4e2d\u63d0\u53d6\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u9886\u57df\u7279\u5b9a\u4ee3\u7801\u793a\u4f8b\u5e93\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5728250\u4e2a\u6750\u6599\u79d1\u5b66\u4ed3\u5e93\u4e0a\u8fdb\u884c\u9a8c\u8bc1", "result": "\u6700\u4f73\u6a21\u578b\u80fd\u4e3a74%\u7684\u4ed3\u5e93\u751f\u6210\u529f\u80fd\u793a\u4f8b\uff1b\u4f7f\u7528CodeDistiller\u751f\u6210\u7684\u5e93\u589e\u5f3a\u7684ASD\u4ee3\u7406\u6bd4\u4ec5\u4f7f\u7528\u901a\u7528\u6750\u6599\u79d1\u5b66\u4ee3\u7801\u793a\u4f8b\u7684\u4ee3\u7406\u4ea7\u751f\u66f4\u51c6\u786e\u3001\u5b8c\u6574\u548c\u79d1\u5b66\u5408\u7406\u7684\u5b9e\u9a8c", "conclusion": "CodeDistiller\u80fd\u591f\u81ea\u52a8\u6269\u5c55\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u7684\u80fd\u529b\u800c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u663e\u8457\u63d0\u9ad8\u5b9e\u9a8c\u751f\u6210\u7684\u8d28\u91cf\u548c\u79d1\u5b66\u6027", "topic": "code agent"}}
{"id": "2512.00499", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00499", "abs": "https://arxiv.org/abs/2512.00499", "authors": ["Yuepeng Sheng", "Yuwei Huang", "Shuman Liu", "Haibo Zhang", "Anxiang Zeng"], "title": "ESPO: Entropy Importance Sampling Policy Optimization", "comment": null, "summary": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.", "AI": {"tldr": "ESPO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u91cd\u8981\u6027\u91c7\u6837\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u9a71\u52a8\u7684\u5206\u7ec4\u548c\u81ea\u9002\u5e94\u88c1\u526a\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f18\u5316\u7c92\u5ea6\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7fa4\u4f53\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GRPO\u548cGSPO\uff09\u5b58\u5728\u4f18\u5316\u7c92\u5ea6\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\u3002GSPO\u867d\u7136\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u6574\u4f53\u5904\u7406\u5e8f\u5217\u7684\u65b9\u5f0f\u5f15\u5165\u4e86\u4e25\u91cd\u4f4e\u6548\u6027\uff1a\u4fdd\u5b88\u7684\u88c1\u526a\u673a\u5236\u4f1a\u4e0d\u52a0\u533a\u5206\u5730\u4e22\u5f03\u6709\u6548\u8bad\u7ec3\u6837\u672c\uff08\u68af\u5ea6\u5229\u7528\u4e0d\u8db3\uff09\uff0c\u7edf\u4e00\u7684\u4fe1\u7528\u5206\u914d\u65e0\u6cd5\u6355\u6349\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u7684\u5f02\u8d28\u6027\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u71b5\u91cd\u8981\u6027\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08ESPO\uff09\u6846\u67b6\uff0c\u5c06\u5e8f\u5217\u57fa\u4e8e\u9884\u6d4b\u71b5\u5206\u89e3\u4e3a\u7ec4\uff0c\u5b9e\u73b0\uff1a\uff081\uff09\u71b5\u9a71\u52a8\u7684\u91cd\u8981\u6027\u91c7\u6837\u4ee5\u6355\u6349\u5e8f\u5217\u5185\u5f02\u8d28\u6027\uff1b\uff082\uff09\u71b5\u81ea\u9002\u5e94\u88c1\u526a\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u5206\u914d\u4fe1\u4efb\u533a\u57df\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cESPO\u4e0d\u4ec5\u52a0\u901f\u4e86\u6536\u655b\uff0c\u8fd8\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684HMMT\u57fa\u51c6\u4e0a\u5c06\u51c6\u786e\u7387\u4ece4.4%\u663e\u8457\u63d0\u5347\u81f313.13%\u3002", "conclusion": "ESPO\u6210\u529f\u8c03\u548c\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u901a\u8fc7\u71b5\u9a71\u52a8\u7684\u5206\u7ec4\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u68af\u5ea6\u5229\u7528\u4e0d\u8db3\u548c\u4fe1\u7528\u5206\u914d\u4e0d\u5747\u95ee\u9898\uff0c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01174", "abs": "https://arxiv.org/abs/2512.01174", "authors": ["Hyunjun Kim", "Sooyoung Ryu"], "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks", "comment": "AAAI 2026 TrustAgent Workshop", "summary": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench", "AI": {"tldr": "DrawingBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53LLM\u53ef\u4fe1\u5ea6\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff08\u751f\u6210\u4f4e\u7ea7GUI\u64cd\u4f5c\u5e8f\u5217\uff09\u8fdb\u884c\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc4\u4f30\uff0c\u5305\u542b250\u4e2a\u591a\u6837\u5316\u63d0\u793a\u3001\u786e\u5b9a\u6027\u8bc4\u4f30\u6307\u6807\u548c\u5916\u90e8\u76d1\u7763\u673a\u5236\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u7cfb\u7edf\u65e5\u76ca\u81ea\u4e3b\u8fd0\u884c\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u5efa\u7acb\u4fe1\u4efb\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u8bc4\u4f30\u667a\u80fd\u4f53\u884c\u4e3a\u53ef\u9760\u6027\u7684\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "method": "\u63d0\u51fa\u4e86DrawingBench\u9a8c\u8bc1\u6846\u67b6\uff1a\u5305\u542b250\u4e2a\u591a\u6837\u5316\u63d0\u793a\uff0820\u4e2a\u7c7b\u522b\uff0c4\u4e2a\u96be\u5ea6\u7ea7\u522b\uff09\uff0c8\u4e2a\u5ba2\u89c2\u8bc4\u4f30\u6807\u51c6\uff0c\u786e\u5b9a\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u53ca\u901a\u8fc7\u591a\u8f6e\u53cd\u9988\u5b9e\u73b0\u7684\u5916\u90e8\u76d1\u7763\u673a\u5236\uff0c\u5141\u8bb8\u4eba\u7c7b\u63a7\u5236\u667a\u80fd\u4f53\u4f18\u5316\u3002", "result": "\u8bc4\u4f304\u4e2aSOTA LLM\uff08Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash\uff09\u76841000\u6b21\u6d4b\u8bd5\u663e\u793a\uff1a\u6a21\u578b\u5728\u6709\u7ed3\u6784\u5316\u5916\u90e8\u53cd\u9988\u65f6\u8fbe\u523092.8%\u5b8c\u7f8e\u6027\u80fd\uff08\u5e73\u5747\u63d0\u53473.2%\uff0c\u590d\u6742\u573a\u666f\u6700\u9ad8\u63d0\u534732.8%\uff09\uff0c\u4f46\u5728\u5de5\u5177\u72b6\u6001\u7ba1\u7406\u548c\u957f\u65f6\u89c4\u5212\u4e2d\u51fa\u73b0\u7cfb\u7edf\u6027\u9519\u8bef\u6a21\u5f0f\u3002\u89c4\u8303\u6e05\u6670\u5ea6\u6bd4\u4efb\u52a1\u590d\u6742\u5ea6\u66f4\u91cd\u8981\u2014\u2014\u5f53\u7ed9\u51fa\u660e\u786e\u3001\u53ef\u9a8c\u8bc1\u6807\u51c6\u65f6\uff0c\u6a21\u578b\u8fbe\u5230100%\u5b8c\u7f8e\u6027\u80fd\u3002", "conclusion": "\u900f\u660e\u8bc4\u4f30\u6846\u67b6\u53ef\u4ee5\u5efa\u7acb\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4fe1\u4efb\uff0c\u5916\u90e8\u76d1\u7763\u5728\u5f15\u5bfc\u667a\u80fd\u4f53\u884c\u4e3a\u65b9\u9762\u6bd4\u81ea\u6211\u7ea0\u6b63\u66f4\u53ef\u9760\u3002\u5f00\u6e90\u6846\u67b6\u4e3a\u53ef\u4fe1\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6a21\u677f\u3002", "topic": "agent analysis"}}
{"id": "2512.00545", "categories": ["cs.LG", "cs.SI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00545", "abs": "https://arxiv.org/abs/2512.00545", "authors": ["Akrati Saxena", "Harshith Kumar Yadav", "Bart Rutten", "Shashi Shekhar Jha"], "title": "DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning", "comment": null, "summary": "The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u516c\u5e73\u6027\u5f71\u54cd\u529b\u6700\u5927\u5316\u65b9\u6cd5DQ4FairIM\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6700\u5c0f\u516c\u5e73\u76ee\u6807\u786e\u4fdd\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u5f71\u54cd\u529b\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u597d\u7684\u516c\u5e73-\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u73b0\u5b9e\u793e\u4ea4\u7f51\u7edc\u5b58\u5728\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\uff0c\u5982\u591a\u6570\u7fa4\u4f53\u4e3b\u5bfc\u548c\u5c11\u6570\u7fa4\u4f53\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u4f20\u7edf\u5f71\u54cd\u529b\u6700\u5927\u5316\u7b97\u6cd5\u4e0d\u8003\u8651\u8fd9\u4e9b\u4e0d\u5e73\u7b49\u4f1a\u5bfc\u81f4\u7ed3\u679c\u504f\u5411\u591a\u6570\u7fa4\u4f53\uff0c\u8fb9\u7f18\u5316\u5c11\u6570\u7fa4\u4f53\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u516c\u5e73\u611f\u77e5\u7684\u5f71\u54cd\u529b\u6700\u5927\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDQ4FairIM\u65b9\u6cd5\uff1a1) \u5c06\u516c\u5e73\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff1b2) \u4f7f\u7528\u6df1\u5ea6Q\u5b66\u4e60\u548cStructure2Vec\u7f51\u7edc\u5d4c\u5165\u76f8\u7ed3\u5408\uff1b3) \u91c7\u7528\u6700\u5927\u5316\u6700\u5c0f\u516c\u5e73\u76ee\u6807\uff0c\u4f18\u5148\u63d0\u5347\u5f71\u54cd\u529b\u6700\u5f31\u7fa4\u4f53\u7684\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u516c\u5e73\u65e0\u5173\u548c\u516c\u5e73\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u516c\u5e73\u6027\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u597d\u7684\u516c\u5e73-\u6027\u80fd\u6743\u8861\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u79cd\u5b50\u9009\u62e9\u7b56\u7565\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u95ee\u9898\u5b9e\u4f8b\uff08\u5982\u7f51\u7edc\u89c4\u6a21\u6216\u79cd\u5b50\u8282\u70b9\u6570\u91cf\u53d8\u5316\uff09\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "DQ4FairIM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u793e\u4ea4\u7f51\u7edc\u5f71\u54cd\u529b\u6700\u5927\u5316\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u786e\u4fdd\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u5f71\u54cd\u529b\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.00553", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00553", "abs": "https://arxiv.org/abs/2512.00553", "authors": ["Bohan Zhang", "Michael Chen", "A. Pavan", "N. V. Vinodchandran", "Lin F. Yang", "Ruosong Wang"], "title": "List Replicable Reinforcement Learning", "comment": null, "summary": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u590d\u5236\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u5217\u8868\u53ef\u590d\u5236\u6027\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86\u4fdd\u8bc1\u591a\u9879\u5f0f\u5217\u8868\u590d\u6742\u5ea6\u7684\u8868\u683cRL\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u5bf9\u8bad\u7ec3\u6761\u4ef6\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u8fd9\u5bfc\u81f4\u4e86\u53ef\u590d\u5236\u6027\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u5f62\u5f0f\u5316\u5730\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5217\u8868\u53ef\u590d\u5236\u6027\u6982\u5ff5\u6765\u91cf\u5316RL\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u5217\u8868\u53ef\u590d\u5236\u6027\u7684\u5f31\u5f62\u5f0f\u548c\u5f3a\u5f62\u5f0f\u5b9a\u4e49\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8ePAC RL\u6846\u67b6\u7684\u8868\u683cRL\u7b97\u6cd5\u3002\u5173\u952e\u521b\u65b0\u5305\u62ec\uff1a(1) \u57fa\u4e8e\u8bcd\u5178\u987a\u5e8f\u5728\u8fd1\u4f18\u9009\u62e9\u4e2d\u9009\u53d6\u52a8\u4f5c\u7684\u89c4\u5212\u7b56\u7565\uff0c(2) \u5728\u968f\u673a\u73af\u5883\u4e2d\u6d4b\u8bd5\u72b6\u6001\u53ef\u8fbe\u6027\u540c\u65f6\u4fdd\u6301\u53ef\u590d\u5236\u6027\u7684\u673a\u5236\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6240\u63d0\u7b97\u6cd5\u80fd\u4fdd\u8bc1\u5217\u8868\u590d\u6742\u5ea6\u5728\u72b6\u6001\u6570\u3001\u52a8\u4f5c\u6570\u548c\u65f6\u95f4\u8303\u56f4\u957f\u5ea6\u4e0a\u662f\u591a\u9879\u5f0f\u7684\uff0c\u5b9e\u73b0\u4e86\u5f31\u5f62\u5f0f\u548c\u5f3a\u5f62\u5f0f\u7684\u5217\u8868\u53ef\u590d\u5236\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u89c4\u5212\u7b56\u7565\u80fd\u589e\u5f3a\u5b9e\u9645RL\u6846\u67b6\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5217\u8868\u53ef\u590d\u5236\u6027\u4e3a\u5f62\u5f0f\u5316\u89e3\u51b3RL\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u590d\u5236\u6027\uff0c\u4e3a\u89e3\u51b3RL\u7b97\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01311", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01311", "abs": "https://arxiv.org/abs/2512.01311", "authors": ["Shinji Mai", "Yunpeng Zhai", "Ziqian Chen", "Cheng Chen", "Anni Zou", "Shuchang Tao", "Zhaoyang Liu", "Bolin Ding"], "title": "CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL", "comment": null, "summary": "Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.", "AI": {"tldr": "CuES\u6846\u67b6\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u548c\u73af\u5883\u57fa\u7840\u7684\u4efb\u52a1\u751f\u6210\uff0c\u89e3\u51b3\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u4efb\u52a1\u7a00\u7f3a\u95ee\u9898\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u4efb\u52a1\u5373\u53ef\u81ea\u4e3b\u751f\u6210\u591a\u6837\u5316\u53ef\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u590d\u6742\u5de5\u5177\u589e\u5f3a\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u4efb\u52a1\u7a00\u7f3a\u95ee\u9898\uff0c\u5373\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bad\u7ec3\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u4efb\u52a1\u96c6\uff0c\u8fd9\u5728\u5de5\u5177\u8bed\u4e49\u548c\u529f\u80fd\u672a\u77e5\u7684\u65b0\u73af\u5883\u4e2d\u5931\u6548\uff0c\u6210\u4e3a\u6269\u5c55\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51faCuES\u6846\u67b6\uff1a\u597d\u5947\u5fc3\u9a71\u52a8\u548c\u73af\u5883\u57fa\u7840\u7684\u4efb\u52a1\u5408\u6210\u65b9\u6cd5\u3002\u901a\u8fc7\u5185\u5728\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\uff0c\u4ece\u73af\u5883\u7ed3\u6784\u548c\u529f\u80fd\u4e2d\u62bd\u8c61\u4ea4\u4e92\u6a21\u5f0f\u4e3a\u53ef\u91cd\u7528\u4efb\u52a1\u6a21\u5f0f\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u4e0a\u800c\u4e0b\u6307\u5bfc\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u8d28\u91cf\u63a7\u5236\u8fdb\u884c\u7cbe\u70bc\uff0c\u65e0\u9700\u624b\u5de5\u79cd\u5b50\u6216\u5916\u90e8\u8bed\u6599\u3002", "result": "\u5728AppWorld\u3001BFCL\u548cWebShop\u4e09\u4e2a\u4ee3\u8868\u6027\u73af\u5883\u4e2d\uff0cCuES\u751f\u6210\u7684\u4efb\u52a1\u5206\u5e03\u5728\u591a\u6837\u6027\u548c\u53ef\u6267\u884c\u6027\u65b9\u9762\u5339\u914d\u6216\u8d85\u8d8a\u624b\u52a8\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5e26\u6765\u663e\u8457\u7684\u4e0b\u6e38\u7b56\u7565\u6539\u8fdb\u3002", "conclusion": "\u597d\u5947\u5fc3\u9a71\u52a8\u3001\u73af\u5883\u57fa\u7840\u7684\u4efb\u52a1\u751f\u6210\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4f7f\u667a\u80fd\u4f53\u4e0d\u4ec5\u80fd\u5b66\u4e60\u5982\u4f55\u884c\u52a8\uff0c\u8fd8\u80fd\u5b66\u4e60\u5b66\u4e60\u4ec0\u4e48\uff0c\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u7a00\u7f3a\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01282", "abs": "https://arxiv.org/abs/2512.01282", "authors": ["Jiahao Yuan", "Zhiqing Cui", "Hanqing Wang", "Yuansheng Gao", "Yucheng Zhou", "Usman Naseem"], "title": "Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning", "comment": null, "summary": "As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.", "AI": {"tldr": "KardiaBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7528\u6237\u57fa\u7840\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b178,080\u4e2a\u95ee\u7b54\u5bf9\uff0c\u8986\u76d622,080\u4e2a\u591a\u8f6e\u5bf9\u8bdd\uff0c\u57fa\u4e8e671\u4e2a\u771f\u5b9e\u4e16\u754c\u7528\u6237\u6863\u6848\u3002Kardia-R1\u6846\u67b6\u4f7f\u7528\u53ef\u89e3\u91ca\u7684Rubric-ERL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u5206\u6807\u51c6\u5956\u52b1\u6765\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u79fb\u60c5\u8ba4\u77e5\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1) \u4f9d\u8d56\u7f3a\u4e4f\u6301\u4e45\u7528\u6237\u8eab\u4efd\u7684\u60c5\u5883\u4e2d\u5fc3\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6355\u6349\u4e2a\u6027\u5316\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\uff1b2) \u4f9d\u8d56\u4e0d\u900f\u660e\u3001\u7c97\u7cd9\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u963b\u788d\u53ef\u9a8c\u8bc1\u79fb\u60c5\u63a8\u7406\u7684\u53d1\u5c55\u3002\u9700\u8981\u8d85\u8d8a\u8868\u9762\u79fb\u60c5\uff0c\u5b9e\u73b0\u8eab\u4efd\u611f\u77e5\u7684\u60c5\u611f\u63a8\u7406\u3002", "method": "1) \u6784\u5efaKardiaBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u6a21\u578b\u5728\u73af\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bc4\u5206\u6807\u51c6\u6307\u5bfc\u7684\u7ec6\u5316\u786e\u4fdd\u5fc3\u7406\u5408\u7406\u6027\u548c\u89d2\u8272\u4e00\u81f4\u6027\uff1b2) \u63d0\u51faKardia-R1\u6846\u67b6\uff0c\u4f7f\u7528Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u89e3\u91ca\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u5206\u6807\u51c6\u5956\u52b1\uff0c\u7d27\u5bc6\u8026\u5408\u7528\u6237\u7406\u89e3\u3001\u60c5\u611f\u63a8\u7406\u548c\u652f\u6301\u6027\u56de\u5e94\u751f\u6210\u3002", "result": "\u5728\u56db\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cKardia-R1\u5728\u60c5\u611f\u51c6\u786e\u6027\u3001\u79fb\u60c5\u6027\u3001\u76f8\u5173\u6027\u3001\u89d2\u8272\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "KardiaBench\u57fa\u51c6\u6d4b\u8bd5\u548cKardia-R1\u6846\u67b6\u4e3a\u89e3\u51b3\u5bf9\u8bdd\u4ee3\u7406\u7684\u8eab\u4efd\u611f\u77e5\u60c5\u611f\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u6807\u51c6\u5956\u52b1\u5b9e\u73b0\u4e86\u9010\u6b65\u79fb\u60c5\u8ba4\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u60c5\u5bf9\u8bdd\u7684\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2512.01420", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01420", "abs": "https://arxiv.org/abs/2512.01420", "authors": ["Yaxuan Wang", "Quan Liu", "Zhenting Wang", "Zichao Li", "Wei Wei", "Yang Liu", "Yujia Bao"], "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models", "comment": null, "summary": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.", "AI": {"tldr": "PromptBridge\uff1a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u63d0\u793a\u6620\u5c04\u89e3\u51b3\u6a21\u578b\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u63d0\u793a\u5728\u4e0d\u540cLLM\u95f4\u7684\u6709\u6548\u8fc1\u79fb", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u80fd\u529b\u3001\u6210\u672c\u3001\u90e8\u7f72\u7ea6\u675f\u548c\u9690\u79c1\u7b49\u56e0\u7d20\uff0c\u9700\u8981\u9891\u7e41\u5207\u6362\u4e0d\u540c\u7684LLM\u6a21\u578b\u3002\u4f46\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u4f18\u5316\u7684\u63d0\u793a\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u79cd\"\u6a21\u578b\u6f02\u79fb\"\u73b0\u8c61\u4e25\u91cd\u5f71\u54cd\u4e86\u63d0\u793a\u7684\u53ef\u79fb\u690d\u6027\u548c\u7cfb\u7edf\u7ef4\u62a4\u6210\u672c\u3002", "method": "\u63d0\u51faPromptBridge\u6846\u67b6\uff1a1) \u4f7f\u7528\u6a21\u578b\u81ea\u9002\u5e94\u53cd\u5c04\u63d0\u793a\u6f14\u5316(MAP-RPE)\u901a\u8fc7\u8fed\u4ee3\u53cd\u5c04\u4f18\u5316\u548c\u5b9a\u91cf\u8bc4\u4f30\u83b7\u5f97\u4efb\u52a1\u548c\u6a21\u578b\u7279\u5b9a\u7684\u6700\u4f18\u63d0\u793a\uff1b2) \u5229\u7528\u6e90\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u7684\u6821\u51c6\u63d0\u793a\u5bf9\u5b66\u4e60\u8de8\u6a21\u578b\u63d0\u793a\u6620\u5c04\uff1b3) \u6d4b\u8bd5\u65f6\uff0c\u7ed9\u5b9a\u6e90\u6a21\u578b\u63d0\u793a\uff0c\u76f4\u63a5\u751f\u6210\u76ee\u6807\u6a21\u578b\u7684\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPromptBridge\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8fc1\u79fb\u5de5\u4f5c\u91cf\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u5bf9\u9f50\u4efb\u52a1\u8fdb\u884c\u6821\u51c6\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6bcf\u4efb\u52a1\u6216\u6bcf\u6a21\u578b\u91cd\u65b0\u4f18\u5316\u3002", "conclusion": "PromptBridge\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63d0\u793a\u5728\u4e0d\u540cLLM\u95f4\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u5207\u6362\u6210\u672c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6a21\u578b\u5207\u6362\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.01434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01434", "abs": "https://arxiv.org/abs/2512.01434", "authors": ["Daull Xavier", "Patrice Bellot", "Emmanuel Bruno", "Vincent Martin", "Elisabeth Murisasco"], "title": "A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building", "comment": null, "summary": "We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.", "AI": {"tldr": "CollabToolBuilder\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u5728\u73af\u6307\u5bfc\u8fed\u4ee3\u5b66\u4e60\u4e3a\u76ee\u6807\u521b\u5efa\u5de5\u5177\uff0c\u6700\u5c0f\u5316\u4efb\u52a1\u9002\u5e94\u548c\u4eba\u5de5\u53cd\u9988\u6536\u96c6\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u8fed\u4ee3\u95ee\u9898\uff08\u5982\u79d1\u5b66\u6587\u6863\u751f\u6210\uff09\u4e2d\u5de5\u5177\u521b\u5efa\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u51cf\u5c11\u4efb\u52a1/\u9886\u57df\u9002\u5e94\u7684\u4eba\u5de5\u52aa\u529b\u548c\u53cd\u9988\u6536\u96c6\u65f6\u95f4\u3002", "method": "\u91c7\u7528\u56db\u667a\u80fd\u4f53\u67b6\u6784\uff08\u6559\u7ec3\u3001\u7f16\u7801\u5458\u3001\u6279\u8bc4\u8005\u3001\u8d44\u672c\u5316\u8005\uff09\uff0c\u901a\u8fc7\u5f3a\u5316\u52a8\u6001\u63d0\u793a\u548c\u7cfb\u7edf\u6027\u4eba\u5de5\u53cd\u9988\u96c6\u6210\u6765\u751f\u6210\u548c\u9a8c\u8bc1\u5de5\u5177\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u4e13\u5bb6\u5728\u73af\u63a7\u5236\u548c\u53ef\u91cd\u7528\u5de5\u5177\u8d44\u672c\u5316\u3002", "result": "\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u5c55\u793a\u4e86\u7cfb\u7edf\u5728\u751f\u6210\u524d\u6cbf\u7814\u7a76\u8bba\u6587\u6216\u4e13\u5229\u7b49\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u8fed\u4ee3\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "CollabToolBuilder\u662f\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u3001\u4eba\u7c7b\u63a7\u5236\u548c\u5de5\u5177\u91cd\u7528\uff0c\u4e3a\u590d\u6742\u8fed\u4ee3\u95ee\u9898\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.00672", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00672", "abs": "https://arxiv.org/abs/2512.00672", "authors": ["Yaswanth Chittepu", "Raghavendra Addanki", "Tung Mai", "Anup Rao", "Branislav Kveton"], "title": "ML-Tool-Bench: Tool-Augmented Planning for ML Tasks", "comment": null, "summary": "The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5de5\u5177\u589e\u5f3a\u578b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b61\u4e2a\u4e13\u4e1a\u5de5\u5177\u548c15\u4e2aKaggle\u8868\u683cML\u6311\u6218\uff0c\u901a\u8fc7\u5185\u5b58\u547d\u540d\u5bf9\u8c61\u7ba1\u7406\u548c\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u4efb\u52a1\u5de5\u5177\u9009\u62e9\u6216\u5de5\u5177\u8c03\u7528\u53c2\u6570\u63d0\u53d6\uff0c\u672a\u80fd\u8bc4\u4f30ML\u4ee3\u7406\u6240\u9700\u7684\u590d\u6742\u89c4\u5212\u80fd\u529b\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5de5\u5177\u589e\u5f3a\u578bML\u4ee3\u7406\u5728\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002", "method": "1) \u5f15\u5165\u5305\u542b61\u4e2a\u4e13\u4e1a\u5de5\u5177\u548c15\u4e2aKaggle\u8868\u683cML\u6311\u6218\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b2) \u91c7\u7528\u5185\u5b58\u547d\u540d\u5bf9\u8c61\u7ba1\u7406\uff0c\u5141\u8bb8\u4ee3\u7406\u7075\u6d3b\u547d\u540d\u3001\u4fdd\u5b58\u548c\u68c0\u7d22\u4e2d\u95f4\u7ed3\u679c\uff1b3) \u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff1a\u4f7f\u7528\u7ed3\u6784\u5316\u6587\u672c\u53cd\u9988\u7684\u786e\u5b9a\u6027\u5956\u52b1\u5851\u9020\uff0c\u4ee5\u53ca\u5c06\u539f\u59cb\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\u3002", "result": "\u6807\u51c6ReAct\u65b9\u6cd5\u5728\u590d\u6742ML\u7ba1\u9053\u4e2d\u96be\u4ee5\u751f\u6210\u6709\u6548\u5de5\u5177\u5e8f\u5217\uff0c\u57fa\u4e8eLLM\u8bc4\u4f30\u7684\u6811\u641c\u7d22\u65b9\u6cd5\u56e0\u72b6\u6001\u8bc4\u5206\u4e0d\u4e00\u81f4\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u7528GPT-4o\u65f6\uff0c\u76f8\u6bd4ReAct\u5728Kaggle\u6311\u6218\u4e2d\u4f4d\u6570\u4e0a\u63d0\u5347\u4e8616.52\u4e2a\u767e\u5206\u4f4d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u589e\u5f3a\u89c4\u5212\u578bML\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u6784\u5316\u53cd\u9988\u548c\u4efb\u52a1\u5206\u89e3\u53ef\u4ee5\u663e\u8457\u6539\u5584\u4ee3\u7406\u5728\u590d\u6742\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2512.01616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01616", "abs": "https://arxiv.org/abs/2512.01616", "authors": ["Chainesh Gautam", "Raghuram Bharadwaj Diddigi"], "title": "CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning", "comment": "4 pages, 2 figures, accepted as a extended abstract at RLDM 2025", "summary": "Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCLIP\u539f\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u8bed\u8a00\u6307\u4ee4\u548c\u7b56\u7565\u5d4c\u5165\u7684\u8868\u5f81\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u9ad8\u6548\u8fc1\u79fb", "motivation": "\u5f53\u524d\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u540c\u4e00\u73af\u5883\u4e2d\u89e3\u51b3\u591a\u4e2a\u4efb\u52a1\u7684\u667a\u80fd\u4f53\uff0c\u7279\u522b\u662f\u5f53\u8fd9\u4e9b\u4efb\u52a1\u4e0e\u8bed\u8a00\u81ea\u7136\u5173\u8054\u65f6\u3002\u9700\u8981\u5efa\u7acb\u9ad8\u6548\u7684\u8de8\u4efb\u52a1\u8fc1\u79fb\u7ba1\u9053\u3002", "method": "\u53d7\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2dCLIP\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u5c06\"\u8868\u793a\u76f8\u540c\u6982\u5ff5\u7684\u4e24\u4e2a\u6a21\u6001\u5e94\u6709\u76f8\u4f3c\u8868\u5f81\"\u7684\u601d\u60f3\u6269\u5c55\u5230\u5f3a\u5316\u5b66\u4e60\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u8bed\u8a00-\u7b56\u7565\u5bf9\u7ec4\u5408\uff0c\u521b\u5efa\u81ea\u7136\u8bed\u8a00\u548c\u7b56\u7565\u5d4c\u5165\u7684\u7edf\u4e00\u8868\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u8de8\u4efb\u52a1\u5feb\u901f\u8fc1\u79fb\u65b9\u9762\u5177\u6709\u5b9e\u7528\u6027", "conclusion": "\u901a\u8fc7\u5c06CLIP\u539f\u7406\u6269\u5c55\u5230\u5f3a\u5316\u5b66\u4e60\u9886\u57df\uff0c\u5efa\u7acb\u4e86\u8bed\u8a00\u6307\u4ee4\u548c\u7b56\u7565\u7684\u7edf\u4e00\u8868\u5f81\u7a7a\u95f4\uff0c\u4e3a\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8de8\u4efb\u52a1\u8fc1\u79fb\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2512.01661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01661", "abs": "https://arxiv.org/abs/2512.01661", "authors": ["Dengyun Peng", "Qiguang Chen", "Bofei Liu", "Jiannan Guan", "Libo Qin", "Zheng Yan", "Jinhao Liu", "Jianshu Zhang", "Wanxiang Che"], "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems", "comment": "preprint", "summary": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUnsolvableQA\u6570\u636e\u96c6\u548cUnsolvableRL\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u533a\u5206\u53ef\u89e3\u4e0e\u4e0d\u53ef\u89e3\u95ee\u9898\uff0c\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u548c\u5e7b\u89c9\u3002", "motivation": "\u5f53\u524dLLM\u96be\u4ee5\u533a\u5206\u5ba2\u89c2\u4e0d\u53ef\u89e3\u95ee\u9898\uff08\u5185\u5728\u77db\u76fe\uff09\u548c\u4e3b\u89c2\u80fd\u529b\u9650\u5236\uff08\u8d85\u51fa\u6a21\u578b\u80fd\u529b\uff09\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u7684\u95ee\u9898\u8bc6\u522b\u80fd\u529b\u3002", "method": "1) \u6784\u5efaUnsolvableQA\u6570\u636e\u96c6\uff1a\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u903b\u8f91\u8c1c\u9898\u548c\"\u53cd\u5411\u6784\u9020\"\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u94fe\u4e2d\u6ce8\u5165\u77db\u76fe\uff1b2) \u63d0\u51faUnsolvableRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u5305\u542b\u51c6\u786e\u6027\u3001\u4e0d\u53ef\u89e3\u6027\u548c\u96be\u5ea6\u4e09\u4e2a\u5956\u52b1\u7ec4\u4ef6\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u4e0d\u53ef\u89e3\u6027\u68c0\u6d4b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002\u53d1\u73b0\u4e86\"\u80fd\u529b\u5d29\u6e83\"\u73b0\u8c61\uff0c\u8bc1\u660e\u660e\u786e\u63a5\u89e6\u4e0d\u53ef\u89e3\u6570\u636e\u5bf9\u9632\u6b62\u7cfb\u7edf\u8fc7\u5ea6\u81ea\u4fe1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u901a\u8fc7UnsolvableQA\u6570\u636e\u96c6\u548cUnsolvableRL\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u8bad\u7ec3LLM\u8bc6\u522b\u4e0d\u53ef\u89e3\u95ee\u9898\uff0c\u51cf\u5c11\u5e7b\u89c9\u548c\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.01710", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.01710", "abs": "https://arxiv.org/abs/2512.01710", "authors": ["Stefano Zeppieri"], "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6df7\u5408\u8bb0\u5fc6\u589e\u5f3a\u751f\u6210\uff08MMAG\uff09\u6a21\u5f0f\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u6784\u5efa\u4e94\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u957f\u5bf9\u8bdd\u4e2d\u76f8\u5173\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u7eed\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5355\u6b21\u63d0\u793a\u4e2d\u80fd\u751f\u6210\u8fde\u8d2f\u6587\u672c\uff0c\u4f46\u5728\u957f\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u7ef4\u6301\u76f8\u5173\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u7eed\u6027\u3002\u4eba\u7c7b\u6c9f\u901a\u4f9d\u8d56\u591a\u79cd\u8bb0\u5fc6\u5f62\u5f0f\uff0c\u9700\u8981\u6784\u5efa\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6846\u67b6\u6765\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u8bb0\u5fc6\u589e\u5f3a\u751f\u6210\uff08MMAG\uff09\u6a21\u5f0f\uff0c\u5c06\u8bb0\u5fc6\u7ec4\u7ec7\u4e3a\u4e94\u5c42\uff1a\u5bf9\u8bdd\u8bb0\u5fc6\u3001\u957f\u671f\u7528\u6237\u8bb0\u5fc6\u3001\u60c5\u666f\u4e0e\u4e8b\u4ef6\u5173\u8054\u8bb0\u5fc6\u3001\u611f\u77e5\u4e0e\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u3001\u77ed\u671f\u5de5\u4f5c\u8bb0\u5fc6\u3002\u501f\u9274\u8ba4\u77e5\u5fc3\u7406\u5b66\u539f\u7406\uff0c\u5c06\u8fd9\u4e9b\u5c42\u6b21\u6620\u5c04\u5230\u6280\u672f\u7ec4\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u534f\u8c03\u3001\u4f18\u5148\u7ea7\u548c\u51b2\u7a81\u89e3\u51b3\u7b56\u7565\u3002", "result": "\u5728Heero\u5bf9\u8bdd\u667a\u80fd\u4f53\u4e2d\u5b9e\u73b0\u4e86\u8be5\u6846\u67b6\uff0c\u52a0\u5bc6\u7684\u957f\u671f\u7528\u6237\u6863\u6848\u548c\u5bf9\u8bdd\u5386\u53f2\u5df2\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u7559\u5b58\u7387\u3002\u5c55\u793a\u4e86\u8bb0\u5fc6\u5206\u5c42\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "MMAG\u4e3a\u6784\u5efa\u8bb0\u5fc6\u4e30\u5bcc\u7684\u8bed\u8a00\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u4f7f\u5176\u66f4\u52a0\u8fde\u8d2f\u3001\u4e3b\u52a8\u4e14\u7b26\u5408\u4eba\u7c7b\u9700\u6c42\u3002\u540c\u65f6\u8ba8\u8bba\u4e86\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u9690\u79c1\u548c\u5ef6\u8fdf\u7b49\u5b9e\u73b0\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2512.01786", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01786", "abs": "https://arxiv.org/abs/2512.01786", "authors": ["Xiaochuan Li", "Ke Wang", "Girija Gouda", "Shubham Choudhary", "Yaqun Wang", "Linwei Hu", "Joel Vaughan", "Freddy Lecue"], "title": "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems", "comment": "66 pages, 22 figures, 37 tables", "summary": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.", "AI": {"tldr": "\u63d0\u51faLLM Jury-on-Demand\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u53ef\u9760\u6027\u9884\u6d4b\u5668\u52a8\u6001\u9009\u62e9\u6700\u53ef\u9760\u7684LLM\u8bc4\u59d4\u7ec4\u6210\u966a\u5ba1\u56e2\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bc4\u4f30\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740LLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u589e\u591a\uff0c\u9700\u8981\u65e2\u53ef\u6269\u5c55\u53c8\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u4eba\u5de5\u8bc4\u4f30\u53ef\u9760\u4f46\u6210\u672c\u9ad8\uff0c\u5355\u4e00LLM\u8bc4\u59d4\u6709\u504f\u89c1\uff0c\u9759\u6001\u966a\u5ba1\u56e2\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "\u8bad\u7ec3\u4e00\u7ec4\u53ef\u9760\u6027\u9884\u6d4b\u5668\u6765\u8bc4\u4f30LLM\u8bc4\u59d4\u4f55\u65f6\u4f1a\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u610f\u89c1\u4e00\u81f4\uff0c\u5229\u7528token\u5206\u5e03\u3001\u5d4c\u5165\u548c\u7ed3\u6784\u5316\u8f93\u5165\u7279\u5f81\u3002\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u52a8\u6001\u9009\u62e9\u6700\u53ef\u9760\u7684\u8bc4\u59d4\u7ec4\u6210\u6700\u4f18\u966a\u5ba1\u56e2\uff0c\u5e76\u4ee5\u53ef\u9760\u6027\u4e3a\u6743\u91cd\u805a\u5408\u8bc4\u5206\u3002", "result": "\u5728\u6458\u8981\u548cRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u52a8\u6001\u966a\u5ba1\u56e2\u7cfb\u7edf\u6bd4\u5355\u4e00\u8bc4\u59d4\u548c\u9759\u6001\u966a\u5ba1\u56e2\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u966a\u5ba1\u56e2\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u7684LLM\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u524d\u666f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2512.01725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01725", "abs": "https://arxiv.org/abs/2512.01725", "authors": ["Jiannan Guan", "Qiguang Chen", "Libo Qin", "Dengyun Peng", "Jinhao Liu", "Liangyu Huo", "Jian Xie", "Wanxiang Che"], "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76LLMs\u5728\u591a\u89e3\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u63d0\u51faMuSoBench\u57fa\u51c6\uff0c\u53d1\u73b0Long-CoT\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u8ba4\u77e5\u521a\u6027\u5047\u8bf4\u89e3\u91ca\u539f\u56e0\u3002", "motivation": "LLMs\u5728\u9700\u8981\u5355\u4e00\u6b63\u786e\u7b54\u6848\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u751f\u6210\u5168\u9762\u591a\u6837\u7b54\u6848\u7684\u591a\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\"\u63a8\u7406\u8fc7\u5ea6\u81ea\u4fe1\"\u5bfc\u81f4\u7684\uff0c\u5373LLMs\u503e\u5411\u4e8e\u5bf9\u4e0d\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\u96c6\u8868\u8fbe\u8fc7\u5ea6\u7684\u786e\u5b9a\u6027\u3002", "method": "1) \u5f15\u5165MuSoBench\u591a\u89e3\u95ee\u9898\u57fa\u51c6\uff1b2) \u5b9e\u9a8c\u6bd4\u8f83Short-CoT\u548cLong-CoT\u63d0\u793a\u8303\u5f0f\uff1b3) \u63d0\u51fa\u8ba4\u77e5\u521a\u6027\u5047\u8bf4\uff0c\u8ba4\u4e3a\u8fc7\u5ea6\u81ea\u4fe1\u6e90\u4e8e\u63a8\u7406\u8fc7\u7a0b\u8fc7\u65e9\u6536\u655b\u4e8e\u72ed\u7a84\u7684\u601d\u7ef4\u8def\u5f84\uff1b4) \u8fdb\u884c\u6ce8\u610f\u529b\u71b5\u5206\u6790\u9a8c\u8bc1\u5047\u8bf4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1) \u4f20\u7edfShort-CoT\u63d0\u793a\u8868\u73b0\u51fa\u660e\u663e\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff1b2) \u65b0\u5174\u7684Long-CoT\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u63a2\u7d22\u548c\u81ea\u6211\u53cd\u601d\u7f13\u89e3\u4e86\u8fc7\u5ea6\u81ea\u4fe1\uff1b3) \u6ce8\u610f\u529b\u71b5\u5206\u6790\u521d\u6b65\u652f\u6301\u8ba4\u77e5\u521a\u6027\u5047\u8bf4\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u63a8\u7406\u5b8c\u6574\u6027\u7684\u5de5\u5177\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5c06\u8bc4\u4f30\u4ece\u5355\u4e00\u7b54\u6848\u51c6\u786e\u6027\u8f6c\u5411\u5168\u9762\u63a2\u7d22\u3002\u8ba4\u77e5\u521a\u6027\u5047\u8bf4\u4e3a\u7406\u89e3LLMs\u5728\u591a\u89e3\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.01822", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.01822", "abs": "https://arxiv.org/abs/2512.01822", "authors": ["Jintian Zhang", "Kewei Xu", "Jingsheng Zheng", "Zhuoyun Yu", "Yuqi Zhu", "Yujie Luo", "Lanning Wei", "Shuofei Qiao", "Lun Du", "Da Zheng", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents", "comment": "Work in progress", "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.", "AI": {"tldr": "InnoGym\u662f\u9996\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u521b\u65b0\u6f5c\u529b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u6027\u80fd\u589e\u76ca\u548c\u65b0\u9896\u6027\u4e24\u4e2a\u4e92\u8865\u6307\u6807\uff0c\u8986\u76d618\u4e2a\u771f\u5b9e\u4e16\u754c\u5de5\u7a0b\u79d1\u5b66\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u521b\u9020\u529b\u4e0e\u6709\u6548\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8861\u91cf\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u89e3\u51b3\u65b9\u6848\u80cc\u540e\u7684\u65b9\u6cd5\u591a\u6837\u6027\u3002\u771f\u6b63\u7684\u521b\u65b0\u4e0d\u4ec5\u9700\u8981\u6b63\u786e\u7b54\u6848\uff0c\u8fd8\u9700\u8981\u65b9\u6cd5\u7684\u539f\u521b\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30AI\u4ee3\u7406\u521b\u65b0\u6f5c\u529b\u7684\u5de5\u5177\u3002", "method": "\u63d0\u51faInnoGym\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6307\u6807\uff1a\u6027\u80fd\u589e\u76ca\uff08\u8861\u91cf\u76f8\u5bf9\u4e8e\u5df2\u77e5\u6700\u4f73\u65b9\u6848\u7684\u6539\u8fdb\uff09\u548c\u65b0\u9896\u6027\uff08\u6355\u6349\u4e0e\u5148\u524d\u65b9\u6cd5\u7684\u65b9\u6cd5\u8bba\u5dee\u5f02\uff09\u3002\u57fa\u51c6\u5305\u542b18\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u771f\u5b9e\u4e16\u754c\u5de5\u7a0b\u79d1\u5b66\u4efb\u52a1\uff0c\u901a\u8fc7\u8d44\u6e90\u8fc7\u6ee4\u3001\u8bc4\u4f30\u5668\u9a8c\u8bc1\u548c\u89e3\u51b3\u65b9\u6848\u6536\u96c6\u8fdb\u884c\u6807\u51c6\u5316\u3002\u540c\u65f6\u63d0\u4f9biGym\u7edf\u4e00\u6267\u884c\u73af\u5883\uff0c\u652f\u6301\u53ef\u91cd\u590d\u548c\u957f\u89c6\u91ce\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u4e00\u4e9b\u4ee3\u7406\u80fd\u4ea7\u751f\u65b0\u9896\u65b9\u6cd5\uff0c\u4f46\u5176\u7f3a\u4e4f\u9c81\u68d2\u6027\u9650\u5236\u4e86\u6027\u80fd\u589e\u76ca\u3002\u8fd9\u63ed\u793a\u4e86\u521b\u9020\u529b\u4e0e\u6709\u6548\u6027\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u540c\u65f6\u8bc4\u4f30\u521b\u9020\u529b\u548c\u6709\u6548\u6027\u7684\u57fa\u51c6\uff0cInnoGym\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30AI\u4ee3\u7406\u7684\u521b\u65b0\u6f5c\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.01848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01848", "abs": "https://arxiv.org/abs/2512.01848", "authors": ["Jinghan Jia", "Nathalie Baracaldo", "Sijia Liu"], "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability", "comment": null, "summary": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u53d1\u73b0\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u8865\u5145\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u548c\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u8fd9\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\u2014\u2014\u4e0d\u5b89\u5168\u884c\u4e3a\u53ef\u80fd\u51fa\u73b0\u5728\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u4e2d\uff0c\u5373\u4f7f\u6700\u7ec8\u7b54\u6848\u770b\u8d77\u6765\u65e0\u5bb3\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6539\u8fdb\u4e0d\u4e00\u81f4\u3001\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b89\u5168\u8bad\u7ec3\u7684\u8865\u5145\u4f18\u5316\u6846\u67b6\u3002\u4e0e\u76d1\u7763\u5fae\u8c03\u4e0d\u540c\uff0c\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u81ea\u9002\u5e94\u548c\u7a33\u5b9a\u7684\u5bf9\u9f50\u3002\u901a\u8fc7\u591a\u6a21\u578b\u7cfb\u5217\u548c\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u53cd\u601d\u52a8\u6001\u548ctoken\u7ea7\u71b5\u7684\u53d8\u5316\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u5f3a\u4e14\u66f4\u4e00\u81f4\u7684\u5b89\u5168\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u80fd\u529b\u3002\u5206\u6790\u663e\u793a\u5f3a\u5316\u5b66\u4e60\u6291\u5236\u4e86\u4e0d\u5b89\u5168\u7684\u63a2\u7d22\u6027\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u53cd\u601d\u6df1\u5ea6\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5b89\u5168\u53ef\u9760\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u7eaf\u76d1\u7763\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u7a33\u5065\u5b89\u5168\u5bf9\u9f50\uff0c\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u8865\u5145\u4f18\u5316\u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u5e73\u8861\u5b89\u5168\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01970", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01970", "abs": "https://arxiv.org/abs/2512.01970", "authors": ["Sitao Cheng", "Xunjian Yin", "Ruiwen Zhou", "Yuxuan Li", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Victor Zhong"], "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning", "comment": "Work in Progress. Code and data will be available at https://github.com/sitaocheng/from_atomic_to_composite", "summary": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.", "AI": {"tldr": "RL\u80fd\u5408\u6210\u65b0\u7684\u63a8\u7406\u7b56\u7565\u800c\u975e\u4ec5\u653e\u5927\u73b0\u6709\u80fd\u529b\uff0c\u4f46\u524d\u63d0\u662f\u57fa\u7840\u6a21\u578b\u9700\u5148\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u638c\u63e1\u539f\u5b50\u6280\u80fd\uff08\u53c2\u6570\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\uff09\u3002", "motivation": "\u7814\u7a76RL\u5982\u4f55\u63d0\u5347\u63a8\u7406\u80fd\u529b\u2014\u2014\u662f\u5408\u6210\u65b0\u6280\u80fd\u8fd8\u662f\u4ec5\u653e\u5927\u73b0\u6709\u884c\u4e3a\uff0c\u901a\u8fc7\u4e92\u8865\u63a8\u7406\u4efb\u52a1\u6765\u63a2\u7a76\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5408\u6210\u7684\u4eba\u7c7b\u4f20\u8bb0\u6570\u636e\u96c6\uff0c\u5c06\u4e92\u8865\u63a8\u7406\u4e25\u683c\u89e3\u8026\u4e3a\u4e24\u4e2a\u539f\u5b50\u6280\u80fd\uff1a\u53c2\u6570\u63a8\u7406\uff08\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\uff09\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\uff08\u4f9d\u8d56\u5916\u90e8\u4fe1\u606f\uff09\u3002\u5728\u4e09\u79cd\u96be\u5ea6\u7ea7\u522b\uff08I.I.D.\u3001\u7ec4\u5408\u3001\u96f6\u6837\u672c\uff09\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u6bd4SFT\u548cRL\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0SFT\u5b58\u5728\u6cdb\u5316\u6096\u8bba\uff1a\u5728\u5206\u5e03\u5185\u8868\u73b0\u5b8c\u7f8e\u4f46\u5728\u5206\u5e03\u5916\u6cdb\u5316\u5d29\u6e83\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u8def\u5f84\u6377\u5f84\u7684\u8bb0\u5fc6\u3002RL\u80fd\u4f5c\u4e3a\u63a8\u7406\u5408\u6210\u5668\u800c\u975e\u6982\u7387\u653e\u5927\u5668\uff0c\u4f46\u524d\u63d0\u662f\u57fa\u7840\u6a21\u578b\u5fc5\u987b\u5148\u901a\u8fc7SFT\u638c\u63e1\u72ec\u7acb\u7684\u539f\u5b50\u6280\u80fd\u3002", "conclusion": "RL\u4e0d\u662f\u7b80\u5355\u7684\u653e\u5927\u5668\uff0c\u5728\u5177\u5907\u8db3\u591f\u539f\u5b50\u57fa\u7840\u7684\u60c5\u51b5\u4e0b\uff0cRL\u80fd\u4e3b\u52a8\u4ece\u5b66\u4e60\u5230\u7684\u539f\u8bed\u4e2d\u5408\u6210\u590d\u6742\u63a8\u7406\u7b56\u7565\uff0c\u65e0\u9700\u5bf9\u6b64\u7c7b\u590d\u6742\u7b56\u7565\u8fdb\u884c\u663e\u5f0f\u76d1\u7763\u3002\u89e3\u8026\u7684\u539f\u5b50\u8bad\u7ec3\u540e\u63a5RL\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6cdb\u5316\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01992", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01992", "abs": "https://arxiv.org/abs/2512.01992", "authors": ["Sai Kolasani", "Maxim Saplin", "Nicholas Crispino", "Kyle Montgomery", "Jared Quincy Davis", "Matei Zaharia", "Chi Wang", "Chenguang Wang"], "title": "LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess", "comment": null, "summary": "We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.", "AI": {"tldr": "LLM CHESS\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u56fd\u9645\u8c61\u68cb\u9886\u57df\u7684\u6269\u5c55\u667a\u80fd\u4f53\u4ea4\u4e92\u6765\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u6cdb\u5316\u80fd\u529b\uff0c\u5bf950\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u6392\u540d\uff0c\u53d1\u73b0\u5373\u4f7f\u9762\u5bf9\u7b80\u5355\u4efb\u52a1\uff0c\u8bb8\u591a\u5148\u8fdb\u6a21\u578b\u4e5f\u96be\u4ee5\u5b8c\u6210\u6e38\u620f\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5bb9\u6613\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\uff0c\u9700\u8981\u4e00\u79cd\u968f\u673a\u52a8\u6001\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u66f4\u771f\u5b9e\u5730\u6d4b\u8bd5LLMs\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6269\u5c55\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e2d\u3002", "method": "\u5f00\u53d1LLM CHESS\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9LLMs\u4e0e\u968f\u673a\u5bf9\u624b\u8fdb\u884c\u56fd\u9645\u8c61\u68cb\u5bf9\u5f08\uff0c\u4f7f\u7528\u80dc\u7387\u3001\u79fb\u52a8\u8d28\u91cf\u3001\u5408\u6cd5\u6027\u3001\u5e7b\u89c9\u52a8\u4f5c\u548c\u6e38\u620f\u65f6\u957f\u7b49\u884c\u4e3a\u6307\u6807\u8bc4\u4f3050\u591a\u4e2a\u6a21\u578b\uff0c\u5bf9\u9876\u7ea7\u63a8\u7406\u6a21\u578b\u4f7f\u7528\u53ef\u53d8\u6280\u80fd\u914d\u7f6e\u7684\u8c61\u68cb\u5f15\u64ce\u8fdb\u884cElo\u8bc4\u5206\u3002", "result": "\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u96be\u4ee5\u5b8c\u6210\u6e38\u620f\u6216\u53d6\u5f97\u7a33\u5b9a\u80dc\u5229\uff0c\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0cLLM CHESS\u7684\u968f\u673a\u52a8\u6001\u7279\u6027\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\uff0c\u9632\u6b62\u57fa\u51c6\u9971\u548c\uff0c\u5373\u4f7f\u9876\u7ea7\u63a8\u7406\u6a21\u578b\u4e5f\u9762\u4e34\u6311\u6218\u3002", "conclusion": "LLM CHESS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u4e14\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u6d4b\u8bd5LLMs\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u51cf\u5c11\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002", "topic": "agent analysis"}}
{"id": "2512.00769", "categories": ["cs.LG", "astro-ph.GA"], "pdf": "https://arxiv.org/pdf/2512.00769", "abs": "https://arxiv.org/abs/2512.00769", "authors": ["Xingchen Zhou", "Nan Li", "Peng Jia", "Yingfeng Liu", "Furen Deng", "Shuanghao Shu", "Ying Li", "Liang Cao", "Huanyuan Shan", "Ayodeji Ibitoye"], "title": "AI Agent for Source Finding by SoFiA-2 for SKA-SDC2", "comment": "20 pages, 10 figures, accepted by RAA", "summary": "Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08SAC\u7b97\u6cd5\uff09\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u81ea\u52a8\u4f18\u5316\u5c04\u7535\u5929\u6587\u6e90\u63d0\u53d6\u8f6f\u4ef6SoFiA\u7684\u5173\u952e\u53c2\u6570\uff0c\u5728SKA\u79d1\u5b66\u6570\u636e\u6311\u6218\u8d5b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u57fa\u51c6\u7684\u6027\u80fd", "motivation": "\u4e0b\u4e00\u4ee3\u5927\u89c4\u6a21\u5c04\u7535\u5de1\u5929\uff08\u5982SKA\uff09\u9700\u8981\u9ad8\u6548\u7684\u6e90\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u5de5\u5177\uff08\u5982SoFiA\u3001Aegean\uff09\u7684\u53c2\u6570\u914d\u7f6e\u4f18\u5316\u56f0\u96be\uff0c\u624b\u52a8\u8c03\u53c2\u8017\u65f6\u4e14\u975e\u5e73\u51e1", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Soft Actor-Critic\uff08SAC\uff09\u6784\u5efaAI\u4ee3\u7406\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\uff08\u8c03\u6574\u53c2\u6570\uff09\u5e76\u6839\u636eSDC2\u8bc4\u5206\u53cd\u9988\u5b66\u4e60\uff0c\u9010\u6b65\u4f18\u5316SoFiA\u5728\u9884\u5904\u7406\u3001\u6e90\u67e5\u627e\u548c\u53ef\u9760\u6027\u8fc7\u6ee4\u6a21\u5757\u7684\u5173\u952e\u53c2\u6570", "result": "AI\u4ee3\u7406\u4ec5\u7528100\u6b21\u8bc4\u4f30\u6b65\u9aa4\u5373\u53ef\u81ea\u52a8\u627e\u5230\u4f18\u4e8eSoFiA\u56e2\u961f\u57fa\u51c6\u7684\u6700\u4f18\u53c2\u6570\u914d\u7f6e\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u65f6\u95f4\u6d88\u8017", "conclusion": "\u8be5\u6846\u67b6\u80fd\u89e3\u51b3\u9700\u8981\u590d\u6742\u53c2\u6570\u8c03\u4f18\u7684\u7c7b\u4f3c\u95ee\u9898\uff0c\u4f46\u9700\u8981\u5305\u542b\u4ee3\u8868\u6027\u89c2\u6d4b\u548c\u771f\u5b9e\u661f\u8868\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u96c6", "topic": "agentic reinforcement learning"}}
{"id": "2512.01909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01909", "abs": "https://arxiv.org/abs/2512.01909", "authors": ["Lihu Chen", "Xiang Yin", "Francesca Toni"], "title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking", "comment": "Preprint", "summary": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.", "AI": {"tldr": "\u63d0\u51falatent debate\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u5355\u4e2a\u6a21\u578b\u5355\u6b21\u63a8\u7406\u4e2d\u7684\u9690\u5f0f\u5185\u90e8\u8fa9\u8bba\u4fe1\u53f7\u6765\u7406\u89e3LLM\u9884\u6d4b\u548c\u5e7b\u89c9\u539f\u56e0", "motivation": "\u7406\u89e3LLM\u5185\u90e8\u601d\u7ef4\u8fc7\u7a0b\u548c\u5e7b\u89c9\u539f\u56e0\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5982self-consistency\u548cmulti-agent debate\u4f9d\u8d56\u591a\u4e2a\u7b54\u6848\u6216\u6a21\u578b\u7684\u663e\u5f0f\u8fa9\u8bba\uff0c\u65e0\u6cd5\u6355\u6349\u5355\u4e2a\u6a21\u578b\u5185\u90e8\u7684\u9690\u5f0f\u8fa9\u8bba\u4fe1\u53f7", "method": "\u63d0\u51fa\u6a21\u578b\u548c\u4efb\u52a1\u65e0\u5173\u7684latent debate\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u5728True/False\u9884\u6d4b\u4efb\u52a1\u4e0a\u7b26\u53f7\u5316\u5b9e\u4f8b\u5316\uff0c\u901a\u8fc7\u6355\u6349\u5355\u4e2a\u6a21\u578b\u5355\u6b21\u63a8\u7406\u4e2d\u7684\u9690\u5f0f\u652f\u6301/\u653b\u51fb\u4fe1\u53f7\u6765\u8fd1\u4f3cLLM\u601d\u7ef4\u8fc7\u7a0b", "result": "latent debate\u662f\u5fe0\u5b9e\u7ed3\u6784\u5316\u4ee3\u7406\u6a21\u578b\uff0c\u4e0e\u539f\u59cbLLM\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\uff1b\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u5f3a\u57fa\u7ebf\uff1b\u53d1\u73b0\u5e7b\u89c9\u4e0e\u8fa9\u8bba\u6a21\u5f0f\u5f3a\u76f8\u5173\uff08\u5982\u4e2d\u95f4\u5c42\u9ad8\u7a0b\u5ea6latent debate\u4e0e\u9ad8\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\uff09", "conclusion": "latent debate\u662f\u7406\u89e3LLM\u5185\u90e8\u673a\u5236\uff08\u7279\u522b\u662f\u63a8\u7406\u6b65\u9aa4\u4e2d\u51fa\u73b0\u5185\u90e8\uff08\u4e0d\uff09\u4e00\u81f4\u6027\u7684\u573a\u666f\uff09\u7684\u6f5c\u5728\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2512.01925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01925", "abs": "https://arxiv.org/abs/2512.01925", "authors": ["Junnan Liu", "Hongwei Liu", "Songyang Zhang", "Kai Chen"], "title": "Rectifying LLM Thought from Lens of Optimization", "comment": "Work in progress", "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.", "AI": {"tldr": "RePro\u662f\u4e00\u79cd\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u5956\u52b1\u4f18\u5316LLM\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u601d\u7ef4\u94fe\u89c6\u4e3a\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u6027\u80fd", "motivation": "\u5f53\u524d\u957f\u601d\u7ef4\u94feLLM\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u548c\u63a8\u7406\u94fe\u8fc7\u957f\u7b49\u6b21\u4f18\u63a8\u7406\u884c\u4e3a\uff0c\u5f71\u54cd\u6027\u80fd\u8868\u73b0\uff0c\u9700\u8981\u6539\u8fdb\u63a8\u7406\u8fc7\u7a0b", "method": "\u5c06\u601d\u7ef4\u94fe\u89c6\u4e3a\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\uff0c\u5b9a\u4e49\u4ee3\u7406\u76ee\u6807\u51fd\u6570\u8bc4\u4f30\u4f18\u5316\u8fc7\u7a0b\uff0c\u4f7f\u7528\u53cc\u91cd\u8bc4\u5206\u673a\u5236\u91cf\u5316\u63a8\u7406\u5f3a\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u6574\u5408\u4e3a\u8fc7\u7a0b\u7ea7\u5956\u52b1\uff0c\u878d\u5165RLVR\u7ba1\u9053\u4f18\u5316LLM", "result": "\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u7f16\u7801\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRePro\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u51cf\u8f7b\u6b21\u4f18\u63a8\u7406\u884c\u4e3a", "conclusion": "RePro\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u5956\u52b1\u4f18\u5316LLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u601d\u7ef4\u94fe\u4e2d\u7684\u6b21\u4f18\u63a8\u7406\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2512.00831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00831", "abs": "https://arxiv.org/abs/2512.00831", "authors": ["Yuchen Zeng", "Shuibai Zhang", "Wonjun Kang", "Shutong Wu", "Lynnix Zou", "Ying Fan", "Heeju Kim", "Ziqian Lin", "Jungtaek Kim", "Hyung Il Koo", "Dimitris Papailiopoulos", "Kangwook Lee"], "title": "ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning \"algorithms\" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.", "AI": {"tldr": "ReJump\u662f\u4e00\u4e2a\u5206\u6790\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u63a8\u7406\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u8868\u793a\u4e3a\u95ee\u9898\u89e3\u51b3\u6b65\u9aa4\u6811\u4e2d\u8282\u70b9\u7684\u8bbf\u95ee\u987a\u5e8f\uff0c\u901a\u8fc7\u91cf\u5316\u63a2\u7d22\u3001\u5229\u7528\u3001\u8fc7\u5ea6\u601d\u8003\u3001\u9057\u5fd8\u548c\u9a8c\u8bc1\u7b49\u884c\u4e3a\u6765\u7406\u89e3LRMs\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5e95\u5c42\u7684\u63a8\u7406\"\u7b97\u6cd5\"\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u5206\u6790LRMs\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u5728\u76f8\u4f3c\u51c6\u786e\u7387\u4e0b\u7684\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u63d0\u51faReJump\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u8868\u793a\u4e3a\u95ee\u9898\u89e3\u51b3\u6b65\u9aa4\u6811\u4e2d\u8282\u70b9\u7684\u8bbf\u95ee\u987a\u5e8f\u3002\u4f7f\u7528\"\u8df3\u8dc3\"\u6982\u5ff5\u63cf\u8ff0\u8282\u70b9\u95f4\u8f6c\u79fb\uff0c\u5305\u62ec\u76f8\u90bb\u79fb\u52a8\uff08\u5982\u8ba1\u7b97\uff09\u548c\u975e\u76f8\u90bb\u79fb\u52a8\uff08\u5982\u56de\u6eaf\u548c\u9a8c\u8bc1\uff09\u3002\u5f00\u53d1LLM\u4ee3\u7406\u5c06\u63a8\u7406\u8f68\u8ff9\u63d0\u53d6\u4e3aReJump\u683c\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u591a\u79cd\u91cf\u5316\u6307\u6807\u6765\u5206\u6790\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u51c6\u786e\u7387\u76f8\u4f3c\u7684\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u622a\u7136\u4e0d\u540c\u7684\u63a8\u7406\u884c\u4e3a\uff1b2\uff09\u4e0d\u540c\u4efb\u52a1\u504f\u597d\u4e0d\u540c\u7684\u63a8\u7406\u98ce\u683c\uff08\u5982\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u4e0d\u540c\uff09\uff1b3\uff09ReJump\u80fd\u6709\u6548\u6bd4\u8f83\u84b8\u998fLRMs\u4e0e\u6559\u5e08\u6a21\u578b\u3001CoT\u63d0\u793aLLMs\u4e0eLRMs\u7684\u5dee\u5f02\uff1b4\uff09ReJump\u53ef\u7528\u4e8e\u63d0\u5347\u63a8\u7406\u8d28\u91cf\uff0c\u5982\u901a\u8fc7ReJump\u5f15\u5bfc\u7684\u6700\u4f73N\u9009\u62e9\u548c\u63d0\u793a\u9009\u62e9\u3002", "conclusion": "ReJump\u4e3a\u7406\u89e3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u5e76\u80fd\u5b9e\u9645\u5e94\u7528\u4e8e\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3LRMs\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.00908", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00908", "abs": "https://arxiv.org/abs/2512.00908", "authors": ["Xinzhu Chen", "Xuesheng Li", "Zhongxiang Sun", "Weijie Yu"], "title": "Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLESS\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4f18\u52bf\u8c03\u5236\u4f4e\u71b5\u6bb5\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709RLVR\u7814\u7a76\u5173\u6ce8\u9ad8\u71b5token\u7684\u63a2\u7d22\u4f5c\u7528\uff0c\u4f46\u5ffd\u7565\u4e86\u63a8\u7406\u8f68\u8ff9\u4e2d\u5927\u90e8\u5206\u662f\u4f4e\u71b5\u6bb5\uff0c\u8fd9\u4e9b\u4f4e\u71b5\u6bb5\u7f16\u7801\u4e86\u7a33\u5b9a\u53ef\u590d\u7528\u7684\u7ed3\u6784\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\u6b63\u786e\u56de\u7b54\u7684\u4f4e\u71b5\u6bb5\u91cd\u53e0\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u9519\u8bef\u56de\u7b54\u7684\u4f4e\u71b5\u6bb5\u5219\u8868\u73b0\u51fa\u7a33\u5b9a\u4f46\u65e0\u6548\u7684\u6a21\u5f0f\u3002", "method": "\u63d0\u51faLESS\u6846\u67b6\uff0c\u5728\u4f4e\u71b5\u6bb5\u4e0a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4f18\u52bf\u8c03\u5236\uff1a\u653e\u5927\u6b63\u786e\u56de\u7b54\u72ec\u6709\u7684\u6bb5\uff0c\u6291\u5236\u9519\u8bef\u56de\u7b54\u72ec\u6709\u7684\u6bb5\uff0c\u4e2d\u548c\u4e24\u8005\u5171\u4eab\u7684\u6bb5\uff0c\u540c\u65f6\u4fdd\u7559\u5e95\u5c42RL\u7b97\u6cd5\u7684\u9ad8\u71b5\u63a2\u7d22\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u6d41\u884c\u7684GRPO\u5b9e\u73b0\u3002", "result": "\u5728\u4e09\u4e2a\u9aa8\u5e72\u6a21\u578b\u548c\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLESS\u76f8\u6bd4\u5f3aRL\u57fa\u7ebf\u6301\u7eed\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u4e0b\u9650\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6b63\u786e\u611f\u77e5\u7684\u4f4e\u71b5\u6bb5\u8c03\u5236\uff0cLESS\u6846\u67b6\u6709\u6548\u6539\u8fdb\u4e86RLVR\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4f4e\u71b5\u6bb5\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.00915", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00915", "abs": "https://arxiv.org/abs/2512.00915", "authors": ["Junwoo Chang", "Minwoo Park", "Joohwan Seo", "Roberto Horowitz", "Jongmin Lee", "Jongeun Choi"], "title": "Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments", "comment": "27 pages, 10 figures", "summary": "Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.", "AI": {"tldr": "\u63d0\u51fa\u90e8\u5206\u7fa4\u4e0d\u53d8MDP\u6846\u67b6\uff0c\u9009\u62e9\u6027\u5e94\u7528\u7fa4\u4e0d\u53d8Bellman\u5907\u4efd\uff0c\u89e3\u51b3\u5c40\u90e8\u5bf9\u79f0\u6027\u7834\u574f\u5bfc\u81f4\u7684\u5168\u5c40\u503c\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\uff0c\u5f00\u53d1PE-DQN\u548cPE-SAC\u7b97\u6cd5\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u5b8c\u5168\u7fa4\u4e0d\u53d8\u7684MDP\u51e0\u4e4e\u4e0d\u5b58\u5728\uff0c\u52a8\u529b\u5b66\u3001\u6267\u884c\u5668\u9650\u5236\u548c\u5956\u52b1\u8bbe\u8ba1\u901a\u5e38\u4f1a\u7834\u574f\u5bf9\u79f0\u6027\uff08\u901a\u5e38\u53ea\u662f\u5c40\u90e8\u7834\u574f\uff09\u3002\u5728\u7fa4\u4e0d\u53d8Bellman\u5907\u4efd\u4e0b\uff0c\u5c40\u90e8\u5bf9\u79f0\u6027\u7834\u574f\u4f1a\u5f15\u5165\u8bef\u5dee\u5e76\u4f20\u64ad\u5230\u6574\u4e2a\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5bfc\u81f4\u5168\u5c40\u503c\u4f30\u8ba1\u9519\u8bef\u3002", "method": "\u5f15\u5165\u90e8\u5206\u7fa4\u4e0d\u53d8MDP\uff08PI-MDP\uff09\u6846\u67b6\uff0c\u6839\u636e\u5bf9\u79f0\u6027\u662f\u5426\u6210\u7acb\u9009\u62e9\u6027\u5e94\u7528\u7fa4\u4e0d\u53d8\u6216\u6807\u51c6Bellman\u5907\u4efd\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u5f00\u53d1PE-DQN\uff08\u79bb\u6563\u63a7\u5236\uff09\u548cPE-SAC\uff08\u8fde\u7eed\u63a7\u5236\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u7b49\u53d8\u6027\u4f18\u52bf\u4e0e\u5bf9\u79f0\u6027\u7834\u574f\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728Grid-World\u3001\u8fd0\u52a8\u63a7\u5236\u548c\u64cd\u4f5c\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPE-DQN\u548cPE-SAC\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9009\u62e9\u6027\u5229\u7528\u5bf9\u79f0\u6027\u5bf9\u9c81\u68d2\u4e14\u6837\u672c\u9ad8\u6548RL\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u90e8\u5206\u7fa4\u4e0d\u53d8MDP\u6846\u67b6\u80fd\u591f\u7f13\u89e3\u5c40\u90e8\u5bf9\u79f0\u6027\u7834\u574f\u5f15\u8d77\u7684\u8bef\u5dee\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u7b49\u53d8\u6027\u7684\u4f18\u52bf\uff0c\u4ece\u800c\u589e\u5f3a\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9009\u62e9\u6027\u5bf9\u79f0\u6027\u5229\u7528\u5bf9\u4e8e\u9c81\u68d2\u9ad8\u6548\u7684RL\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01127", "abs": "https://arxiv.org/abs/2512.01127", "authors": ["Chen Henry Wu", "Sachin Goyal", "Aditi Raghunathan"], "title": "Mode-Conditioning Unlocks Superior Test-Time Scaling", "comment": null, "summary": "Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.", "AI": {"tldr": "\u63d0\u51faMode-Conditioning (ModC)\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5206\u914d\u63a8\u7406\u6a21\u5f0f\u6765\u89e3\u51b3\u5e76\u884c\u91c7\u6837\u4e2d\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u63d0\u5347\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387", "motivation": "\u5e76\u884c\u91c7\u6837\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\u7684\u9650\u5236\u2014\u2014\u6a21\u578b\u96c6\u4e2d\u5728\u5c11\u6570\u6a21\u5f0f\u4e0a\uff0c\u91cd\u590d\u91c7\u6837\u4ea7\u751f\u76f8\u540c\u9519\u8bef\u3002\u6807\u51c6\u8bad\u7ec3\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u591a\u6837\u6027", "method": "\u63d0\u51faMode-Conditioning (ModC)\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u5bb6\u6a21\u578b\u6216\u6a21\u5f0f\u7279\u5b9a\u524d\u7f00\u663e\u5f0f\u5206\u914d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5230\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u3002\u8fd8\u5c55\u793a\u4e86\u68af\u5ea6\u805a\u7c7b\u65b9\u6cd5\u53ef\u4ee5\u5728\u6ca1\u6709\u663e\u5f0f\u6a21\u5f0f\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0ModC", "result": "ModC\u5728\u63a7\u5236\u56fe\u641c\u7d22\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u63a8\u7406\u57fa\u51c6\u4e0a\u4e00\u81f4\u6539\u5584\u6269\u5c55\u6027\uff0c\u6db5\u76d60.5B\u52307B\u6a21\u578b\u3002\u5728OpenThoughts\u4e0a\uff0cQwen2.5-7B\u5fae\u8c03\u540e\u83b7\u5f974\u500d\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u9ad8\u6700\u5927Pass@k\u3002\u68af\u5ea6\u805a\u7c7b\u65b9\u6cd5\u5728NuminaMath\u7b49\u6570\u636e\u96c6\u4e0a\u5e26\u676510%\u589e\u76ca\u3002ModC\u8fd8\u80fd\u6539\u5584\u5f3a\u5316\u5b66\u4e60", "conclusion": "\u6807\u51c6\u8bad\u7ec3\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u591a\u6837\u6027\uff0cModC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u91ca\u653e\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u591a\u6837\u6027\u7684\u5168\u90e8\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2512.01374", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01374", "abs": "https://arxiv.org/abs/2512.01374", "authors": ["Chujie Zheng", "Kai Dang", "Bowen Yu", "Mingze Li", "Huiqiang Jiang", "Junrong Lin", "Yuqiong Liu", "An Yang", "Jingren Zhou", "Junyang Lin"], "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "comment": null, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u516c\u5f0f\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u4ee5\u53ca\u4f55\u65f6\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7406token\u7ea7\u76ee\u6807\u4f18\u5316\u771f\u5b9e\u5e8f\u5217\u7ea7\u5956\u52b1\uff0c\u5e76\u63ed\u793a\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5b58\u5728\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u4e3a\u4f55\u67d0\u4e9b\u6280\u672f\uff08\u5982\u91cd\u8981\u6027\u91c7\u6837\u3001\u88c1\u526a\u3001\u8def\u7531\u56de\u653e\uff09\u80fd\u7a33\u5b9a\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u5206\u6790\uff0c\u63d0\u51fa\u7406\u8bba\u516c\u5f0f\u89e3\u91catoken\u7ea7\u4ee3\u7406\u76ee\u6807\u4f55\u65f6\u80fd\u6709\u6548\u4f18\u5316\u5e8f\u5217\u7ea7\u5956\u52b1\u3002\u5b9e\u9a8c\u4f7f\u752830B\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u8fdb\u884c\u6570\u5341\u4e07GPU\u5c0f\u65f6\u7684\u8bad\u7ec3\uff0c\u6bd4\u8f83\u4e0d\u540c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u5e26\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u7a33\u5b9a\u6027\u6700\u9ad8\uff1b\u79bb\u7b56\u7565\u66f4\u65b0\u65f6\uff0c\u7ed3\u5408\u88c1\u526a\u548c\u8def\u7531\u56de\u653e\u5bf9\u7f13\u89e3\u7b56\u7565\u9648\u65e7\u6027\u5bfc\u81f4\u7684\u5931\u7a33\u81f3\u5173\u91cd\u8981\u3002\u4e00\u65e6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u65e0\u8bba\u51b7\u542f\u52a8\u521d\u59cb\u5316\u5982\u4f55\uff0c\u5ef6\u957f\u4f18\u5316\u90fd\u80fd\u83b7\u5f97\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86RL\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u7406\u8bba\u89e3\u91ca\u548c\u5b9e\u7528\u914d\u65b9\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u548c\u7b56\u7565\u9648\u65e7\u6027\u6700\u5c0f\u5316\u662f\u4ee3\u7406\u76ee\u6807\u6709\u6548\u7684\u5173\u952e\u6761\u4ef6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.00961", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00961", "abs": "https://arxiv.org/abs/2512.00961", "authors": ["Qi Wang", "Mian Wu", "Yuyang Zhang", "Mingqi Yuan", "Wenyao Zhang", "Haoxiang You", "Yunbo Wang", "Xin Jin", "Xiaokang Yang", "Wenjun Zeng"], "title": "Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u76ee\u6807\u9a71\u52a8\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7a0b\u5e8f\u5316\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u8fd9\u4e9b\u51fd\u6570\u8bbe\u8ba1\u56f0\u96be\u4e14\u6cdb\u5316\u6027\u5dee\u3002\u9700\u8981\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e30\u5bcc\u4e16\u754c\u77e5\u8bc6\u6765\u63d0\u4f9b\u76ee\u6807\u9a71\u52a8\u7684\u5956\u52b1\u4fe1\u53f7", "method": "1) \u89c6\u9891\u7ea7\u5956\u52b1\uff1a\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u5176\u89c6\u9891\u7f16\u7801\u5668\u8bc4\u4f30\u667a\u80fd\u4f53\u8f68\u8ff9\u4e0e\u751f\u6210\u76ee\u6807\u89c6\u9891\u7684\u6f5c\u5728\u8868\u793a\u5bf9\u9f50\u5ea6\uff1b2) \u5e27\u7ea7\u5956\u52b1\uff1a\u4f7f\u7528CLIP\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u8bc6\u522b\u6700\u76f8\u5173\u5e27\u4f5c\u4e3a\u76ee\u6807\u72b6\u6001\uff0c\u5b66\u4e60\u524d\u5411-\u540e\u5411\u8868\u793a\u4f5c\u4e3a\u5e27\u7ea7\u5956\u52b1", "result": "\u5728\u591a\u4e2aMeta-World\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u6709\u6548\u7684\u76ee\u6807\u9a71\u52a8\u5956\u52b1\u4fe1\u53f7\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u7684\u4f9d\u8d56", "topic": "agentic reinforcement learning"}}
{"id": "2512.01945", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01945", "abs": "https://arxiv.org/abs/2512.01945", "authors": ["Han Zhou", "Xingchen Wan", "Ivan Vuli\u0107", "Anna Korhonen"], "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution", "comment": "10 pages, 3 figures, 2 tables (18 pages including references and appendices)", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.", "AI": {"tldr": "INSPO\u63d0\u51fa\u4e86\u4e00\u79cd\u6307\u4ee4-\u7b56\u7565\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u6307\u4ee4\u6765\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5728\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u9759\u6001\u6307\u4ee4\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u4eba\u5de5\u8bbe\u8ba1\u7684\u6307\u4ee4\uff0c\u4f46\u8fd9\u4e9b\u6307\u4ee4\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u4e14\u968f\u7740\u7b56\u7565\u6539\u8fdb\u548c\u73af\u5883\u4ea4\u4e92\u53d8\u5316\uff0c\u6700\u4f18\u6307\u4ee4\u4e5f\u4f1a\u6539\u53d8\uff0c\u9700\u8981\u52a8\u6001\u4f18\u5316\u673a\u5236\u3002", "method": "INSPO\u6846\u67b6\u5c06\u6307\u4ee4\u4f18\u5316\u6574\u5408\u5230RL\u5faa\u73af\u4e2d\uff1a\u7ef4\u62a4\u52a8\u6001\u6307\u4ee4\u5019\u9009\u6c60\uff0c\u901a\u8fc7RL\u5956\u52b1\u4fe1\u53f7\u8bc4\u4f30\u6307\u4ee4\uff0c\u5b9a\u671f\u6dd8\u6c70\u4f4e\u6027\u80fd\u6307\u4ee4\uff0c\u57fa\u4e8e\u56de\u653e\u7f13\u51b2\u533a\u7684\u7ecf\u9a8c\u901a\u8fc7LLM\u4f18\u5316\u5668\u751f\u6210\u65b0\u6307\u4ee4\u3002", "result": "\u5728\u591a\u8f6e\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cINSPO\u663e\u8457\u4f18\u4e8e\u4f9d\u8d56\u9759\u6001\u6307\u4ee4\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0\u521b\u65b0\u6027\u6307\u4ee4\u5f15\u5bfc\u4ee3\u7406\u91c7\u53d6\u66f4\u6218\u7565\u6027\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4ec5\u589e\u52a0\u5c11\u91cf\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u6307\u4ee4-\u7b56\u7565\u534f\u540c\u8fdb\u5316\u662f\u6709\u6548\u7684\uff0c\u52a8\u6001\u6307\u4ee4\u4f18\u5316\u80fd\u663e\u8457\u63d0\u5347RL\u4ee3\u7406\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01119", "abs": "https://arxiv.org/abs/2512.01119", "authors": ["Geigh Zollicoffer", "Tanush Chopra", "Mingkuan Yan", "Xiaoxu Ma", "Kenneth Eaton", "Mark Riedl"], "title": "World Model Robustness via Surprise Recognition", "comment": null, "summary": "AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .", "AI": {"tldr": "\u63d0\u51faWISER\u7b97\u6cd5\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u60ca\u8bb6\u5ea6\u6765\u51cf\u5c11\u566a\u58f0\u5bf9\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u6280\u672f\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684AI\u7cfb\u7edf\u9762\u4e34\u5404\u79cd\u5e72\u6270\u548c\u5206\u5e03\u5916\u566a\u58f0\uff0c\u8fd9\u4e9b\u566a\u58f0\u4f1a\u7834\u574f\u7b56\u7565\u7a33\u5b9a\u6027\u5e76\u5bfc\u81f4\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u867d\u7136\u9c81\u68d2\u8bad\u7ec3\u53ef\u4ee5\u51cf\u5c11\u5bf9\u67d0\u4e9b\u566a\u58f0\u7684\u654f\u611f\u6027\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u6240\u6709\u53ef\u80fd\u7684OOD\u6761\u4ef6\u3002", "method": "\u5f00\u53d1WISER\u7b97\u6cd5\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u60ca\u8bb6\u5ea6\u6765\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u3002\u5f15\u5165\u591a\u8868\u793a\u548c\u5355\u8868\u793a\u62d2\u7edd\u91c7\u6837\u6280\u672f\uff0c\u5206\u522b\u5904\u7406\u591a\u4f20\u611f\u5668\u6545\u969c\u548c\u5355\u4f20\u611f\u5668\u6545\u969c\u573a\u666f\u3002", "result": "\u5728CARLA\u548cSafety Gymnasium\u7b49\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u548c\u7a0b\u5ea6\u7684\u566a\u58f0\u4e0b\u90fd\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u589e\u5f3a\u4e24\u79cd\u4e0d\u540c\u67b6\u6784\u7684\u4e16\u754c\u6a21\u578b\uff08Cosmos\u548cDreamerV3\uff09\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u60ca\u8bb6\u5ea6\u6765\u51cf\u8f7b\u566a\u58f0\u5f71\u54cd\uff0c\u5728\u591a\u79cd\u4e16\u754c\u5efa\u6a21\u9886\u57df\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u5e72\u6270\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.01224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01224", "abs": "https://arxiv.org/abs/2512.01224", "authors": ["Ruixiang Feng", "Zhenwei An", "Yuntao Wen", "Ran Le", "Yiming Jia", "Chen Yang", "Zongchao Chen", "Lisi Chen", "Shen Gao", "Shuo Shang", "Yang Song", "Tao Zhang"], "title": "CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions", "comment": null, "summary": "Answer verification methods are widely employed in language model training pipelines spanning data curation, evaluation, and reinforcement learning with verifiable rewards (RLVR). While prior work focus on developing unified verifiers applicable across multiple reasoning scenarios, significant challenges remain in computation-oriented scientific domains, such as algebraic equivalence checking and physical constant substitution. In this paper, we introduce \\model, a tool-augmented verifier that leverages external executors to perform precise computations and symbolic simplifications. \\model enables robust verification that goes beyond simple semantic matching. We propose a novel two-stage pipeline, which begin with cold-start fine-tuning and followed by multi-turn reinforcement learning with tool integration. Extensive experiments conducted on STEM subjects, general QA, and long-form reasoning tasks demonstrates strong generalization of \\model. The results shows that the \\model achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench. And we also employ our \\model in RLVR as a reward model, the results show that it consistently outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25, demonstrating strong potential to enhance reasoning capabilities of LLM. Our model is released at \\hyperlink{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}.", "AI": {"tldr": "CoSineVerifier\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u9a8c\u8bc1\u5668\uff0c\u5229\u7528\u5916\u90e8\u6267\u884c\u5668\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\u548c\u7b26\u53f7\u7b80\u5316\uff0c\u5728STEM\u3001\u901a\u7528QA\u548c\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728RLVR\u4e2d\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u9a8c\u8bc1\u65b9\u6cd5\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u79d1\u5b66\u9886\u57df\uff08\u5982\u4ee3\u6570\u7b49\u4ef7\u68c0\u67e5\u548c\u7269\u7406\u5e38\u6570\u66ff\u6362\uff09\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u8bed\u4e49\u5339\u914d\u7684\u9c81\u68d2\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5de5\u5177\u589e\u5f3a\u7684\u9a8c\u8bc1\u5668CoSineVerifier\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u51b7\u542f\u52a8\u5fae\u8c03 + \u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4e0e\u5de5\u5177\u96c6\u6210\uff0c\u5229\u7528\u5916\u90e8\u6267\u884c\u5668\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\u548c\u7b26\u53f7\u7b80\u5316\u3002", "result": "\u5728VerifyBench-Hard\u548cSCI-Bench\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff1b\u4f5c\u4e3aRLVR\u5956\u52b1\u6a21\u578b\u5728AIME'24\u548cAIME'25\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u9a8c\u8bc1\u5668\u3002", "conclusion": "CoSineVerifier\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u6709\u6548\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u79d1\u5b66\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2512.01775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01775", "abs": "https://arxiv.org/abs/2512.01775", "authors": ["Simon Park", "Simran Kaur", "Sanjeev Arora"], "title": "How Does RL Post-training Induce Skill Composition? A Case Study on Countdown", "comment": null, "summary": "While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.", "AI": {"tldr": "\u7814\u7a76RL\u540e\u8bad\u7ec3\u5982\u4f55\u4fc3\u8fdb\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u957f\u5ea6\u6cdb\u5316\uff0c\u901a\u8fc7Countdown\u4efb\u52a1\u5206\u6790\u6a21\u578b\u5c06\u6280\u80fd\u4f5c\u4e3a\u8868\u8fbe\u5f0f\u5b50\u6811\u91cd\u7528\u7684\u673a\u5236", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5728\u4fc3\u8fdb\u7ec4\u5408\u6cdb\u5316\uff08\u4ece\u5df2\u77e5\u7ec4\u4ef6\u5408\u6210\u65b0\u6280\u80fd\uff09\u65b9\u9762\u7684\u4f5c\u7528\u5e38\u88ab\u4e0e\u7b80\u5355\u7684\u957f\u5ea6\u6cdb\u5316\u6df7\u6dc6\u3002\u9700\u8981\u660e\u786eRL\u540e\u8bad\u7ec3\u5982\u4f55\u6559\u6388\u6280\u80fd\u7ec4\u5408\uff0c\u4ee5\u53ca\u7ec4\u5408\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u6280\u80fd\u8fc1\u79fb", "method": "\u805a\u7126Countdown\u4efb\u52a1\uff08\u7ed9\u5b9an\u4e2a\u6570\u5b57\u548c\u76ee\u6807\u503c\uff0c\u6784\u5efa\u8868\u8fbe\u5f0f\u6c42\u503c\u5230\u76ee\u6807\uff09\uff0c\u5c06\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u5206\u6790\u4e3a\u8868\u8fbe\u5f0f\u6811\uff0c\u6bcf\u4e2a\u5b50\u6811\u5bf9\u5e94\u53ef\u91cd\u7528\u7684\u5b50\u4efb\u52a1\uff08\u6280\u80fd\uff09\u3002\u901a\u8fc7\u8ddf\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6811\u5f62\u7ed3\u6784\u548c\u6210\u529f\u7387\u6765\u5206\u6790", "result": "\u53d1\u73b0\uff1a(1) \u5bf9\u66f4\u5927n\u548c\u672a\u89c1\u6811\u5f62\u7684\u5206\u5e03\u5916\u6cdb\u5316\uff0c\u8868\u660e\u5b50\u4efb\u52a1\u7684\u7ec4\u5408\u91cd\u7528\uff1b(2) \u7ed3\u6784\u4f9d\u8d56\u7684\u5b66\u4e60\u5c42\u6b21\u6027\u2014\u2014\u6a21\u578b\u5148\u638c\u63e1\u6d45\u5c42\u5e73\u8861\u6811\uff08\u5b50\u4efb\u52a1\u95f4\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\uff09\uff0c\u540e\u638c\u63e1\u6df1\u5c42\u4e0d\u5e73\u8861\u6811\uff0c\u5bf9\u53f3\u91cd\u7ed3\u6784\u5b58\u5728\u6301\u7eed\u8106\u5f31\u6027\uff08\u5373\u4f7f\u7ec4\u5408\u6df1\u5ea6\u4e0e\u5de6\u91cd\u7ed3\u6784\u76f8\u540c\uff09", "conclusion": "\u8bca\u65ad\u63ed\u793a\u4e86RL\u540e\u8bad\u7ec3\u5b66\u4e60\u7684\u5185\u5bb9\u3001\u987a\u5e8f\u4ee5\u53ca\u6cdb\u5316\u5931\u8d25\u7684\u5730\u65b9\uff0c\u9610\u660e\u4e86\u4ec5\u901a\u8fc7RL\u540e\u8bad\u7ec3\u5982\u4f55\u8bf1\u5bfc\u8d85\u51fa\u6807\u51c6\u6307\u6807\uff08\u5982pass@k\uff09\u7684\u5206\u5e03\u5916\u6cdb\u5316", "topic": "agent analysis"}}
{"id": "2512.01987", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.01987", "abs": "https://arxiv.org/abs/2512.01987", "authors": ["Suzan Ece Ada", "Georg Martius", "Emre Ugur", "Erhan Oztop"], "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems, NeurIPS 2025", "summary": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.", "AI": {"tldr": "FORL\u6846\u67b6\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u72b6\u6001\u751f\u6210\u548c\u96f6\u6837\u672c\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u95ee\u9898\uff0c\u63d0\u5347\u5728\u73b0\u5b9e\u4e16\u754c\u504f\u79fb\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u5e73\u7a33\u6216\u4ec5\u8003\u8651\u6d4b\u8bd5\u65f6\u7684\u5408\u6210\u6270\u52a8\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7ecf\u5e38\u5931\u6548\u3002\u73b0\u5b9e\u573a\u666f\u5e38\u5b58\u5728\u7a81\u53d1\u3001\u65f6\u53d8\u7684\u504f\u79fb\uff0c\u5bfc\u81f4\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\uff0c\u4f7f\u667a\u80fd\u4f53\u8bef\u5224\u771f\u5b9e\u72b6\u6001\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "FORL\u6846\u67b6\u7edf\u4e00\u4e86\uff1a(1) \u6761\u4ef6\u6269\u6563\u57fa\u7684\u5019\u9009\u72b6\u6001\u751f\u6210\uff0c\u8bad\u7ec3\u65f6\u4e0d\u9884\u8bbe\u4efb\u4f55\u7279\u5b9a\u7684\u672a\u6765\u975e\u5e73\u7a33\u6a21\u5f0f\uff1b(2) \u96f6\u6837\u672c\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6846\u67b6\u9488\u5bf9\u6613\u53d7\u610f\u5916\u3001\u53ef\u80fd\u975e\u9a6c\u5c14\u53ef\u592b\u504f\u79fb\u5f71\u54cd\u7684\u73af\u5883\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u4ece\u6bcf\u4e2aepisode\u5f00\u59cb\u5c31\u5177\u5907\u9c81\u68d2\u6027\u80fd\u3002", "result": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u65f6\u5e8f\u6570\u636e\u6a21\u62df\u73b0\u5b9e\u975e\u5e73\u7a33\u6027\uff0cFORL\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u96f6\u6837\u672c\u9884\u6d4b\u4e0e\u667a\u80fd\u4f53\u7ecf\u9a8c\uff0cFORL\u65e8\u5728\u5f25\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u73b0\u5b9e\u4e16\u754c\u975e\u5e73\u7a33\u73af\u5883\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.7e606f13", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobonomics.substack.com%2Fp%2Fsearch-the-moat-of-the-search-index%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/gLlfBF2hIBAVMN1VKuP4WV6GL_zla129oOpplQvrSu0=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobonomics.substack.com%2Fp%2Fsearch-the-moat-of-the-search-index%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/gLlfBF2hIBAVMN1VKuP4WV6GL_zla129oOpplQvrSu0=433", "authors": ["TLDR Newsletter"], "title": "The Moat of the Search Index", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobonomics.substack.com%2Fp%2Fsearch-the-moat-of-the-search-index%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/gLlfBF2hIBAVMN1VKuP4WV6GL_zla129oOpplQvrSu0=433", "summary": "The Moat of the Search Index (2 minute read) ChatGPT surpasses Google's search by using an \"agent\" approach, extracting relevant information from multiple sources and synthesizing answers, which diminishes Google's traditional search index advantage. This method reduces the impact of any single search result's failure, although it struggles with long-tail, fresh, or SEO-heavy queries. The traditional search engine moat has largely vanished, with AI-driven models increasingly blending search c...", "source": "tldr", "AI": {"tldr": "ChatGPT\u901a\u8fc7\"\u4ee3\u7406\"\u65b9\u6cd5\u8d85\u8d8a\u8c37\u6b4c\u641c\u7d22\uff0c\u4ece\u591a\u4e2a\u6765\u6e90\u63d0\u53d6\u4fe1\u606f\u5e76\u5408\u6210\u7b54\u6848\uff0c\u524a\u5f31\u4e86\u8c37\u6b4c\u4f20\u7edf\u641c\u7d22\u7d22\u5f15\u7684\u4f18\u52bf\uff0c\u4f46\u957f\u5c3e\u3001\u65b0\u9c9c\u6216SEO\u5bc6\u96c6\u67e5\u8be2\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u5206\u6790AI\u9a71\u52a8\u7684\u804a\u5929\u6a21\u578b\u5982\u4f55\u6539\u53d8\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u7684\u7ade\u4e89\u683c\u5c40\uff0c\u7279\u522b\u662fChatGPT\u7684\"\u4ee3\u7406\"\u65b9\u6cd5\u5982\u4f55\u524a\u5f31\u8c37\u6b4c\u641c\u7d22\u7d22\u5f15\u7684\u4f20\u7edf\u62a4\u57ce\u6cb3\u4f18\u52bf\u3002", "method": "\u91c7\u7528\"\u4ee3\u7406\"\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u6765\u6e90\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u5e76\u5408\u6210\u7efc\u5408\u7b54\u6848\uff0c\u51cf\u5c11\u5bf9\u5355\u4e00\u641c\u7d22\u7ed3\u679c\u5931\u8d25\u7684\u4f9d\u8d56\u3002", "result": "ChatGPT\u5728\u4fe1\u606f\u5408\u6210\u65b9\u9762\u8d85\u8d8a\u8c37\u6b4c\u641c\u7d22\uff0c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u7684\u62a4\u57ce\u6cb3\u4f18\u52bf\u57fa\u672c\u6d88\u5931\uff0cAI\u6a21\u578b\u9010\u6e10\u878d\u5408\u641c\u7d22\u529f\u80fd\u3002", "conclusion": "\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u7684\u7d22\u5f15\u4f18\u52bf\u5df2\u88abAI\u9a71\u52a8\u7684\u4ee3\u7406\u65b9\u6cd5\u524a\u5f31\uff0c\u641c\u7d22\u8303\u5f0f\u6b63\u5728\u5411\u4fe1\u606f\u63d0\u53d6\u548c\u5408\u6210\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.84a6b92c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Flangwatch%2Fbetter-agents%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/e1rD7RT2zGzjkqTK1_uA1L60RF1V_UthwDhAn8jap00=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Flangwatch%2Fbetter-agents%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/e1rD7RT2zGzjkqTK1_uA1L60RF1V_UthwDhAn8jap00=433", "authors": ["TLDR Newsletter"], "title": "Better Agents", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Flangwatch%2Fbetter-agents%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/e1rD7RT2zGzjkqTK1_uA1L60RF1V_UthwDhAn8jap00=433", "summary": "Better Agents (GitHub Repo) Better Agents is a CLI tool and a set of standards for agent building. It makes coding assistants experts in any agent framework. The tool generates an AGENTS.md that ensures industry best practices. The CLI guides users through selecting a programming language, agent framework, coding assistant, LLM provider, and API keys.", "source": "tldr", "AI": {"tldr": "Better Agents\u662f\u4e00\u4e2aCLI\u5de5\u5177\u548c\u6807\u51c6\u96c6\uff0c\u7528\u4e8e\u6784\u5efaAI\u4ee3\u7406\uff0c\u901a\u8fc7\u751f\u6210AGENTS.md\u6587\u4ef6\u786e\u4fdd\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\uff0c\u4f7f\u7f16\u7801\u52a9\u624b\u80fd\u591f\u5728\u4efb\u4f55\u4ee3\u7406\u6846\u67b6\u4e2d\u6210\u4e3a\u4e13\u5bb6\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5f00\u53d1\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u5bfc\u81f4\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b\u3001\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u3002\u9700\u8981\u5de5\u5177\u6765\u6807\u51c6\u5316\u4ee3\u7406\u6784\u5efa\u6d41\u7a0b\uff0c\u786e\u4fdd\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\u5f97\u5230\u9075\u5faa\u3002", "method": "\u63d0\u4f9bCLI\u5de5\u5177\u5f15\u5bfc\u7528\u6237\u9009\u62e9\u7f16\u7a0b\u8bed\u8a00\u3001\u4ee3\u7406\u6846\u67b6\u3001\u7f16\u7801\u52a9\u624b\u3001LLM\u63d0\u4f9b\u5546\u548cAPI\u5bc6\u94a5\uff0c\u5e76\u751f\u6210\u5305\u542b\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\u7684AGENTS.md\u6587\u6863\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u5de5\u5177\u96c6\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u9ad8\u8d28\u91cf\u7684AI\u4ee3\u7406\uff0c\u786e\u4fdd\u6700\u4f73\u5b9e\u8df5\u5f97\u5230\u5b9e\u65bd\u3002", "conclusion": "Better Agents\u901a\u8fc7\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u548c\u6587\u6863\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7406\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2511.d48be7de", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FEveryInc%2Fcompounding-engineering-plugin%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/7LVeaQSYbsnVM32Zvt5lZ_l6nb3mm57yMc-VDzGSDMM=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FEveryInc%2Fcompounding-engineering-plugin%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/7LVeaQSYbsnVM32Zvt5lZ_l6nb3mm57yMc-VDzGSDMM=433", "authors": ["TLDR Newsletter"], "title": "Compounding Engineering Plugin", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FEveryInc%2Fcompounding-engineering-plugin%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/7LVeaQSYbsnVM32Zvt5lZ_l6nb3mm57yMc-VDzGSDMM=433", "summary": "Compounding Engineering Plugin (GitHub Repo) The Compounding Engineering Plugin is a Claude Code plugin that transforms how developers plan, build, and review code using AI-powered tools that systematically improve their development workflow. Compound engineering is the idea that each unit of engineering work should make subsequent units of work easier, not harder. The plugin provides the tools to make compound engineering practical.", "source": "tldr", "AI": {"tldr": "Compounding Engineering Plugin\u662f\u4e00\u4e2aClaude Code\u63d2\u4ef6\uff0c\u901a\u8fc7AI\u5de5\u5177\u7cfb\u7edf\u5316\u6539\u8fdb\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u4f7f\u6bcf\u4e2a\u5de5\u7a0b\u5355\u5143\u8ba9\u540e\u7eed\u5de5\u4f5c\u66f4\u5bb9\u6613\u800c\u975e\u66f4\u96be\u3002", "motivation": "\u4f20\u7edf\u5f00\u53d1\u4e2d\uff0c\u5de5\u7a0b\u5de5\u4f5c\u5f80\u5f80\u76f8\u4e92\u72ec\u7acb\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6539\u8fdb\u3002\u4f5c\u8005\u63d0\u51fa\"\u590d\u5408\u5de5\u7a0b\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u6bcf\u4e2a\u5de5\u7a0b\u5355\u5143\u90fd\u5e94\u4f7f\u540e\u7eed\u5de5\u4f5c\u66f4\u5bb9\u6613\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u7528\u5de5\u5177\u6765\u5b9e\u73b0\u8fd9\u4e00\u7406\u5ff5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aClaude Code\u63d2\u4ef6\uff0c\u63d0\u4f9bAI\u9a71\u52a8\u7684\u5de5\u5177\u96c6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u89c4\u5212\u3001\u6784\u5efa\u548c\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u7cfb\u7edf\u5316\u5e94\u7528\u590d\u5408\u5de5\u7a0b\u539f\u5219\uff0c\u4f7f\u8fd9\u4e00\u7406\u5ff5\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002", "result": "\u521b\u5efa\u4e86GitHub\u4ed3\u5e93\u4e2d\u7684Compounding Engineering Plugin\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u73b0\u590d\u5408\u5de5\u7a0b\u7684\u5177\u4f53\u5de5\u5177\uff0c\u80fd\u591f\u8f6c\u53d8\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u5de5\u7a0b\u5de5\u4f5c\u4ea7\u751f\u7d2f\u79ef\u6027\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u8be5\u63d2\u4ef6\u6210\u529f\u5c06\u590d\u5408\u5de5\u7a0b\u7406\u5ff5\u8f6c\u5316\u4e3a\u5b9e\u7528\u5de5\u5177\uff0c\u901a\u8fc7AI\u8f85\u52a9\u7cfb\u7edf\u5316\u6539\u8fdb\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u4f7f\u5de5\u7a0b\u5de5\u4f5c\u80fd\u591f\u4ea7\u751f\u6b63\u5411\u7d2f\u79ef\u6548\u5e94\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "tldr.2511.dd961855", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.primeintellect.ai%2Fblog%2Fintellect-3%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/p8PLjCDjyZh48hBfWgu-nMlRrj__06cQbSkvB161XqI=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.primeintellect.ai%2Fblog%2Fintellect-3%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/p8PLjCDjyZh48hBfWgu-nMlRrj__06cQbSkvB161XqI=433", "authors": ["TLDR Newsletter"], "title": "INTELLECT-3: A 100B+ MoE trained with large-scale RL", "comment": "Source: TLDR Newsletter, Date: 2025-11-28, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.primeintellect.ai%2Fblog%2Fintellect-3%3Futm_source=tldrai/1/0100019acad26bb9-b447c09e-810d-4e84-96cf-5dd64a46b7ed-000000/p8PLjCDjyZh48hBfWgu-nMlRrj__06cQbSkvB161XqI=433", "summary": "INTELLECT-3: A 100B+ MoE trained with large-scale RL (10 minute read) INTELLECT-3 is a 100B+ parameter Mixture-of-Experts that achieves state-of-the-art performance for its size across math, code, science, and reasoning benchmarks. It was trained with both SFT and RL on top of the GLM 4.5 Air base model. The researchers used a diverse and challenging mix of RL environments designed to enhance the reasoning and agentic capabilities of their model. Full details about the training are available.", "source": "tldr", "AI": {"tldr": "INTELLECT-3\u662f\u4e00\u4e2a100B+\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u540c\u89c4\u6a21SOTA\u6027\u80fd\uff0c\u57fa\u4e8eGLM 4.5 Air\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528SFT\u548cRL\u8bad\u7ec3\uff0c\u7279\u522b\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u7684RL\u73af\u5883\u6765\u589e\u5f3a\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u548c\u63a8\u7406\u7b49\u591a\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\u7684\u901a\u7528\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u7684\u6027\u80fd\u9650\u5236\u3002", "method": "\u57fa\u4e8eGLM 4.5 Air\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u53c2\u6570\u8d85\u8fc7100B\u3002\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7279\u522b\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684RL\u73af\u5883\u6765\u4e13\u95e8\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u548c\u63a8\u7406\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cINTELLECT-3\u8fbe\u5230\u4e86\u540c\u89c4\u6a21\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86RL\u8bad\u7ec3\u5bf9\u63d0\u5347\u6a21\u578b\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u5927\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.0275264c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fwhy-engineers-struggle-building-agents%3Futm_source=tldrdata/1/0100019ad998d72f-102b8166-1fa4-4bc9-9f27-ba4bed53dcf4-000000/deZtPxoWK3uFr5Fj1SxcJrgQvdbWOv3IBNPOGwzs28s=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fwhy-engineers-struggle-building-agents%3Futm_source=tldrdata/1/0100019ad998d72f-102b8166-1fa4-4bc9-9f27-ba4bed53dcf4-000000/deZtPxoWK3uFr5Fj1SxcJrgQvdbWOv3IBNPOGwzs28s=433", "authors": ["TLDR Newsletter"], "title": "Why Engineers Struggle to Build AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fwhy-engineers-struggle-building-agents%3Futm_source=tldrdata/1/0100019ad998d72f-102b8166-1fa4-4bc9-9f27-ba4bed53dcf4-000000/deZtPxoWK3uFr5Fj1SxcJrgQvdbWOv3IBNPOGwzs28s=433", "summary": "Why (Senior) Engineers Struggle to Build AI Agents (6 minute read) Senior engineers struggle to build AI agents because their traditional deterministic mindset\u2014centered on strict types, predictable control flow, and error-free code\u2014clashes with the probabilistic systems that thrive on ambiguity, natural language state, and non-linear behavior. Success requires embracing semantic flexibility, handing over control to the agent, treating errors as inputs, replacing unit tests with evaluations, a...", "source": "tldr", "AI": {"tldr": "\u8d44\u6df1\u5de5\u7a0b\u5e08\u6784\u5efaAI\u4ee3\u7406\u56f0\u96be\uff0c\u56e0\u4e3a\u4f20\u7edf\u786e\u5b9a\u6027\u601d\u7ef4\u4e0eAI\u6982\u7387\u7cfb\u7edf\u51b2\u7a81\uff0c\u9700\u8981\u8f6c\u53d8\u5f00\u53d1\u8303\u5f0f", "motivation": "\u5206\u6790\u8d44\u6df1\u5de5\u7a0b\u5e08\u5728\u6784\u5efaAI\u4ee3\u7406\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u63a2\u8ba8\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u601d\u7ef4\u4e0eAI\u4ee3\u7406\u5f00\u53d1\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02", "method": "\u901a\u8fc7\u5bf9\u6bd4\u4f20\u7edf\u786e\u5b9a\u6027\u5de5\u7a0b\u601d\u7ef4\u4e0eAI\u6982\u7387\u7cfb\u7edf\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u601d\u7ef4\u8f6c\u53d8\u548c\u5f00\u53d1\u5b9e\u8df5\u5efa\u8bae", "result": "\u8bc6\u522b\u51fa\u786e\u5b9a\u6027\u601d\u7ef4\u4e0e\u6982\u7387\u7cfb\u7edf\u4e4b\u95f4\u7684\u6838\u5fc3\u51b2\u7a81\uff0c\u63d0\u51fa\u6210\u529f\u6784\u5efaAI\u4ee3\u7406\u9700\u8981\u7684\u5173\u952e\u601d\u7ef4\u8f6c\u53d8", "conclusion": "\u8d44\u6df1\u5de5\u7a0b\u5e08\u9700\u8981\u4ece\u786e\u5b9a\u6027\u601d\u7ef4\u8f6c\u5411\u6982\u7387\u601d\u7ef4\uff0c\u63a5\u53d7\u8bed\u4e49\u7075\u6d3b\u6027\u3001\u4ee3\u7406\u81ea\u4e3b\u63a7\u5236\u3001\u9519\u8bef\u4f5c\u4e3a\u8f93\u5165\u7b49\u65b0\u8303\u5f0f", "topic": "agent analysis"}}
{"id": "tldr.2512.1cec1e7b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.vtrivedy.com%2Fposts%2Fagents-should-be-more-opinionated%3Futm_source=tldrnewsletter/1/0100019ad9a80c55-f376a1b0-aa4a-4392-93cc-535d664e550b-000000/eSm5GT2dscN7wNA-qgawgTYtLspb9puuDNNg08hGFOg=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.vtrivedy.com%2Fposts%2Fagents-should-be-more-opinionated%3Futm_source=tldrnewsletter/1/0100019ad9a80c55-f376a1b0-aa4a-4392-93cc-535d664e550b-000000/eSm5GT2dscN7wNA-qgawgTYtLspb9puuDNNg08hGFOg=433", "authors": ["TLDR Newsletter"], "title": "Agents Should Be More Opinionated", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.vtrivedy.com%2Fposts%2Fagents-should-be-more-opinionated%3Futm_source=tldrnewsletter/1/0100019ad9a80c55-f376a1b0-aa4a-4392-93cc-535d664e550b-000000/eSm5GT2dscN7wNA-qgawgTYtLspb9puuDNNg08hGFOg=433", "summary": "Agents Should Be More Opinionated (10 minute read) The best agent products are the most opinionated. The goal in agent products is to give users a delightful experience. A good baseline for agents is that everything works reliably without tweaking too many settings. Good product design is the result of creators distilling their vision into an intuitive interface that just works.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u6700\u4f73\u7684\u667a\u80fd\u4f53\u4ea7\u54c1\u5e94\u8be5\u66f4\u52a0\"\u56fa\u6267\u5df1\u89c1\"\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u9760\u3001\u65e0\u9700\u8fc7\u591a\u8c03\u6574\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u8ba9\u4e00\u5207\u90fd\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002", "motivation": "\u667a\u80fd\u4f53\u4ea7\u54c1\u7684\u76ee\u6807\u662f\u4e3a\u7528\u6237\u63d0\u4f9b\u6109\u60a6\u7684\u4f53\u9a8c\uff0c\u4f46\u76ee\u524d\u8bb8\u591a\u4ea7\u54c1\u8fc7\u4e8e\u7075\u6d3b\uff0c\u9700\u8981\u7528\u6237\u8fdb\u884c\u5927\u91cf\u8bbe\u7f6e\u548c\u8c03\u6574\uff0c\u8fd9\u964d\u4f4e\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u4ea7\u54c1\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u5c06\u521b\u4f5c\u8005\u7684\u613f\u666f\u63d0\u70bc\u4e3a\u76f4\u89c2\u7684\u754c\u9762\uff0c\u4f7f\u667a\u80fd\u4f53\u4ea7\u54c1\u80fd\u591f\u53ef\u9760\u5de5\u4f5c\u800c\u65e0\u9700\u8fc7\u591a\u8c03\u6574\u3002", "result": "\u63d0\u51fa\"\u56fa\u6267\u5df1\u89c1\"\u7684\u667a\u80fd\u4f53\u4ea7\u54c1\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5f3a\u8c03\u597d\u7684\u667a\u80fd\u4f53\u4ea7\u54c1\u5e94\u8be5\u50cf\u4f18\u79c0\u4ea7\u54c1\u8bbe\u8ba1\u4e00\u6837\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u548c\u9650\u5236\u6765\u63d0\u4f9b\u6700\u4f73\u4f53\u9a8c\u3002", "conclusion": "\u667a\u80fd\u4f53\u4ea7\u54c1\u5f00\u53d1\u8005\u5e94\u8be5\u66f4\u52a0\"\u56fa\u6267\u5df1\u89c1\"\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4ea7\u54c1\u51b3\u7b56\u6765\u63d0\u4f9b\u53ef\u9760\u3001\u6109\u60a6\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u800c\u4e0d\u662f\u8ba9\u7528\u6237\u627f\u62c5\u8fc7\u591a\u7684\u914d\u7f6e\u8d23\u4efb\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.0ebb5c2a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.moderne.ai%2Fcontent-library%2Fhow-uber-migrated-1m-lines-junit-2-weeks-webinar%3Futm_source=tldr%26utm_medium=email%26utm_campaign=dec_uber_webinar_dec_1%26utm_content=lp%26utm_term=register/1/0100019ad9cebe1c-dc6c3160-65da-4955-8559-516851e60b81-000000/SPM7zfFe26tomvLeduYAn4m9ElhVo4HvY2Uv8s1qoN0=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.moderne.ai%2Fcontent-library%2Fhow-uber-migrated-1m-lines-junit-2-weeks-webinar%3Futm_source=tldr%26utm_medium=email%26utm_campaign=dec_uber_webinar_dec_1%26utm_content=lp%26utm_term=register/1/0100019ad9cebe1c-dc6c3160-65da-4955-8559-516851e60b81-000000/SPM7zfFe26tomvLeduYAn4m9ElhVo4HvY2Uv8s1qoN0=433", "authors": ["TLDR Newsletter"], "title": "Migrating 1,000,000 lines of JUnit", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.moderne.ai%2Fcontent-library%2Fhow-uber-migrated-1m-lines-junit-2-weeks-webinar%3Futm_source=tldr%26utm_medium=email%26utm_campaign=dec_uber_webinar_dec_1%26utm_content=lp%26utm_term=register/1/0100019ad9cebe1c-dc6c3160-65da-4955-8559-516851e60b81-000000/SPM7zfFe26tomvLeduYAn4m9ElhVo4HvY2Uv8s1qoN0=433", "summary": "Migrating 1,000,000 lines of JUnit (Sponsor) Using OpenRewrite and AI-assisted static analysis, Uber automated 4,000 pull requests and modernized 1 million lines of code safely. Learn how they used context-aware refactoring to pull off a migration that would take most teams months in just two weeks. Watch the Moderne webinar on-demand", "source": "tldr", "AI": {"tldr": "Uber\u4f7f\u7528OpenRewrite\u548cAI\u8f85\u52a9\u9759\u6001\u5206\u6790\u81ea\u52a8\u5316\u4e864000\u4e2aPR\uff0c\u5b89\u5168\u5730\u5c06100\u4e07\u884c\u4ee3\u7801\u73b0\u4ee3\u5316\uff0c\u5c06\u539f\u672c\u9700\u8981\u6570\u6708\u7684\u8fc1\u79fb\u7f29\u77ed\u81f3\u4e24\u5468", "motivation": "\u9700\u8981\u5927\u89c4\u6a21\u8fc1\u79fb\u548c\u73b0\u4ee3\u5316JUnit\u4ee3\u7801\u5e93\uff0c\u4f20\u7edf\u624b\u52a8\u8fc1\u79fb\u65b9\u6cd5\u8017\u65f6\u6570\u6708\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528OpenRewrite\u9759\u6001\u5206\u6790\u5de5\u5177\u7ed3\u5408AI\u8f85\u52a9\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u91cd\u6784\uff0c\u81ea\u52a8\u5316\u751f\u6210\u548c\u63d0\u4ea4pull requests", "result": "\u6210\u529f\u81ea\u52a8\u5316\u5904\u74064000\u4e2apull requests\uff0c\u5b89\u5168\u8fc1\u79fb100\u4e07\u884c\u4ee3\u7801\uff0c\u5c06\u539f\u672c\u9700\u8981\u6570\u6708\u7684\u8fc1\u79fb\u65f6\u95f4\u7f29\u77ed\u5230\u4ec5\u4e24\u5468", "conclusion": "AI\u8f85\u52a9\u7684\u9759\u6001\u5206\u6790\u548c\u81ea\u52a8\u5316\u5de5\u5177\u80fd\u591f\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u4ee3\u7801\u8fc1\u79fb\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u5e76\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf", "topic": "swe application"}}
{"id": "tldr.2512.aa8376e1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fwriting-a-good-claude-md%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/nA3aBWQ11MR9ZSg-mf23yn1_B0VM0bpfjU_NVQoTrwU=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fwriting-a-good-claude-md%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/nA3aBWQ11MR9ZSg-mf23yn1_B0VM0bpfjU_NVQoTrwU=433", "authors": ["TLDR Newsletter"], "title": "Writing a good CLAUDE.md", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fwriting-a-good-claude-md%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/nA3aBWQ11MR9ZSg-mf23yn1_B0VM0bpfjU_NVQoTrwU=433", "summary": "Writing a good CLAUDE.md (10 minute read) CLAUDE.md, a file for onboarding Claude to a codebase, provides important context about the project's purpose, technology stack, and operational instructions. The CLAUDE.md should be kept concise and universally applicable. Instead of stuffing everything into CLAUDE.md, progressive disclosure should be used by pointing to separate, task-specific markdown files when needed and relying on deterministic tools like linters for code styling.", "source": "tldr", "AI": {"tldr": "CLAUDE.md\u662f\u7528\u4e8e\u5f15\u5bfcClaude\u7406\u89e3\u4ee3\u7801\u5e93\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5e94\u4fdd\u6301\u7b80\u6d01\u901a\u7528\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u62ab\u9732\u6307\u5411\u7279\u5b9a\u4efb\u52a1\u6587\u6863\uff0c\u5e76\u4f9d\u8d56\u786e\u5b9a\u6027\u5de5\u5177\u5904\u7406\u4ee3\u7801\u98ce\u683c", "motivation": "\u4e3aClaude AI\u52a9\u624b\u63d0\u4f9b\u6709\u6548\u7684\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u7406\u89e3\u548c\u64cd\u4f5c\u9879\u76ee\uff0c\u540c\u65f6\u907f\u514d\u4fe1\u606f\u8fc7\u8f7d\uff0c\u4fdd\u6301\u914d\u7f6e\u6587\u4ef6\u7684\u7b80\u6d01\u6027\u548c\u53ef\u7ef4\u62a4\u6027", "method": "\u521b\u5efa\u7b80\u6d01\u901a\u7528\u7684CLAUDE.md\u6587\u4ef6\uff0c\u5305\u542b\u9879\u76ee\u76ee\u7684\u3001\u6280\u672f\u6808\u548c\u64cd\u4f5c\u6307\u4ee4\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u62ab\u9732\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u7528\u7279\u5b9a\u4efb\u52a1\u6587\u6863\u548c\u4f7f\u7528\u786e\u5b9a\u6027\u5de5\u5177\uff08\u5982linters\uff09\u6765\u5904\u7406\u590d\u6742\u9700\u6c42", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u6709\u6548\u7684CLAUDE.md\u7f16\u5199\u6307\u5357\uff0c\u5f3a\u8c03\u7b80\u6d01\u6027\u3001\u901a\u7528\u6027\u548c\u6e10\u8fdb\u5f0f\u62ab\u9732\u539f\u5219\uff0c\u4e3aAI\u52a9\u624b\u4e0e\u4ee3\u7801\u5e93\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5", "conclusion": "CLAUDE.md\u5e94\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7684\u5165\u53e3\u70b9\uff0c\u901a\u8fc7\u5f15\u7528\u5916\u90e8\u6587\u6863\u548c\u5de5\u5177\u6765\u6269\u5c55\u529f\u80fd\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u6a21\u5f0f\u80fd\u63d0\u9ad8AI\u52a9\u624b\u7684\u5de5\u4f5c\u6548\u7387\u548c\u4ee3\u7801\u5e93\u7684\u53ef\u7ef4\u62a4\u6027", "topic": "code agent"}}
{"id": "tldr.2512.9c85e894", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwandb.ai%2Fsite%2Fresources%2Fwhitepapers%2Fevaluating-ai-agent-applications%2F%3Futm_source=tldr-dev%26utm_medium=cpc%26utm_campaign=WP-EvalAIAgents%26utm_content=weave%26utm_term=newsletter/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/tSaHZZMz_OMq25XLf8zG8MTUqaAuf44uMCoVl-6APIM=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwandb.ai%2Fsite%2Fresources%2Fwhitepapers%2Fevaluating-ai-agent-applications%2F%3Futm_source=tldr-dev%26utm_medium=cpc%26utm_campaign=WP-EvalAIAgents%26utm_content=weave%26utm_term=newsletter/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/tSaHZZMz_OMq25XLf8zG8MTUqaAuf44uMCoVl-6APIM=433", "authors": ["TLDR Newsletter"], "title": "How to evaluate AI agents, a guide by Weights & Biases", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwandb.ai%2Fsite%2Fresources%2Fwhitepapers%2Fevaluating-ai-agent-applications%2F%3Futm_source=tldr-dev%26utm_medium=cpc%26utm_campaign=WP-EvalAIAgents%26utm_content=weave%26utm_term=newsletter/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/tSaHZZMz_OMq25XLf8zG8MTUqaAuf44uMCoVl-6APIM=433", "summary": "How to evaluate AI agents, a guide by Weights & Biases (Sponsor) AI agent development breaks the rules of traditional software development. Learn from teams at Canva, Cisco, Snowflake and more, on how to establish an evaluation framework so you can assess your agent across accuracy, latency, cost, safety, and user experience and more. Get a practical five-step recipe for effective evaluations in this guide from Weights & Biases.", "source": "tldr", "AI": {"tldr": "Weights & Biases\u53d1\u5e03\u7684AI\u667a\u80fd\u4f53\u8bc4\u4f30\u6307\u5357\uff0c\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e94\u6b65\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u3001\u6210\u672c\u3001\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u7b49\u591a\u4e2a\u7ef4\u5ea6", "motivation": "AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u89c4\u5219\u4e0d\u540c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u6765\u81eaCanva\u3001Cisco\u3001Snowflake\u7b49\u516c\u53f8\u7684\u56e2\u961f\u7ecf\u9a8c\u8868\u660e\uff0c\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u8bc4\u4f30\u667a\u80fd\u4f53\u6027\u80fd\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u4f9b\u5b9e\u7528\u7684\u4e94\u6b65\u8bc4\u4f30\u914d\u65b9\uff0c\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\u4ee5\u8bc4\u4f30AI\u667a\u80fd\u4f53\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u3001\u6210\u672c\u3001\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u7b49\u591a\u4e2a\u65b9\u9762\u7684\u8868\u73b0", "result": "\u901a\u8fc7Weights & Biases\u7684\u6307\u5357\uff0c\u5f00\u53d1\u56e2\u961f\u53ef\u4ee5\u83b7\u5f97\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u501f\u9274\u884c\u4e1a\u9886\u5148\u516c\u53f8\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5efa\u7acb\u6709\u6548\u7684AI\u667a\u80fd\u4f53\u8bc4\u4f30\u4f53\u7cfb", "conclusion": "AI\u667a\u80fd\u4f53\u5f00\u53d1\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\uff0cWeights & Biases\u63d0\u4f9b\u7684\u4e94\u6b65\u8bc4\u4f30\u6307\u5357\u4e3a\u56e2\u961f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e2e\u52a9\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u8868\u73b0", "topic": "agent analysis"}}
{"id": "tldr.2512.501ff875", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fthe-space-of-minds%2F%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/k0QpdkBSfuVWqWrjSs8Cxq8gJvyfCH7MDjKMROGI8Bk=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fthe-space-of-minds%2F%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/k0QpdkBSfuVWqWrjSs8Cxq8gJvyfCH7MDjKMROGI8Bk=433", "authors": ["TLDR Newsletter"], "title": "The space of minds", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fthe-space-of-minds%2F%3Futm_source=tldrdev/1/0100019ad9e5200e-b1072cdc-6856-4756-8a9c-55324dd1efff-000000/k0QpdkBSfuVWqWrjSs8Cxq8gJvyfCH7MDjKMROGI8Bk=433", "summary": "The space of minds (3 minute read) Both animal and LLM intelligences are forms of optimization.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u52a8\u7269\u548cLLM\u667a\u80fd\u90fd\u662f\u4f18\u5316\u7684\u5f62\u5f0f\uff0c\u63d0\u51fa\u4e86\"\u5fc3\u667a\u7a7a\u95f4\"\u7684\u6982\u5ff5\u6846\u67b6", "motivation": "\u7406\u89e3\u52a8\u7269\u667a\u80fd\u548c\u4eba\u5de5\u667a\u80fd\uff08\u7279\u522b\u662fLLM\uff09\u4e4b\u95f4\u7684\u672c\u8d28\u8054\u7cfb\uff0c\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u4e0d\u540c\u5f62\u5f0f\u7684\u667a\u80fd", "method": "\u91c7\u7528\u6982\u5ff5\u5206\u6790\u6846\u67b6\uff0c\u5c06\u52a8\u7269\u548cLLM\u667a\u80fd\u90fd\u89c6\u4e3a\u4f18\u5316\u7684\u5f62\u5f0f\uff0c\u63d0\u51fa\"\u5fc3\u667a\u7a7a\u95f4\"\u7684\u6982\u5ff5\u6765\u7edf\u4e00\u7406\u89e3\u4e0d\u540c\u667a\u80fd\u7cfb\u7edf", "result": "\u5efa\u7acb\u4e86\u52a8\u7269\u667a\u80fd\u4e0eLLM\u667a\u80fd\u7684\u7edf\u4e00\u7406\u8bba\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u4f5c\u4e3a\u4f18\u5316\u8fc7\u7a0b\u7684\u5171\u540c\u672c\u8d28", "conclusion": "\u52a8\u7269\u548cLLM\u667a\u80fd\u90fd\u662f\u4f18\u5316\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u8fd9\u4e00\u89c6\u89d2\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3\u667a\u80fd\u7684\u672c\u8d28\u548c\u6f14\u5316", "topic": "agent analysis"}}
{"id": "wechat.2512.a784badd", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4OTU1MjcyMw==&mid=2247523119&idx=1&sn=3b7056525b0175b42d14493f08454dca&chksm=fc8355a071774c7a1054eec0ffc5ebc2f51a4115bf4a01a0e0c3b2e0c6fb02ade2aa7061547c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4OTU1MjcyMw==&mid=2247523119&idx=1&sn=3b7056525b0175b42d14493f08454dca&chksm=fc8355a071774c7a1054eec0ffc5ebc2f51a4115bf4a01a0e0c3b2e0c6fb02ade2aa7061547c#rd", "authors": ["\u540c\u6d4e\u667a\u80fd\u6c7d\u8f66\u7814\u7a76\u6240"], "title": "\u57fa\u4e8e\u201c\u6050\u60e7-\u597d\u5947\u201d\u5f15\u5bfc\u7684\u5b89\u5168\u9ad8\u6548\u7aef\u5230\u7aef\u81ea\u4e3b\u5bfc\u822a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5", "comment": "Source: WeChat, Published: 2025-12-02 08:31:20", "summary": "\u201cFeariosity\u201d\u6a21\u578b\u7528\u4e8e\u7ea6\u675f\u5e76\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff1b\u5bfc\u822a\u573a\u666f\u7684\u611f\u77e5\u4fe1\u606f\u7531\u8bed\u4e49\u5206\u5272\u56fe\u50cf\u63d0\u4f9b\u2161 \u76f8\u5173\u5de5\u4f5cA. \u9762\u5411\u8f66\u8f86\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5b89\u5168\u68c0\u6d4b\u5668\u901a\u8fc7\u88ab\u52a8\u963b\u6b62\u9ad8\u98ce\u9669\u52a8\u4f5c\u6765\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f8b\u5982\u5728 RL \u8f93\u51fa\u540e\u589e\u52a0\u5916\u90e8\u6a21\u5757\uff0c\u8fd9\u867d\u7136\u63d0\u9ad8\u4e86\u53ef\u89e3", "AI": {"tldr": "\u201cFeariosity\u201d\u6a21\u578b\u7528\u4e8e\u7ea6\u675f\u5e76\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff1b\u5bfc\u822a\u573a\u666f\u7684\u611f\u77e5\u4fe1\u606f\u7531\u8bed\u4e49\u5206\u5272\u56fe\u50cf\u63d0\u4f9b\u2161 \u76f8\u5173\u5de5\u4f5cA. \u9762\u5411\u8f66\u8f86\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5b89\u5168\u68c0\u6d4b\u5668\u901a\u8fc7\u88ab\u52a8\u963b\u6b62\u9ad8\u98ce\u9669\u52a8\u4f5c\u6765\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f8b\u5982\u5728 RL \u8f93\u51fa\u540e\u589e\u52a0\u5916\u90e8\u6a21\u5757\uff0c\u8fd9\u867d\u7136\u63d0\u9ad8\u4e86\u53ef\u89e3", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.05d9e943", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwNjg5OQ==&mid=2247486967&idx=1&sn=f0a7561729120b2ff30f22b5d7182d2d&chksm=fa1e08a6b23193c3ac62a827dc0e9445ad68e9e7567a88e319ed9e1d8b2e691b7f8350ac9505#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1MjkwNjg5OQ==&mid=2247486967&idx=1&sn=f0a7561729120b2ff30f22b5d7182d2d&chksm=fa1e08a6b23193c3ac62a827dc0e9445ad68e9e7567a88e319ed9e1d8b2e691b7f8350ac9505#rd", "authors": ["\u667a\u7b97\u50ac\u5316"], "title": "\u4ece\u8bad\u7ec3DeepSeek\u5230\u638c\u63a7\u673a\u5668\u4eba\uff0c\u90fd\u79bb\u4e0d\u5f00\u7684\u201c\u795e\u79d8\u6838\u5fc3\"\u2014\u2014<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff01\u4f60\u7684\u201c\u4eba\u751f\u7b97\u6cd5\"\u53ef\u80fd\u4e5f\u662f\u5b83", "comment": "Source: WeChat, Published: 2025-12-02 05:53:39", "summary": "\u70b9\u51fb\u5361\u7247 \u5173\u6ce8\u6211\u4eec 1. \u4e3a\u4ec0\u4e48\u8981\u5b66\u5f3a\u5316\u5b66\u4e60\uff1f\u5728\u673a\u5668\u5b66\u4e60\u8fd9\u95e8\u201c\u8ba9\u673a\u5668\u5b66\u4f1a\u505a\u51b3\u5b9a\u201d\u7684\u5b66\u79d1\u91cc\uff0c\u6700\u5e38\u88ab\u63d0\u8d77\u7684\u662f\u4e09\u4f4d\u201c\u5144\u5f1f\u201d\uff1a\u76d1\u7763\u5b66\u4e60\uff08Supervised Learning\uff09\u3001\u65e0\u76d1\u7763\u5b66\u4e60\uff08Unsupervised Learning\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09", "AI": {"tldr": "\u70b9\u51fb\u5361\u7247 \u5173\u6ce8\u6211\u4eec 1. \u4e3a\u4ec0\u4e48\u8981\u5b66\u5f3a\u5316\u5b66\u4e60\uff1f\u5728\u673a\u5668\u5b66\u4e60\u8fd9\u95e8\u201c\u8ba9\u673a\u5668\u5b66\u4f1a\u505a\u51b3\u5b9a\u201d\u7684\u5b66\u79d1\u91cc\uff0c\u6700\u5e38\u88ab\u63d0\u8d77\u7684\u662f\u4e09\u4f4d\u201c\u5144\u5f1f\u201d\uff1a\u76d1\u7763\u5b66\u4e60\uff08Supervised Learning\uff09\u3001\u65e0\u76d1\u7763\u5b66\u4e60\uff08Unsupervised Learning\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.f09f842e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMDY5MzYxNg==&mid=2247492836&idx=1&sn=fc7a463bf2a409fe03edceff606a4876&chksm=c3efc5561ada55186dcb74aa1996c84871e899c2e86b69355430acad35e353de74c7aba44544#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMDY5MzYxNg==&mid=2247492836&idx=1&sn=fc7a463bf2a409fe03edceff606a4876&chksm=c3efc5561ada55186dcb74aa1996c84871e899c2e86b69355430acad35e353de74c7aba44544#rd", "authors": ["\u5b57\u8282\u8df3\u52a8Seed"], "title": "Seed Research\uff5cGR-RL \u53d1\u5e03\uff1a\u7a81\u7834VLA\u7cbe\u7ec6\u64cd\u4f5c\u74f6\u9888\uff0c\u9996\u6b21\u5b9e\u73b0\u771f\u673a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7a7f\u978b\u5e26", "comment": "Source: WeChat, Published: 2025-12-02 04:23:38", "summary": "\u5728\u7ebf\u9636\u6bb5\uff1a\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7cbe\u7ec6\u8bad\u7ec3 \u56e2\u961f\u53d1\u73b0\uff0c\u6a21\u578b\u5fc5\u987b\u628a\u7cfb\u7edf\u7ea7\u4f18\u5316\u90fd\u5f53\u6210\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc7\u95ed\u73af\u5728\u7ebf\u4ea4\u4e92\u8fdb\u884c\u63a2\u7d22\u4e0e\u81ea\u6211\u4f18\u5316\uff0c\u8fd9\u4e00\u70b9\u5bf9\u4e8e\u7cbe\u7ec6\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "AI": {"tldr": "\u5728\u7ebf\u9636\u6bb5\uff1a\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7cbe\u7ec6\u8bad\u7ec3 \u56e2\u961f\u53d1\u73b0\uff0c\u6a21\u578b\u5fc5\u987b\u628a\u7cfb\u7edf\u7ea7\u4f18\u5316\u90fd\u5f53\u6210\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc7\u95ed\u73af\u5728\u7ebf\u4ea4\u4e92\u8fdb\u884c\u63a2\u7d22\u4e0e\u81ea\u6211\u4f18\u5316\uff0c\u8fd9\u4e00\u70b9\u5bf9\u4e8e\u7cbe\u7ec6\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.e265d6ea", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MTU5OTg1OQ==&mid=2247500211&idx=2&sn=6578bad77a39a8ac6608603379ba0180&chksm=c32587a2de5ab42626fafd76220d361687cb18bed1b080a822453cbd69b6f7004a438f9db264#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MTU5OTg1OQ==&mid=2247500211&idx=2&sn=6578bad77a39a8ac6608603379ba0180&chksm=c32587a2de5ab42626fafd76220d361687cb18bed1b080a822453cbd69b6f7004a438f9db264#rd", "authors": ["AI\u5e1d\u56fd"], "title": "INTELLECT-3\uff1a\u5f00\u6e90\u5927\u89c4\u6a21<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3\u7684\u524d\u6cbf\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-12-02 00:19:00", "summary": "\uff082\uff09 \u5f3a\u5316\u5b66\u4e60\uff1a\u4f7f\u7528 256 \u4e2a prompt \u7684 batch size\uff0c\u6bcf\u4e2a prompt 16 \u4e2a rollout\uff0c\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6 65536\u3002\u8bad\u7ec3\u73af\u5883\u6df7\u5408\u5305\u62ec\uff1a\u6570\u5b66\uff0821.2K \u95ee\u9898\uff09\u3001\u4ee3\u7801\uff088.6K \u95ee\u9898\uff09\u3001\u79d1\u5b66\uff0829.3K \u95ee\u9898\uff09\u3001\u903b\u8f91\uff0811.6K \u95ee\u9898\uff09\u3001Deep Research\uff082.2K \u95ee\u9898\uff09\u548c\u8f6f\u4ef6\u5de5", "AI": {"tldr": "\uff082\uff09 \u5f3a\u5316\u5b66\u4e60\uff1a\u4f7f\u7528 256 \u4e2a prompt \u7684 batch size\uff0c\u6bcf\u4e2a prompt 16 \u4e2a rollout\uff0c\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6 65536\u3002\u8bad\u7ec3\u73af\u5883\u6df7\u5408\u5305\u62ec\uff1a\u6570\u5b66\uff0821.2K \u95ee\u9898\uff09\u3001\u4ee3\u7801\uff088.6K \u95ee\u9898\uff09\u3001\u79d1\u5b66\uff0829.3K \u95ee\u9898\uff09\u3001\u903b\u8f91\uff0811.6K \u95ee\u9898\uff09\u3001Deep Research\uff082.2K \u95ee\u9898\uff09\u548c\u8f6f\u4ef6\u5de5", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.c6286070", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505160&idx=1&sn=781f0b54e4a83ef6f12a33d70b09359f&chksm=9aabbc0e2d043430b65e8137851d5a93cfba88c39e993a55336fbb1dc6e44ff47436bcb3afd6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505160&idx=1&sn=781f0b54e4a83ef6f12a33d70b09359f&chksm=9aabbc0e2d043430b65e8137851d5a93cfba88c39e993a55336fbb1dc6e44ff47436bcb3afd6#rd", "authors": ["\u5173\u4e8eNLP\u90a3\u4e9b\u4f60\u4e0d\u77e5\u9053\u7684\u4e8b"], "title": "NeurIPS 2025 | <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>-\u76f8\u5173\u8bba\u658712\u7bc7", "comment": "Source: WeChat, Published: 2025-12-01 16:00:00", "summary": "\u3010\u8981\u70b9\u3011\u8bba\u6587\u63d0\u51fa\u4e86\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u5355\u6b21\u90e8\u7f72\u4e0b\u7684\u8fde\u7eed\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u4f18\u52bf\u3002", "AI": {"tldr": "\u3010\u8981\u70b9\u3011\u8bba\u6587\u63d0\u51fa\u4e86\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u5355\u6b21\u90e8\u7f72\u4e0b\u7684\u8fde\u7eed\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u4f18\u52bf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.16913dc6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MzEwMDEzNQ==&mid=2650534932&idx=4&sn=49bdaaa46fe951d18c0cf467c0eead02&chksm=8912b9177af25cf246a3d7a28f67ceb28b5c73aa417d64fad72e858f5996d2d534337cc18ce3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MzEwMDEzNQ==&mid=2650534932&idx=4&sn=49bdaaa46fe951d18c0cf467c0eead02&chksm=8912b9177af25cf246a3d7a28f67ceb28b5c73aa417d64fad72e858f5996d2d534337cc18ce3#rd", "authors": ["\u56fd\u6cf0\u6d77\u901a\u8bc1\u5238\u7814\u7a76"], "title": "\u56fd\u6cf0\u6d77\u901a\uff5c\u8ba1\u7b97\u673a\uff1a\u8c46\u5305<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5d4c\u5165\u624b\u673a\u7cfb\u7edf\uff0c\u63a8\u52a8\u7aef\u4fa7 AI \u4ece\u5e94\u7528\u7ea7\u8fc8\u5411\u7cfb\u7edf\u7ea7", "comment": "Source: WeChat, Published: 2025-12-02 09:53:49", "summary": "\u8c46\u5305\u5927\u6a21\u578b\u80fd\u529b\u5168\u9762\uff0c\u6a21\u578b\u5728\u63a8\u7406\u3001\u89c6\u89c9\u7406\u89e3\u3001\u56fe\u50cf\u521b\u4f5c\u3001\u89c6\u9891\u751f\u6210\u3001\u8bed\u97f3\u7b49\u65b9\u9762\u7684\u6027\u80fd\u8fbe\u5230\u56fd\u9645\u4e00\u6d41\u6c34\u5e73\uff0c\u56fe\u5f62\u754c\u9762\u64cd\u4f5c\u80fd\u529b\u66f4\u662f\u5728\u591a\u9879\u6743\u5a01\u8bc4\u6d4b\u4e2d\u83b7\u5f97\u4e1a\u754c\u6700\u4f73\u6210\u7ee9\uff0c\u6709\u4e86\u7cfb\u7edf\u7ea7\u7684\u64cd\u4f5c\u6743\u9650\uff0c\u901a\u8fc7\u8bc6\u522b\u624b\u673a\u5c4f\u5e55\u4e0a\u7684 UI \u5143\u7d20", "AI": {"tldr": "\u8c46\u5305\u5927\u6a21\u578b\u80fd\u529b\u5168\u9762\uff0c\u6a21\u578b\u5728\u63a8\u7406\u3001\u89c6\u89c9\u7406\u89e3\u3001\u56fe\u50cf\u521b\u4f5c\u3001\u89c6\u9891\u751f\u6210\u3001\u8bed\u97f3\u7b49\u65b9\u9762\u7684\u6027\u80fd\u8fbe\u5230\u56fd\u9645\u4e00\u6d41\u6c34\u5e73\uff0c\u56fe\u5f62\u754c\u9762\u64cd\u4f5c\u80fd\u529b\u66f4\u662f\u5728\u591a\u9879\u6743\u5a01\u8bc4\u6d4b\u4e2d\u83b7\u5f97\u4e1a\u754c\u6700\u4f73\u6210\u7ee9\uff0c\u6709\u4e86\u7cfb\u7edf\u7ea7\u7684\u64cd\u4f5c\u6743\u9650\uff0c\u901a\u8fc7\u8bc6\u522b\u624b\u673a\u5c4f\u5e55\u4e0a\u7684 UI \u5143\u7d20", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2512.d40c7761", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MTAzODAzNw==&mid=2651840057&idx=1&sn=a4e1bc0e142cbd45b2395f961daf8d0d&chksm=f3110c8dcfc55247fce6e9e6d705f4ca0ba4acbdba4b6938bd5da39f56c12f5e0f7f6226fa29#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MTAzODAzNw==&mid=2651840057&idx=1&sn=a4e1bc0e142cbd45b2395f961daf8d0d&chksm=f3110c8dcfc55247fce6e9e6d705f4ca0ba4acbdba4b6938bd5da39f56c12f5e0f7f6226fa29#rd", "authors": ["\u4e2d\u56fd\u4e2d\u94c1"], "title": "\u4e2d\u56fd\u4e2d\u94c1\u201c\u5148\u950b\u201d\u7cfb\u5217\u4e09\u5927\u5782\u57df<em class=\"highlight\">\u5927\u6a21\u578b</em>\u96c6\u4e2d\u4e0a\u7ebf \u6784\u5efa\u6570\u667a\u8d4b\u80fd\u5168\u4ea7\u4e1a\u94fe\u65b0\u751f\u6001", "comment": "Source: WeChat, Published: 2025-12-02 09:33:59", "summary": "\u201c\u5148\u950b\u6865\u6881\u5927\u6a21\u578b\u201d\uff1a\u56fd\u5185\u9996\u4e2a\u7279\u6b8a\u53ca\u5927\u8de8\u5ea6\u6865\u6881\u9886\u57df\u5782\u76f4\u5927\u6a21\u578b11\u670829\u65e5\uff0c\u5728\u7b2c\u4e8c\u5c4a\u6865\u6881\u667a\u80fd\u4e0e\u7eff\u8272\u5efa\u9020\u5b66\u672f\u4f1a\u8bae\u4e0a\uff0c\u7531\u4e2d\u94c1\u5927\u6865\u5c40\u7275\u5934\uff0c\u8054\u5408\u4e2d\u94c1\u5927\u6865\u9662\u3001\u4e2d\u94c1\u5de5\u4e1a\u3001\u4e2d\u94c1\u4fe1\u79d1\u7b49\u5355\u4f4d\u5171\u540c\u5f00\u53d1\u7684\u56fd\u5185\u9996\u4e2a\u7279\u6b8a\u53ca\u5927\u8de8\u5ea6\u6865\u6881\u9886", "AI": {"tldr": "\u201c\u5148\u950b\u6865\u6881\u5927\u6a21\u578b\u201d\uff1a\u56fd\u5185\u9996\u4e2a\u7279\u6b8a\u53ca\u5927\u8de8\u5ea6\u6865\u6881\u9886\u57df\u5782\u76f4\u5927\u6a21\u578b11\u670829\u65e5\uff0c\u5728\u7b2c\u4e8c\u5c4a\u6865\u6881\u667a\u80fd\u4e0e\u7eff\u8272\u5efa\u9020\u5b66\u672f\u4f1a\u8bae\u4e0a\uff0c\u7531\u4e2d\u94c1\u5927\u6865\u5c40\u7275\u5934\uff0c\u8054\u5408\u4e2d\u94c1\u5927\u6865\u9662\u3001\u4e2d\u94c1\u5de5\u4e1a\u3001\u4e2d\u94c1\u4fe1\u79d1\u7b49\u5355\u4f4d\u5171\u540c\u5f00\u53d1\u7684\u56fd\u5185\u9996\u4e2a\u7279\u6b8a\u53ca\u5927\u8de8\u5ea6\u6865\u6881\u9886", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.4b37b92e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694360&idx=1&sn=c9f94aa58192d7904e693b76cbb1ac57&chksm=81ee9433c39d03e823260e056ccaf9992baaa8d4ac8dbd6ec3c6fba643c928e612fb1d471f66#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694360&idx=1&sn=c9f94aa58192d7904e693b76cbb1ac57&chksm=81ee9433c39d03e823260e056ccaf9992baaa8d4ac8dbd6ec3c6fba643c928e612fb1d471f66#rd", "authors": ["\u7ae0\u9c7c\u5927\u6570\u636e"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5728\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u2014\u2014\u600e\u4e48\u8ba9\u6a21\u578b\u751f\u6210\u66f4\u51c6\u786e\u7684SQL\u8bed\u53e5\u5b9e\u64cd", "comment": "Source: WeChat, Published: 2025-12-02 07:56:54", "summary": "\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u6570\u636e\u5206\u6790 \u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u5206\u6790\uff1f\u6211\u4eec\u90fd\u77e5\u9053\u4e0e\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u90fd\u662f\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u7684\uff0c\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u6362\u4e2a\u95ee\u6cd5\uff0c\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u7684\u7f16\u5199\u6570\u636e\u5206\u6790\u4ee3\u7801\uff08\u5305\u62ecSQL\uff0cshell\uff0cpython\u7b49\u811a\u672c\u4ee3\u7801\uff09\uff1f", "AI": {"tldr": "\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u6570\u636e\u5206\u6790 \u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u5206\u6790\uff1f\u6211\u4eec\u90fd\u77e5\u9053\u4e0e\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u90fd\u662f\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u7684\uff0c\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u6362\u4e2a\u95ee\u6cd5\uff0c\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u7684\u7f16\u5199\u6570\u636e\u5206\u6790\u4ee3\u7801\uff08\u5305\u62ecSQL\uff0cshell\uff0cpython\u7b49\u811a\u672c\u4ee3\u7801\uff09\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.07f1b60c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNjI4NjMzNw==&mid=2247483712&idx=1&sn=ab060e593c815aaba209172a2b27e5cb&chksm=f116147cb6183fe6bd71078f00f813abebeeca75a81e8b9f013fdc08267ea44d0bbe2d5ee8ce#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNjI4NjMzNw==&mid=2247483712&idx=1&sn=ab060e593c815aaba209172a2b27e5cb&chksm=f116147cb6183fe6bd71078f00f813abebeeca75a81e8b9f013fdc08267ea44d0bbe2d5ee8ce#rd", "authors": ["\u5927\u6a21\u578b\u89c6\u754c"], "title": "DeepSeek-V3.2\uff1a\u5f00\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5982\u4f55\u6458\u5f97IMO\u91d1\u724c\uff0c\u6311\u6218GPT-5\u4e0eGemini-3.0\uff1f", "comment": "Source: WeChat, Published: 2025-12-02 06:48:32", "summary": "\u591a\u6837\u5316\u7684 RL \u4efb\u52a1\u5bf9\u4e8e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002DeepSeek-AI \u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u5de5\u5177\uff0c\u5305\u62ec\u5b9e\u9645\u7684\u7f51\u7edc\u641c\u7d22 API\u3001\u7f16\u7801\u5de5\u5177\u548c Jupyter Notebook\uff0c\u6765\u5904\u7406\u641c\u7d22\u3001\u4ee3\u7801\u5de5\u7a0b\u548c\u4ee3\u7801\u89e3\u91ca\u7b49\u4efb\u52a1\u3002", "AI": {"tldr": "\u591a\u6837\u5316\u7684 RL \u4efb\u52a1\u5bf9\u4e8e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002DeepSeek-AI \u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u5de5\u5177\uff0c\u5305\u62ec\u5b9e\u9645\u7684\u7f51\u7edc\u641c\u7d22 API\u3001\u7f16\u7801\u5de5\u5177\u548c Jupyter Notebook\uff0c\u6765\u5904\u7406\u641c\u7d22\u3001\u4ee3\u7801\u5de5\u7a0b\u548c\u4ee3\u7801\u89e3\u91ca\u7b49\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.477da4ed", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772652&idx=1&sn=98c7723f1f8d4ecd595b87e45d4531a4&chksm=fab853d5a36948aeaf981c21ab0e7a0a5c923f36f34bc8de975f6158f4e6c1243b9c255431b8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772652&idx=1&sn=98c7723f1f8d4ecd595b87e45d4531a4&chksm=fab853d5a36948aeaf981c21ab0e7a0a5c923f36f34bc8de975f6158f4e6c1243b9c255431b8#rd", "authors": ["DataFunTalk"], "title": "\u8c46\u5305<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5bb6\u65cf\u5168\u7ebf\u7115\u65b0\uff01", "comment": "Source: WeChat, Published: 2025-12-02 05:00:00", "summary": "\u8c46\u5305\u5927\u6a21\u578b\u5bb6\u65cf\u5168\u7ebf\u7115\u65b0\u4e0d\u662f\u5c0f\u4fee\u5c0f\u8865\uff0c\u662f\u5168\u5bb6\u65cfAgent\u4ece\u6982\u5ff5\u5230\u843d\u5730\u7684\u5b8c\u6574\u8def\u5f84Agent\u5f00\u53d1\u5de5\u5177\u5168\u9762\u5347\u7ea7\u751f\u6001\u6269\u5bb9\uff0c\u66f4\u591a\u5408\u4f5c\u4f19\u4f34\u52a0\u5165\u6784\u5efaAI\u843d\u5730\u5168\u94fe\u8def\u80fd\u529b", "AI": {"tldr": "\u8c46\u5305\u5927\u6a21\u578b\u5bb6\u65cf\u5168\u7ebf\u7115\u65b0\u4e0d\u662f\u5c0f\u4fee\u5c0f\u8865\uff0c\u662f\u5168\u5bb6\u65cfAgent\u4ece\u6982\u5ff5\u5230\u843d\u5730\u7684\u5b8c\u6574\u8def\u5f84Agent\u5f00\u53d1\u5de5\u5177\u5168\u9762\u5347\u7ea7\u751f\u6001\u6269\u5bb9\uff0c\u66f4\u591a\u5408\u4f5c\u4f19\u4f34\u52a0\u5165\u6784\u5efaAI\u843d\u5730\u5168\u94fe\u8def\u80fd\u529b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.9ab846e6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzAzMDMwOA==&mid=2247488067&idx=1&sn=771935680c5619fbc7489eabcb12e5a4&chksm=cfbdae34281c27f37fb0a573454e8c16da8845867a60a0c247c11b0d469b650bd6800bc25fc5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzAzMDMwOA==&mid=2247488067&idx=1&sn=771935680c5619fbc7489eabcb12e5a4&chksm=cfbdae34281c27f37fb0a573454e8c16da8845867a60a0c247c11b0d469b650bd6800bc25fc5#rd", "authors": ["AI\u63a2\u7d22\u65f6\u4ee3"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5728\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u2014\u2014\u600e\u4e48\u8ba9\u6a21\u578b\u751f\u6210\u66f4\u51c6\u786e\u7684SQL\u8bed\u53e5\u5b9e\u64cd", "comment": "Source: WeChat, Published: 2025-12-02 03:01:51", "summary": "\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u6570\u636e\u5206\u6790 \u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u5206\u6790\uff1f\u6211\u4eec\u90fd\u77e5\u9053\u4e0e\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u90fd\u662f\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u7684\uff0c\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u6362\u4e2a\u95ee\u6cd5\uff0c\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u7684\u7f16\u5199\u6570\u636e\u5206\u6790\u4ee3\u7801\uff08\u5305\u62ecSQL\uff0cshell\uff0cpython\u7b49\u811a\u672c\u4ee3\u7801\uff09\uff1f", "AI": {"tldr": "\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u6570\u636e\u5206\u6790 \u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u5730\u8fdb\u884c\u5206\u6790\uff1f\u6211\u4eec\u90fd\u77e5\u9053\u4e0e\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u90fd\u662f\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u7684\uff0c\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u6362\u4e2a\u95ee\u6cd5\uff0c\u600e\u4e48\u8ba9\u5927\u6a21\u578b\u66f4\u597d\u7684\u7f16\u5199\u6570\u636e\u5206\u6790\u4ee3\u7801\uff08\u5305\u62ecSQL\uff0cshell\uff0cpython\u7b49\u811a\u672c\u4ee3\u7801\uff09\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
