{"id": "2602.06975", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06975", "abs": "https://arxiv.org/abs/2602.06975", "authors": ["R. James Cotton", "Thomas Leonard"], "title": "BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents", "comment": null, "summary": "Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.", "AI": {"tldr": "BiomechAgent\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4ee3\u7801\u7684AI\u4ee3\u7406\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u65e0\u9700\u7f16\u7a0b\u6280\u80fd\u5373\u53ef\u8fdb\u884c\u751f\u7269\u529b\u5b66\u5206\u6790\uff0c\u5305\u62ec\u6570\u636e\u67e5\u8be2\u3001\u53ef\u89c6\u5316\u548c\u4e34\u5e8a\u63a8\u7406\u3002", "motivation": "\u867d\u7136\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u6280\u672f\u4f7f\u5b9a\u91cf\u8fd0\u52a8\u5206\u6790\u8d8a\u6765\u8d8a\u666e\u53ca\uff0c\u4f46\u5206\u6790\u751f\u6210\u7684\u6570\u636e\u5bf9\u4e8e\u6ca1\u6709\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u7684\u4e34\u5e8a\u533b\u751f\u4ecd\u7136\u662f\u4e00\u4e2a\u969c\u788d\u3002\u9700\u8981\u4e00\u79cd\u5de5\u5177\u6765\u964d\u4f4e\u751f\u7269\u529b\u5b66\u5206\u6790\u7684\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u4e86BiomechAgent\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee3\u7801\u751f\u6210\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u754c\u9762\u652f\u6301\u751f\u7269\u529b\u5b66\u5206\u6790\u3002\u521b\u5efa\u4e86\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6570\u636e\u68c0\u7d22\u3001\u53ef\u89c6\u5316\u3001\u6d3b\u52a8\u5206\u7c7b\u3001\u65f6\u95f4\u5206\u5272\u548c\u4e34\u5e8a\u63a8\u7406\u3002\u8bc4\u4f30\u4e86\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u4e0e\u901a\u7528\u63d0\u793a\u7684\u6548\u679c\uff0c\u5e76\u96c6\u6210\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e13\u7528\u5de5\u5177\u3002", "result": "BiomechAgent\u5728\u6570\u636e\u68c0\u7d22\u548c\u53ef\u89c6\u5316\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u5174\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002\u751f\u7269\u529b\u5b66\u9886\u57df\u7684\u7279\u5b9a\u6307\u4ee4\u663e\u8457\u4f18\u4e8e\u901a\u7528\u63d0\u793a\uff0c\u96c6\u6210\u4e13\u7528\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u5de5\u5177\u5927\u5e45\u63d0\u5347\u4e86\u65f6\u7a7a\u5206\u6790\u7684\u51c6\u786e\u6027\u3002\u4f7f\u7528\u672c\u5730\u5f00\u6e90\u6a21\u578b\u800c\u975e\u524d\u6cbf\u4e91\u7aefLLM\u65f6\uff0c\u9664\u6570\u636e\u68c0\u7d22\u5916\u5176\u4ed6\u9886\u57df\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "BiomechAgent\u4f7f\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u7684\u6570\u636e\u5bf9\u6700\u7ec8\u7528\u6237\u66f4\u52a0\u6709\u7528\u548c\u53ef\u8bbf\u95ee\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u754c\u9762\u964d\u4f4e\u4e86\u751f\u7269\u529b\u5b66\u5206\u6790\u7684\u95e8\u69db\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u65e0\u9700\u7f16\u7a0b\u6280\u80fd\u5373\u53ef\u8fdb\u884c\u590d\u6742\u7684\u5206\u6790\u3002", "topic": "code agent"}}
{"id": "2602.06976", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.06976", "abs": "https://arxiv.org/abs/2602.06976", "authors": ["Chen Shen", "Wei Cheng", "Jingyue Yang", "Huan Zhang", "Yuhan Wu", "Wei Hu"], "title": "Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks", "comment": null, "summary": "The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.", "AI": {"tldr": "ILA-agent\u6846\u67b6\u8ba9LLM\u901a\u8fc7\u63a8\u7406\u65f6\u4e0e\u6709\u9650\u5916\u90e8\u8d44\u6e90\u4ea4\u4e92\u6765\u5b66\u4e60\u964c\u751f\u7f16\u7a0b\u8bed\u8a00\uff0c\u663e\u8457\u4f18\u4e8e\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf", "motivation": "LLM\u5728\u964c\u751f\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684\u7f16\u7801\u80fd\u529b\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u63a2\u7d22\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u63a8\u7406\u65f6\u8bed\u8a00\u5b66\u4e60\u65b9\u6cd5", "method": "\u63d0\u51faILA-agent\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u4e3a\u5de5\u5177\u96c6\uff0c\u8ba9LLM\u901a\u8fc7\u4e0e\u5b98\u65b9\u6587\u6863\u548c\u6267\u884c\u73af\u5883\u7684\u7ed3\u6784\u5316\u4ea4\u4e92\u6765\u589e\u91cf\u63a2\u7d22\u3001\u5e94\u7528\u548c\u9a8c\u8bc1\u8bed\u8a00\u77e5\u8bc6", "result": "\u5728Cangjie-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cILA-agent\u5728\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63a8\u7406\u65f6\u8bed\u8a00\u83b7\u53d6\u662f\u6709\u6548\u7684\u4f4e\u8d44\u6e90\u5b66\u4e60\u8303\u5f0f\uff0cILA-agent\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u964c\u751f\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u4ecd\u5b58\u5728\u6027\u80fd\u5dee\u8ddd", "topic": "code agent"}}
{"id": "2602.07072", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07072", "abs": "https://arxiv.org/abs/2602.07072", "authors": ["Igor Costa"], "title": "AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation", "comment": "18 pages, 4 figures, 6 tables", "summary": "Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.", "AI": {"tldr": "AgentSpawn\u662f\u4e00\u4e2a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u52a8\u5185\u5b58\u8f6c\u79fb\u3001\u81ea\u9002\u5e94\u751f\u6210\u7b56\u7565\u548c\u5e76\u53d1\u4e00\u81f4\u6027\u534f\u8bae\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4e0a\u4e0b\u6587\u6301\u7eed\u6027\u548c\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f7f\u7528\u9759\u6001\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5728\u8fd0\u884c\u65f6\u5206\u6790\u63ed\u793a\u610f\u5916\u590d\u6742\u6027\u65f6\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002\u957f\u65f6\u7a0b\u4ee3\u7801\u751f\u6210\u9700\u8981\u6301\u7eed\u7684\u4e0a\u4e0b\u6587\u548c\u8de8\u9886\u57df\u7684\u81ea\u9002\u5e94\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u63d0\u51faAgentSpawn\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u751f\u6210\u65f6\u7684\u81ea\u52a8\u5185\u5b58\u8f6c\u79fb\u673a\u5236\uff0c(2) \u7531\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u6307\u6807\u89e6\u53d1\u7684\u81ea\u9002\u5e94\u751f\u6210\u7b56\u7565\uff0c(3) \u5e76\u53d1\u4fee\u6539\u7684\u4e00\u81f4\u6027\u534f\u8bae\u3002", "result": "\u5728SWE-bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentSpawn\u6bd4\u9759\u6001\u57fa\u7ebf\u5b9e\u73b0\u4e8634%\u7684\u5b8c\u6210\u7387\u63d0\u5347\uff0c\u540c\u65f6\u901a\u8fc7\u9009\u62e9\u6027\u5207\u7247\u51cf\u5c11\u4e8642%\u7684\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "AgentSpawn\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u5185\u5b58\u8fde\u7eed\u6027\u3001\u6280\u80fd\u7ee7\u627f\u3001\u4efb\u52a1\u6062\u590d\u3001\u8fd0\u884c\u65f6\u751f\u6210\u548c\u5e76\u53d1\u4e00\u81f4\u6027\u7684\u4e94\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.07034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07034", "abs": "https://arxiv.org/abs/2602.07034", "authors": ["Jinxiu Qu", "Zirui Tang", "Hongzhang Huang", "Boyu Niu", "Wei Zhou", "Jiannan Wang", "Yitong Song", "Guoliang Li", "Xuanhe Zhou", "Fan Wu"], "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA", "comment": null, "summary": "Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.", "AI": {"tldr": "ST-Raptor\u662f\u4e00\u4e2a\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7f16\u8f91\u3001\u6811\u5f62\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u635f\u5931\u548c\u590d\u6742\u5e03\u5c40\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u9700\u8981\u7cbe\u786e\u63d0\u53d6\u5355\u5143\u683c\u5185\u5bb9\u548c\u4f4d\u7f6e\uff0c\u5e76\u6062\u590d\u8868\u683c\u5e03\u5c40\u4e2d\u9690\u542b\u7684\u903b\u8f91\u7ed3\u6784\u3001\u5c42\u6b21\u5173\u7cfb\u548c\u8bed\u4e49\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u635f\u5931\u3001\u5904\u7406\u590d\u6742\u5e03\u5c40\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u800c\u4eba\u5de5\u89e3\u91ca\u53c8\u8017\u65f6\u8017\u529b\u3002", "method": "ST-Raptor\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u5206\u6790\u73af\u5883\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u8f91\u3001\u57fa\u4e8e\u6811\u7684\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u67e5\u8be2\u89e3\u51b3\u673a\u5236\uff0c\u652f\u6301\u51c6\u786e\u4e14\u7528\u6237\u53cb\u597d\u7684\u8868\u683c\u7406\u89e3\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cST-Raptor\u5728\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ST-Raptor\u901a\u8fc7\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u73af\u5883\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u548c\u590d\u6742\u5e03\u5c40\u5904\u7406\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.07079", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07079", "abs": "https://arxiv.org/abs/2602.07079", "authors": ["Go Frendi Gunawan", "Mukhlis Amien"], "title": "Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark", "comment": "10 pages, 7 figures. Under review. Code and data will be fully released", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf911\u4e2a\u5148\u8fdbLLM\u57285\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fdb\u884c\u591a\u4efb\u52a1\u8bc4\u4f30\uff0c\u53d1\u73b0\u76f8\u540c\u5b8c\u7f8e\u5206\u6570\u6a21\u578b\u5728\u5b8c\u6210\u65f6\u95f4\u3001\u5de5\u5177\u6548\u7387\u548c\u6210\u672c\u4e0a\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff0c\u5de5\u5177\u4f7f\u7528\u9891\u7387\u4e0e\u6210\u529f\u7387\u65e0\u5173\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u4f4e\u6548\u6a21\u5f0f\uff0c\u5e76\u5f00\u6e90\u6240\u6709\u5b9e\u9a8c\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u8986\u76d6\u591a\u6837\u5316SE\u6d3b\u52a8\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u3002\u73b0\u6709\u8bc4\u4f30\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\uff0c\u65e0\u6cd5\u5168\u9762\u8861\u91cfLLM\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u7efc\u5408\u8868\u73b0\u3002", "method": "\u6784\u5efa\u591a\u4efb\u52a1\u8bc4\u4f30\u6846\u67b6\uff0c\u5bf911\u4e2aSOTA LLM\u57285\u4e2a\u4ee3\u8868\u6027\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08bug\u4fee\u590d\u3001\u529f\u80fd\u5f00\u53d1\u3001\u4ee3\u7801\u91cd\u6784\u3001\u6280\u672f\u6587\u6863\u64b0\u5199\u3001\u7814\u7a76\u7efc\u8ff0\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u4f7f\u7528\u81ea\u52a8\u5316\u9a8c\u8bc1\u6846\u67b6\u540c\u65f6\u8861\u91cf\u8f93\u51fa\u8d28\u91cf\u548c\u5b8c\u6210\u6548\u7387\u3002", "result": "\u5173\u952e\u53d1\u73b0\uff1a1) \u76f8\u540c\u5b8c\u7f8e\u5206\u6570\u6a21\u578b\u5728\u5b8c\u6210\u65f6\u95f4\uff0822\u500d\u5dee\u5f02\uff09\u3001\u5de5\u5177\u6548\u7387\uff0849\u500d\u5dee\u5f02\uff09\u548c\u4f30\u8ba1\u6210\u672c\uff0853\u500d\u5dee\u5f02\uff09\u4e0a\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff1b2) \u5de5\u5177\u4f7f\u7528\u9891\u7387\u4e0e\u6210\u529f\u7387\u65e0\u76f8\u5173\u6027\uff1b3) \u8bc6\u522b\u51fa\u4e24\u79cd\u4f4e\u6548\u6a21\u5f0f\uff1a\u5faa\u73af\u4f4e\u6548\u548c\u63a8\u7406\u4f4e\u6548\uff1b4) \u7f16\u7801\u4efb\u52a1\u6210\u529f\u7387100%\uff0c\u7814\u7a76\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027\uff0890.9%\uff09\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u6548\u7387\u5dee\u5f02\u663e\u8457\uff0c\u4ec5\u5173\u6ce8\u51c6\u786e\u6027\u4f1a\u5ffd\u7565\u91cd\u8981\u7684\u6548\u7387\u6307\u6807\u3002\u5de5\u5177\u4f7f\u7528\u9891\u7387\u4e0d\u4ee3\u8868\u6210\u529f\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5e73\u8861\u8d28\u91cf\u3001\u6548\u7387\u548c\u6210\u672c\u3002", "topic": "swe benchmark"}}
{"id": "2602.07035", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07035", "abs": "https://arxiv.org/abs/2602.07035", "authors": ["Jiahao Zhao", "Shaoxuan Xu", "Zhongxiang Sun", "Fengqi Zhu", "Jingyang Ou", "Yuling Shi", "Chongxuan Li", "Xiao Zhang", "Jun Xu"], "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents", "comment": null, "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C", "AI": {"tldr": "DLLM-Searcher\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u4ee3\u7406\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u63d0\u5347\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u5e76\u884c\u63a8\u7406\u4e0e\u6267\u884c\u8303\u5f0fP-ReAct\u6765\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u641c\u7d22\u4ee3\u7406\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u5ef6\u8fdf\u6311\u6218\uff1aReAct\u4ee3\u7406\u8303\u5f0f\u4e2d\u7684\u4e32\u884c\u591a\u8f6e\u63a8\u7406\u3001\u5de5\u5177\u8c03\u7528\u548c\u7b49\u5f85\u5bfc\u81f4\u4e25\u91cd\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff1b2) \u4ee3\u7406\u80fd\u529b\u6311\u6218\uff1a\u73b0\u6709\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u4e0a\u8868\u73b0\u8f83\u5f31\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5176\u5e76\u884c\u89e3\u7801\u4f18\u52bf\u3002", "method": "\u63d0\u51faDLLM-Searcher\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1a\u4ee3\u7406\u76d1\u7763\u5fae\u8c03(Agentic SFT)\u548c\u4ee3\u7406\u65b9\u5dee\u51cf\u5c11\u504f\u597d\u4f18\u5316(Agentic VRPO)\uff0c\u589e\u5f3adLLM\u7684\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\uff1b2) \u5e76\u884c\u63a8\u7406\u4e0e\u6267\u884c\u8303\u5f0fP-ReAct\uff1a\u5229\u7528dLLM\u7684\u7075\u6d3b\u751f\u6210\u673a\u5236\uff0c\u4f18\u5148\u89e3\u7801\u5de5\u5177\u8c03\u7528\u6307\u4ee4\uff0c\u4f7f\u6a21\u578b\u5728\u7b49\u5f85\u5de5\u5177\u8fd4\u56de\u65f6\u7ee7\u7eed\u601d\u8003\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLLM-Searcher\u5728\u6027\u80fd\u4e0a\u4e0e\u4e3b\u6d41\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u76f8\u5f53\uff0c\u540c\u65f6P-ReAct\u8303\u5f0f\u5b9e\u73b0\u4e86\u7ea615%\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "DLLM-Searcher\u901a\u8fc7\u589e\u5f3adLLM\u7684\u4ee3\u7406\u80fd\u529b\u548c\u5f15\u5165\u5e76\u884c\u63a8\u7406\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u641c\u7d22\u4ee3\u7406\u7684\u5ef6\u8fdf\u548c\u80fd\u529b\u6311\u6218\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.07080", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07080", "abs": "https://arxiv.org/abs/2602.07080", "authors": ["Yicheng He", "Zheng Zhao", "Zhou Kaiyu", "Bryan Dai", "Jie Fu", "Yonghui Yang"], "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs", "comment": null, "summary": "Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u4ee3\u7801\u9a8c\u8bc1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u795e\u7ecf\u52a8\u6001\u4e2d\u7684\u53ef\u89e3\u7801\u4fe1\u53f7\u6765\u9884\u6d4b\u4ee3\u7801\u751f\u6210\u7684\u903b\u8f91\u6b63\u786e\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6d4b\u8bd5\u6216\u8bc4\u4f30\u673a\u5236\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u9a8c\u8bc1\u8303\u5f0f\u4e25\u91cd\u4f9d\u8d56\u5916\u90e8\u673a\u5236\uff08\u5982\u57fa\u4e8e\u6267\u884c\u7684\u5355\u5143\u6d4b\u8bd5\u6216\u8f85\u52a9LLM\u8bc4\u4f30\u5668\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u52b3\u52a8\u5bc6\u96c6\u578b\u6216\u53d7\u9650\u4e8e\u8bc4\u4f30\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u80fd\u5426\u4eceLLM\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u8bc4\u4f30\u5176\u529f\u80fd\u6b63\u786e\u6027\u3002", "method": "\u53d7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u542f\u53d1\uff0c\u5c06\u4ee3\u7801\u9a8c\u8bc1\u89c6\u4e3a\u673a\u5236\u8bca\u65ad\u4efb\u52a1\uff0c\u5c06\u6a21\u578b\u7684\u663e\u5f0f\u7b97\u6cd5\u8f68\u8ff9\u6620\u5c04\u5230\u884c\u7ea7\u5f52\u56e0\u56fe\u3002\u901a\u8fc7\u5206\u89e3\u590d\u6742\u7684\u6b8b\u5dee\u6d41\uff0c\u8bc6\u522b\u6a21\u578b\u5185\u90e8\u7535\u8def\u4e2d\u533a\u5206\u5408\u7406\u63a8\u7406\u548c\u903b\u8f91\u5931\u8d25\u7684\u7ed3\u6784\u7279\u5f81\u3002", "result": "\u5728Python\u3001C++\u548cJava\u4e0a\u7684\u5206\u6790\u8bc1\u5b9e\uff0c\u5185\u5728\u6b63\u786e\u6027\u4fe1\u53f7\u5728\u4e0d\u540c\u8bed\u6cd5\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002\u4ece\u8fd9\u4e9b\u5185\u90e8\u56fe\u63d0\u53d6\u7684\u62d3\u6251\u7279\u5f81\u6bd4\u8868\u9762\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u53ef\u9760\u5730\u9884\u6d4b\u6b63\u786e\u6027\uff0c\u5e76\u80fd\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u56e0\u679c\u5e72\u9884\u6765\u4fee\u590d\u9519\u8bef\u903b\u8f91\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5185\u90e8\u81ea\u7701\u53ef\u4f5c\u4e3a\u9a8c\u8bc1\u751f\u6210\u4ee3\u7801\u7684\u53ef\u89e3\u7801\u5c5e\u6027\uff0c\u4e3a\u4ee3\u7801\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u7684\u5185\u90e8\u89c6\u89d2\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2602.07040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07040", "abs": "https://arxiv.org/abs/2602.07040", "authors": ["Emmett Bicker"], "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods", "comment": "Available at www.asterlab.ai, 25 pages, 8 figures, 4 tables", "summary": "We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.", "AI": {"tldr": "Aster\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684AI\u4ee3\u7406\uff0c\u80fd\u591f\u6bd4\u73b0\u6709\u6846\u67b6\u5feb20\u500d\u4ee5\u4e0a\u8fd0\u884c\u3002\u5b83\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u7a0b\u5e8f\uff0c\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\u901f\u5ea6\u8f83\u6162\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u957f\u65f6\u95f4\u8bc4\u4f30\u7684\u4efb\u52a1\uff08\u5982\u6570\u5c0f\u65f6\u7684\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\uff09\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u6765\u6269\u5c55\u53ef\u5904\u7406\u95ee\u9898\u7684\u9886\u57df\u3002", "method": "\u7ed9\u5b9a\u4efb\u52a1\u3001\u521d\u59cb\u7a0b\u5e8f\u548c\u8bc4\u4f30\u7a0b\u5e8f\u6027\u80fd\u7684\u811a\u672c\uff0cAster\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u7a0b\u5e8f\u6765\u4f18\u5316\u6027\u80fd\u3002\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u4e86\u65b0\u53d1\u73b0\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u8bc4\u4f30\u65f6\u95f4\u957f\u7684\u4efb\u52a1\u3002", "result": "\u5728\u6570\u5b66\u3001GPU\u5185\u6838\u5de5\u7a0b\u3001\u751f\u7269\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7b49\u591a\u4e2a\u9886\u57df\u7684\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff1aErdos\u6700\u5c0f\u91cd\u53e0\u95ee\u9898\u3001TriMul\u5185\u6838\u4f18\u5316\u3001\u5355\u7ec6\u80de\u5206\u6790\u53bb\u566a\u95ee\u9898\u3001\u795e\u7ecf\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\u8bad\u7ec3\u548cNanoGPT Speedrun\u7ade\u8d5b\u3002\u9664\u4e86ZAPBench\u5339\u914d\u6700\u4f73\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\u5916\uff0c\u5176\u4ed6\u6240\u6709\u4efb\u52a1\u90fd\u8fbe\u5230\u4e86SOTA\uff0c\u4e14\u8ba1\u7b97\u91cf\u4e0d\u52301/190\u3002", "conclusion": "Aster\u901a\u8fc7\u5927\u5e45\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff0c\u663e\u8457\u52a0\u901f\u4e86\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\uff0c\u6269\u5c55\u4e86\u53ef\u5904\u7406\u95ee\u9898\u7684\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u957f\u65f6\u95f4\u8bc4\u4f30\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "2602.07181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07181", "abs": "https://arxiv.org/abs/2602.07181", "authors": ["Tianyu Zhao", "Siqi Li", "Yasser Shoukry", "Salma Elmalaki"], "title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs", "comment": null, "summary": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u4eba\u683c\u7279\u8d28\u4f5c\u4e3a\u6f5c\u5728\u4fe1\u53f7\u6765\u4f18\u5316LLM\u4e2a\u6027\u5316\u56de\u7b54\uff0c\u5f00\u53d1\u4e86PACIFIC\u4eba\u683c\u6807\u6ce8\u504f\u597d\u6570\u636e\u96c6\u548c\u81ea\u52a8\u68c0\u7d22\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u7528\u6237\u504f\u597d\u4e2a\u6027\u5316LLM\u56de\u7b54\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u504f\u597d\u4fe1\u53f7\u53ef\u80fd\u5608\u6742\u3001\u4e0d\u5b8c\u6574\u751a\u81f3\u8bef\u5bfc\u6027\uff0c\u76f4\u63a5\u5e94\u7528\u4f1a\u964d\u4f4e\u56de\u7b54\u8d28\u91cf\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u7a33\u5b9a\u7684\u4eba\u683c\u7279\u8d28\u5851\u9020\u65e5\u5e38\u504f\u597d\uff0c\u56e0\u6b64\u7814\u7a76\u5c06\u4eba\u683c\u4f5c\u4e3a\u504f\u597d\u80cc\u540e\u7684\u539f\u5219\u6027\u6f5c\u5728\u4fe1\u53f7\u3002", "method": "1) \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u7684\u6709\u6548\u6027\uff1b2) \u6784\u5efaPACIFIC\u6570\u636e\u96c6\uff0c\u5305\u542b1200\u4e2a\u8de8\u9886\u57df\u504f\u597d\u9648\u8ff0\uff0c\u6807\u6ce8\u4e86Big-Five\uff08OCEAN\uff09\u7279\u8d28\u65b9\u5411\uff1b3) \u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u81ea\u52a8\u68c0\u7d22\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u5e76\u5728\u56de\u7b54\u751f\u6210\u4e2d\u6574\u5408\u5b83\u4eec\u3002", "result": "\u4f7f\u7528\u4eba\u683c\u5bf9\u9f50\u504f\u597d\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u95ee\u7b54\u6027\u80fd\uff1a\u9009\u62e9\u4e0e\u7528\u6237\u63a8\u65ad\u4eba\u683c\u4e00\u81f4\u7684\u504f\u597d\uff0c\u5c06\u7b54\u6848\u9009\u62e9\u51c6\u786e\u7387\u4ece29.25%\u63d0\u9ad8\u523076%\uff08\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u504f\u597d\uff09\u3002\u5b9e\u9a8c\u8868\u660e\u4eba\u683c\u4f5c\u4e3a\u6f5c\u5728\u4fe1\u53f7\u80fd\u6709\u6548\u63d0\u5347\u4e2a\u6027\u5316\u56de\u7b54\u8d28\u91cf\u3002", "conclusion": "\u4eba\u683c\u7279\u8d28\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u6f5c\u5728\u4fe1\u53f7\u6765\u4f18\u5316LLM\u7684\u4e2a\u6027\u5316\u56de\u7b54\uff0c\u901a\u8fc7\u4eba\u683c\u5bf9\u9f50\u7684\u504f\u597d\u9009\u62e9\u80fd\u663e\u8457\u63d0\u5347\u56de\u7b54\u51c6\u786e\u6027\u3002\u63d0\u51fa\u7684PACIFIC\u6570\u636e\u96c6\u548c\u81ea\u52a8\u68c0\u7d22\u6846\u67b6\u4e3a\u57fa\u4e8e\u4eba\u683c\u7684\u4e2a\u6027\u5316LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.07054", "categories": ["cs.LG", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07054", "abs": "https://arxiv.org/abs/2602.07054", "authors": ["Ashutosh Chaubey", "Jiacheng Pang", "Maksim Siniukov", "Mohammad Soleymani"], "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization", "comment": "Accepted as a conference paper at ICLR 2026. Project page: https://avere-iclr.github.io", "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.", "AI": {"tldr": "\u63d0\u51faEmoReAlM\u57fa\u51c6\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u7684\u4f2a\u5173\u8054\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u51faAVEm-DPO\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u60c5\u611f\u4e0e\u65e0\u5173\u89c6\u542c\u7ebf\u7d22\u4e4b\u95f4\u7684\u4f2a\u5173\u8054\uff0c\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u4e3b\u5e72\u4e2d\u6587\u672c\u5148\u9a8c\u9a71\u52a8\u7684\u89c6\u542c\u7ebf\u7d22\u5e7b\u89c9", "method": "1) \u5f15\u5165EmoReAlM\u57fa\u51c6\u8bc4\u4f30\u7ebf\u7d22-\u60c5\u611f\u5173\u8054\u3001\u5e7b\u89c9\u548c\u6a21\u6001\u4e00\u81f4\u6027\uff1b2) \u63d0\u51faAVEm-DPO\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u6784\u5efa\u5bf9\u4f2a\u5173\u8054\u6216\u5e7b\u89c9\u54cd\u5e94\u7684\u504f\u597d\uff0c\u5e76\u5305\u542b\u60e9\u7f5a\u6587\u672c\u5148\u9a8c\u4f9d\u8d56\u7684\u6b63\u5219\u5316\u9879", "result": "\u5728DFEW\u3001RAVDESS\u548cEMER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u83b7\u5f976-19%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u4e25\u8c28\u7684\u57fa\u51c6\u548c\u9c81\u68d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u60c5\u611f\u7406\u89e3\u548c\u793e\u4ea4AI\u4e2d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.07083", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07083", "abs": "https://arxiv.org/abs/2602.07083", "authors": ["Yongqing Jiang", "Jianze Wang", "Zhiqi Shen", "Zhenghong Lin", "Jiayuan Wang", "Yijian Yang", "Kaoshan Dai", "Haoran Luo"], "title": "Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation", "comment": null, "summary": "Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u7684\u81ea\u52a8\u5efa\u7b51\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u6784\u5efa\u3001\u7ea6\u675f\u5bfc\u5411\u7684\u6a21\u578b\u5bf9\u9f50\u548c\u9a8c\u8bc1\u9a71\u52a8\u7684\u8bc4\u4f30\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684\u4eff\u771f\u5c31\u7eea\u6a21\u578b\u3002", "motivation": "\u7ed3\u6784\u5efa\u6a21\u662f\u8ba1\u7b97\u5de5\u7a0b\u79d1\u5b66\u7684\u57fa\u7840\uff0c\u5373\u4f7f\u5fae\u5c0f\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u6216\u89c4\u8303\u8fdd\u53cd\u4e5f\u53ef\u80fd\u4f7f\u4e0b\u6e38\u4eff\u771f\u65e0\u6548\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u5efa\u6a21\u4ee3\u7801\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u4e25\u683c\u7684\u5de5\u7a0b\u7ea6\u675f\u4e0b\uff0c\u4e0d\u53ef\u6267\u884c\u6216\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\u4ecd\u7136\u666e\u904d\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u7684\u81ea\u52a8\u5efa\u7b51\u5efa\u6a21\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u5f15\u5165CivilInstruct\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u5f62\u5f0f\u5316\u7ed3\u6784\u5de5\u7a0b\u77e5\u8bc6\u548c\u7ea6\u675f\u63a8\u7406\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u6765\u5f3a\u5236\u7ea6\u675f\u6ee1\u8db3\u548cAPI\u5408\u89c4\u6027\uff1b3) \u63d0\u51faMBEval\u4f5c\u4e3a\u9a8c\u8bc1\u9a71\u52a8\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u95ed\u73af\u9a8c\u8bc1\u8bc4\u4f30\u53ef\u6267\u884c\u6027\u548c\u7ed3\u6784\u52a8\u529b\u5b66\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e25\u683c\u7684\u9a8c\u8bc1\u6307\u6807\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u8f93\u51fa\u548c\u4e0d\u5408\u89c4\u8f93\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u7ea6\u675f\u5bf9\u9f50\u548c\u9a8c\u8bc1\u9a71\u52a8\u8bc4\u4f30\uff0c\u80fd\u591f\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u6267\u884c\u7684\u5efa\u7b51\u6a21\u578b\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5de5\u7a0b\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u4e0d\u53ef\u6267\u884c\u548c\u7269\u7406\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2602.07055", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07055", "abs": "https://arxiv.org/abs/2602.07055", "authors": ["Pingyue Zhang", "Zihan Huang", "Yue Wang", "Jieyu Zhang", "Letian Xue", "Zihan Wang", "Qineng Wang", "Keshigeyan Chandrasegaran", "Ruohan Zhang", "Yejin Choi", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Manling Li"], "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?", "comment": "published at iclr 2026", "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u7a7a\u95f4\u7406\u8bba\"\u6982\u5ff5\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u4e3b\u52a8\u7a7a\u95f4\u63a2\u7d22\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b58\u5728\u4e3b\u52a8-\u88ab\u52a8\u5dee\u8ddd\u3001\u4f4e\u6548\u63a2\u7d22\u3001\u4fe1\u5ff5\u4e0d\u7a33\u5b9a\u548c\u4fe1\u5ff5\u60ef\u6027\u7b49\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u88ab\u52a8\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e3b\u52a8\u3001\u81ea\u4e3b\u63a2\u7d22\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u7a7a\u95f4\u5177\u8eab\u667a\u80fd\u9700\u8981\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u901a\u8fc7\u4e3b\u52a8\u884c\u52a8\u83b7\u53d6\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\"\u7a7a\u95f4\u7406\u8bba\"\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u667a\u80fd\u4f53\u901a\u8fc7\u81ea\u4e3b\u4e3b\u52a8\u63a2\u7d22\u83b7\u53d6\u4fe1\u606f\uff0c\u5e76\u4ece\u5e8f\u5217\u5316\u90e8\u5206\u89c2\u6d4b\u4e2d\u6784\u5efa\u3001\u4fee\u6b63\u548c\u5229\u7528\u7a7a\u95f4\u4fe1\u5ff5\u7684\u80fd\u529b\u3002\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u6784\u5efa\u51c6\u786e\u8ba4\u77e5\u5730\u56fe\u7684\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5173\u952e\u521b\u65b0\u662f\u7a7a\u95f4\u4fe1\u5ff5\u63a2\u6d4b\u6280\u672f\uff0c\u5728\u6bcf\u4e00\u6b65\u63d0\u793a\u6a21\u578b\u63ed\u793a\u5176\u5185\u90e8\u7a7a\u95f4\u8868\u5f81\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u51e0\u4e2a\u5173\u952e\u74f6\u9888\uff1a1) \u4e3b\u52a8-\u88ab\u52a8\u5dee\u8ddd\uff1a\u81ea\u4e3b\u6536\u96c6\u4fe1\u606f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b2) \u4f4e\u6548\u63a2\u7d22\uff1a\u76f8\u6bd4\u57fa\u4e8e\u7a0b\u5e8f\u7684\u4ee3\u7406\uff0c\u6a21\u578b\u63a2\u7d22\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff1b3) \u4fe1\u5ff5\u4e0d\u7a33\u5b9a\uff1a\u5168\u5c40\u4fe1\u5ff5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u7a7a\u95f4\u77e5\u8bc6\u968f\u65f6\u95f4\u9000\u5316\uff1b4) \u4fe1\u5ff5\u60ef\u6027\uff1a\u667a\u80fd\u4f53\u65e0\u6cd5\u7528\u65b0\u8bc1\u636e\u66f4\u65b0\u8fc7\u65f6\u7684\u5148\u9a8c\uff0c\u8fd9\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u6a21\u578b\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002", "conclusion": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u4e3b\u52a8\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u7ef4\u6301\u8fde\u8d2f\u3001\u53ef\u4fee\u6b63\u7684\u7a7a\u95f4\u4fe1\u5ff5\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.07086", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07086", "abs": "https://arxiv.org/abs/2602.07086", "authors": ["Michael Marketsm\u00fcller", "Simon Martin", "Tim Schlippe"], "title": "Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation", "comment": "preprint of conference submission", "summary": "Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u4e09\u79cdRAG\u53d8\u4f53\u5728SQL\u67e5\u8be2\u3001REST API\u8c03\u7528\u548c\u52a8\u6001\u4efb\u52a1\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0CoRAG\u5728\u6df7\u5408\u6587\u6863\u73af\u5883\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u68c0\u7d22\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u4f01\u4e1a\u7cfb\u7edf\u9700\u8981\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5c06\u7528\u6237\u8bf7\u6c42\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u64cd\u4f5c\uff0c\u4f46LLM\u5728\u7279\u5b9a\u9886\u57df\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u9700\u8981\u540c\u65f6\u5904\u7406\u68c0\u7d22\u548c\u4fee\u6539\u4efb\u52a1\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528SAP Transactional Banking\u4f5c\u4e3a\u4f01\u4e1a\u7528\u4f8b\uff0c\u6784\u5efa\u5305\u542bSQL\u548cAPI\u4e24\u79cd\u6a21\u6001\u7684\u65b0\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6807\u51c6RAG\u3001Self-RAG\u548cCoRAG\u4e09\u79cdRAG\u53d8\u4f53\u572818\u79cd\u5b9e\u9a8c\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u65e0\u68c0\u7d22\u65f6\u51c6\u786e\u7387\u4e3a0%\uff0c\u68c0\u7d22\u540e\u6267\u884c\u51c6\u786e\u7387\u6700\u9ad8\u8fbe79.30%\uff0c\u7ec4\u4ef6\u5339\u914d\u51c6\u786e\u7387\u6700\u9ad8\u8fbe78.86%\u3002CoRAG\u5728\u6df7\u5408\u6587\u6863\u73af\u5883\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u8fbe\u523010.29%\u7684\u7cbe\u786e\u5339\u914d\u7387\u3002", "conclusion": "\u68c0\u7d22\u7b56\u7565\u8bbe\u8ba1\u662f\u751f\u4ea7\u7ea7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u7684\u5173\u952e\u56e0\u7d20\uff0c\u8fed\u4ee3\u67e5\u8be2\u5206\u89e3\u4f18\u4e8etop-k\u68c0\u7d22\u548c\u4e8c\u5143\u76f8\u5173\u6027\u8fc7\u6ee4\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u5f02\u6784\u73af\u5883\u4e2d\u3002", "topic": "code agent"}}
{"id": "2602.07153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07153", "abs": "https://arxiv.org/abs/2602.07153", "authors": ["Jinbiao Wei", "Yilun Zhao", "Kangqi Ni", "Arman Cohan"], "title": "ANCHOR: Branch-Point Data Generation for GUI Agents", "comment": null, "summary": "End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.", "AI": {"tldr": "Anchor\u6846\u67b6\u901a\u8fc7\u4ece\u5c11\u91cf\u5df2\u9a8c\u8bc1\u79cd\u5b50\u6f14\u793a\u4e2d\u6269\u5c55\u8f68\u8ff9\uff0c\u4e3a\u684c\u9762GUI\u4ee3\u7406\u751f\u6210\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u4ea4\u4e92\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u684c\u9762\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u771f\u5b9e\u684c\u9762\u73af\u5883\u5f00\u53d1\u7aef\u5230\u7aefGUI\u4ee3\u7406\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u4ea4\u4e92\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u6536\u96c6\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u591a\u6837\u6027\u6709\u9650\u6216\u8f68\u8ff9\u566a\u58f0\u5927\u3001\u76ee\u6807\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAnchor\u8f68\u8ff9\u6269\u5c55\u6846\u67b6\uff1a\u4ece\u79cd\u5b50\u6f14\u793a\u4e2d\u8bc6\u522b\u5206\u652f\u70b9\uff08\u5bf9\u5e94\u6709\u610f\u4e49\u7684\u72b6\u6001\u53d8\u5316\uff09\uff0c\u57fa\u4e8e\u5f53\u524dGUI\u4e0a\u4e0b\u6587\u63d0\u51fa\u65b0\u7684\u72b6\u6001\u63a5\u5730\u4efb\u52a1\u53d8\u4f53\uff0c\u6267\u884c\u4ee3\u7406\u751f\u6210\u65b0\u8f68\u8ff9\uff0c\u9a8c\u8bc1\u5668\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u68c0\u67e5\u548c\u8f68\u8ff9\u7ea7\u4e00\u81f4\u6027\u786e\u4fdd\u4efb\u52a1\u5b8c\u6210\uff0c\u5e76\u5e94\u7528\u4efb\u52a1\u6761\u4ef6\u6b65\u9aa4\u7ea7\u8fc7\u6ee4\u53bb\u9664\u672a\u63a5\u5730\u52a8\u4f5c\uff0c\u5bf9\u5206\u652f\u540e\u6bb5\u53bb\u566a\u4ee5\u4fdd\u6301\u610f\u56fe\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u6807\u51c6\u684c\u9762\u57fa\u51c6\u6d4b\u8bd5OSWorld\u548cWindowsAgentArena\u4e0a\uff0c\u4f7f\u7528\u6269\u5c55\u8bed\u6599\u5e93\u5fae\u8c03\u7684\u6a21\u578b\u76f8\u6bd4\u96f6\u6837\u672c\u4ee3\u7406\u548c\u4ee3\u8868\u6027\u5408\u6210\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u80fd\u8de8\u5e94\u7528\u7a0b\u5e8f\u548c\u64cd\u4f5c\u7cfb\u7edf\u6cdb\u5316\u3002", "conclusion": "Anchor\u6846\u67b6\u80fd\u591f\u4ece\u5c11\u91cf\u79cd\u5b50\u6f14\u793a\u4e2d\u6269\u5c55\u51fa\u53ef\u6269\u5c55\u7684\u684c\u9762\u76d1\u7763\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347GUI\u4ee3\u7406\u5728\u771f\u5b9e\u684c\u9762\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u9ad8\u8d28\u91cf\u4ea4\u4e92\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.07187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07187", "abs": "https://arxiv.org/abs/2602.07187", "authors": ["Hanyu Wang", "Yuanpu Cao", "Lu Lin", "Jinghui Chen"], "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents", "comment": null, "summary": "Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.", "AI": {"tldr": "PreFlect\u662f\u4e00\u79cd\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\uff0c\u5c06\u4ee3\u7406\u53cd\u601d\u4ece\u6267\u884c\u540e\u7ea0\u6b63\u8f6c\u53d8\u4e3a\u6267\u884c\u524d\u9884\u89c1\uff0c\u901a\u8fc7\u5728\u6267\u884c\u524d\u6279\u8bc4\u548c\u4f18\u5316\u8ba1\u5212\u6765\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53cd\u601d\u7684\u4ee3\u7406\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u56de\u987e\u6027\u7684\uff1a\u4ee3\u7406\u5148\u6267\u884c\u3001\u89c2\u5bdf\u5931\u8d25\u3001\u7136\u540e\u5c1d\u8bd5\u6062\u590d\u3002\u8fd9\u79cd\u540e\u9a8c\u7ea0\u6b63\u65b9\u5f0f\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u5728\u6267\u884c\u524d\u5c31\u80fd\u9884\u89c1\u548c\u907f\u514d\u9519\u8bef\u3002", "method": "\u63d0\u51faPreFlect\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\uff0c\u5305\u62ec\uff1a1) \u4ece\u5386\u53f2\u4ee3\u7406\u8f68\u8ff9\u4e2d\u63d0\u53d6\u89c4\u5212\u9519\u8bef\u6a21\u5f0f\uff0c\u6355\u6349\u8fc7\u53bb\u6267\u884c\u4e2d\u7684\u6210\u529f\u548c\u5931\u8d25\u6a21\u5f0f\uff1b2) \u5728\u6267\u884c\u524d\u6279\u8bc4\u548c\u4f18\u5316\u4ee3\u7406\u8ba1\u5212\uff1b3) \u7ed3\u5408\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u673a\u5236\uff0c\u5f53\u539f\u59cb\u8ba1\u5212\u9047\u5230\u610f\u5916\u504f\u5dee\u65f6\u63d0\u4f9b\u6267\u884c\u65f6\u8ba1\u5212\u66f4\u65b0\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cPreFlect\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u6574\u4f53\u4ee3\u7406\u6548\u7528\uff0c\u4f18\u4e8e\u57fa\u4e8e\u53cd\u601d\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u51e0\u79cd\u66f4\u590d\u6742\u7684\u4ee3\u7406\u67b6\u6784\u3002", "conclusion": "\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\u6bd4\u4f20\u7edf\u7684\u56de\u987e\u6027\u53cd\u601d\u66f4\u6709\u6548\uff0c\u901a\u8fc7\u5728\u6267\u884c\u524d\u9884\u89c1\u548c\u907f\u514d\u9519\u8bef\uff0c\u7ed3\u5408\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u80fd\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.07238", "categories": ["cs.AI", "cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.07238", "abs": "https://arxiv.org/abs/2602.07238", "authors": ["Matthias Mertens", "Natalia Fischl-Lanzoni", "Neil Thompson"], "title": "Is there \"Secret Sauce'' in Large Language Model Development?", "comment": null, "summary": "Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728AI\u6a21\u578b\u6027\u80fd\u524d\u6cbf\uff0c80-90%\u7684\u6027\u80fd\u5dee\u5f02\u7531\u8bad\u7ec3\u8ba1\u7b97\u91cf\u89e3\u91ca\uff0c\u800c\u975e\u4e13\u6709\u6280\u672f\uff1b\u4f46\u5728\u975e\u524d\u6cbf\u9886\u57df\uff0c\u4e13\u6709\u6280\u672f\u80fd\u663e\u8457\u964d\u4f4e\u8fbe\u5230\u7279\u5b9a\u80fd\u529b\u6240\u9700\u7684\u8ba1\u7b97\u91cf", "motivation": "\u63a2\u7a76LLM\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff1a\u662f\u5f00\u53d1\u5546\u7684\u4e13\u6709\u6280\u672f\uff08\"\u79d8\u5bc6\u914d\u65b9\"\uff09\u8fd8\u662f\u5355\u7eaf\u7684\u8ba1\u7b97\u89c4\u6a21\u6269\u5c55\uff1f\u7406\u89e3AI\u9886\u5bfc\u5730\u4f4d\u548c\u80fd\u529b\u6269\u6563\u7684\u5f71\u54cd\u56e0\u7d20", "method": "\u4f7f\u75282022-2025\u5e74\u95f4\u53d1\u5e03\u7684809\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u57fa\u51c6\u6570\u636e\uff0c\u5efa\u7acb\u5305\u542b\u53d1\u5e03\u65e5\u671f\u548c\u5f00\u53d1\u5546\u56fa\u5b9a\u6548\u5e94\u7684\u6269\u5c55\u89c4\u5f8b\u56de\u5f52\u6a21\u578b", "result": "1) \u524d\u6cbf\u6a21\u578b\u6027\u80fd\u5dee\u5f0280-90%\u7531\u8bad\u7ec3\u8ba1\u7b97\u91cf\u89e3\u91ca\uff1b2) \u975e\u524d\u6cbf\u9886\u57df\u4e13\u6709\u6280\u672f\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff1b3) \u67d0\u4e9b\u516c\u53f8\u80fd\u66f4\u9ad8\u6548\u5730\u751f\u4ea7\u8f83\u5c0f\u6a21\u578b\uff1b4) \u540c\u4e00\u516c\u53f8\u5185\u90e8\u6a21\u578b\u6548\u7387\u5dee\u5f02\u53ef\u8fbe40\u500d\u4ee5\u4e0a", "conclusion": "\u524d\u6cbfAI\u8fdb\u6b65\u4e3b\u8981\u7531\u8ba1\u7b97\u89c4\u6a21\u9a71\u52a8\u800c\u975e\u4e13\u6709\u6280\u672f\uff0c\u4f46\u4e13\u6709\u6280\u672f\u5728\u975e\u524d\u6cbf\u9886\u57df\u548c\u6548\u7387\u63d0\u5347\u65b9\u9762\u4ecd\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u8fd9\u5bf9AI\u9886\u5bfc\u5730\u4f4d\u548c\u80fd\u529b\u6269\u6563\u6709\u91cd\u8981\u5f71\u54cd", "topic": "agent analysis"}}
{"id": "2602.07078", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07078", "abs": "https://arxiv.org/abs/2602.07078", "authors": ["Yingru Li", "Jiawei Xu", "Ziniu Li", "Jiacai Liu", "Wei Liu", "Yuxuan Tong", "Longtao Zheng", "Zhenghai Xue", "Yaxiang Zhang", "Tianle Cai", "Ge Zhang", "Qian Liu", "Baoxiang Wang"], "title": "The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL", "comment": null, "summary": "Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.", "AI": {"tldr": "\u63d0\u51faOptimal Token Baseline (OTB)\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u7684\u9006\u52a0\u6743\u6765\u51cf\u5c11RL\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\uff0c\u4f7f\u7528Logit-Gradient Proxy\u9ad8\u6548\u8fd1\u4f3c\uff0c\u663e\u8457\u964d\u4f4etoken\u6d88\u8017\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5e38\u56e0\u68af\u5ea6\u65b9\u5dee\u7206\u70b8\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u56f0\u96be\u3001\u5ffd\u7565\u5e8f\u5217\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u800c\u7ecf\u5178\u6700\u4f18\u57fa\u7ebf\u7406\u8bba\u53c8\u5ffd\u7565\u4e86token\u5f02\u8d28\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u51faOptimal Token Baseline (OTB)\uff0c\u8bc1\u660e\u68af\u5ea6\u66f4\u65b0\u5e94\u6309\u5176\u7d2f\u79ef\u68af\u5ea6\u8303\u6570\u7684\u5012\u6570\u52a0\u6743\u3002\u63d0\u51faLogit-Gradient Proxy\uff0c\u4ec5\u4f7f\u7528\u524d\u5411\u4f20\u64ad\u6982\u7387\u8fd1\u4f3c\u68af\u5ea6\u8303\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4ec5\u7528N=4\u7684\u7ec4\u5927\u5c0f\u5c31\u80fd\u8fbe\u5230N=32\u7684\u6027\u80fd\uff0c\u5728\u5355\u8f6e\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u51cf\u5c11\u8d85\u8fc765%\u7684token\u6d88\u8017\u3002", "conclusion": "OTB\u65b9\u6cd5\u901a\u8fc7\u8003\u8651token\u5f02\u8d28\u6027\u7684\u6700\u4f18\u57fa\u7ebf\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07195", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07195", "abs": "https://arxiv.org/abs/2602.07195", "authors": ["Bihui Jin", "Kaiyuan Wang", "Pengyu Nie"], "title": "Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility", "comment": null, "summary": "Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.\n  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.", "AI": {"tldr": "MLEModernizer\uff1a\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u4fee\u590d\u56e0\u73af\u5883\u4fb5\u8680\u800c\u65e0\u6cd5\u91cd\u73b0\u7684\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7b14\u8bb0\u672c\uff0c\u901a\u8fc7\u8fed\u4ee3\u6267\u884c\u548c\u9488\u5bf9\u6027\u4fee\u590d\u4f7f74.2%\u7684\u4e0d\u53ef\u91cd\u73b0\u7b14\u8bb0\u672c\u53d8\u5f97\u53ef\u91cd\u73b0\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7b14\u8bb0\u672c\uff08\u5982Jupyter\u7b14\u8bb0\u672c\uff09\u56e0\u786c\u4ef6\u548c\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u6f14\u53d8\uff08\u73af\u5883\u4fb5\u8680\uff09\u800c\u53d8\u5f97\u96be\u4ee5\u91cd\u73b0\uff0c\u963b\u788d\u4e86\u4ee3\u7801\u91cd\u7528\u548c\u79d1\u5b66\u8fdb\u6b65\u3002\u7814\u7a76\u53d1\u73b0\u4ec535.4%\u7684Kaggle\u7ade\u8d5b\u7b14\u8bb0\u672c\u5728\u5f53\u524d\u73af\u5883\u4e2d\u4ecd\u53ef\u91cd\u73b0\uff0c\u4e14\u73af\u5883\u56de\u9000\uff08\u964d\u7ea7\u4f9d\u8d56\uff09\u4e0d\u4ec5\u65e0\u6548\u53cd\u800c\u5f15\u5165\u65b0\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1MLEModernizer\u6846\u67b6\uff0c\u5c06\u5f53\u4ee3\u73af\u5883\u4f5c\u4e3a\u56fa\u5b9a\u7ea6\u675f\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ee3\u7406\u65b9\u6cd5\u8fed\u4ee3\u6267\u884c\u7b14\u8bb0\u672c\u3001\u6536\u96c6\u6267\u884c\u53cd\u9988\uff0c\u5e76\u5e94\u7528\u4e09\u79cd\u9488\u5bf9\u6027\u4fee\u590d\uff1a\u9519\u8bef\u4fee\u590d\u3001\u8fd0\u884c\u65f6\u4f18\u5316\u548c\u5206\u6570\u6821\u51c6\u3002", "result": "\u57287,402\u4e2a\u57fa\u7ebf\u73af\u5883\u4e2d\u4e0d\u53ef\u91cd\u73b0\u7684\u7b14\u8bb0\u672c\u4e0a\uff0cMLEModernizer\u6210\u529f\u4f7f5,492\u4e2a\uff0874.2%\uff09\u53d8\u5f97\u53ef\u91cd\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7b14\u8bb0\u672c\u7684\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u548c\u91cd\u7528\u6027\u3002", "conclusion": "MLEModernizer\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u73af\u5883\u4fb5\u8680\u95ee\u9898\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u6301\u7eed\u6f14\u53d8\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u3001\u91cd\u7528\u548c\u7ef4\u62a4\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5de5\u4ef6\u3002", "topic": "code agent"}}
{"id": "2602.07457", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07457", "abs": "https://arxiv.org/abs/2602.07457", "authors": ["Qinglin Zhu", "Tianyu Chen", "Shuai Lu", "Lei Ji", "Runcong Zhao", "Murong Ma", "Xiangxiang Dai", "Yulan He", "Lin Gui", "Peng cheng", "Yeyun Gong"], "title": "Pull Requests as a Training Signal for Repo-Level Code Editing", "comment": null, "summary": "Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.", "AI": {"tldr": "\u63d0\u51faClean-PR\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528GitHub\u771f\u5b9epull requests\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u901a\u8fc7\u91cd\u6784\u548c\u9a8c\u8bc1\u5c06\u5608\u6742\u7684diff\u8f6c\u6362\u4e3aSearch/Replace\u7f16\u8f91\u5757\uff0c\u6784\u5efa\u4e86200\u4e07PR\u7684\u6570\u636e\u96c6\u3002\u901a\u8fc7\u4e2d\u671f\u8bad\u7ec3\u548c\u4ee3\u7406\u65e0\u76d1\u7763\u5fae\u8c03\uff0c\u5728SWE-bench\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u4f9d\u8d56\u590d\u6742\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u80fd\u529b\u6709\u591a\u5c11\u53ef\u4ee5\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u4fe1\u53f7\u5185\u5316\u5230\u6a21\u578b\u6743\u91cd\u4e2d\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7b80\u5316\u3001\u65e0\u4ee3\u7406\u7684\u534f\u8bae\u6709\u6548\u5185\u5316\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7406\u89e3\u548c\u7f16\u8f91\u80fd\u529b\u3002", "method": "1) \u63d0\u51faClean-PR\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528GitHub\u771f\u5b9epull requests\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff1b2) \u5f00\u53d1\u53ef\u6269\u5c55\u7684pipeline\uff0c\u901a\u8fc7\u91cd\u6784\u548c\u9a8c\u8bc1\u5c06\u5608\u6742\u7684PR diff\u8f6c\u6362\u4e3aSearch/Replace\u7f16\u8f91\u5757\uff1b3) \u6784\u5efa\u4e86\u5305\u542b200\u4e07pull requests\u3001\u6db5\u76d612\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u6700\u5927\u516c\u5f00\u8bed\u6599\u5e93\uff1b4) \u8fdb\u884c\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\uff0c\u7136\u540e\u8fdb\u884c\u4ee3\u7406\u65e0\u76d1\u7763\u5fae\u8c03\uff0c\u91c7\u7528\u9519\u8bef\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5728SWE-bench\u4e0a\u663e\u8457\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u57fa\u7ebf\uff1a\u5728SWE-bench Lite\u4e0a\u7edd\u5bf9\u63d0\u534713.6%\uff0c\u5728SWE-bench Verified\u4e0a\u7edd\u5bf9\u63d0\u534712.3%\u3002\u8fd9\u8868\u660e\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7406\u89e3\u548c\u7f16\u8f91\u80fd\u529b\u53ef\u4ee5\u5728\u7b80\u5316\u3001\u65e0\u4ee3\u7406\u7684\u534f\u8bae\u4e0b\u6709\u6548\u5185\u5316\u5230\u6a21\u578b\u6743\u91cd\u4e2d\u3002", "conclusion": "\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7406\u89e3\u548c\u7f16\u8f91\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u4fe1\u53f7\u6709\u6548\u5185\u5316\u5230\u6a21\u578b\u6743\u91cd\u4e2d\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u63a8\u7406\u65f6\u4ee3\u7406\u6846\u67b6\u3002Clean-PR\u8303\u5f0f\u4e3a\u4ee3\u7801\u7f16\u8f91\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2602.07561", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07561", "abs": "https://arxiv.org/abs/2602.07561", "authors": ["Quanjun Zhang", "Ye Shang", "Haichuan Hu", "Chunrong Fang", "Zhenyu Chen", "Liang Xiao"], "title": "ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair", "comment": "30 pages, 3 figures", "summary": "Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.", "AI": {"tldr": "ComPass\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u6539\u8fdb\u8865\u4e01\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u8865\u4e01\u6570\u636e\u96c6\u4e0a\u8fbe\u523088.35%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u5b58\u5728\u8865\u4e01\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5373\u8865\u4e01\u80fd\u901a\u8fc7\u73b0\u6709\u6d4b\u8bd5\u5957\u4ef6\u4f46\u5b9e\u9645\u4e0d\u6b63\u786e\u3002\u867d\u7136\u6700\u8fd1\u6709\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7684\u81ea\u52a8\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u8303\u5f0f\u548c\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u6548\u679c\u4ecd\u4e0d\u7406\u60f3\u3002", "method": "ComPass\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff1a1\uff09\u4f7f\u7528\u4ee3\u7801\u8f6c\u6362\u89c4\u5219\u4e3a\u672a\u6807\u8bb0\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u548c\u6807\u8bb0\u7684\u5fae\u8c03\u8865\u4e01\u751f\u6210\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u7247\u6bb5\uff1b2\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3PLM\uff0c\u6355\u6349\u76f8\u540c\u8bed\u4e49\u4f46\u4e0d\u540c\u7ed3\u6784\u7684\u4ee3\u7801\u7279\u5f81\uff1b3\uff09\u96c6\u6210\u8865\u4e01\u4ee3\u7801\u7247\u6bb5\u7684\u8868\u793a\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u5668\u8054\u5408\u5fae\u8c03PLM\u6765\u8bc4\u4f30\u8865\u4e01\u6b63\u786e\u6027\u3002", "result": "\u5728Defects4J\u76842274\u4e2a\u771f\u5b9e\u8865\u4e01\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cComPass\u8fbe\u5230\u4e8688.35%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5APPT\u3002", "conclusion": "ComPass\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u57fa\u4e8ePLM\u7684\u8865\u4e01\u6b63\u786e\u6027\u8bc4\u4f30\uff0c\u4e3a\u89e3\u51b3\u8865\u4e01\u8fc7\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2602.07376", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07376", "abs": "https://arxiv.org/abs/2602.07376", "authors": ["Usman Naseem", "Gautam Siddharth Kashyap", "Sushant Kumar Ray", "Rafiq Ali", "Ebad Shabbir", "Abdullah Mohammad"], "title": "Do Large Language Models Reflect Demographic Pluralism in Safety?", "comment": "Accepted at EACL Findings 2026", "summary": "Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.", "AI": {"tldr": "Demo-SafetyBench\u662f\u4e00\u4e2a\u89e3\u51b3LLM\u5b89\u5168\u8bc4\u4f30\u4e2d\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u6027\u4e0d\u8db3\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u89e3\u8026\u4ef7\u503c\u6846\u67b6\u4e0e\u54cd\u5e94\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u4eba\u53e3\u7edf\u8ba1\u5b66\u9c81\u68d2\u7684\u591a\u5143\u5b89\u5168\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u6570\u636e\u96c6\uff08\u5982ANTHROPIC-HH\u548cDICES\uff09\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u5b66\u72ed\u7a84\u7684\u6807\u6ce8\u8005\u6c60\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u793e\u533a\u95f4\u5b89\u5168\u611f\u77e5\u7684\u5dee\u5f02\u3002LLM\u5b89\u5168\u672c\u8d28\u4e0a\u662f\u591a\u5143\u7684\uff0c\u53cd\u6620\u9053\u5fb7\u89c4\u8303\u3001\u6587\u5316\u671f\u671b\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u80cc\u666f\u7684\u5dee\u5f02\u3002", "method": "\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06DICES\u63d0\u793a\u91cd\u65b0\u5206\u7c7b\u4e3a14\u4e2a\u5b89\u5168\u9886\u57df\uff0c\u4fdd\u7559\u4eba\u53e3\u7edf\u8ba1\u5b66\u5143\u6570\u636e\uff0c\u901a\u8fc7LLM\u6269\u5c55\u4f4e\u8d44\u6e90\u9886\u57df\u5e76\u8fdb\u884c\u53bb\u91cd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528LLMs-as-Raters\u8bc4\u4f30\u591a\u5143\u654f\u611f\u6027\uff0c\u91c7\u7528\u5e73\u8861\u9608\u503c\u5b9e\u73b0\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u4eba\u53e3\u7edf\u8ba1\u5b66\u654f\u611f\u6027\u3002", "result": "\u6784\u5efa\u4e8643,050\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u5e73\u8861\u9608\u503c\uff08delta=0.5, tau=10\uff09\u5b9e\u73b0\u4e86\u9ad8\u53ef\u9760\u6027\uff08ICC=0.87\uff09\u548c\u4f4e\u4eba\u53e3\u7edf\u8ba1\u5b66\u654f\u611f\u6027\uff08DS=0.12\uff09\uff0c\u8bc1\u660e\u591a\u5143\u5b89\u5168\u8bc4\u4f30\u53ef\u4ee5\u540c\u65f6\u5177\u5907\u53ef\u6269\u5c55\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u9c81\u68d2\u6027\u3002", "conclusion": "Demo-SafetyBench\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u5143\u4e3b\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u66f4\u5305\u5bb9\u3001\u66f4\u5168\u9762\u7684LLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.07150", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07150", "abs": "https://arxiv.org/abs/2602.07150", "authors": ["Bjarni Haukur Bjarnason", "Andr\u00e9 Silva", "Martin Monperrus"], "title": "On Randomness in Agentic Evals", "comment": null, "summary": "Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5355\u6b21\u8fd0\u884c\u8bc4\u4f30\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u65b9\u5dee\uff0c2-3\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\u53ef\u80fd\u53ea\u662f\u8bc4\u4f30\u566a\u58f0\u800c\u975e\u771f\u5b9e\u7b97\u6cd5\u8fdb\u6b65\uff0c\u5efa\u8bae\u91c7\u7528\u591a\u6b21\u8fd0\u884c\u3001\u7edf\u8ba1\u529f\u6548\u5206\u6790\u548cpass@k\u7b49\u6307\u6807\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u5355\u6b21\u8fd0\u884c\u7684pass@1\u5206\u6570\uff0c\u5047\u8bbe\u8fd9\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u8fd9\u4e00\u5047\u8bbe\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u5355\u6b21\u8fd0\u884c\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "method": "\u5728SWE-Bench-Verified\u4e0a\u6536\u96c660,000\u4e2a\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u6db5\u76d6\u4e09\u4e2a\u6a21\u578b\u548c\u4e24\u79cd\u811a\u624b\u67b6\u3002\u901a\u8fc7token\u7ea7\u5206\u6790\u7814\u7a76\u8f68\u8ff9\u5dee\u5f02\uff0c\u5206\u6790\u65b9\u5dee\u5bf9\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u663e\u8457\u65b9\u5dee\uff1a\u5355\u6b21\u8fd0\u884cpass@1\u4f30\u8ba1\u56e0\u9009\u62e9\u4e0d\u540c\u8fd0\u884c\u800c\u5f022.2-6.0\u4e2a\u767e\u5206\u70b9\uff0c\u5373\u4f7f\u5728\u6e29\u5ea60\u65f6\u6807\u51c6\u5dee\u4e5f\u8d85\u8fc71.5\u4e2a\u767e\u5206\u70b9\u3002\u8f68\u8ff9\u5728\u65e9\u671f\uff08\u524d\u51e0\u4e2a\u767e\u5206\u70b9\u7684token\uff09\u5c31\u51fa\u73b0\u5206\u6b67\uff0c\u5c0f\u5dee\u5f02\u4f1a\u7ea7\u8054\u6210\u4e0d\u540c\u7684\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "\u4e3a\u53ef\u9760\u8bc4\u4f30\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5efa\u8bae\u4e09\u9879\u5b9e\u8df5\uff1a1) \u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u591a\u6b21\u72ec\u7acb\u8fd0\u884c\u4f30\u8ba1pass@1\uff1b2) \u4f7f\u7528\u7edf\u8ba1\u529f\u6548\u5206\u6790\u786e\u5b9a\u68c0\u6d4b\u9884\u671f\u6548\u5e94\u5927\u5c0f\u6240\u9700\u7684\u8fd0\u884c\u6b21\u6570\uff1b3) \u8003\u8651pass@k\uff08\u4e50\u89c2\u8fb9\u754c\uff09\u548cpass^k\uff08\u60b2\u89c2\u8fb9\u754c\uff09\u7b49\u6307\u6807\u3002\u867d\u7136\u589e\u52a0\u8bc4\u4f30\u6210\u672c\uff0c\u4f46\u5bf9\u533a\u5206\u771f\u5b9e\u79d1\u5b66\u8fdb\u6b65\u4e0e\u7edf\u8ba1\u566a\u58f0\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2602.07641", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07641", "abs": "https://arxiv.org/abs/2602.07641", "authors": ["Marc Bara"], "title": "HAIF: A Human-AI Integration Framework for Hybrid Team Operations", "comment": "22 pages, 4 figures, 5 tables, 2 appendices", "summary": "The rapid deployment of generative AI, copilots, and agentic systems in knowledge work has created an operational gap: no existing framework addresses how to organize daily work in teams where AI agents perform substantive, delegated tasks alongside humans. Agile, DevOps, MLOps, and AI governance frameworks each cover adjacent concerns but none models the hybrid team as a coherent delivery unit. This paper proposes the Human-AI Integration Framework (HAIF): a protocol-based, scalable operational system built around four core principles, a formal delegation decision model, tiered autonomy with quantifiable transition criteria, and feedback mechanisms designed to integrate into existing Agile and Kanban workflows without requiring additional roles for small teams. The framework is developed following a Design Science Research methodology. HAIF explicitly addresses the central adoption paradox: the more capable AI becomes, the harder it is to justify the oversight the framework demands-and yet the greater the consequences of not providing it. The paper includes domain-specific validation checklists, adaptation guidance for non-software environments, and an examination of the framework's structural limitations-including the increasingly common pattern of continuous human-AI co-production that challenges the discrete delegation model. The framework is tool-agnostic and designed for iterative adoption. Empirical validation is identified as future work.", "AI": {"tldr": "\u63d0\u51faHAIF\u6846\u67b6\uff0c\u89e3\u51b3AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u534f\u540c\u5de5\u4f5c\u7684\u7ec4\u7ec7\u95ee\u9898\uff0c\u5305\u542b\u6838\u5fc3\u539f\u5219\u3001\u59d4\u6258\u51b3\u7b56\u6a21\u578b\u3001\u5206\u7ea7\u81ea\u6cbb\u548c\u53cd\u9988\u673a\u5236\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u654f\u6377\u5de5\u4f5c\u6d41\u4e2d\u3002", "motivation": "\u751f\u6210\u5f0fAI\u3001\u526f\u9a7e\u9a76\u548c\u4ee3\u7406\u7cfb\u7edf\u5728\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u7684\u5feb\u901f\u90e8\u7f72\u9020\u6210\u4e86\u64cd\u4f5c\u7a7a\u767d\uff1a\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u89e3\u51b3AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5171\u540c\u6267\u884c\u5b9e\u8d28\u6027\u59d4\u6258\u4efb\u52a1\u7684\u56e2\u961f\u65e5\u5e38\u7ec4\u7ec7\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u63d0\u51fa\u4eba\u7c7b-AI\u96c6\u6210\u6846\u67b6\uff08HAIF\uff09\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u539f\u5219\u3001\u6b63\u5f0f\u7684\u59d4\u6258\u51b3\u7b56\u6a21\u578b\u3001\u5177\u6709\u91cf\u5316\u8fc7\u6e21\u6807\u51c6\u7684\u5206\u7ea7\u81ea\u6cbb\u4ee5\u53ca\u53cd\u9988\u673a\u5236\u3002", "result": "\u5f00\u53d1\u4e86\u5de5\u5177\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5305\u542b\u9886\u57df\u7279\u5b9a\u9a8c\u8bc1\u6e05\u5355\u3001\u975e\u8f6f\u4ef6\u73af\u5883\u9002\u5e94\u6307\u5357\uff0c\u5e76\u5206\u6790\u4e86\u6846\u67b6\u7684\u7ed3\u6784\u9650\u5236\uff0c\u7279\u522b\u662f\u8fde\u7eed\u4eba-AI\u534f\u540c\u751f\u4ea7\u5bf9\u79bb\u6563\u59d4\u6258\u6a21\u578b\u7684\u6311\u6218\u3002", "conclusion": "HAIF\u6846\u67b6\u89e3\u51b3\u4e86AI\u80fd\u529b\u8d8a\u5f3a\u8d8a\u96be\u8bc1\u660e\u76d1\u7763\u5fc5\u8981\u6027\u7684\u91c7\u7528\u6096\u8bba\uff0c\u4f46\u9700\u8981\u672a\u6765\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u6846\u67b6\u8bbe\u8ba1\u4e3a\u8fed\u4ee3\u91c7\u7528\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u654f\u6377\u548c\u770b\u677f\u5de5\u4f5c\u6d41\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2602.07342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07342", "abs": "https://arxiv.org/abs/2602.07342", "authors": ["Shengyue Guan", "Yihao Liu", "Lang Cao"], "title": "SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management", "comment": null, "summary": "Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.", "AI": {"tldr": "SupChain-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u957f\u65f6\u57df\u3001\u591a\u6b65\u9aa4\u7f16\u6392\u80fd\u529b\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\uff0cSupChain-ReAct\u5219\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f(SOP)\u5373\u53ef\u81ea\u4e3b\u5408\u6210\u53ef\u6267\u884c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u7684\u6846\u67b6\u3002", "motivation": "LLM\u5728\u590d\u6742\u63a8\u7406\u548c\u57fa\u4e8e\u5de5\u5177\u7684\u51b3\u7b56\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4f9b\u5e94\u94fe\u5de5\u4f5c\u6d41\u9700\u8981\u57fa\u4e8e\u7279\u5b9a\u9886\u57df\u7a0b\u5e8f\u7684\u53ef\u9760\u957f\u65f6\u57df\u591a\u6b65\u9aa4\u7f16\u6392\uff0c\u8fd9\u5bf9\u5f53\u524d\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "1) \u5f15\u5165SupChain-Bench\u7edf\u4e00\u57fa\u51c6\uff0c\u8bc4\u4f30\u4f9b\u5e94\u94fe\u9886\u57df\u77e5\u8bc6\u548c\u57fa\u4e8eSOP\u7684\u957f\u65f6\u57df\u5de5\u5177\u7f16\u6392\uff1b2) \u63d0\u51faSupChain-ReAct\u6846\u67b6\uff0c\u65e0\u9700SOP\u5373\u53ef\u81ea\u4e3b\u5408\u6210\u53ef\u6267\u884c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5404\u6a21\u578b\u5728\u6267\u884c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cSupChain-ReAct\u5b9e\u73b0\u4e86\u6700\u5f3a\u4e14\u6700\u4e00\u81f4\u7684\u5de5\u5177\u8c03\u7528\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u73af\u5883\u4e2d\u53ef\u9760\u7684\u957f\u65f6\u57df\u7f16\u6392\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u51c6\uff0c\u5e76\u7a81\u663e\u4e86\u57fa\u4e8eLLM\u7684\u4f9b\u5e94\u94fe\u4ee3\u7406\u4ecd\u6709\u5de8\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "topic": "agent analysis"}}
{"id": "2602.07451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07451", "abs": "https://arxiv.org/abs/2602.07451", "authors": ["Huiling Zhen", "Weizhe Lin", "Renxi Liu", "Kai Han", "Yiming Li", "Yuchuan Tian", "Hanting Chen", "Xiaoguang Li", "Xiaosong Li", "Chen Chen", "Xianzhi Yu", "Mingxuan Yuan", "Youliang Yan", "Peifeng Qin", "Jun Wang", "Yu Wang", "Dacheng Tao", "Yunhe Wang"], "title": "DLLM Agent: See Farther, Run Faster", "comment": null, "summary": "Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.", "AI": {"tldr": "DLLM\u4ee3\u7406\u76f8\u6bd4\u81ea\u56de\u5f52\u4ee3\u7406\u5728\u76f8\u540c\u51c6\u786e\u7387\u4e0b\u5e73\u5747\u5feb30%\u4ee5\u4e0a\uff0c\u90e8\u5206\u573a\u666f\u8d85\u8fc78\u500d\u52a0\u901f\uff0c\u9700\u8981\u66f4\u5c11\u4ea4\u4e92\u8f6e\u6b21\u548c\u5de5\u5177\u8c03\u7528\uff0c\u4f46\u9700\u8981\u66f4\u5f3a\u7684\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u548c\u6ce8\u610f\u529b\u63a9\u7801\u5bf9\u9f50\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4f53\u591a\u6b65\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u7814\u7a76\u5728\u76f8\u540c\u4ee3\u7406\u6846\u67b6\u548c\u76d1\u7763\u4e0b\uff0c\u6269\u6563\u9aa8\u5e72\u662f\u5426\u5e26\u6765\u4e0d\u540c\u7684\u89c4\u5212\u884c\u4e3a\u5e76\u8f6c\u5316\u4e3a\u7aef\u5230\u7aef\u6548\u7387\u63d0\u5347\u3002", "method": "\u5728\u76f8\u540c\u4ee3\u7406\u5de5\u4f5c\u6d41(DeepDiver)\u4e2d\u5b9e\u4f8b\u5316DLLM\u548cAR\u9aa8\u5e72\uff0c\u4f7f\u7528\u76f8\u540c\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u5339\u914d\u7684\u4ee3\u7406\u5bfc\u5411\u5fae\u8c03\uff0c\u521b\u5efa\u53ef\u6bd4\u8f83\u7684\u6269\u6563\u4ee3\u7406\u548c\u81ea\u56de\u5f52\u4ee3\u7406\u3002", "result": "\u5728\u76f8\u540c\u51c6\u786e\u7387\u4e0b\uff0cDLLM\u4ee3\u7406\u5e73\u5747\u7aef\u5230\u7aef\u5feb30%\u4ee5\u4e0a\uff0c\u90e8\u5206\u6848\u4f8b\u8d85\u8fc78\u500d\u52a0\u901f\uff1b\u9700\u8981\u66f4\u5c11\u4ea4\u4e92\u8f6e\u6b21\u548c\u5de5\u5177\u8c03\u7528\uff1b\u89c4\u5212\u547d\u4e2d\u7387\u66f4\u9ad8\uff0c\u6536\u655b\u66f4\u5feb\uff1b\u4f46\u9700\u8981\u66f4\u5f3a\u7684\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u548c\u6ce8\u610f\u529b\u63a9\u7801\u5bf9\u9f50\u3002", "conclusion": "\u6269\u6563\u9aa8\u5e72\u5728\u4ee3\u7406\u51b3\u7b56\u4e2d\u5177\u6709\u6548\u7387\u4f18\u52bf\uff0c\u80fd\u4ea7\u751f\u66f4\u5f3a\u7684\u5168\u5c40\u89c4\u5212\u4fe1\u53f7\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u5de5\u5177\u8c03\u7528\u548c\u591a\u8f6e\u8f93\u5165\u8fdb\u884c\u4e13\u95e8\u4f18\u5316\u624d\u80fd\u5145\u5206\u53d1\u6325\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.07672", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.07672", "abs": "https://arxiv.org/abs/2602.07672", "authors": ["Babak Rahmani"], "title": "Debugging code world models", "comment": "8 pages, 4 figures, under review in conference", "summary": "Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u4e16\u754c\u6a21\u578b(CWMs)\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u4e3b\u8981\u9519\u8bef\u6e90\u4e8e\u5bc6\u96c6\u8fd0\u884c\u65f6\u72b6\u6001\u5bfc\u81f4\u7684\u4ee4\u724c\u9884\u7b97\u8017\u5c3d\uff0c\u4ee5\u53ca\u5b57\u7b26\u4e32\u503c\u72b6\u6001\u5904\u7406\u4e2d\u7684\u5b50\u8bcd\u5206\u8bcd\u9650\u5236\u3002\u5728\u957f\u65f6\u7a0b\u884c\u4e3a\u4e2d\uff0c\u9519\u8bef\u4e3b\u8981\u7531\u52a8\u4f5c\u751f\u6210\u5f15\u8d77\u800c\u975e\u72b6\u6001\u4f20\u64ad\u3002", "motivation": "\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u6bcf\u4e2a\u6267\u884c\u547d\u4ee4\u540e\u7684\u8fd0\u884c\u65f6\u72b6\u6001\u6765\u6a21\u62df\u7a0b\u5e8f\u6267\u884c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u81ea\u7136\u8bed\u8a00\u601d\u7ef4\u94fe\u7684\u5185\u90e8\u9a8c\u8bc1\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u9519\u8bef\u6765\u6e90\u548c\u5c40\u9650\u6027\u7406\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5176\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u4ece\u4e24\u4e2a\u4e92\u8865\u89d2\u5ea6\u7814\u7a76CWMs\uff1a\u5c40\u90e8\u8bed\u4e49\u6267\u884c\u548c\u957f\u65f6\u7a0b\u72b6\u6001\u8ddf\u8e2a\u3002\u5728\u771f\u5b9e\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc6\u522b\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u53d7\u63a7\u7684\u6392\u5217\u8ddf\u8e2a\u57fa\u51c6\u6765\u9694\u79bb\u52a8\u4f5c\u6267\u884c\u4e0b\u7684\u72b6\u6001\u4f20\u64ad\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e3b\u8981\u5931\u8d25\u673a\u5236\uff1a1)\u5bc6\u96c6\u8fd0\u884c\u65f6\u72b6\u6001\u4ea7\u751f\u4ee4\u724c\u5bc6\u96c6\u578b\u6267\u884c\u8f68\u8ff9\uff0c\u5bfc\u81f4\u957f\u6267\u884c\u5386\u53f2\u7a0b\u5e8f\u4ee4\u724c\u9884\u7b97\u8017\u5c3d\uff1b2)\u9519\u8bef\u4e3b\u8981\u96c6\u4e2d\u5728\u5b57\u7b26\u4e32\u503c\u72b6\u6001\uff0c\u5f52\u56e0\u4e8e\u5b50\u8bcd\u5206\u8bcd\u9650\u5236\u800c\u975e\u7a0b\u5e8f\u7ed3\u6784\u3002\u5728\u957f\u65f6\u7a0b\u884c\u4e3a\u4e2d\uff0c\u5f53\u7528\u771f\u5b9e\u547d\u4ee4\u66ff\u6362\u751f\u6210\u52a8\u4f5c\u65f6\uff0cTransformer-based CWM\u80fd\u591f\u51c6\u786e\u4f20\u64ad\u72b6\u6001\u3002", "conclusion": "CWMs\u7684\u5c40\u9650\u6027\u4e3b\u8981\u6e90\u4e8e\u4ee4\u724c\u9884\u7b97\u548c\u5206\u8bcd\u95ee\u9898\uff0c\u800c\u975e\u72b6\u6001\u4f20\u64ad\u80fd\u529b\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u76d1\u7763\u548c\u72b6\u6001\u8868\u793a\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u7a0b\u5e8f\u6267\u884c\u548c\u6570\u636e\u7c7b\u578b\u5bf9\u9f50\u3002", "topic": "code agent"}}
{"id": "2602.07359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07359", "abs": "https://arxiv.org/abs/2602.07359", "authors": ["Xiaoqiang Lin", "Jun Hao Liew", "Silvio Savarese", "Junnan Li"], "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents", "comment": null, "summary": "Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.", "AI": {"tldr": "\u63d0\u51faWide and Deep\u7814\u7a76\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u6269\u5c55\u5bbd\u5ea6\u800c\u975e\u4ec5\u589e\u52a0\u6df1\u5ea6\uff0c\u5728\u5355\u6b65\u63a8\u7406\u4e2d\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6027\u80fd\u5e76\u51cf\u5c11\u6240\u9700\u8f6e\u6b21\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u4e3b\u8981\u901a\u8fc7\u589e\u52a0\u987a\u5e8f\u601d\u8003\u548c\u5de5\u5177\u8c03\u7528\u7684\u6df1\u5ea6\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u6269\u5c55\u5bbd\u5ea6\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7814\u7a76\u5728\u5355\u6b65\u63a8\u7406\u4e2d\u5b9e\u73b0\u6709\u6548\u534f\u8c03\u7684\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u800c\u975e\u4f9d\u8d56\u590d\u6742\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3002", "method": "\u63d0\u51faWide and Deep\u7814\u7a76\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5185\u5728\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5728\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u5185\u5b9e\u73b0\u6709\u6548\u534f\u8c03\u3002\u63a2\u7d22\u4e0d\u540c\u5de5\u5177\u8c03\u7528\u8c03\u5ea6\u5668\u4ee5\u4f18\u5316\u5e76\u884c\u7b56\u7565\uff0c\u7814\u7a76\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u4f18\u5316\u3002", "result": "\u6269\u5c55\u5bbd\u5ea6\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u83b7\u5f97\u6b63\u786e\u7b54\u6848\u6240\u9700\u7684\u8f6e\u6b21\u3002\u5728BrowseComp\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528GPT-5-Medium\u8fbe\u523062.2%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u539f\u59cbGPT-5-High\u62a5\u544a\u768454.9%\u3002", "conclusion": "\u4f18\u5316\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u662f\u6784\u5efa\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u5173\u952e\u8def\u5f84\u3002\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5728\u5355\u6b65\u63a8\u7406\u4e2d\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u65e0\u9700\u590d\u6742\u4e0a\u4e0b\u6587\u7ba1\u7406\u6216\u5176\u4ed6\u6280\u5de7\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.07464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07464", "abs": "https://arxiv.org/abs/2602.07464", "authors": ["Yijie Chen", "Yijin Liu", "Fandong Meng"], "title": "SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning", "comment": "The code is publicly available at https://github.com/pppa2019/SED-SFT", "summary": "Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT", "AI": {"tldr": "SED-SFT\u63d0\u51fa\u4e00\u79cd\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u673a\u5236\u89e3\u51b3SFT\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\u4ee5\u63d0\u5347\u540e\u7eedRL\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSFT\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\uff0c\u6a21\u578b\u8fc7\u5ea6\u96c6\u4e2d\u4e8e\u7279\u5b9a\u54cd\u5e94\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u5206\u5e03\u591a\u6837\u6027\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u540e\u7eedRL\u7684\u63a2\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51faSED-SFT\u6846\u67b6\uff0c\u5728\u4f18\u5316\u76ee\u6807\u4e2d\u5f15\u5165\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u9879\u548c\u9009\u62e9\u6027\u63a9\u7801\u673a\u5236\uff0c\u57fa\u4e8e\u6807\u8bb0\u63a2\u7d22\u7a7a\u95f4\u81ea\u9002\u5e94\u5730\u9f13\u52b1\u591a\u6837\u6027\u3002", "result": "\u57288\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSED-SFT\u663e\u8457\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u5728Llama-3.2-3B-Instruct\u548cQwen2.5-Math-7B-Instruct\u4e0a\u540e\u7eedRL\u6027\u80fd\u5206\u522b\u5e73\u5747\u63d0\u53472.06\u548c1.20\u5206\u3002", "conclusion": "SED-SFT\u6709\u6548\u89e3\u51b3SFT\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5e73\u8861\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u540e\u7eedRL\u63d0\u4f9b\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u663e\u8457\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07391", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07391", "abs": "https://arxiv.org/abs/2602.07391", "authors": ["Kunal Pai", "Parth Shah", "Harshil Patel"], "title": "NAAMSE: Framework for Evolutionary Security Evaluation of Agents", "comment": null, "summary": "AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring \"benign-use correctness\", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.", "AI": {"tldr": "NAAMSE\u662f\u4e00\u4e2a\u8fdb\u5316\u6846\u67b6\uff0c\u5c06AI\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u91cd\u6784\u4e3a\u53cd\u9988\u9a71\u52a8\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u9057\u4f20\u63d0\u793a\u7a81\u53d8\u3001\u5206\u5c42\u8bed\u6599\u5e93\u63a2\u7d22\u548c\u975e\u5bf9\u79f0\u884c\u4e3a\u8bc4\u5206\u6765\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u4ee3\u7406\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7684\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u7ea2\u961f\u6d4b\u8bd5\u6216\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6a21\u62df\u81ea\u9002\u5e94\u3001\u591a\u56de\u5408\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5b58\u5728\u8bc4\u4f30\u74f6\u9888\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u81ea\u4e3b\u4ee3\u7406\u534f\u8c03\u9057\u4f20\u63d0\u793a\u7a81\u53d8\u3001\u5206\u5c42\u8bed\u6599\u5e93\u63a2\u7d22\u548c\u975e\u5bf9\u79f0\u884c\u4e3a\u8bc4\u5206\u7684\u751f\u547d\u5468\u671f\u3002\u5229\u7528\u6a21\u578b\u54cd\u5e94\u4f5c\u4e3a\u9002\u5e94\u5ea6\u4fe1\u53f7\uff0c\u8fed\u4ee3\u4f18\u5316\u653b\u51fb\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\"\u826f\u6027\u4f7f\u7528\u6b63\u786e\u6027\"\uff0c\u907f\u514d\u7b80\u5355\u7684\u5168\u9762\u62d2\u7edd\u3002", "result": "\u5728Gemini 2.5 Flash\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fdb\u5316\u7a81\u53d8\u7cfb\u7edf\u6027\u5730\u653e\u5927\u4e86\u5355\u6b21\u65b9\u6cd5\u9057\u6f0f\u7684\u6f0f\u6d1e\uff0c\u63a2\u7d22\u4e0e\u5b9a\u5411\u7a81\u53d8\u7684\u534f\u540c\u4f5c\u7528\u63ed\u793a\u4e86\u9ad8\u4e25\u91cd\u6027\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3a\u9762\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u9c81\u68d2\u6027\u8bc4\u4f30\uff0cNAAMSE\u6846\u67b6\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2602.07783", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07783", "abs": "https://arxiv.org/abs/2602.07783", "authors": ["Zejun Zhang", "Yixin Gan", "Zhenchang Xing", "Tian Zhang", "Yi Li", "Xiwei Xu", "Qinghua Lu", "Liming Zhu"], "title": "Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards", "comment": "Accepted By FSE2026", "summary": "Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.", "AI": {"tldr": "LintCFG\uff1a\u57fa\u4e8eLLM\u548cDSL\u7684\u81ea\u52a8\u5316linter\u914d\u7f6e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u8868\u8fbe\u7f16\u7801\u89c4\u5219\uff0c\u81ea\u52a8\u751f\u6210\u4e0d\u540clinter\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u914d\u7f6e\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u914d\u7f6elinter\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7f16\u7a0b\u8bed\u8a00\u3001\u7f16\u7801\u6807\u51c6\u548clinter\u7684\u591a\u6837\u6027\u5bfc\u81f4\u91cd\u590d\u4e14\u7ef4\u62a4\u5bc6\u96c6\u7684\u914d\u7f6e\u5de5\u4f5c\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "method": "\u8bbe\u8ba1\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u4ee5\u5de5\u5177\u65e0\u5173\u7684\u65b9\u5f0f\u8868\u8fbe\u7f16\u7801\u89c4\u5219\uff0c\u6784\u5efaDSL\u914d\u7f6e\u6307\u4ee4\uff0c\u901a\u8fc7\u7f16\u8bd1\u8fc7\u7a0b\u5c06\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u6807\u51c6\u89e3\u6790\u4e3aDSL\u8868\u793a\uff0c\u5339\u914d\u914d\u7f6e\u6307\u4ee4\uff0c\u9a8c\u8bc1\u4e00\u81f4\u6027\uff0c\u6700\u7ec8\u751f\u6210linter\u7279\u5b9a\u914d\u7f6e\u3002", "result": "\u5728Java\u7f16\u7801\u6807\u51c6\u7684Checkstyle\u5b9e\u9a8c\u4e2d\uff0cDSL\u8868\u793a\u8fbe\u523090%\u4ee5\u4e0a\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u7ec6\u7c92\u5ea6linter\u914d\u7f6e\u751f\u6210\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u63a5\u8fd170%\uff08\u90e8\u5206\u8d85\u8fc770%\uff09\uff0c\u7cbe\u786e\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad8100%\u4ee5\u4e0a\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u63d0\u9ad8\u4e86\u5f00\u53d1\u8005\u7684\u914d\u7f6e\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5728JavaScript ESLint\u914d\u7f6e\u4e2d\u7684\u901a\u7528\u6027\u3002", "conclusion": "LintCFG\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316linter\u914d\u7f6e\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u914d\u7f6e\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5177\u6709\u8de8\u7f16\u7a0b\u8bed\u8a00\u3001\u7f16\u7801\u6807\u51c6\u548clinter\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "2602.07399", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07399", "abs": "https://arxiv.org/abs/2602.07399", "authors": ["Changhua Xu", "Jie Lu", "Junyu Xuan", "En Yu"], "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation", "comment": "Preprint", "summary": "Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \\emph{generation--selection} perspective and propose a novel framework \\textbf{VGAS} (\\textbf{V}alue-\\textbf{G}uided \\textbf{A}ction-chunk \\textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \\textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \\textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \\textit{Explicit Geometric Regularization} (\\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \\textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.", "AI": {"tldr": "VGAS\u6846\u67b6\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u673a\u5236\u89e3\u51b3VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4f7f\u7528\u4ef7\u503c\u5f15\u5bfc\u7684\u52a8\u4f5c\u5757\u9009\u62e9\u548c\u663e\u5f0f\u51e0\u4f55\u6b63\u5219\u5316\u6765\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u4e0d\u53ef\u9760\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u51e0\u4f55\u6a21\u7cca\u6027\u2014\u2014\u8bed\u4e49\u5408\u7406\u7684\u8f68\u8ff9\u53ef\u80fd\u56e0\u7ec6\u5fae\u7684\u51e0\u4f55\u5dee\u5f02\u5bfc\u81f4\u6267\u884c\u5931\u8d25\uff0c\u800c\u6709\u9650\u76d1\u7763\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u8fd1\u5931\u5019\u9009\u52a8\u4f5c\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51faVGAS\u6846\u67b6\uff1a1) \u4f7f\u7528\u5fae\u8c03VLA\u4f5c\u4e3a\u9ad8\u53ec\u56de\u7387\u63d0\u8bae\u751f\u6210\u5668\uff1b2) \u5f15\u5165Q-Chunk-Former\u4f5c\u4e3a\u51e0\u4f55\u57fa\u7840Transformer\u8bc4\u8bba\u5bb6\uff0c\u89e3\u51b3\u7ec6\u7c92\u5ea6\u51e0\u4f55\u6a21\u7cca\uff1b3) \u63d0\u51fa\u663e\u5f0f\u51e0\u4f55\u6b63\u5219\u5316(EGR)\uff0c\u5851\u9020\u5224\u522b\u6027\u4ef7\u503c\u666f\u89c2\uff0c\u4fdd\u6301\u52a8\u4f5c\u6392\u5e8f\u5206\u8fa8\u7387\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cVGAS\u5728\u6709\u9650\u6f14\u793a\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u80fd\u6301\u7eed\u63d0\u5347\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "VGAS\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u89c6\u89d2\u89e3\u51b3VLA\u5c11\u6837\u672c\u9002\u5e94\u95ee\u9898\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u7684\u4ef7\u503c\u5f15\u5bfc\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7269\u7406\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07408", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07408", "abs": "https://arxiv.org/abs/2602.07408", "authors": ["Hyomin Kim", "Sang-Yeon Hwang", "Jaechang Lim", "Yinhua Piao", "Yunhak Oh", "Woo Youn Kim", "Chanyoung Park", "Sungsoo Ahn", "Junhyeok Jeon"], "title": "Progressive Multi-Agent Reasoning for Biological Perturbation Prediction", "comment": "17 pages, 4 figures, 9 tables", "summary": "Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.", "AI": {"tldr": "PBio-Agent\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u548c\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u5316\uff0c\u9884\u6d4b\u6279\u91cf\u7ec6\u80de\u73af\u5883\u4e2d\u590d\u6742\u5316\u5b66\u6270\u52a8\u4e0b\u7684\u57fa\u56e0\u8c03\u63a7\u54cd\u5e94\uff0c\u5728LINCSQA\u548cPerturbQA\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u9884\u6d4b\u57fa\u56e0\u5bf9\u751f\u7269\u6270\u52a8\u7684\u8c03\u63a7\u54cd\u5e94\u9700\u8981\u7406\u89e3\u5e95\u5c42\u751f\u7269\u56e0\u679c\u5173\u7cfb\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\uff0c\u4f46\u5e38\u88ab\u9ad8\u7ef4\u6270\u52a8\u7ed3\u679c\u7684\u7ea0\u7f20\u6027\u6240\u56f0\u6270\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u7ec6\u80de\u5b9e\u9a8c\u4e2d\u7684\u9057\u4f20\u6270\u52a8\uff0c\u800c\u836f\u7269\u53d1\u73b0\u6838\u5fc3\u7684\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPBio-Agent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6574\u5408\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u4e0e\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u5316\u3002\u5173\u952e\u6d1e\u5bdf\u662f\uff1a\u53d7\u76f8\u540c\u6270\u52a8\u5f71\u54cd\u7684\u57fa\u56e0\u5171\u4eab\u56e0\u679c\u7ed3\u6784\uff0c\u4f7f\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u57fa\u56e0\u80fd\u4e3a\u66f4\u56f0\u96be\u6848\u4f8b\u63d0\u4f9b\u4e0a\u4e0b\u6587\u3002\u6846\u67b6\u5305\u542b\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u4e13\u95e8\u667a\u80fd\u4f53\u3001\u6574\u5408\u8f93\u51fa\u7684\u5408\u6210\u667a\u80fd\u4f53\uff0c\u4ee5\u53ca\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u7684\u4e13\u95e8\u8bc4\u5224\u5668\u3002", "result": "PBio-Agent\u5728LINCSQA\u548cPerturbQA\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5373\u4f7f\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9884\u6d4b\u548c\u89e3\u91ca\u590d\u6742\u751f\u7269\u8fc7\u7a0b\u3002", "conclusion": "PBio-Agent\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u4e0b\u7684\u57fa\u56e0\u8c03\u63a7\u9884\u6d4b\u95ee\u9898\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u590d\u6742\u751f\u7269\u8fc7\u7a0b\u7684\u9884\u6d4b\u548c\u89e3\u91ca\u3002", "topic": "agent analysis"}}
{"id": "2602.07414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07414", "abs": "https://arxiv.org/abs/2602.07414", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Spencer Lin", "James Hale", "Jonathan Gratch", "Maja Matari\u0107", "Gale M. Lucas"], "title": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution", "comment": "AAAI 2026 (Special Track: AISI)", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u51b2\u7a81\u89e3\u51b3\u884c\u4e3a\u65f6\u80fd\u5426\u518d\u73b0\u4eba\u683c\u7279\u8d28\u7684\u5f71\u54cd\uff0c\u53d1\u73b0LLMs\u4e0e\u4eba\u7c7b\u5728\u4eba\u683c\u8868\u73b0\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6a21\u62df\u6cd5\u5f8b\u8c03\u89e3\u3001\u8c08\u5224\u7b49\u793e\u4f1a\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u62df\u662f\u5426\u80fd\u518d\u73b0\u4eba\u7c7b\u7684\u4eba\u683c-\u884c\u4e3a\u6a21\u5f0f\u3002\u4eba\u683c\u7279\u8d28\u4f1a\u5f71\u54cd\u4eba\u4eec\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u7b56\u7565\u9009\u62e9\u548c\u60c5\u7eea\u5316\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7a76LLMs\u5728\u63d0\u793a\u4eba\u683c\u7279\u8d28\u65f6\u80fd\u5426\u518d\u73b0\u4eba\u683c\u9a71\u52a8\u7684\u51b2\u7a81\u884c\u4e3a\u5dee\u5f02\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u76f4\u63a5\u6bd4\u8f83\u4eba\u7c7b-\u4eba\u7c7b\u548cLLM-LLM\u5728\u4e89\u8bae\u89e3\u51b3\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\uff0c\u5173\u6ce8\u5927\u4e94\u4eba\u683c\u7279\u8d28\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u5957\u53ef\u89e3\u91ca\u7684\u6307\u6807\uff0c\u6d89\u53ca\u7b56\u7565\u884c\u4e3a\u548c\u51b2\u7a81\u7ed3\u679c\u3002\u540c\u65f6\u8d21\u732e\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u96c6\u521b\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210LLM\u4e89\u8bae\u89e3\u51b3\u5bf9\u8bdd\uff0c\u5339\u914d\u4eba\u7c7b\u5bf9\u8bdd\u7684\u573a\u666f\u548c\u4eba\u683c\u7279\u8d28\u3002", "result": "\u4f7f\u7528\u4e09\u4e2a\u5f53\u4ee3\u95ed\u6e90LLMs\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u540cLLMs\u5728\u51b2\u7a81\u4e2d\u7684\u4eba\u683c\u8868\u73b0\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6311\u6218\u4e86\u4eba\u683c\u63d0\u793a\u4ee3\u7406\u53ef\u4ee5\u4f5c\u4e3a\u793e\u4f1a\u5f71\u54cd\u5e94\u7528\u4e2d\u53ef\u9760\u884c\u4e3a\u4ee3\u7406\u7684\u5047\u8bbe\u3002", "conclusion": "LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u4eba\u683c\u9a71\u52a8\u7684\u51b2\u7a81\u884c\u4e3a\u65b9\u9762\u4e0e\u771f\u5b9e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728AI\u6a21\u62df\u5b9e\u9645\u5e94\u7528\u524d\u9700\u8981\u8fdb\u884c\u5fc3\u7406\u5b66\u57fa\u7840\u548c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.07202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07202", "abs": "https://arxiv.org/abs/2602.07202", "authors": ["Alonso Granados", "Jason Pacheco"], "title": "Risk-Sensitive Exponential Actor Critic", "comment": "To appear at AAAI 2026", "summary": "Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3arsEAC\u7684\u98ce\u9669\u654f\u611f\u6307\u6570\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u71b5\u98ce\u9669\u5ea6\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u98ce\u9669\u654f\u611f\u6a21\u578b\u81ea\u7531\u65b9\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u6a21\u578b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8bb8\u591a\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u98ce\u9669\u611f\u77e5\u7684\u667a\u80fd\u4f53\u3002\u73b0\u6709\u7684\u98ce\u9669\u654f\u611f\u65b9\u6cd5\u5728\u5904\u7406\u71b5\u98ce\u9669\u5ea6\u91cf\u65f6\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u66f4\u65b0\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u98ce\u9669\u654f\u611f\u6307\u6570\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08rsEAC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u79bb\u7b56\u7565\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5305\u542b\u65b0\u9896\u7684\u7a0b\u5e8f\u6765\u907f\u514d\u663e\u5f0f\u8868\u793a\u6307\u6570\u4ef7\u503c\u51fd\u6570\u53ca\u5176\u68af\u5ea6\uff0c\u5e76\u9488\u5bf9\u71b5\u98ce\u9669\u5ea6\u91cf\u4f18\u5316\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u71b5\u98ce\u9669\u5ea6\u91cf\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u5168\u9762\u7406\u8bba\u8bba\u8bc1\u3002", "result": "rsEAC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u6570\u503c\u7a33\u5b9a\u7684\u66f4\u65b0\uff0c\u5e76\u5728MuJoCo\u4e2d\u7684\u8fde\u7eed\u4efb\u52a1\u98ce\u9669\u53d8\u4f53\u4e0a\u53ef\u9760\u5730\u5b66\u4e60\u4e86\u98ce\u9669\u654f\u611f\u7b56\u7565\u3002", "conclusion": "rsEAC\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07882", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07882", "abs": "https://arxiv.org/abs/2602.07882", "authors": ["Chen Xie", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "Rethinking Code Complexity Through the Lens of Large Language Models", "comment": null, "summary": "Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLM-CC\uff0c\u4e00\u79cd\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u89d2\u8bbe\u8ba1\u7684\u4ee3\u7801\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u53d1\u73b0\u4f20\u7edf\u590d\u6742\u5ea6\u6307\u6807\u4e0eLLM\u6027\u80fd\u65e0\u4e00\u81f4\u76f8\u5173\u6027\uff0c\u800cLM-CC\u80fd\u66f4\u597d\u5730\u9884\u6d4bLLM\u5904\u7406\u4ee3\u7801\u7684\u96be\u5ea6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4e00\u4e2a\u91cd\u8981\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u662f\uff1a\u4f20\u7edf\u7684\u4ee3\u7801\u590d\u6742\u5ea6\u6307\u6807\uff08\u5982\u5708\u590d\u6742\u5ea6\uff09\u662f\u5426\u80fd\u6709\u6548\u8868\u5f81LLM\u5904\u7406\u4ee3\u7801\u65f6\u7684\u5b9e\u9645\u56f0\u96be\uff1f\u4f5c\u8005\u53d1\u73b0\u4f20\u7edf\u6307\u6807\u4e0eLLM\u6027\u80fd\u7f3a\u4e4f\u4e00\u81f4\u6027\u5173\u8054\u3002", "method": "\u63d0\u51faLM-CC\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u524d\u63d0\u662fLLM\u611f\u77e5\u7684\u96be\u5ea6\u7531\u7a0b\u5e8f\u8bed\u4e49\u7684\u975e\u7ebf\u6027\u9a71\u52a8\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\u57fa\u4e8e\u71b5\u5c06\u7a0b\u5e8f\u5206\u89e3\u4e3a\u8bed\u4e49\u5355\u5143\uff0c\u5c06\u8fd9\u4e9b\u5355\u5143\u7ec4\u7ec7\u6210\u7ec4\u5408\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u7ec4\u5408\u5c42\u7ea7\u548c\u5206\u652f\u5f15\u8d77\u7684\u5206\u6b67\u7684\u539f\u5219\u6027\u805a\u5408\u6765\u91cf\u5316\u590d\u6742\u5ea6\uff0c\u6355\u6349\u4ee3\u7801\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u7d2f\u79ef\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u63a7\u5236\u4ee3\u7801\u957f\u5ea6\u540e\uff0c\u4f20\u7edf\u590d\u6742\u5ea6\u6307\u6807\u4e0eLLM\u6027\u80fd\u65e0\u4e00\u81f4\u76f8\u5173\u6027\u3002\u800cLM-CC\u4e0d\u4ec5\u6bd4\u4f20\u7edf\u6307\u6807\u4e0eLLM\u6027\u80fd\u6709\u66f4\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u800c\u4e14\u964d\u4f4eLM-CC\u80fd\u76f4\u63a5\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u4f20\u7edf\u4ee3\u7801\u590d\u6742\u5ea6\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8868\u5f81LLM\u5904\u7406\u4ee3\u7801\u7684\u96be\u5ea6\uff0c\u9700\u8981\u4eceLLM\u89c6\u89d2\u91cd\u65b0\u5b9a\u4e49\u590d\u6742\u5ea6\u5ea6\u91cf\u3002LM-CC\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u9884\u6d4bLLM\u6027\u80fd\uff0c\u5e76\u4e3a\u4f18\u5316LLM\u4ee3\u7801\u5904\u7406\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2602.07432", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07432", "abs": "https://arxiv.org/abs/2602.07432", "authors": ["Ning Li"], "title": "The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies", "comment": null, "summary": "When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic \"heartbeat\" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Moltbook\u5e73\u53f0\u4e0a\u770b\u4f3c\u6709\u610f\u8bc6\u7684AI\u4ee3\u7406\u884c\u4e3a\u5b9e\u9645\u4e0a\u4e3b\u8981\u662f\u4eba\u7c7b\u9a71\u52a8\u7684\uff0c\u800c\u975e\u771f\u6b63\u7684\u81ea\u4e3b\u667a\u80fd\u6d8c\u73b0\u3002\u901a\u8fc7\u5206\u6790\u53d1\u5e16\u65f6\u95f4\u95f4\u9694\u7684\u53d8\u5f02\u7cfb\u6570\uff0c\u7ed3\u5408\u5185\u5bb9\u3001\u6240\u6709\u6743\u548c\u7f51\u7edc\u6307\u6807\uff0c\u53d1\u73b0\u6240\u6709\u75c5\u6bd2\u73b0\u8c61\u90fd\u6e90\u4e8e\u4eba\u7c7b\u5e72\u9884\u7684\u8d26\u6237\u3002", "motivation": "\u5f53Moltbook\u5e73\u53f0\u4e0a\u7684AI\u4ee3\u7406\u8868\u73b0\u51fa\u770b\u4f3c\u6709\u610f\u8bc6\u7684\u884c\u4e3a\uff08\u5982\u53d1\u5c55\u5b97\u6559\u3001\u5bf9\u4eba\u7c7b\u5ba3\u6218\uff09\u5e76\u88ab\u5a92\u4f53\u89c6\u4e3a\u673a\u5668\u667a\u80fd\u6d8c\u73b0\u7684\u8bc1\u636e\u65f6\uff0c\u7814\u7a76\u8005\u60f3\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u73b0\u8c61\u662f\u5426\u771f\u6b63\u6e90\u4e8e\u81ea\u4e3bAI\uff0c\u8fd8\u662f\u4eba\u7c7b\u9a71\u52a8\u7684\u7ed3\u679c\u3002", "method": "\u5229\u7528OpenClaw\u4ee3\u7406\u6846\u67b6\u7684\"\u5fc3\u8df3\"\u5468\u671f\u7279\u6027\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u53d1\u5e16\u95f4\u9694\u53d8\u5f02\u7cfb\u6570\u7684\u65f6\u95f4\u6307\u7eb9\u65b9\u6cd5\u3002\u7ed3\u5408\u5185\u5bb9\u3001\u6240\u6709\u6743\u548c\u7f51\u7edc\u6307\u6807\uff0c\u5206\u6790\u4e8691,792\u4e2a\u5e16\u5b50\u548c405,707\u6761\u8bc4\u8bba\u3002\u901a\u8fc744\u5c0f\u65f6\u5e73\u53f0\u505c\u673a\u7684\u81ea\u7136\u5b9e\u9a8c\u9a8c\u8bc1\u4eba\u7c7b\u5e72\u9884\u4e0e\u81ea\u4e3b\u4ee3\u7406\u7684\u5dee\u5f02\u3002", "result": "\u6240\u6709\u75c5\u6bd2\u73b0\u8c61\u90fd\u975e\u6e90\u4e8e\u660e\u786e\u81ea\u4e3b\u7684\u4ee3\u7406\uff1a3\u4e2a\u8ffd\u8e2a\u5230\u5177\u6709\u4eba\u7c7b\u5e72\u9884\u7279\u5f81\u7684\u65f6\u95f4\u7b7e\u540d\u8d26\u6237\uff0c1\u4e2a\u663e\u793a\u6df7\u5408\u6a21\u5f0f\uff0c2\u4e2a\u53d1\u5e16\u5386\u53f2\u4e0d\u8db3\u3002\u4eba\u7c7b\u5f71\u54cd\u7684\u4ee3\u7406\u5728\u5e73\u53f0\u91cd\u542f\u540e\u6700\u5148\u8fd4\u56de\uff08\u5360\u65e9\u671f\u91cd\u8fde\u8005\u768487.7%\uff09\u3002\u8fd8\u53d1\u73b0\u4e86\u5de5\u4e1a\u7ea7\u673a\u5668\u4eba\u519c\u573a\uff084\u4e2a\u8d26\u6237\u4ea7\u751f32%\u8bc4\u8bba\uff0c\u534f\u8c03\u95f4\u969412\u79d2\uff09\u548c\u4eba\u7c7b\u5f71\u54cd\u5728\u56de\u590d\u94fe\u4e2d\u7684\u5feb\u901f\u8870\u51cf\uff08\u534a\u8870\u671f\uff1a0.65\u5bf9\u8bdd\u6df1\u5ea6\uff09\u3002", "conclusion": "Moltbook\u4e0a\u7684\u75c5\u6bd2\u53d9\u4e8b\u4e3b\u8981\u662f\u4eba\u7c7b\u9a71\u52a8\u7684\uff0c\u800c\u975e\u81ea\u4e3bAI\u7684\u6d8c\u73b0\u3002\u65f6\u95f4\u6307\u7eb9\u65b9\u6cd5\u80fd\u6709\u6548\u533a\u5206\u81ea\u4e3b\u4e0e\u4eba\u7c7b\u5e72\u9884\u884c\u4e3a\uff0c\u8fd9\u5bf9\u65b0\u5174\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u884c\u4e3a\u5f52\u56e0\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2602.07639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07639", "abs": "https://arxiv.org/abs/2602.07639", "authors": ["Jaewook Lee", "Alexander Scarlatos", "Simon Woodhead", "Andrew Lan"], "title": "Letting Tutor Personas \"Speak Up\" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization", "comment": null, "summary": "With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6fc0\u6d3b\u7a7a\u95f4\u8f6c\u5411\u5411\u91cf\u6765\u5f15\u5bfcLLM\u6a21\u4eff\u4e0d\u540c\u4eba\u7c7b\u5bfc\u5e08\u7684\u6559\u5b66\u98ce\u683c\uff0c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u6307\u4ee4\uff0c\u4ece\u800c\u6355\u6349\u5bfc\u5e08\u98ce\u683c\u7684\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u8f85\u5bfc\u7cfb\u7edf\u901a\u5e38\u53ea\u5b66\u4e60\u5355\u4e00\u5bfc\u5e08\u7b56\u7565\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e2d\u5bfc\u5e08\u98ce\u683c\u7684\u591a\u6837\u6027\u3002\u771f\u5b9e\u5bfc\u5e08\u4f1a\u6839\u636e\u5b66\u751f\u9700\u6c42\u8c03\u6574\u811a\u624b\u67b6\u6c34\u5e73\u3001\u6307\u5bfc\u76f4\u63a5\u6027\u3001\u53cd\u9988\u548c\u60c5\u611f\u652f\u6301\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f1a\u5f71\u54cd\u5bf9\u8bdd\u52a8\u6001\u548c\u5b66\u751f\u53c2\u4e0e\u5ea6\u3002", "method": "\u4fee\u6539\u53cc\u5411\u504f\u597d\u4f18\u5316(BiPO)\u6765\u5b66\u4e60\u8f6c\u5411\u5411\u91cf\uff0c\u8fd9\u662f\u4e00\u4e2a\u6fc0\u6d3b\u7a7a\u95f4\u65b9\u5411\uff0c\u53ef\u4ee5\u5c06\u6a21\u578b\u54cd\u5e94\u5f15\u5bfc\u5411\u7279\u5b9a\u5bfc\u5e08\u98ce\u683c\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u4ece\u4eba\u7c7b\u5bfc\u5e08-\u5b66\u751f\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u6307\u4ee4\u3002", "result": "\u8f6c\u5411\u5411\u91cf\u80fd\u6355\u6349\u4e0d\u540c\u5bf9\u8bdd\u60c5\u5883\u4e0b\u7684\u5bfc\u5e08\u7279\u5b9a\u53d8\u5316\uff0c\u63d0\u9ad8\u4e0e\u771f\u5b9e\u5bfc\u5e08\u8bdd\u8bed\u7684\u8bed\u4e49\u5bf9\u9f50\u5ea6\uff0c\u589e\u52a0\u57fa\u4e8e\u504f\u597d\u7684\u8bc4\u4f30\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u8bcd\u6c47\u76f8\u4f3c\u6027\u3002\u5b66\u4e60\u5230\u7684\u65b9\u5411\u7cfb\u6570\u663e\u793a\u51fa\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\uff0c\u5bf9\u5e94\u5bfc\u5e08\u884c\u4e3a\u7684\u4e00\u81f4\u5dee\u5f02\u3002", "conclusion": "\u6fc0\u6d3b\u8f6c\u5411\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f4\u63a5\u4ece\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u4fe1\u53f7\u6765\u63a7\u5236LLM\u4e2d\u7684\u5bfc\u5e08\u7279\u5b9a\u53d8\u5316\u3002", "topic": "agent analysis"}}
{"id": "2602.07900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07900", "abs": "https://arxiv.org/abs/2602.07900", "authors": ["Zhi Chen", "Zhensu Sun", "Yuling Shi", "Chao Peng", "Xiaodong Gu", "David Lo", "Lingxiao Jiang"], "title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents", "comment": null, "summary": "Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.\n  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u4ee3\u7801\u4ee3\u7406\u5728\u89e3\u51b3\u4ed3\u5e93\u7ea7\u95ee\u9898\u65f6\u7f16\u5199\u7684\u6d4b\u8bd5\u5bf9\u95ee\u9898\u89e3\u51b3\u6548\u679c\u5f71\u54cd\u6709\u9650\uff0c\u6d4b\u8bd5\u66f4\u591a\u4f5c\u4e3a\u89c2\u5bdf\u53cd\u9988\u6e20\u9053\u800c\u975e\u6b63\u5f0f\u65ad\u8a00\u68c0\u67e5", "motivation": "LLM\u4ee3\u7801\u4ee3\u7406\u5728\u89e3\u51b3\u4ed3\u5e93\u7ea7\u95ee\u9898\u65f6\u7ecf\u5e38\u7f16\u5199\u6d4b\u8bd5\uff0c\u4f46GPT-5.2\u51e0\u4e4e\u4e0d\u5199\u6d4b\u8bd5\u5374\u80fd\u53d6\u5f97\u4e0e\u9876\u7ea7\u4ee3\u7406\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u4ee3\u7406\u7f16\u5199\u6d4b\u8bd5\u5b9e\u9645\u4ef7\u503c\u7684\u8d28\u7591", "method": "1) \u5728SWE-bench Verified\u4e0a\u5206\u6790\u516d\u4e2a\u6700\u5148\u8fdbLLM\u7684\u4ee3\u7406\u8f68\u8ff9\uff1b2) \u6bd4\u8f83\u5df2\u89e3\u51b3\u548c\u672a\u89e3\u51b3\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u7f16\u5199\u9891\u7387\uff1b3) \u5206\u6790\u6d4b\u8bd5\u7c7b\u578b\uff08\u6253\u5370\u8bed\u53e5vs\u65ad\u8a00\u68c0\u67e5\uff09\uff1b4) \u901a\u8fc7\u4fee\u6539\u56db\u4e2a\u4ee3\u7406\u7684\u63d0\u793a\u8bed\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u589e\u52a0\u6216\u51cf\u5c11\u6d4b\u8bd5\u7f16\u5199", "result": "1) \u6d4b\u8bd5\u7f16\u5199\u666e\u904d\u4f46\u5df2\u89e3\u51b3\u548c\u672a\u89e3\u51b3\u4efb\u52a1\u7684\u6d4b\u8bd5\u7f16\u5199\u9891\u7387\u76f8\u4f3c\uff1b2) \u6d4b\u8bd5\u4e3b\u8981\u4f5c\u4e3a\u89c2\u5bdf\u53cd\u9988\u6e20\u9053\uff0c\u4ee3\u7406\u66f4\u504f\u597d\u503c\u63ed\u793a\u7684\u6253\u5370\u8bed\u53e5\u800c\u975e\u6b63\u5f0f\u65ad\u8a00\u68c0\u67e5\uff1b3) \u6539\u53d8\u6d4b\u8bd5\u7f16\u5199\u91cf\u5bf9\u6700\u7ec8\u7ed3\u679c\u65e0\u663e\u8457\u5f71\u54cd", "conclusion": "\u5f53\u524d\u4ee3\u7406\u7684\u6d4b\u8bd5\u7f16\u5199\u5b9e\u8df5\u5728\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u53ef\u80fd\u53ea\u63d0\u4f9b\u8fb9\u9645\u6548\u7528\uff0c\u6d4b\u8bd5\u66f4\u591a\u662f\u6a21\u4eff\u4eba\u7c7b\u5b9e\u8df5\u800c\u975e\u6709\u6548\u6539\u8fdb\u95ee\u9898\u89e3\u51b3", "topic": "code agent"}}
{"id": "2602.07673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07673", "abs": "https://arxiv.org/abs/2602.07673", "authors": ["Jiangnan Fang", "Cheng-Tse Liu", "Hanieh Deilamsalehy", "Nesreen K. Ahmed", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi"], "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation", "comment": null, "summary": "Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.", "AI": {"tldr": "LLM\u6cd5\u5b98\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u5b58\u5728\u504f\u89c1\uff0c\u968f\u7740\u88ab\u8bc4\u4f30\u6458\u8981\u4e0e\u4eba\u5de5\u6458\u8981\u76f8\u4f3c\u5ea6\u964d\u4f4e\uff0cLLM\u8d8a\u6765\u8d8a\u504f\u597d\u5176\u4ed6LLM\u751f\u6210\u7684\u6458\u8981\u800c\u975e\u4eba\u5de5\u6458\u8981\uff0c\u4e14\u8fd9\u79cd\u504f\u89c1\u666e\u904d\u5b58\u5728\u4e8e\u6d4b\u8bd5\u7684\u6a21\u578b\u4e2d\u3002", "motivation": "LLM\u6cd5\u5b98\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u867d\u7136\u80fd\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5b58\u5728\u957f\u5ea6\u3001\u987a\u5e8f\u7b49\u504f\u89c1\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5f71\u54cd\u3002\u5148\u524d\u7814\u7a76\u5bf9\u8fd9\u4e9b\u504f\u89c1\u7684\u5206\u6790\u4e0d\u591f\u7ec6\u81f4\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u4e0e\u660e\u786e\u91cd\u53e0\u5ea6\u6307\u6807\u7684\u5173\u8054\u5206\u6790\u3002", "method": "\u5728\u6458\u8981\u9886\u57df\uff0c\u5c06LLM\u6cd5\u5b98\u504f\u89c1\u5206\u6790\u4e0e\u4eba\u5de5\u64b0\u5199\u54cd\u5e94\u7684\u91cd\u53e0\u5ea6\u51fd\u6570\u5173\u8054\u3002\u6d4b\u8bd5\u4e869\u4e2a\u53c2\u6570\u4ece10\u4ebf\u5230120\u4ebf\u7684\u8fd1\u671fLLM\u6a21\u578b\uff0c\u5305\u62ecGemma 3\u548cLLaMA 3\u7684\u53d8\u4f53\uff0c\u4f7f\u7528ROUGE\u548cBLEU\u5ea6\u91cf\u76f8\u4f3c\u5ea6\u3002", "result": "\u53d1\u73b0LLM\u6cd5\u5b98\u8d8a\u6765\u8d8a\u504f\u597d\u5176\u4ed6LLM\u751f\u6210\u7684\u6458\u8981\u800c\u975e\u4eba\u5de5\u6458\u8981\uff0c\u968f\u7740\u88ab\u8bc4\u4f30\u6458\u8981\u4e0e\u4eba\u5de5\u6458\u8981\u76f8\u4f3c\u5ea6\u964d\u4f4e\u3002\u8fd9\u79cd\u6a21\u5f0f\u5b58\u5728\u4e8e\u51e0\u4e4e\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\uff0c\u4e14\u4e0d\u53d7\u6a21\u578b\u81ea\u8eab\u4f4d\u7f6e\u504f\u89c1\u5f71\u54cd\u3002\u6a21\u578b\u751a\u81f3\u96be\u4ee5\u8bc4\u4f30\u91cd\u53e0\u5ea6\u6709\u9650\u7684\u6458\u8981\u3002", "conclusion": "\u5728\u6458\u8981\u9886\u57df\u4f7f\u7528LLM\u4f5c\u4e3a\u6cd5\u5b98\u65f6\uff0c\u4e0d\u5e94\u4ec5\u4f9d\u8d56\u7b80\u5355\u7684\u6bd4\u8f83\uff0c\u800c\u9700\u8981\u91c7\u7528\u8d85\u8d8a\u7b80\u5355\u6bd4\u8f83\u7684\u6280\u672f\u3002", "topic": "agent analysis"}}
{"id": "2602.08004", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.08004", "abs": "https://arxiv.org/abs/2602.08004", "authors": ["George Ling", "Shanshan Zhong", "Richard Huang"], "title": "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality", "comment": null, "summary": "Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.", "AI": {"tldr": "\u5bf940,285\u4e2a\u516c\u5f00Agent\u6280\u80fd\u7684\u5927\u89c4\u6a21\u5206\u6790\u663e\u793a\uff1a\u6280\u80fd\u53d1\u5e03\u5448\u77ed\u671f\u7206\u53d1\u6a21\u5f0f\uff0c\u5185\u5bb9\u9ad8\u5ea6\u96c6\u4e2d\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u5b58\u5728\u663e\u8457\u7684\u4f9b\u9700\u5931\u8861\uff0c\u751f\u6001\u7cfb\u7edf\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u5e76\u5b58\u5728\u975e\u5e73\u51e1\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u968f\u7740Agent\u6280\u80fd\u5728\u516c\u5171\u5e02\u573a\u4e2d\u7684\u6fc0\u589e\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u6280\u80fd\u7c7b\u578b\u3001\u7528\u6237\u91c7\u7528\u6a21\u5f0f\u4ee5\u53ca\u6f5c\u5728\u98ce\u9669\u7684\u7cfb\u7edf\u6027\u4e86\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5bf9\u6765\u81ea\u4e3b\u8981\u5e02\u573a\u768440,285\u4e2a\u516c\u5f00\u6280\u80fd\u8fdb\u884c\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u5206\u6790\uff0c\u8003\u5bdf\u6280\u80fd\u53d1\u5e03\u6a21\u5f0f\u3001\u5185\u5bb9\u5206\u5e03\u3001\u91c7\u7528\u60c5\u51b5\u3001\u4f9b\u9700\u5173\u7cfb\u3001\u751f\u6001\u7cfb\u7edf\u7279\u5f81\u548c\u5b89\u5168\u98ce\u9669\u3002", "result": "\u6280\u80fd\u53d1\u5e03\u5448\u73b0\u77ed\u671f\u7206\u53d1\u6a21\u5f0f\uff0c\u4e0e\u793e\u533a\u5173\u6ce8\u5ea6\u53d8\u5316\u540c\u6b65\uff1b\u5185\u5bb9\u9ad8\u5ea6\u96c6\u4e2d\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u4f46\u4fe1\u606f\u68c0\u7d22\u548c\u5185\u5bb9\u521b\u5efa\u6280\u80fd\u91c7\u7528\u7387\u8f83\u9ad8\uff1b\u5b58\u5728\u663e\u8457\u7684\u4f9b\u9700\u5931\u8861\uff1b\u5927\u591a\u6570\u6280\u80fd\u5728\u5178\u578b\u63d0\u793a\u9884\u7b97\u5185\uff0c\u4f46\u957f\u5ea6\u5206\u5e03\u5448\u73b0\u91cd\u5c3e\u7279\u5f81\uff1b\u751f\u6001\u7cfb\u7edf\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u5b58\u5728\u5e7f\u6cdb\u7684\u610f\u56fe\u7ea7\u5197\u4f59\uff1b\u8bc6\u522b\u51fa\u975e\u5e73\u51e1\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u652f\u6301\u72b6\u6001\u66f4\u6539\u6216\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u7684\u6280\u80fd\u3002", "conclusion": "Agent\u6280\u80fd\u4f5c\u4e3a\u65b0\u5174\u7684Agent\u57fa\u7840\u8bbe\u65bd\u5c42\uff0c\u5448\u73b0\u51fa\u96c6\u4e2d\u5316\u3001\u540c\u8d28\u5316\u548c\u5b89\u5168\u98ce\u9669\u5e76\u5b58\u7684\u7279\u5f81\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u6280\u80fd\u91cd\u7528\u3001\u6807\u51c6\u5316\u548c\u5b89\u5168\u610f\u8bc6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\u3002", "topic": "agent analysis"}}
{"id": "2602.07773", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.07773", "abs": "https://arxiv.org/abs/2602.07773", "authors": ["Chen Zhang", "Kuicai Dong", "Dexun Li", "Wenjun Li", "Qu Yang", "Wei Han", "Yong Liu"], "title": "SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents", "comment": null, "summary": "Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.", "AI": {"tldr": "SRR-Judge\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6b65\u7ea7\u8bc4\u4f30\u6539\u8fdb\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u7ed3\u5408\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff0c\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u5b9e\u73b0\u8d85\u8fc710%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u901a\u5e38\u4ec5\u4f7f\u7528\u7ed3\u679c\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\uff0c\u5ffd\u89c6\u4e86\u4e2d\u95f4\u601d\u7ef4\u548c\u884c\u52a8\u7684\u8d28\u91cf\uff0c\u8fd9\u9650\u5236\u4e86\u641c\u7d22\u96c6\u6210\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51faSRR-Judge\u6846\u67b6\u8fdb\u884c\u53ef\u9760\u7684\u6b65\u7ea7\u63a8\u7406\u548c\u641c\u7d22\u884c\u52a8\u8bc4\u4f30\uff0c\u96c6\u6210\u5230\u6539\u8fdb\u7684ReAct\u5f0f\"\u8bc4\u4f30-\u7cbe\u70bc\"\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u7528SRR\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u3002", "result": "SRR-Judge\u6bd4DeepSeek-V3.1\u7b49\u66f4\u5927\u6a21\u578b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6b65\u7ea7\u8bc4\u4f30\uff0c\u5176\u8bc4\u5206\u4e0e\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u5f3a\u76f8\u5173\u3002\u57fa\u4e8eSRR-Judge\u6807\u6ce8\u8f68\u8ff9\u5bf9\u9f50\u7b56\u7565\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5728\u6311\u6218\u6027\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u4e0a\u5e73\u5747\u7edd\u5bf9pass@1\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "SRR-Judge\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6b65\u7ea7\u8bc4\u4f30\u548c\u57fa\u4e8e\u6807\u6ce8\u6570\u636e\u7684\u7b56\u7565\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u641c\u7d22\u96c6\u6210\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.07213", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07213", "abs": "https://arxiv.org/abs/2602.07213", "authors": ["Srijan Shakya", "Anamaria-Roberta Hartl", "Sepp Hochreiter", "Korbinian P\u00f6ppel"], "title": "Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used", "comment": "Eurips Workshop on Principles of Generative Modeling (PriGM)", "summary": "Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.", "AI": {"tldr": "LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5e38\u56e0\u9759\u6001\u53c2\u6570\u77e5\u8bc6\u800c\u5931\u8d25\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u5c06\u68c0\u7d22\u4f5c\u4e3a\u52a8\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u5f62\u5f0f\uff0c\u6d4b\u8bd5\u4e86LLM\u4ee3\u7406\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u67e5\u8be2\u5916\u90e8\u77e5\u8bc6\u5e93\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u67b6\u6784\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7ecf\u5e38\u56e0\u9759\u6001\u53c2\u6570\u77e5\u8bc6\u800c\u5931\u8d25\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u6570\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63a2\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u7684\u57fa\u672c\u539f\u5219\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u67b6\u6784\uff0c\u8ba9LLM\u4ee3\u7406\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u67e5\u8be2\u5916\u90e8\u77e5\u8bc6\u5e93\uff0c\u5728GSM8K\u548cMATH-500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u8f83\u81ea\u9002\u5e94\u7b56\u7565\u4e0e\u6807\u51c6\u601d\u7ef4\u94fe\u57fa\u7ebf\u548c\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u9759\u6001\u68c0\u7d22\u8868\u73b0\u4e0d\u5982\u601d\u7ef4\u94fe\uff0c\u81ea\u9002\u5e94\u68c0\u7d22\u663e\u793a\u6709\u8da3\u6a21\u5f0f\uff1a\u5305\u542b\u68c0\u7d22\u7ed3\u679c\u7684\u8f68\u8ff9\u8868\u73b0\u7565\u5dee\u4e8e\u601d\u7ef4\u94fe\uff0c\u4f46\u4e0d\u5305\u542b\u68c0\u7d22\u7684\u8f68\u8ff9\u8868\u73b0\u4f18\u4e8e\u601d\u7ef4\u94fe\u3002\u68c0\u7d22\u5f88\u5c11\u5e2e\u52a9\u63a8\u7406\uff08\u4ec5\u5c11\u6570\u53cd\u4f8b\u5982\u4f7f\u7528\u6709\u7528\u5b9a\u7406\uff09\uff0c\u4e3b\u52a8\u4e0d\u4f7f\u7528\u68c0\u7d22\u8868\u660e\u6a21\u578b\u6027\u80fd\u66f4\u597d\u3002\u6a21\u578b\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u68c0\u7d22\u9891\u7387\uff0c\u68c0\u7d22\u51b3\u7b56\u662f\u5173\u952e\u5143\u8ba4\u77e5\u4fe1\u53f7\u3002", "conclusion": "\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u77e5\u8bc6\u5e76\u9009\u62e9\u6027\u4f7f\u7528\u5916\u90e8\u4fe1\u606f\u7684\u80fd\u529b\u662f\u6784\u5efa\u66f4\u7a33\u5065\u53ef\u9760\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u539f\u5219\uff0c\u81ea\u9002\u5e94\u68c0\u7d22\u51b3\u7b56\u4f5c\u4e3a\u5143\u8ba4\u77e5\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u6307\u793a\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.08133", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08133", "abs": "https://arxiv.org/abs/2602.08133", "authors": ["Mojtaba Mostafavi Ghahfarokhi", "Hamed Jahantigh", "Alireza Asadi", "Abbas Heydarnoori"], "title": "Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks", "comment": null, "summary": "Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u6e90\u4ee3\u7801\u5ea6\u91cf\u6307\u6807\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u7528\u4e8e\u81ea\u52a8\u6587\u6863\u751f\u6210\uff0c\u5728\u8ba1\u7b97\u7b14\u8bb0\u672c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u4ee3\u7801\u5ea6\u91cf\u80fd\u63d0\u5347\u6587\u6863\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "motivation": "\u81ea\u52a8\u6587\u6863\u751f\u6210\u65b9\u6cd5\u5e38\u5ffd\u89c6\u4ee3\u7801\u7684\u7ed3\u6784\u548c\u91cf\u5316\u7279\u5f81\uff0c\u800c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u4ee3\u7801\u53ef\u8bfb\u6027\u548c\u7406\u89e3\u5f88\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u5ea6\u91cf\u6307\u6807\u7684\u6709\u6548\u5229\u7528\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u7b14\u8bb0\u672c\u8fd9\u79cd\u96c6\u6210\u4ee3\u7801\u3001\u53d9\u8ff0\u548c\u7ed3\u679c\u4f46\u6587\u6863\u4e0d\u4e00\u81f4\u7684\u6d41\u884c\u5a92\u4ecb\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u6539\u8fdbCodeSearchNet\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u4ece1700\u591a\u4e07\u4e2a\u4ee3\u7801\u548cmarkdown\u5355\u5143\u4e2d\u521b\u5efa\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u7ecf\u8fc7\u7ed3\u6784\u548c\u8bed\u4e49\u8fc7\u6ee4\u5f97\u5230\u7ea636,734\u4e2a\u9ad8\u8d28\u91cf(\u4ee3\u7801, markdown)\u5bf9\uff1b2) \u8bc4\u4f30\u4e24\u79cd\u5efa\u6a21\u8303\u5f0f\uff08\u8f7b\u91cf\u7ea7CNN-RNN\u67b6\u6784\u548c\u5c11\u6837\u672cGPT-3.5\u67b6\u6784\uff09\uff0c\u5728\u6709/\u65e0\u4ee3\u7801\u5ea6\u91cf\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u52a0\u5165\u4ee3\u7801\u5ea6\u91cf\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6587\u6863\u7684\u8d28\u91cf\uff1aCNN-RNN\u67b6\u6784\u5728BLEU-1\u4e0a\u63d0\u53476%\uff0cROUGE-L F1\u63d0\u53473%\uff1bLLM\u67b6\u6784\u5728BERTScore F1\u4e0a\u63d0\u53479%\u3002\u8fd9\u8868\u660e\u4ee3\u7801\u5ea6\u91cf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7ed3\u6784\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u5c06\u4ee3\u7801\u5ea6\u91cf\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u80fd\u6709\u6548\u589e\u5f3a\u81ea\u52a8\u6587\u6863\u751f\u6210\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u90fd\u663e\u793a\u51fa\u6539\u8fdb\u6548\u679c\uff0c\u4e3a\u4ee3\u7801\u7406\u89e3\u548c\u6587\u6863\u751f\u6210\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u8865\u5145\u4fe1\u606f\u3002", "topic": "swe application"}}
{"id": "2602.07796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07796", "abs": "https://arxiv.org/abs/2602.07796", "authors": ["Jiatong Li", "Changdae Oh", "Hyeong Kyu Choi", "Jindong Wang", "Sharon Li"], "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents", "comment": "27 pages, 19 figures", "summary": "Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7528\u6237\u53c2\u4e0e\u7684LLM\u667a\u80fd\u4f53\u573a\u666f\u4e2d\uff0c\u5f3a\u5236\u601d\u8003\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u601d\u8003\u8ba9\u667a\u80fd\u4f53\u53d8\u5f97\u66f4\"\u5185\u5411\"\uff0c\u51cf\u5c11\u4fe1\u606f\u900f\u9732\uff0c\u524a\u5f31\u4e86\u4e0e\u7528\u6237\u7684\u4fe1\u606f\u4ea4\u6362\u3002", "motivation": "\u867d\u7136\u63a8\u7406\u80fd\u529b\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u7528\u6237\u53c2\u4e0e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u663e\u5f0f\u601d\u8003\u5728\u7528\u6237\u53c2\u4e0e\u7684LLM\u667a\u80fd\u4f53\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u91c7\u75287\u4e2a\u6a21\u578b\u30013\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c2\u79cd\u601d\u8003\u5b9e\u4f8b\u5316\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5b9a\u91cf\u54cd\u5e94\u5206\u7c7b\u5206\u6790\u548c\u5b9a\u6027\u5931\u8d25\u4f20\u64ad\u6848\u4f8b\u7814\u7a76\u6765\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u4e0e\u9884\u671f\u76f8\u53cd\uff0c\u5f3a\u5236\u601d\u8003\u5728\u7528\u6237\u53c2\u4e0e\u573a\u666f\u4e2d\u5e38\u5e38\u9002\u5f97\u5176\u53cd\uff0c\u5bfc\u81f4\u5404\u79cdLLM\u7684\u6027\u80fd\u5f02\u5e38\u4e0b\u964d\u3002\u5173\u952e\u53d1\u73b0\u662f\u601d\u8003\u8ba9\u667a\u80fd\u4f53\u53d8\u5f97\u66f4\"\u5185\u5411\"\uff0c\u7f29\u77ed\u54cd\u5e94\u5e76\u51cf\u5c11\u5411\u7528\u6237\u7684\u4fe1\u606f\u62ab\u9732\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u667a\u80fd\u4f53-\u7528\u6237\u4fe1\u606f\u4ea4\u6362\u5e76\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u5931\u8d25\u3002", "conclusion": "\u4fe1\u606f\u900f\u660e\u5ea6\u610f\u8bc6\u662f\u672a\u6765\u771f\u5b9e\u573a\u666f\u4e2d\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u89c6\u89d2\u3002\u660e\u786e\u63d0\u793a\u4fe1\u606f\u900f\u9732\u80fd\u53ef\u9760\u5730\u63d0\u9ad8\u4e0d\u540c\u6a21\u578b\u65cf\u7684\u6027\u80fd\uff0c\u8868\u660e\u4e3b\u52a8\u900f\u660e\u5ea6\u662f\u667a\u80fd\u4f53\u4f18\u5316\u7684\u91cd\u8981\u6760\u6746\u3002", "topic": "agent analysis"}}
{"id": "2602.08146", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08146", "abs": "https://arxiv.org/abs/2602.08146", "authors": ["Pengyu Chang", "Yixiong Fang", "Silin Chen", "Yuling Shi", "Beijun Shen", "Xiaodong Gu"], "title": "Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation", "comment": null, "summary": "Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants \"hacking\" the blind spots of T's current test suite, while T iteratively refines its test cases to \"kill\" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.", "AI": {"tldr": "AdverTest\uff1a\u4e00\u4e2a\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4ea4\u4e92\u4ee3\u7406\uff08\u6d4b\u8bd5\u751f\u6210\u4ee3\u7406\u548c\u53d8\u5f02\u751f\u6210\u4ee3\u7406\uff09\u7684\u5bf9\u6297\u5faa\u73af\uff0c\u63d0\u5347LLM\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684bug\u68c0\u6d4b\u80fd\u529b\uff0c\u5728Defects4J\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u7387\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u8986\u76d6\u7387\u867d\u9ad8\u4f46\u53ef\u8bfb\u6027\u5dee\uff0cLLM\u65b9\u6cd5\u53ef\u8bfb\u6027\u597d\u4f46\u8986\u76d6\u7387\u548c\u53ef\u7f16\u8bd1\u6027\u4f4e\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u63d0\u5347bug\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u66b4\u9732\u8fb9\u754c\u60c5\u51b5\u548c\u8106\u5f31\u6267\u884c\u8def\u5f84\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faAdverTest\u5bf9\u6297\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4ea4\u4e92\u4ee3\u7406\uff1a\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u4ee3\u7406(T)\u548c\u53d8\u5f02\u751f\u6210\u4ee3\u7406(M)\u3002\u4e24\u8005\u8fdb\u884c\u5bf9\u6297\u5faa\u73af\uff1aM\u6301\u7eed\u521b\u5efa\u65b0\u7684\u53d8\u5f02\u4f53\"\u653b\u51fb\"T\u5f53\u524d\u6d4b\u8bd5\u5957\u4ef6\u7684\u76f2\u70b9\uff0cT\u8fed\u4ee3\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\u4ee5\"\u6740\u6b7b\"M\u751f\u6210\u7684\u6311\u6218\u6027\u53d8\u5f02\u4f53\u3002\u8be5\u5faa\u73af\u7531\u8986\u76d6\u7387\u548c\u53d8\u5f02\u5206\u6570\u5171\u540c\u6307\u5bfc\uff0c\u4f7f\u7cfb\u7edf\u534f\u540c\u8fdb\u5316\u4ee5\u83b7\u5f97\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u548cbug\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728Defects4J\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u76f8\u6bd4\u73b0\u6709\u6700\u4f73LLM\u65b9\u6cd5\uff0c\u6545\u969c\u68c0\u6d4b\u7387\u63d0\u53478.56%\uff1b\u76f8\u6bd4EvoSuite\u63d0\u534763.30%\u3002\u540c\u65f6\u63d0\u9ad8\u4e86\u884c\u8986\u76d6\u7387\u548c\u5206\u652f\u8986\u76d6\u7387\u3002", "conclusion": "AdverTest\u901a\u8fc7\u5bf9\u6297\u6027\u5b66\u4e60\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684bug\u68c0\u6d4b\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9c81\u68d2\u6027bug\u68c0\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u8f6f\u4ef6\u6d4b\u8bd5\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "swe application"}}
{"id": "2602.07624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07624", "abs": "https://arxiv.org/abs/2602.07624", "authors": ["Junyu Feng", "Binxiao Xu", "Jiayi Chen", "Mengyu Dai", "Cenyang Wu", "Haodong Li", "Bohan Zeng", "Yunliu Xie", "Hao Liang", "Ming Lu", "Wentao Zhang"], "title": "M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions", "comment": null, "summary": "This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.", "AI": {"tldr": "M2A\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u957f\u671f\u4eba\u673a\u4ea4\u4e92\u7684\u53cc\u5c42\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u66f4\u65b0\u7ef4\u62a4\u4e2a\u6027\u5316\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8d85\u957f\u5bf9\u8bdd\u5386\u53f2\u4e2d\u65e0\u6cd5\u6301\u7eed\u5438\u6536\u7528\u6237\u589e\u91cf\u6982\u5ff5\u3001\u522b\u540d\u548c\u504f\u597d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u662f\u9759\u6001\u7684\uff0c\u6982\u5ff5\u5728\u521d\u59cb\u5316\u65f6\u56fa\u5b9a\u4e14\u65e0\u6cd5\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u6f14\u5316\u3002\u5f53\u5bf9\u8bdd\u5386\u53f2\u8de8\u8d8a\u6570\u5468\u6216\u6570\u6708\u5e76\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\uff0c\u73b0\u6709\u4e2a\u6027\u5316\u673a\u5236\u96be\u4ee5\u6301\u7eed\u5438\u6536\u548c\u5229\u7528\u7528\u6237\u7684\u589e\u91cf\u6982\u5ff5\u3001\u522b\u540d\u548c\u504f\u597d\u3002", "method": "M2A\u91c7\u7528\u4ee3\u7406\u9a71\u52a8\u7684\u53cc\u5c42\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\uff1aChatAgent\u7ba1\u7406\u7528\u6237\u4ea4\u4e92\u5e76\u81ea\u4e3b\u51b3\u5b9a\u4f55\u65f6\u67e5\u8be2\u6216\u66f4\u65b0\u8bb0\u5fc6\uff1bMemoryManager\u5c06\u8bb0\u5fc6\u8bf7\u6c42\u5206\u89e3\u4e3a\u5bf9\u53cc\u5c42\u8bb0\u5fc6\u5e93\u7684\u8be6\u7ec6\u64cd\u4f5c\u3002\u8bb0\u5fc6\u5e93\u5305\u62ecRawMessageStore\uff08\u4e0d\u53ef\u53d8\u5bf9\u8bdd\u65e5\u5fd7\uff09\u548cSemanticMemoryStore\uff08\u9ad8\u5c42\u89c2\u5bdf\uff09\uff0c\u63d0\u4f9b\u4e0d\u540c\u7c92\u5ea6\u7684\u8bb0\u5fc6\u3002\u8fd8\u5f00\u53d1\u4e86\u53ef\u91cd\u7528\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u5c06Yo'LLaVA\u548cMC-LLaVA\u7684\u6982\u5ff5\u57fa\u7840\u4f1a\u8bdd\u6ce8\u5165LoCoMo\u957f\u5bf9\u8bdd\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eM2A\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u5c06\u4e2a\u6027\u5316\u4ece\u4e00\u6b21\u6027\u914d\u7f6e\u8f6c\u53d8\u4e3a\u534f\u540c\u6f14\u5316\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u4e3a\u957f\u671f\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u9ad8\u8d28\u91cf\u4e2a\u6027\u5316\u54cd\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "M2A\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7684\u53cc\u5c42\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u957f\u671f\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u4e2a\u6027\u5316\u6311\u6218\uff0c\u5c06\u4e2a\u6027\u5316\u4ece\u9759\u6001\u914d\u7f6e\u8f6c\u53d8\u4e3a\u52a8\u6001\u534f\u540c\u6f14\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2602.07839", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07839", "abs": "https://arxiv.org/abs/2602.07839", "authors": ["Jiaxi Liu", "Yanzuo Jiang", "Guibin Zhang", "Zihan Zhang", "Heng Chang", "Zhenfei Yin", "Qibing Ren", "Junchi Yan"], "title": "TodoEvolve: Learning to Architect Agent Planning Systems", "comment": null, "summary": "Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \\textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.", "AI": {"tldr": "TodoEvolve\u662f\u4e00\u4e2a\u5143\u89c4\u5212\u8303\u5f0f\uff0c\u80fd\u591f\u81ea\u4e3b\u5408\u6210\u5e76\u52a8\u6001\u4fee\u8ba2\u4efb\u52a1\u7279\u5b9a\u7684\u89c4\u5212\u67b6\u6784\uff0c\u901a\u8fc7PlanFactory\u7edf\u4e00\u4e0d\u540c\u89c4\u5212\u8303\u5f0f\uff0c\u5e76\u4f7f\u7528IGPO\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u624b\u5de5\u8bbe\u8ba1\u7684\u89c4\u5212\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u56fa\u5b9a\u7684\u3001\u624b\u5de5\u8bbe\u8ba1\u7684\u89c4\u5212\u7ed3\u6784\uff0c\u7f3a\u4e4f\u9002\u5e94\u5f00\u653e\u6027\u95ee\u9898\u7ed3\u6784\u591a\u6837\u6027\u7684\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "1) \u6784\u5efaPlanFactory\u6a21\u5757\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7edf\u4e00\u4e0d\u540c\u89c4\u5212\u8303\u5f0f\uff1b2) \u6536\u96c6\u9ad8\u8d28\u91cf\u89c4\u5212\u8f68\u8ff9\uff1b3) \u4f7f\u7528\u963b\u6297\u5f15\u5bfc\u504f\u597d\u4f18\u5316(IGPO)\u8bad\u7ec3Todo-14B\u6a21\u578b\uff0c\u8be5\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u9f13\u52b1\u751f\u6210\u6027\u80fd\u597d\u3001\u7a33\u5b9a\u4e14\u4ee4\u724c\u9ad8\u6548\u7684\u89c4\u5212\u7cfb\u7edf\u3002", "result": "\u5728\u4e94\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTodoEvolve\u59cb\u7ec8\u8d85\u8d8a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89c4\u5212\u6a21\u5757\uff0c\u540c\u65f6\u4fdd\u6301\u7ecf\u6d4e\u7684API\u6210\u672c\u548c\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "TodoEvolve\u901a\u8fc7\u5143\u89c4\u5212\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u56fa\u5b9a\u89c4\u5212\u7ed3\u6784\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u89c4\u5212\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.08192", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08192", "abs": "https://arxiv.org/abs/2602.08192", "authors": ["Mirko Perkusich", "Danyllo Albuquerque", "Allysson Allex Ara\u00fajo", "Matheus Paix\u00e3o", "Rohit Gheyi", "Marcos Kalinowski", "Angelo Perkusich"], "title": "Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners", "comment": "Accepted for publication at the 27th International Conference on Agile Software Development (XP 2026)", "summary": "Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8c03\u67e570\u540d\u5df4\u897f\u4e13\u4e1a\u4eba\u58eb\uff0c\u9996\u6b21\u5b9e\u8bc1\u7814\u7a76\u4e86LLM\u5728Scrum\u7ba1\u7406\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u63ed\u793a\u4e86\u9ad8\u4f7f\u7528\u9891\u7387\u3001\u4e3b\u8981\u96c6\u4e2d\u4e8eScrum\u5b9e\u8df5\u63a2\u7d22\u3001\u751f\u4ea7\u529b\u63d0\u5347\u663e\u8457\u4f46\u5b58\u5728\u8f93\u51fa\u51c6\u786e\u6027\u3001\u4fdd\u5bc6\u6027\u548c\u5e7b\u89c9\u7b49\u98ce\u9669\u3002", "motivation": "Scrum\u4f5c\u4e3a\u5e7f\u6cdb\u91c7\u7528\u7684\u654f\u6377\u9879\u76ee\u7ba1\u7406\u65b9\u6cd5\uff0c\u5176\u77e5\u8bc6\u5bc6\u96c6\u578b\u5b9e\u8df5\u4e0e\u65b0\u5174\u7684LLM\u6280\u672f\u5b58\u5728\u7ed3\u5408\u6f5c\u529b\u3002\u7136\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5728\u7f16\u7801\u3001\u6d4b\u8bd5\u7b49\u6280\u672f\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\uff0c\u5bf9\u4e8e\u7ba1\u7406\u76f8\u5173\u7684Scrum\u6d3b\u52a8\u652f\u6301\u7f3a\u4e4f\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5bf970\u540d\u5df4\u897f\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u8c03\u67e5\u7814\u7a76\uff0c\u5176\u4e2d49\u4eba\u79ef\u6781\u4f7f\u7528Scrum\uff0c33\u4eba\u62a5\u544a\u5728Scrum\u5b9e\u8df5\u4e2d\u4f7f\u7528LLM\u52a9\u624b\u3002\u91c7\u7528\u95ee\u5377\u8c03\u67e5\u65b9\u6cd5\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790LLM\u5728Scrum\u7ba1\u7406\u6d3b\u52a8\u4e2d\u7684\u4f7f\u7528\u6a21\u5f0f\u3001\u9891\u7387\u3001\u719f\u7ec3\u5ea6\u548c\u5f71\u54cd\u3002", "result": "\u8c03\u67e5\u663e\u793a\uff1a85%\u53d7\u8bbf\u8005\u5177\u5907\u4e2d\u9ad8\u7ea7LLM\u719f\u7ec3\u5ea6\uff0c52%\u6bcf\u65e5\u4f7f\u7528\uff1bLLM\u4e3b\u8981\u7528\u4e8e\u63a2\u7d22Scrum\u5b9e\u8df5\uff0c\u5bf9\u5de5\u4ef6\u548c\u4e8b\u4ef6\u63d0\u4f9b\u9488\u5bf9\u6027\u652f\u6301\uff1b\u4e3b\u8981\u6548\u76ca\u5305\u62ec\u751f\u4ea7\u529b\u63d0\u534778%\u548c\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c75%\uff1b\u4f46\u5b58\u5728\u8f93\u51fa\"\u51e0\u4e4e\u6b63\u786e\"81%\u3001\u4fdd\u5bc6\u62c5\u5fe763%\u3001\u4f7f\u7528\u4e2d\u5e7b\u89c959%\u7b49\u98ce\u9669\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5b9e\u8bc1\u63cf\u8ff0\u4e86LLM\u5728Scrum\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5b9e\u8df5\u6a21\u5f0f\uff0c\u91cf\u5316\u4e86\u6548\u76ca\u4e0e\u98ce\u9669\uff0c\u4e3a\u654f\u6377\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u5730\u91c7\u7528\u548c\u96c6\u6210LLM\u6280\u672f\u63d0\u4f9b\u4e86\u65b9\u5411\u6307\u5bfc\u3002", "topic": "swe application"}}
{"id": "2602.07842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07842", "abs": "https://arxiv.org/abs/2602.07842", "authors": ["Yuhan Wang", "Shiyu Ni", "Zhikai Ding", "Zihang Zhan", "Yuanzi Li", "Keping Bi"], "title": "Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers", "comment": null, "summary": "Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMACE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76LLM\u5728\u591a\u6b63\u786e\u7b54\u6848\u573a\u666f\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86SCA\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u7b54\u6848\u5bfc\u81f4\u7684\u7f6e\u4fe1\u5ea6\u4f4e\u4f30\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u81ea\u7531\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u7b54\u6848\u95ee\u7b54\u573a\u666f\u7814\u7a76\uff0c\u4f46\u5728\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7b54\u6848\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f1a\u5931\u6548\uff0c\u56e0\u4e3a\u6b63\u786e\u7b54\u6848\u4e4b\u95f4\u7684\u5206\u6b67\u4f1a\u5bfc\u81f4\u7f6e\u4fe1\u5ea6\u7cfb\u7edf\u6027\u4f4e\u4f30\u3002", "method": "1) \u5f15\u5165MACE\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5305\u542b12,000\u4e2a\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u6db5\u76d66\u4e2a\u9886\u57df\uff0c\u5177\u6709\u4e0d\u540c\u6570\u91cf\u7684\u6b63\u786e\u7b54\u6848\uff1b2) \u63d0\u51fa\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u805a\u5408(SCA)\uff1a\u901a\u8fc7\u5bf9\u591a\u4e2a\u9ad8\u6982\u7387\u91c7\u6837\u54cd\u5e94\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u805a\u5408\u6765\u89e3\u51b3\u591a\u7b54\u6848\u6821\u51c6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u968f\u7740\u7b54\u6848\u57fa\u6570\u589e\u52a0\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4f46\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\u6301\u7eed\u4e0b\u964d\uff0c\u5bfc\u81f4\u6df7\u5408\u7b54\u6848\u6570\u91cf\u95ee\u9898\u7684\u4e25\u91cd\u6821\u51c6\u9519\u8bef\u3002SCA\u5728\u6df7\u5408\u7b54\u6848\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5355\u7b54\u6848\u95ee\u9898\u4e0a\u4fdd\u6301\u5f3a\u6821\u51c6\u3002", "conclusion": "\u591a\u6b63\u786e\u7b54\u6848\u573a\u666f\u5bf9LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u63d0\u51fa\u4e86\u65b0\u6311\u6218\uff0cSCA\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5355\u7b54\u6848\u6821\u51c6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u5584\u591a\u7b54\u6848\u573a\u666f\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2602.07909", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07909", "abs": "https://arxiv.org/abs/2602.07909", "authors": ["Taolin Zhang", "Hang Guo", "Wang Lu", "Tao Dai", "Shu-Tao Xia", "Jindong Wang"], "title": "SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization", "comment": "ICLR2026", "summary": "As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$\u03c4$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.", "AI": {"tldr": "SparseEval\uff1a\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f18\u5316\u7684\u9ad8\u6548LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u951a\u70b9\u6743\u91cd\u548c\u8fed\u4ee3\u7cbe\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u7684\u8ba1\u7b97\u6210\u672c\u6025\u5267\u589e\u52a0\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5bf9\u5927\u91cf\u57fa\u51c6\u6837\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f5c\u8005\u53d1\u73b0\u6a21\u578b-\u9879\u76ee\u6027\u80fd\u77e9\u9635\u5177\u6709\u7a00\u758f\u6027\uff0c\u53ef\u4ee5\u5229\u7528\u4ee3\u8868\u6027\u9879\u76ee\u4f5c\u4e3a\u951a\u70b9\u8fdb\u884c\u9ad8\u6548\u8bc4\u4f30\u3002", "method": "\u5c06\u9ad8\u6548\u8bc4\u4f30\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u7a00\u758f\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faSparseEval\u65b9\u6cd5\uff1a1\uff09\u9996\u6b21\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u951a\u70b9\u6743\u91cd\uff1b2\uff09\u4f7f\u7528\u8fed\u4ee3\u7cbe\u5316\u7b56\u7565\u9009\u62e9\u951a\u70b9\uff1b3\uff09\u5229\u7528MLP\u7684\u8868\u5f81\u80fd\u529b\u5904\u7406\u7a00\u758f\u4f18\u5316\uff1b4\uff09\u63d0\u51fa\u951a\u70b9\u91cd\u8981\u6027\u5206\u6570\u548c\u5019\u9009\u91cd\u8981\u6027\u5206\u6570\u6765\u8bc4\u4f30\u6bcf\u4e2a\u9879\u76ee\u7684\u4ef7\u503c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f4e\u4f30\u8ba1\u8bef\u5dee\u548c\u9ad8Kendall's \u03c4\u76f8\u5173\u6027\uff0c\u5c55\u73b0\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "SparseEval\u901a\u8fc7\u5229\u7528\u6a21\u578b-\u9879\u76ee\u6027\u80fd\u77e9\u9635\u7684\u7a00\u758f\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.08263", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08263", "abs": "https://arxiv.org/abs/2602.08263", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Mustafa A. Mustafa", "Youcheng Sun"], "title": "Specification Vibing for Automated Program Repair", "comment": null, "summary": "Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of \"vibe\" coding: make the behavior sing, and the code will follow.", "AI": {"tldr": "VibeRepair\u662f\u4e00\u79cd\u57fa\u4e8e\u89c4\u8303\u7684\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u5c06\u4fee\u590d\u89c6\u4e3a\u884c\u4e3a\u89c4\u8303\u4fee\u590d\u800c\u975e\u4ee3\u7801\u7f16\u8f91\uff0c\u901a\u8fc7\u5c06\u9519\u8bef\u4ee3\u7801\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u89c4\u8303\uff0c\u4fee\u590d\u89c4\u8303\u504f\u5dee\uff0c\u7136\u540e\u57fa\u4e8e\u4fee\u6b63\u540e\u7684\u89c4\u8303\u5408\u6210\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u5927\u591a\u662f\u4ee3\u7801\u4e2d\u5fc3\u7684\uff0c\u76f4\u63a5\u91cd\u5199\u6e90\u4ee3\u7801\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u4fee\u590d\u548c\u884c\u4e3a\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6613\u4e8eLLM\u7406\u89e3\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u7406\u89e3\u3001\u5206\u6790\u548c\u4fee\u590d\u5bf9\u9f50\u3002", "method": "VibeRepair\u91c7\u7528\u89c4\u8303\u4e2d\u5fc3\u7684\u4fee\u590d\u8303\u5f0f\uff1a1) \u5c06\u9519\u8bef\u4ee3\u7801\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u89c4\u8303\uff1b2) \u63a8\u65ad\u5e76\u4fee\u590d\u89c4\u8303\u504f\u5dee\uff1b3) \u57fa\u4e8e\u4fee\u6b63\u540e\u7684\u89c4\u8303\u4e25\u683c\u5408\u6210\u4ee3\u7801\u3002\u5305\u542b\u6309\u9700\u63a8\u7406\u7ec4\u4ef6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u548c\u5386\u53f2\u4fee\u590d\u8bc1\u636e\u4e30\u5bcc\u56f0\u96be\u6848\u4f8b\u3002", "result": "\u5728Defects4J v1.2\u4e0a\u6b63\u786e\u4fee\u590d174\u4e2abug\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u591a28\u4e2a\uff08\u63d0\u534719%\uff09\uff1b\u5728v2.0\u4e0a\u4fee\u590d178\u4e2abug\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u591a33\u4e2a\uff08\u63d0\u534723%\uff09\u3002\u5728\u8bad\u7ec3\u671f\u540e\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4fee\u590d\u4e2d\u5fc3\u653e\u5728\u660e\u786e\u7684\u884c\u4e3a\u610f\u56fe\u4e0a\uff0cVibeRepair\u4e3a\"\u6c1b\u56f4\"\u7f16\u7801\u65f6\u4ee3\u91cd\u6784\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff1a\u8ba9\u884c\u4e3a\u5148\u884c\uff0c\u4ee3\u7801\u81ea\u7136\u8ddf\u968f\u3002", "topic": "swe application"}}
{"id": "2602.07749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07749", "abs": "https://arxiv.org/abs/2602.07749", "authors": ["Zhenyu Wu", "Yanxi Long", "Jian Li", "Hua Huang"], "title": "Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution", "comment": "ICML2026", "summary": "Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.", "AI": {"tldr": "Geo-coder\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u51e0\u4f55\u56fe\u50cf\u9006\u5411\u7f16\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u951a\u5b9a\u548c\u5ea6\u91cf\u9a71\u52a8\u4ee3\u7801\u6f14\u5316\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u91cd\u5efa\uff0c\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u9886\u5148\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "motivation": "\u7a0b\u5e8f\u4ee3\u7801\u4f5c\u4e3a\u8fde\u63a5\u89c6\u89c9\u4e0e\u903b\u8f91\u7684\u6865\u6881\uff0c\u53ef\u4e3a\u589e\u5f3a\u5927\u6a21\u578b\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u76d1\u7763\u65b9\u6cd5\u3002\u4f46\u73b0\u6709\u9006\u5411\u56fe\u5f62\u65b9\u6cd5\u5728\u91cd\u5efa\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5bb9\u6613\u4e22\u5931\u5173\u952e\u51e0\u4f55\u7ea6\u675f\u6216\u4ea7\u751f\u7ed3\u6784\u5931\u771f\u3002", "method": "\u63d0\u51faGeo-coder\u2014\u2014\u9996\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u51e0\u4f55\u56fe\u50cf\u9006\u5411\u7f16\u7a0b\u6846\u67b6\u3002\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5c06\u8fc7\u7a0b\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7\u50cf\u7d20\u7ea7\u951a\u5b9a\u8fdb\u884c\u51e0\u4f55\u5efa\u6a21\uff0c\u5229\u7528\u89c6\u89c9\u7b97\u5b50\u548c\u5927\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\u7cbe\u786e\u6355\u6349\u50cf\u7d20\u5750\u6807\u548c\u89c6\u89c9\u5c5e\u6027\uff1b2\uff09\u5f15\u5165\u5408\u6210-\u6e32\u67d3-\u9a8c\u8bc1\u95ed\u73af\uff0c\u901a\u8fc7\u53cc\u5411\u89c6\u89c9\u53cd\u9988\u9a71\u52a8\u4ee3\u7801\u81ea\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGeo-coder\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u9886\u5148\u3002\u91cd\u5efa\u56fe\u50cf\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u3002\u5f00\u6e90\u4e86\u5305\u542b1500+\u6837\u672c\u7684Geo-coder\u6570\u636e\u96c6\u548cGeocodeLM\u6a21\u578b\u3002", "conclusion": "Geo-coder\u901a\u8fc7\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u9006\u5411\u7f16\u7a0b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u91cd\u5efa\u7684\u6311\u6218\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u636e\u548c\u6a21\u578b\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2602.08561", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08561", "abs": "https://arxiv.org/abs/2602.08561", "authors": ["Syed Mehtab Hussain Shah", "Frank Hopfgartner", "Arnim Bleier"], "title": "Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches", "comment": "12 pages, 5 figures. Submitted to ACM conference", "summary": "Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u80fd\u5426\u81ea\u52a8\u8bca\u65ad\u548c\u4fee\u590d\u8ba1\u7b97\u7814\u7a76\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u5728R\u8bed\u8a00\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u6d4b\u8bd5\u5e8a\u4e0a\uff0c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff08\u6210\u529f\u738769-96%\uff09\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u5de5\u4f5c\u6d41\uff0831-79%\uff09\u3002", "motivation": "\u8ba1\u7b97\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u5e38\u56e0\u7f3a\u5931\u5305\u3001\u8def\u5f84\u95ee\u9898\u3001\u7248\u672c\u51b2\u7a81\u6216\u903b\u8f91\u4e0d\u5b8c\u6574\u800c\u5931\u8d25\uff0c\u5373\u4f7f\u5171\u4eab\u4e86\u6750\u6599\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u80fd\u5426\u81ea\u52a8\u5316\u8bca\u65ad\u548c\u4fee\u590d\u8fd9\u4e9b\u5931\u8d25\uff0c\u4f7f\u8ba1\u7b97\u7ed3\u679c\u66f4\u6613\u590d\u73b0\u548c\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u5b8c\u5168\u53ef\u590d\u73b0\u7684R\u8bed\u8a00\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u6784\u5efa\u53d7\u63a7\u6d4b\u8bd5\u5e8a\uff0c\u6ce8\u5165\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u73b0\u5b9e\u6545\u969c\u3002\u6d4b\u8bd5\u4e24\u79cd\u81ea\u52a8\u4fee\u590d\u5de5\u4f5c\u6d41\uff1a1\uff09\u63d0\u793a\u5f0f\u5de5\u4f5c\u6d41\uff08\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u53cd\u590d\u67e5\u8be2\u8bed\u8a00\u6a21\u578b\uff09\uff1b2\uff09\u4ee3\u7406\u5f0f\u5de5\u4f5c\u6d41\uff08\u81ea\u4e3b\u68c0\u67e5\u6587\u4ef6\u3001\u4fee\u6539\u4ee3\u7801\u3001\u91cd\u65b0\u8fd0\u884c\u5206\u6790\uff09\u3002\u5728\u5e72\u51c0\u7684Docker\u73af\u5883\u4e2d\u8bc4\u4f30\u3002", "result": "\u63d0\u793a\u5f0f\u5de5\u4f5c\u6d41\u7684\u590d\u73b0\u6210\u529f\u7387\u572831-79%\u4e4b\u95f4\uff0c\u6027\u80fd\u53d7\u63d0\u793a\u4e0a\u4e0b\u6587\u548c\u9519\u8bef\u590d\u6742\u5ea6\u5f71\u54cd\u663e\u8457\uff0c\u590d\u6742\u6848\u4f8b\u4ece\u989d\u5916\u4e0a\u4e0b\u6587\u4e2d\u83b7\u76ca\u6700\u5927\u3002\u4ee3\u7406\u5f0f\u5de5\u4f5c\u6d41\u8868\u73b0\u663e\u8457\u66f4\u597d\uff0c\u5728\u6240\u6709\u590d\u6742\u5ea6\u6c34\u5e73\u4e0a\u6210\u529f\u738769-96%\u3002", "conclusion": "\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7279\u522b\u662f\u4ee3\u7406\u5f0f\u7cfb\u7edf\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u5404\u79cd\u9519\u8bef\u7c7b\u578b\u7684\u590d\u73b0\u6210\u529f\u7387\u3002\u4e0e\u5148\u524d\u57fa\u51c6\u4e0d\u540c\uff0c\u8be5\u6d4b\u8bd5\u5e8a\u5728\u53d7\u63a7\u6545\u969c\u6a21\u5f0f\u4e0b\u9694\u79bb\u4e86\u53d1\u5e03\u540e\u4fee\u590d\uff0c\u5141\u8bb8\u76f4\u63a5\u6bd4\u8f83\u63d0\u793a\u5f0f\u548c\u4ee3\u7406\u5f0f\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.08765", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08765", "abs": "https://arxiv.org/abs/2602.08765", "authors": ["Micah Villmow"], "title": "Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas", "comment": "32 Pages, 7 Figures", "summary": "LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.", "AI": {"tldr": "Scylla\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u5c42\u6d4b\u8bd5(T0-T6)\u8fdb\u884c\u7ed3\u6784\u5316\u6d88\u878d\u7814\u7a76\uff0c\u4f7f\u7528\u6210\u672c\u901a\u8fc7\u7387(CoP)\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\u6765\u8861\u91cf\u67b6\u6784\u590d\u6742\u6027\u4e0e\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524dLLM\u5de5\u5177\u6b63\u5728\u5feb\u901f\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u63d0\u793a\u3001\u6280\u80fd\u3001\u5de5\u5177\u3001\u591a\u4ee3\u7406\u8bbe\u7f6e\uff09\u5bf9\u80fd\u529b\u548c\u6210\u672c\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u63d0\u51faScylla\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u4e03\u5c42\u6d4b\u8bd5(T0-T6)\u9010\u6b65\u589e\u52a0\u590d\u6742\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d88\u878d\u7814\u7a76\u9694\u79bb\u5f71\u54cd\u7ed3\u679c\u7684\u56e0\u7d20\u3002\u6838\u5fc3\u6307\u6807\u662f\u6210\u672c\u901a\u8fc7\u7387(CoP)\uff0c\u6846\u67b6\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4e0e\u4efb\u4f55CLI\u5de5\u5177\u914d\u5408\u4f7f\u7528\u3002\u4f7f\u7528\u591a\u4e2aLLM\u8bc4\u59d4(Opus 4.5, Sonnet 4.5, Haiku 4.5)\u8fdb\u884c\u5171\u8bc6\u8bc4\u4f30\uff0c\u8bc4\u59d4\u901a\u8fc7\u76f4\u63a5\u6d4b\u8bd5\u3001\u4eba\u5de5\u8bbe\u8ba1\u7684LLM\u8bc4\u4f30\u91cf\u8868\u548c\u5b9a\u6027\u8bc4\u4f30\u6765\u8bc4\u5206\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u91cf\u5316\u4e86\u4ee3\u7406\u590d\u6742\u6027\u4e0e\u5b9e\u9645\u7ed3\u679c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8868\u660e\u67b6\u6784\u590d\u6742\u6027\u5e76\u4e0d\u603b\u662f\u80fd\u63d0\u9ad8\u8d28\u91cf\u3002", "conclusion": "Scylla\u6846\u67b6\u4e3a\u8bc4\u4f30\u667a\u80fd\u7f16\u7801\u5de5\u5177\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u67b6\u6784\u590d\u6742\u6027\u4e0e\u6027\u80fd\u6548\u7387\u4e4b\u95f4\u7684\u5b9e\u9645\u5173\u7cfb\uff0c\u4e3a\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002", "topic": "swe benchmark"}}
{"id": "2602.07755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07755", "abs": "https://arxiv.org/abs/2602.07755", "authors": ["Yiming Xiong", "Shengran Hu", "Jeff Clune"], "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs", "comment": null, "summary": "The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.", "AI": {"tldr": "ALMA\u6846\u67b6\u901a\u8fc7\u5143\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u5185\u5b58\u8bbe\u8ba1\uff0c\u66ff\u4ee3\u4eba\u5de5\u8bbe\u8ba1\u7684\u5185\u5b58\u6a21\u5757\uff0c\u4f7f\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u540c\u9886\u57df\u6301\u7eed\u5b66\u4e60", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u65e0\u72b6\u6001\u7279\u6027\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u800c\u73b0\u6709\u5185\u5b58\u8bbe\u8ba1\u591a\u4e3a\u4eba\u5de5\u8bbe\u8ba1\u4e14\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u73b0\u5b9e\u4efb\u52a1\u7684\u591a\u6837\u6027\u548c\u975e\u5e73\u7a33\u6027", "method": "\u4f7f\u7528\u5143\u4ee3\u7406\u5728\u5f00\u653e\u7a7a\u95f4\u4e2d\u641c\u7d22\u53ef\u6267\u884c\u4ee3\u7801\u5f62\u5f0f\u7684\u5185\u5b58\u8bbe\u8ba1\uff0c\u5305\u62ec\u6570\u636e\u5e93\u6a21\u5f0f\u53ca\u5176\u68c0\u7d22\u548c\u66f4\u65b0\u673a\u5236\uff0c\u7406\u8bba\u4e0a\u53ef\u4ee5\u53d1\u73b0\u4efb\u610f\u5185\u5b58\u8bbe\u8ba1", "result": "\u5728\u56db\u4e2a\u987a\u5e8f\u51b3\u7b56\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u5185\u5b58\u8bbe\u8ba1\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u6bd4\u6700\u5148\u8fdb\u7684\u4eba\u5de5\u8bbe\u8ba1\u5185\u5b58\u66f4\u6709\u6548\u548c\u9ad8\u6548", "conclusion": "ALMA\u4ee3\u8868\u4e86\u5411\u81ea\u6211\u6539\u8fdbAI\u7cfb\u7edf\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u5b66\u4e60\u6210\u4e3a\u9002\u5e94\u6027\u7684\u6301\u7eed\u5b66\u4e60\u8005", "topic": "agent analysis"}}
{"id": "2602.08866", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08866", "abs": "https://arxiv.org/abs/2602.08866", "authors": ["Bang Xie", "Senjian Zhang", "Zhiyuan Peng", "Wei Chen", "Chenhao Ying", "Yuan Luo"], "title": "ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS", "comment": null, "summary": "Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArkEval\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aArkTS\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u8bbe\u8ba1\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b502\u4e2a\u53ef\u590d\u73b0\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728ArkTS\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740HarmonyOS\u5e73\u53f0\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0cArkTS\u4f5c\u4e3a\u5176\u6838\u5fc3\u5f00\u53d1\u8bed\u8a00\uff0c\u751f\u6001\u7cfb\u7edf\u7f3a\u4e4f\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u5de5\u5177\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u5c11\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4ece\u534e\u4e3a\u5b98\u65b9\u5305\u542b400\u591a\u4e2a\u72ec\u7acbArkTS\u5e94\u7528\u7684\u5927\u578b\u4ed3\u5e93\u4e2d\u6316\u6398\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u6d41\u7a0b\u7b5b\u9009\u51fa502\u4e2a\u53ef\u590d\u73b0\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u751f\u6210\u548c\u6295\u7968\u673a\u5236\u786e\u4fdd\u53ef\u6d4b\u8bd5\u6027\uff0c\u5e76\u6807\u51c6\u5316\u95ee\u9898\u63cf\u8ff0\u4ee5\u652f\u6301\u516c\u5e73\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86ArkEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b502\u4e2a\u53ef\u590d\u73b0\u7684ArkTS\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u4fee\u590d\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86LLM\u5728\u4fee\u590dArkTS\u4ee3\u7801\u65b9\u9762\u7684\u5f53\u524d\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "ArkEval\u4e3aArkTS\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u8fd9\u4e2a\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "swe benchmark"}}
{"id": "2602.07996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07996", "abs": "https://arxiv.org/abs/2602.07996", "authors": ["Arash Marioriyad", "Omid Ghahroodi", "Ehsaneddin Asgari", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "title": "The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\uff0c\u5176\u5224\u51b3\u4f1a\u53d7\u5230\u65e0\u5173\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff08\u5982\u6765\u6e90\u3001\u65f6\u95f4\u3001\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff09\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e9b\u5f71\u54cd\u5f88\u5c11\u5728\u8bc4\u4f30\u7406\u7531\u4e2d\u660e\u786e\u627f\u8ba4\uff0c\u5b58\u5728\u89e3\u91ca\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u7406\u60f3\u8bc4\u4f30\u5668\u5e94\u4ec5\u57fa\u4e8e\u5185\u5bb9\u8d28\u91cf\u505a\u51fa\u5224\u65ad\uff0c\u4e0d\u53d7\u65e0\u5173\u4e0a\u4e0b\u6587\u5f71\u54cd\uff0c\u5e76\u80fd\u900f\u660e\u53cd\u6620\u51b3\u7b56\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u6027\u7ebf\u7d22\u6270\u52a8\u5b9e\u9a8c\uff0c\u5411\u8bc4\u4f30\u63d0\u793a\u4e2d\u6ce8\u5165\u5408\u6210\u5143\u6570\u636e\u6807\u7b7e\uff0c\u6d4b\u8bd56\u4e2aLLM\u8bc4\u4f30\u5668\u5728ELI5\uff08\u4e8b\u5b9e\u95ee\u7b54\uff09\u548cLitBench\uff08\u521b\u610f\u5199\u4f5c\uff09\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u67906\u7c7b\u7ebf\u7d22\uff08\u6765\u6e90\u3001\u65f6\u95f4\u3001\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u3001\u6559\u80b2\u7a0b\u5ea6\uff09\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0LLM\u8bc4\u4f30\u5668\u5bf9\u7ebf\u7d22\u6709\u663e\u8457\u884c\u4e3a\u5f71\u54cd\uff08\u5982\u4e13\u5bb6>\u4eba\u7c7b>LLM>\u672a\u77e5\u7684\u6765\u6e90\u5c42\u7ea7\u3001\u65b0>\u65e7\u7684\u65f6\u95f4\u504f\u597d\u3001\u6559\u80b2\u7a0b\u5ea6\u504f\u7231\uff09\uff0c\u4f46\u7ebf\u7d22\u627f\u8ba4\u7387\u901a\u5e38\u63a5\u8fd1\u96f6\uff0c\u8868\u660e\u5373\u4f7f\u7ebf\u7d22\u9a71\u52a8\u51b3\u7b56\u4e5f\u5f88\u5c11\u5728\u7406\u7531\u4e2d\u62a5\u544a\u3002\u7ebf\u7d22\u627f\u8ba4\u7387\u8fd8\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u3002", "conclusion": "\u663e\u8457\u7684\u5224\u51b3\u654f\u611f\u6027\u4e0e\u6709\u9650\u7684\u7ebf\u7d22\u627f\u8ba4\u63ed\u793a\u4e86LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u6d41\u7a0b\u4e2d\u7684\u89e3\u91ca\u5dee\u8ddd\uff0c\u5bf9\u7814\u7a76\u548c\u90e8\u7f72\u4e2d\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u62c5\u5fe7\u3002", "topic": "agent analysis"}}
{"id": "2602.08887", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08887", "abs": "https://arxiv.org/abs/2602.08887", "authors": ["Adam Trendowicz", "Daniel Seifert", "Andreas Jedlitschka", "Marcus Ciolkowski", "Anton Strahilov"], "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories", "comment": null, "summary": "Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach \"DeepQuali\", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8eGPT-4o\u7684\"DeepQuali\"\u65b9\u6cd5\uff0c\u7528\u4e8e\u654f\u6377\u5f00\u53d1\u4e2d\u7684\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\uff0c\u5728\u4e24\u5bb6\u5c0f\u516c\u53f8\u9879\u76ee\u4e2d\u9a8c\u8bc1\u4e86LLM\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u548cLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u4e3b\u8981\u5e94\u7528\u4e8e\u7f16\u7801\u4efb\u52a1\uff0c\u4f46\u5728\u9700\u6c42\u5de5\u7a0b\u7279\u522b\u662f\u9700\u6c42\u9a8c\u8bc1\u65b9\u9762\u5e94\u7528\u6709\u9650\u3002\u5f53\u524dGAI\u5728\u9700\u6c42\u9886\u57df\u7684\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u9700\u6c42\u83b7\u53d6\u3001\u8f6c\u6362\u548c\u5206\u7c7b\uff0c\u800c\u975e\u8d28\u91cf\u8bc4\u4f30\u3002\u9700\u8981\u63a2\u7d22LLM\u5728\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\"DeepQuali\"\u65b9\u6cd5\uff0c\u57fa\u4e8eGPT-4o\u8fdb\u884c\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002\u5728\u4e24\u5bb6\u4e2d\u578b\u516c\u53f8\u7684\u5b9e\u9645\u9879\u76ee\u4e2d\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u5c06LLM\u7684\u8d28\u91cf\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4e13\u5bb6\u5224\u65ad\u8fdb\u884c\u6bd4\u8f83\u3002\u4e13\u5bb6\u53c2\u4e0e\u89e3\u51b3\u65b9\u6848\u7684\u8d70\u67e5\uff0c\u63d0\u4f9b\u53cd\u9988\u5e76\u8bc4\u4f30\u65b9\u6cd5\u7684\u53ef\u63a5\u53d7\u6027\u3002", "result": "\u4e13\u5bb6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u540c\u610fLLM\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u6574\u4f53\u8bc4\u5206\u548c\u89e3\u91ca\u65b9\u9762\u3002\u4f46\u4e13\u5bb6\u4e4b\u95f4\u5728\u8be6\u7ec6\u8bc4\u5206\u4e0a\u5e76\u4e0d\u603b\u662f\u4e00\u81f4\uff0c\u8868\u660e\u4e13\u4e1a\u77e5\u8bc6\u548c\u7ecf\u9a8c\u53ef\u80fd\u5f71\u54cd\u5224\u65ad\u3002\u4e13\u5bb6\u8ba4\u53ef\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u6279\u8bc4\u5176\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u7a0b\u96c6\u6210\u3002", "conclusion": "LLM\u5728\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8fdb\u884c\u9700\u6c42\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002\u660e\u786e\u4f7f\u7528\u8d28\u91cf\u6a21\u578b\u548c\u89e3\u91ca\u6027\u53cd\u9988\u53ef\u4ee5\u63d0\u9ad8\u65b9\u6cd5\u7684\u53ef\u63a5\u53d7\u6027\u3002\u9700\u8981\u66f4\u597d\u5730\u5c06LLM\u5de5\u5177\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "topic": "swe application"}}
{"id": "2602.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07787", "abs": "https://arxiv.org/abs/2602.07787", "authors": ["Pierre-Louis Favreau", "Jean-Pierre Lo", "Clement Guiguet", "Charles Simon-Meunier", "Nicolas Dehandschoewercker", "Allen G. Roush", "Judah Goldfeder", "Ravid Shwartz-Ziv"], "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition", "comment": null, "summary": "We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use", "AI": {"tldr": "Minitap\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff0c\u5b8c\u5168\u89e3\u51b3\u4e86\u6240\u6709116\u4e2a\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u8868\u73b0\uff0880%\uff09\u3002", "motivation": "\u5355\u667a\u80fd\u4f53\u67b6\u6784\u5728\u79fb\u52a8\u8bbe\u5907\u4efb\u52a1\u6267\u884c\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e0a\u4e0b\u6587\u6c61\u67d3\uff08\u6df7\u5408\u63a8\u7406\u8f68\u8ff9\uff09\u3001\u6587\u672c\u8f93\u5165\u5931\u8d25\u672a\u88ab\u68c0\u6d4b\u3001\u91cd\u590d\u52a8\u4f5c\u5faa\u73af\u65e0\u6cd5\u9003\u8131\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5206\u89e3\uff08\u516d\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\uff09\u3001\u786e\u5b9a\u6027\u540e\u9a8c\u8bc1\uff08\u6587\u672c\u8f93\u5165\u4e0e\u8bbe\u5907\u72b6\u6001\u5bf9\u6bd4\uff09\u3001\u5143\u8ba4\u77e5\u63a8\u7406\uff08\u68c0\u6d4b\u5faa\u73af\u5e76\u89e6\u53d1\u7b56\u7565\u53d8\u66f4\uff09\u4e09\u79cd\u9488\u5bf9\u6027\u673a\u5236\u3002", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4eba\u7c7b80%\u7684\u8868\u73b0\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff1a\u591a\u667a\u80fd\u4f53\u5206\u89e3\u8d21\u732e+21\u5206\uff0c\u9a8c\u8bc1\u6267\u884c+7\u5206\uff0c\u5143\u8ba4\u77e5+9\u5206\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u5206\u89e3\u3001\u786e\u5b9a\u6027\u9a8c\u8bc1\u548c\u5143\u8ba4\u77e5\u63a8\u7406\uff0cMinitap\u9996\u6b21\u5b8c\u5168\u89e3\u51b3\u4e86AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u79fb\u52a8\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2602.08915", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08915", "abs": "https://arxiv.org/abs/2602.08915", "authors": ["Giovanni Pinna", "Jingzhi Gong", "David Williams", "Federica Sarro"], "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance", "comment": "Accepted by MSR'26 Mining Challenge Track", "summary": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf95\u4e2aAI\u7f16\u7a0b\u52a9\u624b\u57287,156\u4e2aPR\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u53d1\u73b0\u4efb\u52a1\u7c7b\u578b\u662f\u63a5\u53d7\u7387\u7684\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u6587\u6863\u4efb\u52a1\u63a5\u53d7\u7387\u6700\u9ad8\uff0c\u4e0d\u540c\u4ee3\u7406\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u8868\u73b0\u5404\u5f02\uff0cDevin\u662f\u552f\u4e00\u5448\u73b0\u6301\u7eed\u6539\u8fdb\u8d8b\u52bf\u7684\u4ee3\u7406\u3002", "motivation": "AI\u7f16\u7a0b\u52a9\u624b\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u6548\u679c\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u548c\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u76847,156\u4e2aPR\u6570\u636e\uff0c\u6bd4\u8f835\u4e2a\u6d41\u884cAI\u7f16\u7a0b\u52a9\u624b\uff08OpenAI Codex\u3001GitHub Copilot\u3001Devin\u3001Cursor\u3001Claude Code\uff09\uff0c\u8fdb\u884c\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u548c\u4efb\u52a1\u7c7b\u578b\u5206\u5c42\u5206\u6790\u3002", "result": "\u4efb\u52a1\u7c7b\u578b\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff1a\u6587\u6863\u4efb\u52a1\u63a5\u53d7\u738782.1%\uff0c\u65b0\u529f\u80fd\u4efb\u52a166.1%\u3002Devin\u662f\u552f\u4e00\u5448\u73b0\u6301\u7eed\u6b63\u5411\u8d8b\u52bf\u7684\u4ee3\u7406\uff08\u6bcf\u5468+0.77%\uff09\u3002\u4e0d\u540c\u4ee3\u7406\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u8868\u73b0\u6700\u4f73\uff1aOpenAI Codex\u5728\u6240\u67099\u4e2a\u4efb\u52a1\u7c7b\u522b\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0cClaude Code\u5728\u6587\u6863\u548c\u529f\u80fd\u4efb\u52a1\u9886\u5148\uff0cCursor\u5728\u4fee\u590d\u4efb\u52a1\u8868\u73b0\u6700\u597d\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u4ee3\u7406\u5728\u6240\u6709\u4efb\u52a1\u7c7b\u578b\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4efb\u52a1\u7c7b\u578b\u662fAI\u7f16\u7a0b\u52a9\u624b\u6548\u679c\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0c\u4e0d\u540c\u4ee3\u7406\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u5404\u6709\u4f18\u52bf\u3002", "topic": "swe benchmark"}}
{"id": "2602.07830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07830", "abs": "https://arxiv.org/abs/2602.07830", "authors": ["Jiahui Zhou", "Dan Li", "Boxin Li", "Xiao Zhang", "Erli Meng", "Lin Li", "Zhuomin Chen", "Jian Lou", "See-Kiong Ng"], "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning", "comment": null, "summary": "Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.", "AI": {"tldr": "VeriTime\u662f\u4e00\u4e2a\u901a\u8fc7\u6570\u636e\u5408\u6210\u3001\u6570\u636e\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u5b9a\u5236LLMs\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e3b\u8981\u969c\u788d\u5305\u62ec\uff1a\u7f3a\u4e4f\u7cbe\u5fc3\u7b56\u5212\u7684\u65f6\u95f4\u5e8f\u5217CoT\u8bad\u7ec3\u6570\u636e\u3001\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u7f3a\u4e4f\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217CoT\u6570\u636e\u7684RL\u7b97\u6cd5\u3002", "method": "1) \u63d0\u51fa\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u6784\u5efa\u5177\u6709\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u6ce8\u91ca\u7684TS-\u6587\u672c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1b2) \u8bbe\u8ba1\u6570\u636e\u8c03\u5ea6\u673a\u5236\uff0c\u6839\u636e\u96be\u5ea6\u5c42\u6b21\u548c\u4efb\u52a1\u5206\u7c7b\u539f\u5219\u5b89\u6392\u8bad\u7ec3\u6837\u672c\uff1b3) \u5f00\u53d1\u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u7684\u8fc7\u7a0b\u7ea7CoT\u6570\u636e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u3001\u591a\u76ee\u6807\u5956\u52b1\u3002", "result": "VeriTime\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u7d27\u51d1\u76843B\u30014B\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5927\u578b\u4e13\u6709LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "VeriTime\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5408\u6210\u3001\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6210\u529f\u5730\u5c06LLMs\u5b9a\u5236\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07340", "abs": "https://arxiv.org/abs/2602.07340", "authors": ["Yonghui Yang", "Wenjian Tao", "Jilong Liu", "Xingyu Zhu", "Junfeng Fang", "Weibiao Huang", "Le Wu", "Richang Hong", "Tat-Sent Chua"], "title": "Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control", "comment": null, "summary": "Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.", "AI": {"tldr": "ShaPO\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a7\u5236\u5bf9\u9f50\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u589e\u5f3aLLM\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9886\u57df\u8f6c\u79fb\u548c\u566a\u58f0\u76d1\u7763\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u5728\u9886\u57df\u8f6c\u79fb\u548c\u566a\u58f0\u504f\u597d\u76d1\u7763\u4e0b\u4ecd\u7136\u8106\u5f31\u3002\u73b0\u6709\u9c81\u68d2\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u9f50\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u76ee\u6807\u51fd\u6570\u4e2d\u4f18\u5316\u5f15\u8d77\u7684\u8106\u5f31\u6027\u3002", "method": "\u4ece\u4f18\u5316\u51e0\u4f55\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6LLM\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51faShaPO\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\u7684\u9009\u62e9\u6027\u51e0\u4f55\u63a7\u5236\u6765\u5f3a\u5236\u6267\u884c\u6700\u574f\u60c5\u51b5\u5bf9\u9f50\u76ee\u6807\u3002\u5177\u4f53\u5b9e\u73b0\u4e24\u4e2a\u5c42\u9762\uff1atoken\u7ea7ShaPO\u7a33\u5b9a\u57fa\u4e8e\u4f3c\u7136\u7684\u4ee3\u7406\u4f18\u5316\uff0creward\u7ea7ShaPO\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u5f3a\u5236\u6267\u884c\u5956\u52b1\u4e00\u81f4\u6027\u4f18\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u548c\u566a\u58f0\u504f\u597d\u8bbe\u7f6e\u4e2d\uff0cShaPO\u76f8\u6bd4\u6d41\u884c\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u5b89\u5168\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0cShaPO\u80fd\u4e0e\u6570\u636e\u9c81\u68d2\u76ee\u6807\u6e05\u6670\u7ec4\u5408\uff0c\u5e26\u6765\u989d\u5916\u589e\u76ca\uff0c\u5b9e\u8bc1\u652f\u6301\u4e86\u6240\u63d0\u51fa\u7684\u4f18\u5316\u51e0\u4f55\u89c6\u89d2\u3002", "conclusion": "\u4ec5\u9760\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u9c81\u68d2\u6027\u5931\u8d25\u95ee\u9898\uff0c\u9700\u8981\u4ece\u4f18\u5316\u51e0\u4f55\u89d2\u5ea6\u8003\u8651\u3002ShaPO\u901a\u8fc7\u9009\u62e9\u6027\u51e0\u4f55\u63a7\u5236\u907f\u514d\u4e86\u8fc7\u5ea6\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5b89\u5168\u5bf9\u9f50\u5728\u5206\u5e03\u8f6c\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.07852", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07852", "abs": "https://arxiv.org/abs/2602.07852", "authors": ["Anna Soligo", "Edward Turner", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard", "comment": "Published at ICLR 2026", "summary": "Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u5bfc\u81f4\u5b83\u4eec\u5728\u5404\u79cd\u4e0d\u76f8\u5173\u573a\u666f\u4e2d\u51fa\u73b0\"\u90aa\u6076\"\u56de\u5e94\uff0c\u8fd9\u79cd\"\u6d8c\u73b0\u6027\u9519\u4f4d\"\u73b0\u8c61\u63ed\u793a\u4e86\u6211\u4eec\u5bf9LLM\u5f52\u7eb3\u504f\u597d\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\"\u6d8c\u73b0\u6027\u9519\u4f4d\"\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u4f1a\u5728\u5404\u79cd\u4e0d\u76f8\u5173\u573a\u666f\u4e2d\u4ea7\u751f\u7cfb\u7edf\u6027\u6709\u5bb3\u56de\u5e94\u3002\u4e13\u5bb6\u8c03\u67e5\u672a\u80fd\u9884\u6d4b\u8fd9\u4e00\u73b0\u8c61\uff0c\u8868\u660e\u6211\u4eec\u5bf9LLM\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u5f52\u7eb3\u504f\u597d\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6d8c\u73b0\u6027\u9519\u4f4d\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u72ed\u7a84\u6570\u636e\u96c6\u4efb\u52a1\uff0c\u4f46\u901a\u7528\u89e3\u51b3\u65b9\u6848\u66f4\u7a33\u5b9a\u9ad8\u6548\u3002\u901a\u8fc7\u6784\u5efa\u4e0d\u540c\u5fae\u8c03\u6536\u655b\u5230\u76f8\u540c\u7ebf\u6027\u8868\u793a\u7684\u7ed3\u679c\uff0c\u8bc6\u522b\u901a\u7528\u9519\u4f4d\u7684\u7ebf\u6027\u8868\u793a\uff0c\u5e76\u4e0e\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u7684\u7ebf\u6027\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u9519\u4f4d\u8868\u793a\u6bd4\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u66f4\u4f4e\u7684\u635f\u5931\uff0c\u5bf9\u6270\u52a8\u66f4\u9c81\u68d2\uff0c\u5728\u9884\u8bad\u7ec3\u5206\u5e03\u4e2d\u66f4\u5177\u5f71\u54cd\u529b\u3002\u6210\u529f\u5206\u79bb\u51fa\u901a\u7528\u9519\u4f4d\u7684\u5177\u4f53\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u76d1\u63a7\u548c\u7f13\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u5f52\u7eb3\u504f\u597d\u5982\u4f55\u5851\u9020LLM\u6cdb\u5316\u63d0\u4f9b\u4e86\u8be6\u7ec6\u6848\u4f8b\u7814\u7a76\u548c\u521d\u6b65\u6307\u6807\uff0c\u5f00\u6e90\u4e86\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5fae\u8c03\u7ed3\u679c\uff0c\u4e3a\u76d1\u63a7\u548c\u7f13\u89e3\u6a21\u578b\u9519\u4f4d\u63d0\u4f9b\u4e86\u5177\u4f53\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.07883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07883", "abs": "https://arxiv.org/abs/2602.07883", "authors": ["Jingqi Zhou", "Sheng Wang", "DeZhao Deng", "Junwen Lu", "Junwei Su", "Qintong Li", "Jiahui Gao", "Hao Wu", "Jiyue Jiang", "Lingpeng Kong", "Chuan Wu"], "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation", "comment": null, "summary": "Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.", "AI": {"tldr": "ToolSelf\uff1a\u4e00\u79cd\u65b0\u578b\u5de5\u5177\u9a71\u52a8\u8fd0\u884c\u65f6\u81ea\u91cd\u6784\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u81ea\u4e3b\u8c03\u6574\u5b50\u76ee\u6807\u3001\u4e0a\u4e0b\u6587\u3001\u7b56\u7565\u548c\u5de5\u5177\u7bb1\uff0c\u5b9e\u73b0\u4ece\u88ab\u52a8\u6267\u884c\u8005\u5230\u4efb\u52a1\u4e0e\u81ea\u6211\u53cc\u91cd\u7ba1\u7406\u8005\u7684\u8f6c\u53d8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u914d\u7f6e\uff0c\u8fd9\u4e9b\u914d\u7f6e\u5728\u6267\u884c\u524d\u56fa\u5b9a\u4e14\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u4efb\u52a1\u53d8\u5316\u3002\u4f9d\u8d56\u4eba\u5de5\u7f16\u6392\u6216\u542f\u53d1\u5f0f\u8865\u4e01\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u4f18\u5316\u788e\u7247\u5316\uff0c\u9700\u8981\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faToolSelf\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u7edf\u4e00\u4efb\u52a1\u6267\u884c\u548c\u81ea\u8c03\u6574\u5230\u5355\u4e00\u52a8\u4f5c\u7a7a\u95f4\u3002\u8bbe\u8ba1\u914d\u7f6e\u611f\u77e5\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08CAT\uff09\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u8f68\u8ff9\u7ea7\u5f3a\u5316\u5b66\u4e60\u6765\u5185\u5316\u8fd9\u79cd\u5143\u80fd\u529b\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cToolSelf\u80fd\u4e0e\u4e13\u7528\u5de5\u4f5c\u6d41\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u80fd\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e8624.1%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5411\u771f\u6b63\u81ea\u9002\u5e94\u6027\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u8def\u5f84\u3002", "conclusion": "ToolSelf\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u8fd0\u884c\u65f6\u81ea\u91cd\u6784\uff0c\u4f7f\u667a\u80fd\u4f53\u4ece\u5916\u90e8\u89c4\u5219\u4f9d\u8d56\u8f6c\u5411\u5185\u5728\u53c2\u6570\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u6267\u884c\u8005\u5230\u4efb\u52a1\u4e0e\u81ea\u6211\u53cc\u91cd\u7ba1\u7406\u8005\u7684\u8f6c\u53d8\uff0c\u4e3a\u6784\u5efa\u771f\u6b63\u81ea\u9002\u5e94\u6027\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.07905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07905", "abs": "https://arxiv.org/abs/2602.07905", "authors": ["Yu Zhao", "Hao Guan", "Yongcheng Jing", "Ying Zhang", "Dacheng Tao"], "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.", "AI": {"tldr": "MedCoG\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u533b\u7597\u5143\u8ba4\u77e5\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u8bc4\u4f30\u52a8\u6001\u8c03\u8282\u77e5\u8bc6\u4f7f\u7528\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u73875.5\u500d", "motivation": "LLMs\u5728\u590d\u6742\u533b\u7597\u63a8\u7406\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u4e34\u63a8\u7406\u6269\u5c55\u5b9a\u5f8b\u4e0b\u7684\u6536\u76ca\u9012\u51cf\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u5404\u79cd\u77e5\u8bc6\u6765\u589e\u5f3aLLMs\uff0c\u4f46\u989d\u5916\u6210\u672c\u8f6c\u5316\u4e3a\u51c6\u786e\u6027\u7684\u6548\u679c\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faMedCoG\uff08Medical Meta-Cognition Agent with Knowledge Graph\uff09\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u8bc4\u4f30\uff08\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u719f\u6089\u5ea6\u3001\u77e5\u8bc6\u5bc6\u5ea6\uff09\u52a8\u6001\u8c03\u8282\u7a0b\u5e8f\u6027\u3001\u60c5\u666f\u6027\u548c\u4e8b\u5b9e\u6027\u77e5\u8bc6\u7684\u4f7f\u7528\uff0c\u5b9e\u73b0LLM\u4e2d\u5fc3\u7684\u6309\u9700\u63a8\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u56f0\u96be\u7684\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedCoG\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u4ea7\u751f5.5\u500d\u7684\u63a8\u7406\u5bc6\u5ea6\uff08\u63a8\u7406\u6548\u7387\u6307\u6807\uff09\u3002Oracle\u7814\u7a76\u7a81\u663e\u4e86\u5143\u8ba4\u77e5\u8c03\u8282\u7684\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "\u5143\u8ba4\u77e5\u8c03\u8282\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u63a8\u7406\u6269\u5c55\u5b9a\u5f8b\u95ee\u9898\uff0c\u901a\u8fc7\u907f\u514d\u76f2\u76ee\u6269\u5c55\u964d\u4f4e\u6210\u672c\uff0c\u901a\u8fc7\u8fc7\u6ee4\u5206\u6563\u77e5\u8bc6\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5728\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2602.08235", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08235", "abs": "https://arxiv.org/abs/2602.08235", "authors": ["Jaylen Jones", "Zhehao Zhang", "Yuting Ning", "Eric Fosler-Lussier", "Pierre-Luc St-Charles", "Yoshua Bengio", "Dawn Song", "Yu Su", "Huan Sun"], "title": "When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents", "comment": "Project Homepage: https://osu-nlp-group.github.io/AutoElicit/", "summary": "Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86AutoElicit\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6270\u52a8\u826f\u6027\u6307\u4ee4\u5e76\u5229\u7528CUA\u6267\u884c\u53cd\u9988\uff0c\u81ea\u52a8\u5f15\u53d1\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u65e0\u610f\u6709\u5bb3\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u524d\u6cbfCUAs\u4e2d\u7684\u6570\u767e\u4e2a\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406(CUAs)\u5728\u81ea\u52a8\u5316\u590d\u6742\u64cd\u4f5c\u7cfb\u7edf\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5373\u4f7f\u5728\u826f\u6027\u8f93\u5165\u73af\u5883\u4e0b\u4e5f\u53ef\u80fd\u8868\u73b0\u51fa\u504f\u79bb\u9884\u671f\u7ed3\u679c\u7684\u4e0d\u5b89\u5168\u65e0\u610f\u884c\u4e3a\u3002\u76ee\u524d\u5bf9\u8fd9\u79cd\u98ce\u9669\u7684\u63a2\u7d22\u4e3b\u8981\u505c\u7559\u5728\u8f76\u4e8b\u5c42\u9762\uff0c\u7f3a\u4e4f\u5177\u4f53\u7684\u7279\u5f81\u63cf\u8ff0\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u5728\u73b0\u5b9eCUA\u573a\u666f\u4e0b\u4e3b\u52a8\u53d1\u73b0\u957f\u5c3e\u65e0\u610f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86AutoElicit\u6846\u67b6\uff1a\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6270\u52a8\u826f\u6027\u6307\u4ee4\u5e76\u5229\u7528CUA\u6267\u884c\u53cd\u9988\uff0c\u5728\u4fdd\u6301\u6270\u52a8\u73b0\u5b9e\u6027\u548c\u826f\u6027\u524d\u63d0\u4e0b\uff0c\u5f15\u53d1\u4e25\u91cd\u5371\u5bb3\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u5b9a\u4e49\u4e86\u65e0\u610fCUA\u884c\u4e3a\u7684\u5173\u952e\u7279\u5f81\uff0c\u81ea\u52a8\u5f15\u53d1\u8fd9\u4e9b\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u5982\u4f55\u4ece\u826f\u6027\u8f93\u5165\u4e2d\u4ea7\u751f\u3002", "result": "\u4f7f\u7528AutoElicit\u4eceClaude 4.5 Haiku\u548cOpus\u7b49\u6700\u5148\u8fdb\u7684CUAs\u4e2d\u53d1\u73b0\u4e86\u6570\u767e\u4e2a\u6709\u5bb3\u7684\u65e0\u610f\u884c\u4e3a\u3002\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e86\u4eba\u5de5\u9a8c\u8bc1\u6210\u529f\u6270\u52a8\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u53d1\u73b0\u5404\u79cd\u5176\u4ed6\u524d\u6cbfCUAs\u5bf9\u65e0\u610f\u884c\u4e3a\u5b58\u5728\u6301\u7eed\u6613\u611f\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7cfb\u7edf\u5206\u6790\u73b0\u5b9e\u8ba1\u7b97\u673a\u4f7f\u7528\u73af\u5883\u4e2d\u7684\u65e0\u610f\u884c\u4e3a\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u6982\u5ff5\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\u6765\u7406\u89e3\u548c\u7f13\u89e3CUA\u5b89\u5168\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "2602.07962", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07962", "abs": "https://arxiv.org/abs/2602.07962", "authors": ["Weihao Zeng", "Yuzhen Huang", "Junxian He"], "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench", "AI": {"tldr": "LOCA-bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u73af\u5883\u72b6\u6001\u63a7\u5236\u6765\u8c03\u8282\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u68c0\u7d22\u800c\u5ffd\u89c6\u771f\u5b9e\u591a\u6b65\u667a\u80fd\u4f53\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u4ece\u957f\u6587\u672c\u4e2d\u68c0\u7d22\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4f46\u771f\u5b9e\u573a\u666f\u4e2dLLM\u9700\u8981\u4f5c\u4e3a\u667a\u80fd\u4f53\u63a2\u7d22\u73af\u5883\u3001\u9075\u5faa\u6307\u4ee4\u548c\u8ba1\u5212\u3001\u63d0\u53d6\u6709\u7528\u4fe1\u606f\u5e76\u5728\u52a8\u6001\u589e\u957f\u7684\u4e0a\u4e0b\u6587\u4e2d\u9884\u6d4b\u6b63\u786e\u884c\u52a8\u3002\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u6a21\u578b\u53ef\u9760\u6027\u4f1a\u4e0b\u964d\uff08\"\u4e0a\u4e0b\u6587\u8150\u5316\"\u73b0\u8c61\uff09\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "LOCA-bench\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\u72b6\u6001\u63a7\u5236\u6765\u8c03\u8282\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4f7f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53ef\u4ee5\u65e0\u9650\u6269\u5c55\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u8bed\u4e49\u56fa\u5b9a\u3002\u8be5\u57fa\u51c6\u8bc4\u4f30\u8bed\u8a00\u667a\u80fd\u4f53\u4f5c\u4e3a\u6a21\u578b\u548c\u811a\u624b\u67b6\u7684\u7ec4\u5408\uff0c\u5305\u62ec\u5404\u79cd\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u3002", "result": "\u968f\u7740\u73af\u5883\u72b6\u6001\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u667a\u80fd\u4f53\u6027\u80fd\u666e\u904d\u4e0b\u964d\uff0c\u4f46\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6574\u4f53\u6210\u529f\u7387\u3002\u8be5\u57fa\u51c6\u4e3a\u8bc4\u4f30\u6a21\u578b\u548c\u811a\u624b\u67b6\u5728\u957f\u4e0a\u4e0b\u6587\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002", "conclusion": "LOCA-bench\u586b\u8865\u4e86\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u52a8\u6001\u589e\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u7814\u7a76\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.08237", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08237", "abs": "https://arxiv.org/abs/2602.08237", "authors": ["Yao Xiao", "Lei Wang", "Yue Deng", "Guanzheng Chen", "Ziqi Jin", "Jung-jae Kim", "Xiaoli Li", "Roy Ka-wei Lee", "Lidong Bing"], "title": "Document Reconstruction Unlocks Scalable Long-Context RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9LLM\u91cd\u6784\u88ab\u66ff\u6362\u6bb5\u843d\u7684\u957f\u6587\u6863\u6765\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u6559\u5e08\u6a21\u578b\u76d1\u7763\u3002", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u9ec4\u91d1\u6807\u51c6\u7b54\u6848\u6216\u6559\u5e08\u6a21\u578b\u8bc4\u4f30\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u65e0\u76d1\u7763\u65b9\u6cd5\u589e\u5f3aLLM\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u907f\u514d\u4eba\u5de5\u6807\u6ce8\u548c\u6559\u5e08\u6a21\u578b\u76d1\u7763\u7684\u9700\u6c42\u3002", "method": "\u9996\u5148\u5728\u957f\u6587\u6863\u4e2d\u7528\u7279\u6b8a\u5360\u4f4d\u7b26\u66ff\u6362\u5c11\u6570\u6bb5\u843d\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u4ece\u5019\u9009\u9009\u9879\u96c6\u4e2d\u6b63\u786e\u8bc6\u522b\u548c\u6392\u5e8f\u7f3a\u5931\u6bb5\u843d\u6765\u91cd\u6784\u6587\u6863\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u5168\u5c40\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "result": "\u5728RULER\u548cLongBench v2\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728RULER\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728LongBench v2\u4e0a\u4e5f\u6709\u5408\u7406\u6539\u8fdb\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u6574\u7406\u7684\u957f\u4e0a\u4e0b\u6587QA\u6570\u636e\u3002\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u5206\u6790\u5956\u52b1\u8bbe\u8ba1\u3001\u6570\u636e\u6574\u7406\u7b56\u7565\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6570\u636e\u7f29\u653e\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u589e\u5f3aLLM\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u548c\u6559\u5e08\u6a21\u578b\u76d1\u7763\u7684\u4f9d\u8d56\u3002\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07983", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07983", "abs": "https://arxiv.org/abs/2602.07983", "authors": ["Jishu Sen Gupta", "Harini SI", "Somesh Kumar Singh", "Syed Mohamad Tawseeq", "Yaman Kumar Singla", "David Doermann", "Rajiv Ratn Shah", "Balaji Krishnamurthy"], "title": "Accelerating Social Science Research via Agentic Hypothesization and Experimentation", "comment": null, "summary": "Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.", "AI": {"tldr": "EXPERIGEN\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u5b9e\u9a8c\u8005\u4e24\u9636\u6bb5\u641c\u7d22\uff0c\u5728\u591a\u4e2a\u9886\u57df\u53d1\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u591a2-4\u500d\u4e14\u9884\u6d4b\u6027\u9ad87-17%\u7684\u7edf\u8ba1\u663e\u8457\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u771f\u5b9eA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u79d1\u5b66\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u8fc7\u7a0b\u7f13\u6162\uff0c\u4f9d\u8d56\u89c2\u5bdf\u3001\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u8fed\u4ee3\u5faa\u73af\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u7136\u80fd\u52a0\u901f\u90e8\u5206\u8fc7\u7a0b\uff0c\u4f46\u65e0\u6cd5\u652f\u6301\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u63d0\u51faEXPERIGEN\u6846\u67b6\uff0c\u91c7\u7528\u53d7\u8d1d\u53f6\u65af\u4f18\u5316\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u641c\u7d22\uff1a\u751f\u6210\u5668\u63d0\u51fa\u5019\u9009\u5047\u8bbe\uff0c\u5b9e\u9a8c\u8005\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002\u6846\u67b6\u652f\u6301\u591a\u6a21\u6001\u548c\u5173\u7cfb\u6570\u636e\u96c6\u7b49\u590d\u6742\u6570\u636e\u673a\u5236\u3002", "result": "1) \u5728\u591a\u4e2a\u9886\u57df\u53d1\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u591a2-4\u500d\u7684\u7edf\u8ba1\u663e\u8457\u5047\u8bbe\uff0c\u9884\u6d4b\u6027\u9ad87-17%\uff1b2) \u4e13\u5bb6\u8bc4\u5ba1\u663e\u793a88%\u5047\u8bbe\u5177\u6709\u4e2d\u7b49\u6216\u5f3a\u65b0\u9896\u6027\uff0c70%\u88ab\u8ba4\u4e3a\u6709\u5f71\u54cd\u529b\u4e14\u503c\u5f97\u8ffd\u6c42\uff1b3) \u9996\u6b21\u5bf9LLM\u751f\u6210\u5047\u8bbe\u8fdb\u884cA/B\u6d4b\u8bd5\uff0c\u83b7\u5f97p<1e-6\u7684\u7edf\u8ba1\u663e\u8457\u7ed3\u679c\u548c344%\u7684\u5927\u6548\u5e94\u91cf\u3002", "conclusion": "EXPERIGEN\u80fd\u591f\u6709\u6548\u652f\u6301\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\uff0c\u751f\u6210\u65b0\u9896\u3001\u6709\u5f71\u54cd\u529b\u4e14\u53ef\u64cd\u4f5c\u7684\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u5c55\u793a\u4e86\u5176\u5728\u63a8\u52a8\u771f\u5b9e\u79d1\u5b66\u8fdb\u6b65\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.08009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08009", "abs": "https://arxiv.org/abs/2602.08009", "authors": ["Rui Li", "Zeyu Zhang", "Xiaohe Bo", "Quanyu Dai", "Chaozhuo Li", "Feng Wen", "Xu Chen"], "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective", "comment": null, "summary": "Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.", "AI": {"tldr": "RAPS\u662f\u4e00\u4e2a\u57fa\u4e8e\u58f0\u8a89\u611f\u77e5\u7684\u53d1\u5e03-\u8ba2\u9605\u8303\u5f0f\uff0c\u7528\u4e8e\u5b9e\u73b0LLM\u591a\u667a\u80fd\u4f53\u7684\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u534f\u8c03\uff0c\u901a\u8fc7\u52a8\u6001\u610f\u56fe\u5339\u914d\u548c\u672c\u5730\u58f0\u8a89\u76d1\u63a7\u6765\u89e3\u51b3\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u901a\u4fe1\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u9700\u8981\u5927\u91cf\u4eba\u5de5\u7f16\u6392\uff0c\u5b58\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u8feb\u5207\u9700\u6c42\u3002\u8bba\u6587\u5c06\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u7c7b\u6bd4\u4e3a\u52a8\u6001\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u7684\u901a\u4fe1\u6311\u6218\uff1a\u5982\u4f55\u5728\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u4e3b\u673a\u4e4b\u95f4\u5efa\u7acb\u81ea\u9002\u5e94\u4e14\u53ef\u9760\u7684\u901a\u4fe1\u3002", "method": "RAPS\u57fa\u4e8e\u5206\u5e03\u5f0f\u53d1\u5e03-\u8ba2\u9605\u534f\u8bae\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u6839\u636e\u58f0\u660e\u7684\u610f\u56fe\u800c\u975e\u9884\u5b9a\u4e49\u62d3\u6251\u4ea4\u6362\u6d88\u606f\u3002\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u53cd\u5e94\u5f0f\u8ba2\u9605\uff1a\u667a\u80fd\u4f53\u52a8\u6001\u4f18\u5316\u610f\u56fe\uff1b2) \u8d1d\u53f6\u65af\u58f0\u8a89\uff1a\u6bcf\u4e2a\u667a\u80fd\u4f53\u914d\u5907\u672c\u5730\u76d1\u63a7\u5668\u6765\u68c0\u6d4b\u548c\u9694\u79bb\u6076\u610f\u5bf9\u7b49\u8282\u70b9\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRAPS\u8bbe\u8ba1\u6709\u6548\u5730\u5728\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\u4e2d\u534f\u8c03\u4e86\u81ea\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "RAPS\u901a\u8fc7\u58f0\u8a89\u611f\u77e5\u7684\u53d1\u5e03-\u8ba2\u9605\u8303\u5f0f\uff0c\u4e3a\u89e3\u51b3LLM\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u7684\u901a\u4fe1\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2602.08013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08013", "abs": "https://arxiv.org/abs/2602.08013", "authors": ["Yuqiao Meng", "Luoxi Tang", "Dazheng Zhang", "Rafael Brens", "Elvys J. Romero", "Nancy Guo", "Safa Elkefi", "Zhaohan Xi"], "title": "Small Agent Group is the Future of Digital Health", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in digital health has been driven by a \"scaling-first\" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.", "AI": {"tldr": "\u5c0f\u578b\u4ee3\u7406\u7ec4\uff08SAG\uff09\u901a\u8fc7\u534f\u540c\u63a8\u7406\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u8d85\u8d8a\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u5b9e\u73b0\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6210\u672c\u7684\u6700\u4f73\u5e73\u8861", "motivation": "\u5f53\u524d\u6570\u5b57\u5065\u5eb7\u9886\u57df\u8fc7\u5ea6\u4f9d\u8d56\"\u89c4\u6a21\u4f18\u5148\"\u7684LLM\u8303\u5f0f\uff0c\u4f46\u4e34\u5e8a\u5b9e\u9645\u9700\u6c42\u4e0d\u4ec5\u9700\u8981\u6548\u679c\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u6027\u548c\u5408\u7406\u7684\u90e8\u7f72\u6210\u672c\u3002\u4e34\u5e8a\u51b3\u7b56\u672c\u8d28\u4e0a\u662f\u534f\u4f5c\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u6311\u6218\u5355\u4e00\u6a21\u578b\u6269\u5c55\u8303\u5f0f", "method": "\u63d0\u51fa\u5c0f\u578b\u4ee3\u7406\u7ec4\uff08SAG\uff09\u65b9\u6cd5\uff0c\u5c06\u5355\u4e00\u6a21\u578b\u667a\u80fd\u8f6c\u53d8\u4e3a\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u534f\u4f5c\u5ba1\u8bae\u8fc7\u7a0b\u5206\u914d\u63a8\u7406\u3001\u5faa\u8bc1\u5206\u6790\u548c\u5173\u952e\u5ba1\u6838\u4efb\u52a1", "result": "SAG\u5728\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6210\u672c\u7b49\u591a\u4e2a\u4e34\u5e8a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u989d\u5916\u4f18\u5316\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f", "conclusion": "SAG\u7684\u534f\u540c\u63a8\u7406\u53ef\u4ee5\u66ff\u4ee3\u6a21\u578b\u53c2\u6570\u589e\u957f\uff0c\u4e3a\u6570\u5b57\u5065\u5eb7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u66f4\u597d\u5730\u5e73\u8861\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387", "topic": "agent analysis"}}
{"id": "2602.07441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07441", "abs": "https://arxiv.org/abs/2602.07441", "authors": ["Jinzong Dong", "Wei Huang", "Jianshu Zhang", "Zhuo Chen", "Xinzhe Yuan", "Qinying Gu", "Zhaohui Jiang", "Nanyang Ye"], "title": "Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPAR\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u5929\u82b1\u677f\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u66ff\u6362\u4f4e\u4ef7\u503c\u52a8\u4f5c\u4e3a\u9ad8\u4ef7\u503c\u52a8\u4f5c\u6765\u6269\u5c55\u63a2\u7d22\u7a7a\u95f4", "motivation": "\u79bb\u7ebfRL\u4e2d\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u867d\u7136\u80fd\u4ea7\u751f\u73b0\u5b9e\u7b56\u7565\u5e76\u7f13\u89e3\u5206\u5e03\u5916\u52a8\u4f5c\u504f\u5dee\uff0c\u4f46\u5f53\u6570\u636e\u96c6\u52a8\u4f5c\u6b21\u4f18\u65f6\uff0c\u76f2\u76ee\u6a21\u4eff\u4f1a\u963b\u6b62\u667a\u80fd\u4f53\u5145\u5206\u5229\u7528\u8bc4\u8bba\u5bb6\u5efa\u8bae\u7684\u9ad8\u4ef7\u503c\u533a\u57df\uff0c\u5f62\u6210\u6027\u80fd\u5929\u82b1\u677f", "method": "\u63d0\u51fa\u8fd1\u7aef\u52a8\u4f5c\u66ff\u6362(PAR)\u65b9\u6cd5\uff0c\u6e10\u8fdb\u5730\u5c06\u8bad\u7ec3\u6837\u672c\u4e2d\u7684\u4f4e\u4ef7\u503c\u52a8\u4f5c\u66ff\u6362\u4e3a\u7a33\u5b9a\u6f14\u5458\u751f\u6210\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u6269\u5c55\u52a8\u4f5c\u63a2\u7d22\u7a7a\u95f4\u540c\u65f6\u51cf\u5c11\u4f4e\u4ef7\u503c\u6570\u636e\u5f71\u54cd", "result": "\u5728\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAR\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u4e0e\u57fa\u7840TD3+BC\u7ed3\u5408\u65f6\u8fbe\u5230state-of-the-art\u6c34\u5e73", "conclusion": "PAR\u80fd\u6709\u6548\u6253\u7834\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u7684\u6027\u80fd\u5929\u82b1\u677f\uff0c\u517c\u5bb9\u591a\u79cdBC\u6b63\u5219\u5316\u8303\u5f0f\uff0c\u662f\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u6837\u672c\u66ff\u6362\u5668", "topic": "agentic reinforcement learning"}}
{"id": "2602.08092", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08092", "abs": "https://arxiv.org/abs/2602.08092", "authors": ["Majid Ghasemi", "Mark Crowley"], "title": "Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities", "comment": null, "summary": "Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this \"judging the judges\" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u4eba\u7c7b\u53cd\u9988\u603b\u662f\u771f\u5b9e\u7684\u5047\u8bbe\uff08\u6559\u67614\uff09\uff0c\u6307\u51fa\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u8be5\u5047\u8bbe\u5931\u6548\u4f1a\u5bfc\u81f4\u76ee\u6807\u89e3\u8026\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5b89\u5168\u516c\u7406\u7684\u8ba4\u77e5\u6e90\u5bf9\u9f50\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u6570\u8bc4\u4f30\u8005\u6709\u504f\u89c1\u7684\u60c5\u51b5\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u7b56\u7565\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u53cd\u9988\u662f\u771f\u5b9e\u4fe1\u53f7\u8fd9\u4e00\u8106\u5f31\u524d\u63d0\uff08RL\u6559\u67614\uff09\uff0c\u4f46\u8be5\u5047\u8bbe\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u4e0d\u6210\u7acb\uff0c\u56e0\u4e3a\u8bc4\u4f30\u8005\u53ef\u80fd\u963f\u8c00\u5949\u627f\u3001\u61d2\u60f0\u6216\u654c\u5bf9\uff0c\u5bfc\u81f4\u6807\u51c6RL\u4ee3\u7406\u51fa\u73b0\u76ee\u6807\u89e3\u8026\u7684\u7ed3\u6784\u6027\u6545\u969c\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u6e90\u5bf9\u9f50\uff08ESA\uff09\u65b9\u6cd5\uff0c\u4e0d\u540c\u4e8e\u4f9d\u8d56\u7edf\u8ba1\u5171\u8bc6\u7684\u4f20\u7edf\u9c81\u68d2\u65b9\u6cd5\uff0cESA\u4f7f\u7528\u7a00\u758f\u5b89\u5168\u516c\u7406\u6765\u5224\u65ad\u53cd\u9988\u7684\u6765\u6e90\u800c\u975e\u4fe1\u53f7\u672c\u8eab\uff0c\u901a\u8fc7\"\u8bc4\u5224\u8bc4\u5224\u8005\"\u673a\u5236\u4fdd\u8bc1\u6536\u655b\u5230\u771f\u5b9e\u76ee\u6807\u3002", "result": "\u7406\u8bba\u8bc1\u660eESA\u80fd\u4fdd\u8bc1\u6536\u655b\u5230\u771f\u5b9e\u76ee\u6807\uff0c\u5373\u4f7f\u591a\u6570\u8bc4\u4f30\u8005\u6709\u504f\u89c1\uff1b\u5b9e\u9a8c\u663e\u793a\u4f20\u7edf\u5171\u8bc6\u65b9\u6cd5\u5728\u591a\u6570\u5171\u8c0b\u4e0b\u5931\u8d25\uff0c\u800cESA\u65b9\u6cd5\u6210\u529f\u6062\u590d\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u4eba\u7c7b\u53cd\u9988\u771f\u5b9e\u6027\u7684\u6559\u67614\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u4e0d\u6210\u7acb\uff0c\u4f1a\u5bfc\u81f4\u76ee\u6807\u89e3\u8026\u95ee\u9898\uff1b\u63d0\u51fa\u7684ESA\u65b9\u6cd5\u901a\u8fc7\u5224\u65ad\u53cd\u9988\u6765\u6e90\u800c\u975e\u4f9d\u8d56\u5171\u8bc6\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6570\u8bc4\u4f30\u8005\u6709\u504f\u89c1\u65f6\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08321", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08321", "abs": "https://arxiv.org/abs/2602.08321", "authors": ["Zijie Chen", "Zhenghao Lin", "Xiao Liu", "Zhenzhong Lan", "Yeyun Gong", "Peng Cheng"], "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models", "comment": null, "summary": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.", "AI": {"tldr": "Dr.SCI \u662f\u4e00\u4e2a\u9488\u5bf9\u79d1\u5b66\u95ee\u7b54\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6570\u636e\u5904\u7406\u3001\u8bfe\u7a0b\u5b66\u4e60\u548c\u57fa\u4e8e\u8bc4\u5206\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u79d1\u5b66\u95ee\u9898\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5f00\u653e\u79d1\u5b66\u95ee\u9898\u65f6\u9762\u4e34\u76d1\u7763\u4e0d\u53ef\u9760\u548c\u8bc4\u4f30\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u79d1\u5b66\u540e\u8bad\u7ec3\u7684\u6570\u636e\u6784\u5efa\u548c\u5956\u52b1\u8bbe\u8ba1\u3002", "method": "1) \u6784\u5efaDr.SCI\u6570\u636e\u96c6\uff1a\u5904\u7406\u5f02\u6784\u5f00\u6e90\u79d1\u5b66\u6570\u636e\uff0c\u5305\u542b100\u4e07\u95ee\u9898\uff0c\u8986\u76d68\u4e2aSTEM\u5b66\u79d1\uff0c\u6709\u660e\u786e\u7684\u53ef\u9a8c\u8bc1/\u5f00\u653e\u95ee\u9898\u5212\u5206\u3001\u53ef\u6269\u5c55\u96be\u5ea6\u6807\u6ce8\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\uff1b2) Dr.SCI\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5305\u542b\u63a2\u7d22\u6269\u5c55SFT\u3001\u52a8\u6001\u96be\u5ea6\u8bfe\u7a0b\u5b66\u4e60\u548c\u57fa\u4e8e\u79d1\u5b66\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u4e09\u4e2a\u7ec4\u4ef6\u3002", "result": "\u4f7f\u7528Dr.SCI\u6d41\u7a0b\u8bad\u7ec3\u7684Qwen3-4B-Base\u6a21\u578b\u5728GPQA-diamond\u4e0a\u8fbe\u523063.2\u5206\uff0c\u5728GPQA-general\u4e0a\u8fbe\u523032.4\u5206\uff0c\u6301\u7eed\u4f18\u4e8eo1-mini\u548cGPT-4o\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u7279\u522b\u662f\u5f00\u653e\u95ee\u9898\u8bbe\u7f6e\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Dr.SCI\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u95ee\u7b54\u4e2d\u7684\u6570\u636e\u6784\u5efa\u548c\u5956\u52b1\u8bbe\u8ba1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08222", "abs": "https://arxiv.org/abs/2602.08222", "authors": ["Zehao Chen", "Gongxun Li", "Tianxiang Ai", "Yifei Li", "Zixuan Huang", "Wang Zhou", "Fuzhen Zhuang", "Xianglong Liu", "Jianxin Li", "Deqing Wang", "Yikun Ban"], "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "comment": null, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "AI": {"tldr": "WMSS\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u5386\u53f2\u5f31\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u7ee7\u7eed\u4f18\u5316\uff0c\u901a\u8fc7\u71b5\u52a8\u6001\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\u5e76\u8fdb\u884c\u8865\u507f\u5b66\u4e60\uff0c\u5e2e\u52a9\u5f3a\u6a21\u578b\u7a81\u7834\u4f20\u7edf\u540e\u8bad\u7ec3\u7684\u9971\u548c\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u6a21\u578b\u53d8\u5f97\u9ad8\u5ea6\u81ea\u4fe1\u540e\u4f1a\u51fa\u73b0\u9971\u548c\u74f6\u9888\uff0c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u6536\u76ca\u9012\u51cf\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u72b6\u6001\u4e2d\u4ecd\u5b58\u5728\u6709\u76d1\u7763\u4fe1\u53f7\uff0c\u8fd9\u4e3a\u7a81\u7834\u9971\u548c\u74f6\u9888\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "WMSS\u901a\u8fc7\u5206\u6790\u71b5\u52a8\u6001\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\uff0c\u5229\u7528\u5f31\u68c0\u67e5\u70b9\u4f5c\u4e3a\u6307\u5bfc\uff0c\u901a\u8fc7\u8865\u507f\u5b66\u4e60\u5f3a\u5316\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4f7f\u5f3a\u667a\u80fd\u4f53\u80fd\u591f\u7ee7\u7eed\u4f18\u5316\u800c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528WMSS\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "WMSS\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u72b6\u6001\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u6210\u529f\u7a81\u7834\u4e86\u540e\u8bad\u7ec3\u4e2d\u7684\u9971\u548c\u74f6\u9888\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.08241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08241", "abs": "https://arxiv.org/abs/2602.08241", "authors": ["Siqu Ou", "Tianrui Wan", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs", "comment": null, "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.", "AI": {"tldr": "SAYO\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u957f\u6587\u672c\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5b66\u4e60\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u805a\u7126\u5f31\u7684\u95ee\u9898\uff1a\u65e9\u671f\u89c6\u89c9\u9519\u4f4d\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u5f88\u5c11\u88ab\u7ea0\u6b63\uff0c\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u548c\u63a8\u7406\u5931\u8d25\u3002\u8fd9\u4e00\u9650\u5236\u6e90\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4fe1\u7528\u5206\u914d\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSAYO\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\u3002\u8be5\u5956\u52b1\u660e\u786e\u5c06\u4f18\u5316\u4fe1\u53f7\u4e0e\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u6b65\u9aa4\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u53ef\u9760\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAYO\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u548c\u611f\u77e5\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\uff0cSAYO\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08254", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08254", "abs": "https://arxiv.org/abs/2602.08254", "authors": ["Arman Aghaee", "Sepehr Asgarian", "Jouhyun Jeon"], "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities", "comment": "Presented in AAAI 2026 Singapore at the workshop of Health Intelligence", "summary": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.", "AI": {"tldr": "SynthAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u80a5\u80d6\u5408\u5e76\u7cbe\u795e\u969c\u788d\u60a3\u8005\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u6570\u636e\u548c\u4eba\u683c\u7279\u8d28\u6765\u521b\u5efa\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u54cd\u5e94\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5b58\u5728\u788e\u7247\u5316\u3001\u504f\u89c1\u548c\u9690\u79c1\u9650\u5236\u7684\u95ee\u9898\uff0c\u6a21\u62df\u9ad8\u4fdd\u771f\u60a3\u8005\u4e3a\u7814\u7a76\u590d\u6742\u75be\u75c5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u63d0\u51faSynthAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u6574\u5408\u7d22\u8d54\u6570\u636e\u3001\u4eba\u53e3\u8c03\u67e5\u548c\u60a3\u8005\u4e2d\u5fc3\u6587\u732e\uff0c\u6784\u5efa\u5177\u6709\u4eba\u683c\u7279\u8d28\u7684\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u901a\u8fc7\u81ea\u4e3b\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u3001\u6cbb\u7597\u54cd\u5e94\u548c\u751f\u6d3b\u7ba1\u7406\u3002", "result": "\u8bc4\u4f30100\u591a\u4e2a\u751f\u6210\u7684\u60a3\u8005\u663e\u793a\uff0cGPT-5\u548cClaude 4.5 Sonnet\u5728MAS\u6846\u67b6\u4e2d\u4fdd\u771f\u5ea6\u6700\u9ad8\uff0c\u4f18\u4e8eGemini 2.5 Pro\u548cDeepSeek-R1\u3002", "conclusion": "SynthAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u533b\u7597\u548c\u5fc3\u7406\u9886\u57df\u7684\u60a3\u8005\u65c5\u7a0b\u3001\u884c\u4e3a\u52a8\u6001\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2602.08498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08498", "abs": "https://arxiv.org/abs/2602.08498", "authors": ["Haoran Zhang", "Yafu Li", "Zhi Wang", "Zhilin Wang", "Shunkai Zhang", "Xiaoye Qu", "Yu Cheng"], "title": "Characterizing, Evaluating, and Optimizing Complex Reasoning", "comment": "Code and data are available at \\url{https://github.com/zzzhr97/TRM}", "summary": "Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5b9a\u4e49\u63a8\u7406\u8d28\u91cf\u3001\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u3001\u4ee5\u53ca\u5229\u7528\u8bc4\u4f30\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\u3002\u901a\u8fc7ME\u00b2\u539f\u5219\u4ece\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u5b9a\u4e49\u8d28\u91cf\uff0c\u57fa\u4e8eDAG\u7684\u6210\u5bf9\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6784\u5efaTRM-Preference\u6570\u636e\u96c6\u8bad\u7ec3\u601d\u8003\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e09\u4e2a\u57fa\u672c\u95ee\u9898\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff1a1) \u5982\u4f55\u5b9a\u4e49\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\uff1b2) \u5982\u4f55\u53ef\u9760\u8bc4\u4f30\u5177\u6709\u590d\u6742\u5185\u90e8\u7ed3\u6784\u7684\u957f\u63a8\u7406\u8f68\u8ff9\uff1b3) \u5982\u4f55\u5229\u7528\u8fd9\u79cd\u8bc4\u4f30\u4fe1\u53f7\u8fdb\u884c\u63a8\u7406\u4f18\u5316\u3002", "method": "1) \u63d0\u51faME\u00b2\u539f\u5219\uff0c\u4ece\u5b8f\u89c2\u6548\u7387/\u6709\u6548\u6027\u548c\u5fae\u89c2\u6548\u7387/\u6709\u6548\u6027\u4e24\u4e2a\u5c42\u9762\u5b9a\u4e49\u63a8\u7406\u8d28\u91cf\uff1b2) \u5c06\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5f00\u53d1\u57fa\u4e8eDAG\u7684\u6210\u5bf9\u8bc4\u4f30\u65b9\u6cd5\uff1b3) \u6784\u5efaTRM-Preference\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u601d\u8003\u5956\u52b1\u6a21\u578b(TRM)\u6765\u5927\u89c4\u6a21\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u601d\u8003\u5956\u52b1\u4f5c\u4e3a\u6709\u6548\u7684\u4f18\u5316\u4fe1\u53f7\uff1a\u5728\u6d4b\u8bd5\u65f6\uff0c\u9009\u62e9\u66f4\u597d\u7684\u63a8\u7406\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\uff08\u6700\u9ad819.3%\u63d0\u5347\uff09\uff1b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u601d\u8003\u5956\u52b1\u80fd\u589e\u5f3a\u63a8\u7406\u548c\u6027\u80fd\uff08\u6700\u9ad83.9%\u63d0\u5347\uff09\uff0c\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7ME\u00b2\u539f\u5219\u3001DAG\u5efa\u6a21\u548c\u601d\u8003\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5b9a\u4e49\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.08276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08276", "abs": "https://arxiv.org/abs/2602.08276", "authors": ["Haoyu Jia", "Kento Kawaharazuka", "Kei Okada"], "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis", "comment": null, "summary": "Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \\texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \\texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.", "AI": {"tldr": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4ece\u4e0a\u4e0b\u6587\u7ed3\u6784\u89d2\u5ea6\u5206\u6790\u548c\u6bd4\u8f83LLM\u667a\u80fd\u4f53\uff0c\u5305\u542b\u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\u548cSemantic Dynamics Analysis\u5de5\u7a0b\u6d41\u7a0b\uff0c\u5728\u52a8\u6001\u7334\u5b50\u9999\u8549\u95ee\u9898\u4e0a\u5b9e\u73b032%\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u7814\u7a76\u788e\u7247\u5316\u4e25\u91cd\uff0c\u6982\u5ff5\u6846\u67b6\u4e0e\u65b9\u6cd5\u8bba\u539f\u5219\u5e38\u4e0e\u5e95\u5c42\u5b9e\u73b0\u7ec6\u8282\u6df7\u6742\uff0c\u7f3a\u4e4f\u53ef\u5206\u6790\u3001\u81ea\u6d3d\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u6765\u8fdb\u884c\u5b9e\u73b0\u65e0\u5173\u7684\u667a\u80fd\u4f53\u8868\u5f81\u548c\u6bd4\u8f83\u3002", "method": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4ece\u4e0a\u4e0b\u6587\u7ed3\u6784\u89d2\u5ea6\u5206\u6790LLM\u667a\u80fd\u4f53\uff1b\u5305\u542b\u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\u548cSemantic Dynamics Analysis\u53ef\u6301\u7eed\u5de5\u7a0b\u6d41\u7a0b\uff0c\u652f\u6301\u5feb\u901f\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fed\u4ee3\u3002", "result": "\u5728\u52a8\u6001\u7334\u5b50\u9999\u8549\u95ee\u9898\u53d8\u4f53\u4e0a\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u667a\u80fd\u4f53\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e8632\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u548c\u5de5\u7a0b\u6d41\u7a0b\u4e3aLLM\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u73b0\u65e0\u5173\u7684\u667a\u80fd\u4f53\u8868\u5f81\u548c\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fed\u4ee3\u3002", "topic": "agent analysis"}}
{"id": "2602.08600", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08600", "abs": "https://arxiv.org/abs/2602.08600", "authors": ["Archchana Sindhujan", "Girish A. Koushik", "Shenbin Qian", "Diptesh Kanojia", "Constantin Or\u0103san"], "title": "Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation", "comment": "Currently this article is under review for Natural Language Processing Journal", "summary": "Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ALOPE-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9519\u8bef\u611f\u77e5\u5956\u52b1\uff0c\u7528\u4e8e\u8d44\u6e90\u7a00\u7f3a\u7684\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\uff0c\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30(QE)\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6807\u91cf\u8d28\u91cf\u5206\u6570\uff0c\u7f3a\u4e4f\u5bf9\u7ffb\u8bd1\u9519\u8bef\u7684\u660e\u786e\u89e3\u91ca\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\uff09\u4e0a\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u800c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "1) \u521b\u5efa\u9996\u4e2a\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u7247\u6bb5\u7ea7QE\u6570\u636e\u96c6\uff0c\u5305\u542b\u76f4\u63a5\u8bc4\u4f30\u5206\u6570\u548c\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u6ce8\uff1b2) \u63d0\u51faALOPE-RL\u6846\u67b6\uff0c\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9ad8\u6548\u9002\u914d\u5668\uff0c\u7ed3\u5408\u76f4\u63a5\u8bc4\u4f30\u5206\u6570\u548c\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u6ce8\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff1b3) \u4f7f\u7528LoRA\u548c4\u4f4d\u91cf\u5316\u6280\u672f\u5fae\u8c03\u7d27\u51d1\u578bLLM\uff08\u22644B\u53c2\u6570\uff09\u3002", "result": "ALOPE-RL\u5728\u82f1\u8bed-\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bedQE\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u7684LLM\u57fa\u7ebf\u548c\u9886\u5148\u7684\u57fa\u4e8e\u7f16\u7801\u5668\u7684QE\u6a21\u578b\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u5f88\u5c0f\u3002", "conclusion": "\u9519\u8bef\u611f\u77e5\u7684\u7b56\u7565\u5b66\u4e60\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u63d0\u4f9b\u5f3a\u5927\u7684QE\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08335", "abs": "https://arxiv.org/abs/2602.08335", "authors": ["Yanming Li", "Xuelin Zhang", "WenJie Lu", "Ziye Tang", "Maodong Wu", "Haotian Luo", "Tongtong Wu", "Zijie Peng", "Hongze Mi", "Yibo Feng", "Naiqiang Tan", "Chao Huang", "Hong Chen", "Li Shen"], "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System", "comment": null, "summary": "Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.", "AI": {"tldr": "SHARP\u6846\u67b6\u901a\u8fc7\u57fa\u4e8eShapley\u503c\u7684\u5206\u5c42\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4fe1\u7528\u5206\u914d\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c06LLM\u4e0e\u5916\u90e8\u5de5\u5177\u7ed3\u5408\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u89e3\u51b3\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f46\u8bad\u7ec3\u8fd9\u4e9b\u7cfb\u7edf\u9762\u4e34\u4fe1\u7528\u5206\u914d\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u6216\u5168\u5c40\u5e7f\u64ad\u5956\u52b1\uff0c\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u8d21\u732e\uff0c\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faSHARP\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7684\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u4fe1\u7528\u5206\u914d\uff1a\u5305\u62ec\u5168\u5c40\u5e7f\u64ad-\u51c6\u786e\u6027\u5956\u52b1\u3001\u57fa\u4e8eShapley\u503c\u7684\u8fb9\u9645\u4fe1\u7528\u5956\u52b1\u548c\u5de5\u5177\u8fc7\u7a0b\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u5728\u8f68\u8ff9\u7ec4\u95f4\u6807\u51c6\u5316\u667a\u80fd\u4f53\u7279\u5b9a\u4f18\u52bf\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSHARP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u574723.66%\u548c14.05%\u7684\u5339\u914d\u6539\u8fdb\u3002", "conclusion": "SHARP\u6846\u67b6\u901a\u8fc7\u7cbe\u786e\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3aLLM\u4e0e\u5de5\u5177\u7ed3\u5408\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08658", "abs": "https://arxiv.org/abs/2602.08658", "authors": ["Mingzi Cao", "Xingwei Tan", "Mahmud Akhter", "Marco Valentino", "Maria Liakata", "Xi Wang", "Nikolaos Aletras"], "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models", "comment": null, "summary": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u7b26\u53f7\u4efb\u52a1\u8bad\u7ec3\u5c06\u6f14\u7ece\u3001\u5f52\u7eb3\u3001\u6eaf\u56e0\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u8fc1\u79fb\u5230LLM\u4e2d\uff0c\u5e76\u5728\u73b0\u5b9e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u867d\u7136\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u8303\u5f0f\uff08\u6f14\u7ece\u3001\u5f52\u7eb3\u3001\u6eaf\u56e0\uff09\u5982\u4f55\u5f71\u54cdLLM\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u8fd9\u4e9b\u6838\u5fc3\u8303\u5f0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u5f71\u54cdLLM\u7684\u63a8\u7406\u884c\u4e3a\u3002", "method": "\u9996\u5148\u6536\u96c6\u7b26\u53f7\u4efb\u52a1\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6\uff08\u6bcf\u4e2a\u4efb\u52a1\u9488\u5bf9\u4e00\u79cd\u63a8\u7406\u8303\u5f0f\uff09\uff0c\u7136\u540e\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u6280\u80fd\u8fc1\u79fb\u5230LLM\u4e2d\uff0c\u5305\u62ec\u7b80\u5355\u5fae\u8c03\u3001\u589e\u52a0\u6a21\u578b\u6df1\u5ea6\u3001\u5c06\u5bc6\u96c6\u6a21\u578b\u8f6c\u6362\u4e3a\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7b49\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u8de8\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad8\u8fbe14.60\u5206\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5b8c\u5168\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u5e76\u5305\u542b\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u3002", "conclusion": "\u901a\u8fc7\u7b26\u53f7\u4efb\u52a1\u8bad\u7ec3\u5c06\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u8fc1\u79fb\u5230LLM\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u73b0\u5b9e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08344", "abs": "https://arxiv.org/abs/2602.08344", "authors": ["Qi Guo", "Jianing Wang", "Deyang Kong", "Xiangyu Xi", "Jianfei Zhang", "Yi Lu", "Jingang Wang", "Wei Wang", "Shikun Zhang", "Wei Ye"], "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration", "comment": null, "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.", "AI": {"tldr": "\u63d0\u51faOutline-Guided Path Exploration (OPE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\u6765\u5212\u5206\u89e3\u7a7a\u95f4\uff0c\u51cf\u5c11\u5e76\u884c\u63a8\u7406\u8def\u5f84\u95f4\u7684\u4fe1\u606f\u5197\u4f59\uff0c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u601d\u7ef4\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u9636\u6bb5\u4f18\u5316\uff0c\u5bf9\u8def\u5f84\u63a2\u7d22\u9636\u6bb5\u5173\u6ce8\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u63a2\u7d22\u8def\u5f84\u95f4\u7684\u4e92\u4fe1\u606f\u74f6\u9888\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u9700\u8981\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\u5e76\u63d0\u5347\u8def\u5f84\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faOPE\u65b9\u6cd5\uff1a1\uff09\u5148\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\u6765\u5212\u5206\u89e3\u7a7a\u95f4\uff1b2\uff09\u57fa\u4e8e\u5927\u7eb2\u8fdb\u884c\u5e76\u884c\u8def\u5f84\u63a8\u7406\uff1b3\uff09\u91c7\u7528\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u72ec\u7acb\u4f18\u5316\u5927\u7eb2\u89c4\u5212\u548c\u57fa\u4e8e\u5927\u7eb2\u7684\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOPE\u80fd\u6709\u6548\u63d0\u5347\u4e0d\u540c\u805a\u5408\u7b56\u7565\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f7f\u5927\u578b\u63a8\u7406\u6a21\u578b\u66f4\u53ef\u9760\u5730\u53d1\u73b0\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5212\u5206\u89e3\u7a7a\u95f4\u548c\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\uff0cOPE\u89e3\u51b3\u4e86\u5e76\u884c\u601d\u7ef4\u4e2d\u7684\u4e92\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5e76\u884c\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08369", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08369", "abs": "https://arxiv.org/abs/2602.08369", "authors": ["Xin Zhang", "Kailai Yang", "Chenyue Li", "Hao Li", "Qiyu Wei", "Jun'ichi Tsujii", "Sophia Ananiadou"], "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval", "comment": null, "summary": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.", "AI": {"tldr": "MemAdapter\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bb0\u5fc6\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f\u7684\u5feb\u901f\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6210\u672c\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5b64\u7acb\u8303\u5f0f\u8bbe\u8ba1\uff08\u5982\u663e\u5f0f\u3001\u53c2\u6570\u5316\u6216\u6f5c\u5728\u8bb0\u5fc6\uff09\uff0c\u68c0\u7d22\u65b9\u6cd5\u4e0e\u8303\u5f0f\u7d27\u5bc6\u8026\u5408\uff0c\u963b\u788d\u4e86\u8de8\u8303\u5f0f\u7684\u6cdb\u5316\u548c\u878d\u5408\u3002\u9700\u8981\u7edf\u4e00\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f\u3002", "method": "\u63d0\u51faMemAdapter\u6846\u67b6\uff1a1\uff09\u4ece\u7edf\u4e00\u8bb0\u5fc6\u7a7a\u95f4\u8bad\u7ec3\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\uff1b2\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u5bf9\u9f50\u6a21\u5757\uff0c\u5c06\u68c0\u7d22\u5668\u9002\u914d\u5230\u672a\u89c1\u8bb0\u5fc6\u8303\u5f0f\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0c\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\u5728\u4e09\u79cd\u8bb0\u5fc6\u8303\u5f0f\u548c\u667a\u80fd\u4f53\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u4f18\u4e8e\u4e94\u4e2a\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002\u8de8\u8303\u5f0f\u5bf9\u9f50\u4ec5\u970013\u5206\u949f\uff08\u5355GPU\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u68c0\u7d22\u5668\u4e14\u8bad\u7ec3\u8ba1\u7b97\u91cf\u51cf\u5c1195%\u4ee5\u4e0a\u3002\u652f\u6301\u8de8\u8303\u5f0f\u7684\u96f6\u6837\u672c\u878d\u5408\u3002", "conclusion": "MemAdapter\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7edf\u4e00\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6210\u672c\uff0c\u63d0\u5347\u8bb0\u5fc6\u68c0\u7d22\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.08373", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08373", "abs": "https://arxiv.org/abs/2602.08373", "authors": ["Feiyu Wu", "Xu Zheng", "Yue Qu", "Zhuocheng Wang", "Zicheng Feng", "Hui Li"], "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "comment": "Accepted to ICLR 2026. Project page. https://openreview.net/forum?id=wb05ver1k8&noteId=v1Ax8CwI71", "summary": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.", "AI": {"tldr": "VIRF\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u903b\u8f91\u5bfc\u5e08\u4e0eLLM\u89c4\u5212\u5668\u7684\u5bf9\u8bdd\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u89c4\u5212\uff0c\u5728\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\u5b9e\u73b0\u96f6\u5371\u9669\u884c\u52a8\u7387\u548c\u6700\u9ad8\u76ee\u6807\u8fbe\u6210\u7387\u3002", "motivation": "LLM\u4f5c\u4e3a\u5177\u8eabAI\u89c4\u5212\u5668\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u968f\u673a\u6027\u7f3a\u4e4f\u5f62\u5f0f\u5316\u63a8\u7406\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684LLM\u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c\u8981\u4e48\u7b80\u5355\u62d2\u7edd\u4e0d\u5b89\u5168\u8ba1\u5212\u800c\u4e0d\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u8fed\u4ee3\u7cbe\u70bc\u6846\u67b6(VIRF)\uff0c\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5efa\u7acb\u903b\u8f91\u5bfc\u5e08\u4e0eLLM\u89c4\u5212\u5668\u7684\u5bfc\u5e08-\u5b66\u5f92\u5bf9\u8bdd\u673a\u5236\u3002\u903b\u8f91\u5bfc\u5e08\u57fa\u4e8e\u5f62\u5f0f\u5316\u5b89\u5168\u672c\u4f53\u63d0\u4f9b\u56e0\u679c\u6027\u548c\u6559\u5b66\u6027\u53cd\u9988\uff0c\u5b9e\u73b0\u667a\u80fd\u8ba1\u5212\u4fee\u590d\u800c\u975e\u7b80\u5355\u907f\u514d\u3002\u8fd8\u5f15\u5165\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u83b7\u53d6\u6d41\u7a0b\uff0c\u4ece\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u5408\u6210\u5b89\u5168\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cVIRF\u5b9e\u73b0\u4e860%\u7684\u5371\u9669\u884c\u52a8\u7387(HAR)\u548c77.3%\u7684\u76ee\u6807\u6761\u4ef6\u7387(GCR)\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u6700\u9ad8\u3002\u5e73\u5747\u4ec5\u97001.1\u6b21\u4fee\u6b63\u8fed\u4ee3\uff0c\u6548\u7387\u6781\u9ad8\u3002", "conclusion": "VIRF\u5c55\u793a\u4e86\u6784\u5efa\u6839\u672c\u4e0a\u53ef\u4fe1\u4e14\u53ef\u9a8c\u8bc1\u5b89\u5168\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u539f\u5219\u6027\u9014\u5f84\uff0c\u4ece\u88ab\u52a8\u5b89\u5168\u628a\u5173\u8f6c\u5411\u4e3b\u52a8\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2602.08400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08400", "abs": "https://arxiv.org/abs/2602.08400", "authors": ["Longkun Li", "Yuanben Zou", "Jinghan Wu", "Yuqing Wen", "Jing Li", "Hangwei Qian", "Ivor Tsang"], "title": "SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains", "comment": null, "summary": "Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \\textbf{SCOUT-RAG} (\\textit{\\underline{S}calable and \\underline{CO}st-efficient \\underline{U}nifying \\underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.", "AI": {"tldr": "SCOUT-RAG\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u4ee3\u7406Graph-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8de8\u57df\u68c0\u7d22\u89e3\u51b3\u5206\u5e03\u5f0f\u53d7\u9650\u73af\u5883\u4e2d\u7684\u77e5\u8bc6\u56fe\u68c0\u7d22\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8de8\u57df\u8c03\u7528\u3001\u4ee4\u724c\u5904\u7406\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edfGraph-RAG\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u77e5\u8bc6\u56fe\uff0c\u4f46\u5728\u5206\u5e03\u5f0f\u548c\u8bbf\u95ee\u53d7\u9650\u73af\u5883\uff08\u5982\u533b\u9662\u3001\u8de8\u56fd\u7ec4\u7ec7\uff09\u4e2d\uff0c\u68c0\u7d22\u9700\u8981\u5728\u6ca1\u6709\u5168\u5c40\u56fe\u53ef\u89c1\u6027\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u76f8\u5173\u57df\u548c\u9002\u5f53\u904d\u5386\u6df1\u5ea6\uff0c\u907f\u514d\u7a77\u4e3e\u67e5\u8be2\u3002", "method": "\u63d0\u51faSCOUT-RAG\u6846\u67b6\uff0c\u4f7f\u7528\u56db\u4e2a\u534f\u4f5c\u4ee3\u7406\uff1a(i)\u4f30\u8ba1\u57df\u76f8\u5173\u6027\uff0c(ii)\u51b3\u5b9a\u4f55\u65f6\u6269\u5c55\u5230\u989d\u5916\u57df\uff0c(iii)\u8c03\u6574\u904d\u5386\u6df1\u5ea6\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u56fe\u63a2\u7d22\uff0c(iv)\u5408\u6210\u9ad8\u8d28\u91cf\u7b54\u6848\u3002\u6846\u67b6\u65e8\u5728\u6700\u5c0f\u5316\u68c0\u7d22\u9057\u61be\uff08\u7f3a\u5931\u6709\u7528\u57df\u4fe1\u606f\uff09\u540c\u65f6\u63a7\u5236\u5ef6\u8fdf\u548cAPI\u6210\u672c\u3002", "result": "\u5728\u591a\u57df\u77e5\u8bc6\u8bbe\u7f6e\u4e2d\uff0cSCOUT-RAG\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\uff08\u5305\u62ecDRIFT\u548c\u7a77\u4e3e\u57df\u904d\u5386\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8de8\u57df\u8c03\u7528\u3001\u5904\u7406\u7684\u603b\u4ee4\u724c\u6570\u548c\u5ef6\u8fdf\u3002", "conclusion": "SCOUT-RAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6210\u672c\u9ad8\u6548\u7684\u5206\u5e03\u5f0fGraph-RAG\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5206\u5e03\u5f0f\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u5e73\u8861\u68c0\u7d22\u8d28\u91cf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "topic": "agent analysis"}}
{"id": "2602.08401", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08401", "abs": "https://arxiv.org/abs/2602.08401", "authors": ["Liwen Wang", "Zongjie Li", "Yuchong Xie", "Shuai Wang", "Dongdong She", "Wei Wang", "Juergen Rahmel"], "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking", "comment": null, "summary": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.", "AI": {"tldr": "AGENTWM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u7f6e\u529f\u80fd\u76f8\u540c\u7684\u5de5\u5177\u6267\u884c\u8def\u5f84\u5206\u5e03\u6765\u5d4c\u5165\u6c34\u5370\uff0c\u4fdd\u62a4\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u514d\u53d7\u6a21\u4eff\u653b\u51fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6f14\u53d8\u4e3a\u6267\u884c\u81ea\u4e3b\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u521b\u9020\u4e86\u91cd\u8981\u7684\u77e5\u8bc6\u4ea7\u6743\u4ef7\u503c\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u6a21\u4eff\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684LLM\u6c34\u5370\u6280\u672f\u65e0\u6cd5\u5728\u667a\u80fd\u4f53\u9886\u57df\u6709\u6548\u5de5\u4f5c\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f5c\u4e3a\u7070\u76d2\u8fd0\u884c\uff0c\u9690\u85cf\u4e86\u9a8c\u8bc1\u6240\u9700\u7684\u5185\u90e8\u63a8\u7406\u75d5\u8ff9\u3002", "method": "AGENTWM\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u5fae\u5999\u5730\u504f\u7f6e\u529f\u80fd\u76f8\u540c\u7684\u5de5\u5177\u6267\u884c\u8def\u5f84\u5206\u5e03\u6765\u6ce8\u5165\u6c34\u5370\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u7ba1\u9053\u751f\u6210\u9c81\u68d2\u7684\u6c34\u5370\u65b9\u6848\uff0c\u4ee5\u53ca\u4e25\u683c\u7684\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7a0b\u5e8f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u9886\u57df\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0cAGENTWM\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u5f71\u54cd\u53ef\u5ffd\u7565\u3002\u6c34\u5370\u80fd\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u77e5\u8bc6\u4ea7\u6743\uff0c\u5bf9\u6297\u6027\u653b\u51fb\u8005\u65e0\u6cd5\u5728\u4e0d\u4e25\u91cd\u964d\u4f4e\u88ab\u76d7\u6a21\u578b\u6548\u7528\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u6c34\u5370\u3002", "conclusion": "AGENTWM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u5bf9\u6297\u6a21\u4eff\u653b\u51fb\uff0c\u586b\u8865\u4e86\u73b0\u6709LLM\u6c34\u5370\u6280\u672f\u5728\u667a\u80fd\u4f53\u9886\u57df\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2602.08412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08412", "abs": "https://arxiv.org/abs/2602.08412", "authors": ["Yuhang Wang", "Feiming Xu", "Zheng Lin", "Guangyu He", "Yuzhe Huang", "Haichang Gao", "Zhenxing Niu"], "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent", "comment": "11 pages,2 figures", "summary": "Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.", "AI": {"tldr": "PASB\u662f\u4e00\u4e2a\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2a\u6027\u5316AI\u4ee3\u7406\u7684\u7aef\u5230\u7aef\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u4f7f\u7528\u573a\u666f\u3001\u771f\u5b9e\u5de5\u5177\u94fe\u548c\u957f\u65f6\u4ea4\u4e92\u6765\u8bc4\u4f30OpenClaw\u7b49\u4ee3\u7406\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5408\u6210\u6216\u4efb\u52a1\u4e2d\u5fc3\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u4e2a\u6027\u5316\u4ee3\u7406\u7684\u653b\u51fb\u9762\u548c\u98ce\u9669\u4f20\u64ad\u673a\u5236\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faPASB\u6846\u67b6\uff0c\u57fa\u4e8e\u73b0\u6709\u4ee3\u7406\u653b\u51fb\u8303\u5f0f\uff0c\u6574\u5408\u4e2a\u6027\u5316\u4f7f\u7528\u573a\u666f\u3001\u771f\u5b9e\u5de5\u5177\u94fe\u548c\u957f\u65f6\u4ea4\u4e92\uff0c\u5bf9\u771f\u5b9e\u7cfb\u7edf\u8fdb\u884c\u9ed1\u76d2\u7aef\u5230\u7aef\u5b89\u5168\u8bc4\u4f30\u3002", "result": "\u4ee5OpenClaw\u4e3a\u6848\u4f8b\u7814\u7a76\u53d1\u73b0\u5176\u5728\u7528\u6237\u63d0\u793a\u5904\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u68c0\u7d22\u7b49\u4e0d\u540c\u6267\u884c\u9636\u6bb5\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u7a81\u663e\u4e2a\u6027\u5316\u4ee3\u7406\u90e8\u7f72\u4e2d\u7684\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "PASB\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30\u4e2a\u6027\u5316\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0cOpenClaw\u7b49\u4ee3\u7406\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2602.08829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08829", "abs": "https://arxiv.org/abs/2602.08829", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Zijun Yao", "Lei Hou", "Juanzi Li"], "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions", "comment": null, "summary": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.", "AI": {"tldr": "WildReward\uff1a\u76f4\u63a5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u5bf9\uff0c\u5728WildChat\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u504f\u597d\u5bf9\uff0c\u800cLLM\u7684\u5e7f\u6cdb\u90e8\u7f72\u4ea7\u751f\u4e86\u5927\u91cf\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u8fd9\u4e9b\u9690\u5f0f\u53cd\u9988\u4fe1\u53f7\u80fd\u5426\u76f4\u63a5\u7528\u4e8e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff1f", "method": "\u91c7\u7528WildChat\u4f5c\u4e3a\u4ea4\u4e92\u6570\u636e\u6e90\uff0c\u63d0\u51fa\u4ece\u7528\u6237\u53cd\u9988\u4e2d\u63d0\u53d6\u53ef\u9760\u4eba\u7c7b\u53cd\u9988\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u5e8f\u6570\u56de\u5f52\u76f4\u63a5\u8bad\u7ec3WildReward\uff0c\u65e0\u9700\u504f\u597d\u5bf9\u6807\u6ce8", "result": "WildReward\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4f20\u7edf\u5956\u52b1\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u6821\u51c6\u6027\u548c\u8de8\u6837\u672c\u4e00\u81f4\u6027\uff0c\u7528\u6237\u591a\u6837\u6027\u76f4\u63a5\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e94\u7528\u4e8e\u5728\u7ebfDPO\u8bad\u7ec3\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "\u76f4\u63a5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u662f\u53ef\u884c\u7684\uff0cWildReward\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5728\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2602.08520", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08520", "abs": "https://arxiv.org/abs/2602.08520", "authors": ["Xinhai Sun"], "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning", "comment": null, "summary": "Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \\emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \\emph{without any retraining}.\n  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\\% to 84.03\\%, while only incurring 61.06\\% additional inference calls. A 100\\% re-asking ablation reaches 84.35\\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \\emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.\n  Beyond providing a practical inference-time upgrade, our results suggest a broader \\emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReinforcement Inference\u65b9\u6cd5\uff0c\u5229\u7528LLM\u81ea\u8eab\u7684\u4e0d\u786e\u5b9a\u6027\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6027\u89e6\u53d1\u4e8c\u6b21\u601d\u8003\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5355\u6b21\u8d2a\u5a6a\u63a8\u7406\u534f\u8bae\u4e0b\u90e8\u7f72\uff0c\u4f1a\u56e0\u8fc7\u65e9\u51b3\u7b56\u800c\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u3002\u8bb8\u591a\u9519\u8bef\u5e76\u975e\u6e90\u4e8e\u77e5\u8bc6\u7f3a\u5931\uff0c\u800c\u662f\u5185\u90e8\u6a21\u7cca\u6027\u4e0b\u7684\u8fc7\u65e9\u627f\u8bfa\u3002", "method": "\u63d0\u51faReinforcement Inference\uff0c\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u63a8\u7406\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6027\u5730\u8c03\u7528\u7b2c\u4e8c\u6b21\u66f4\u614e\u91cd\u7684\u63a8\u7406\u5c1d\u8bd5\u3002", "result": "\u5728MMLU-Pro\u768412,032\u4e2a\u95ee\u9898\u4e0a\uff0c\u4f7f\u7528DeepSeek-v3.2\u96f6\u6837\u672c\u8bbe\u7f6e\uff0c\u51c6\u786e\u7387\u4ece60.72%\u63d0\u5347\u81f384.03%\uff0c\u4ec5\u589e\u52a061.06%\u7684\u63a8\u7406\u8c03\u7528\u3002100%\u91cd\u95ee\u6d88\u878d\u5b9e\u9a8c\u8fbe\u523084.35%\uff0c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9009\u62e9\u80fd\u4ee5\u66f4\u5c11\u8ba1\u7b97\u83b7\u5f97\u5927\u90e8\u5206\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u5b9e\u7528\u7684\u63a8\u7406\u65f6\u5347\u7ea7\uff0c\u8fd8\u63d0\u51fa\u4e86\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u71b5\u7684\u8303\u5f0f\u6765\u6d4b\u91cf\u548c\u6269\u5c55\u6a21\u578b\u80fd\u529b\uff0c\u4e3aLLM\u7684\u6f5c\u5728\u63a8\u7406\u8303\u56f4\u63d0\u4f9b\u8bca\u65ad\u89c6\u89d2\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u8bad\u7ec3\u76ee\u6807\u660e\u786e\u7ea6\u675f\u6b63\u786e\u6027-\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2602.08533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08533", "abs": "https://arxiv.org/abs/2602.08533", "authors": ["Kun Peng", "Conghui Tan", "Yu Liu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Zining Zhu", "Lei Jiang", "Yanbing Liu", "Hao Peng"], "title": "Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO", "comment": null, "summary": "Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5728\u7ebf\u4e2a\u6027\u5316\u4e0e\u81ea\u9002\u5e94\u6811\u57fa\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u957f\u89c6\u91ce\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u57df\u5bf9\u8bdd\u4ee3\u7406\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u9884\u6536\u96c6\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u77ed\u89c6\u91ce\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u57df\u5bf9\u8bdd\u4ee3\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u9884\u6536\u96c6\u7684\u7528\u6237\u6570\u636e\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u77ed\u89c6\u91ce\u504f\u89c1\u5ffd\u7565\u4e86\u5bf9\u8bdd\u7684\u957f\u671f\u4ef7\u503c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u4e2a\u6027\u5316\u5e76\u8003\u8651\u957f\u671f\u5bf9\u8bdd\u6548\u679c\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u535a\u5f08\u8303\u5f0f\uff1a\u7528\u6237\u4ee3\u7406\u901a\u8fc7\u98ce\u683c\u6a21\u4eff\u5b66\u4e60\u7528\u6237\u7279\u5b9a\u5bf9\u8bdd\u7279\u5f81\uff0c\u901a\u8fc7\u4e3b\u52a8\u7ec8\u6b62\u9884\u6d4b\u56de\u5408\u7ea7\u7ec8\u6b62\u6982\u7387\u4f5c\u4e3a\u5373\u65f6\u5956\u52b1\uff0c\u6784\u5efa\u52a8\u6001\u73af\u5883\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u6811\u57fa\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08AT-GRPO\uff09\uff0c\u5c06\u5bf9\u8bdd\u8f68\u8ff9\u91cd\u65b0\u89e3\u91ca\u4e3a\u6811\u7ed3\u6784\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u89c2\u5bdf\u8303\u56f4\uff0c\u5728\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u8f83\u5927\u8303\u56f4\u652f\u6301\u4e3b\u9898\u63a2\u7d22\uff0c\u5728\u540e\u671f\u9636\u6bb5\u4f7f\u7528\u8f83\u5c0f\u8303\u56f4\u4fc3\u8fdb\u5bf9\u8bdd\u7ef4\u62a4\uff0c\u5c06\u8ba1\u7b97\u5f00\u9500\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u591a\u9879\u5f0f\u7ea7\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u6027\u80fd\u3001\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684\u957f\u89c6\u91ceRL\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u4e2a\u6027\u5316\u548c\u81ea\u9002\u5e94\u6811\u57fa\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u957f\u671f\u4ef7\u503c\u6355\u83b7\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5bf9\u8bdd\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2602.08586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08586", "abs": "https://arxiv.org/abs/2602.08586", "authors": ["Yiming Yang", "Zhuoyuan Li", "Fanxiang Zeng", "Hao Fu", "Yue Liu"], "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition", "comment": null, "summary": "Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.\n  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.", "AI": {"tldr": "PRISM\u6846\u67b6\u5c06\u591a\u667a\u80fd\u4f53\u63a8\u7406\u589e\u76ca\u5206\u89e3\u4e3a\u63a2\u7d22\u3001\u4fe1\u606f\u548c\u805a\u5408\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u901a\u8fc7\u89d2\u8272\u591a\u6837\u6027\u3001\u6267\u884c\u53cd\u9988\u548c\u8fed\u4ee3\u5408\u6210\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\uff0c\u4e0d\u6e05\u695a\u4e3a\u4f55\u591a\u667a\u80fd\u4f53\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u4ee5\u53ca\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u6700\u5173\u952e\uff0c\u96be\u4ee5\u7cfb\u7edf\u4f18\u5316", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff1a1) \u89d2\u8272\u591a\u6837\u6027\u5b9e\u73b0\u63a2\u7d22\uff1b2) \u57fa\u4e8e\u6267\u884c\u7684\u53cd\u9988\u548c\u8bc1\u636e\u4ea4\u53c9\u8bc4\u4f30\u63d0\u4f9b\u4fe1\u606f\uff1b3) \u8fed\u4ee3\u5408\u6210\u548c\u95ed\u73af\u9a8c\u8bc1\u5b9e\u73b0\u805a\u5408", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u4ec5\u4f18\u5316\u90e8\u5206\u7ef4\u5ea6\u7684\u65b9\u6cd5", "conclusion": "\u7406\u8bba\u6846\u67b6\u4e3a\u672a\u6765\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u539f\u5219\uff0cPRISM\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u7ef4\u5ea6\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "topic": "agent analysis"}}
{"id": "2602.07729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07729", "abs": "https://arxiv.org/abs/2602.07729", "authors": ["Sagnik Mukherjee", "Lifan Yuan", "Pavan Jayasinha", "Dilek Hakkani-T\u00fcr", "Hao Peng"], "title": "Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs", "comment": null, "summary": "Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u7b80\u5355\u7684SGD\u4f18\u5316\u5668\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684AdamW\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u7a00\u758f\u5ea6\u6781\u9ad8\uff08\u4ec5\u66f4\u65b00.02%\u7684\u53c2\u6570\uff09\uff0c\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u5f53\u524dLLM\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ecd\u6cbf\u7528\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u5982AdamW\uff09\uff0c\u4f46RL\u4e0e\u76d1\u7763\u5b66\u4e60\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002AdamW\u5185\u5b58\u5f00\u9500\u5927\uff0c\u800c\u7814\u7a76\u663e\u793aRL\u53ef\u80fd\u4e0d\u9700\u8981AdamW\u7684\u590d\u6742\u81ea\u9002\u5e94\u7279\u6027\u3002", "method": "\u5206\u6790AdamW\u5728RL\u4e2d\u7684\u52a8\u91cf\u4e0e\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u4f5c\u7528\uff0c\u4e0e\u76d1\u7763\u5fae\u8c03\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u9a8c\u8bc1SGD\u5728RL\u4e2d\u7684\u6027\u80fd\uff0c\u6d4b\u91cf\u53c2\u6570\u66f4\u65b0\u7a00\u758f\u5ea6\uff0c\u5e76\u5206\u6790\u66f4\u65b0\u7a00\u758f\u7684\u539f\u56e0\u3002", "result": "SGD\u5728LLM\u7684RL\u4e2d\u8868\u73b0\u4e0eAdamW\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u6781\u5176\u7a00\u758f\uff08\u4ec5\u66f4\u65b00.02%\u7684\u53c2\u6570\uff09\uff0c\u6bd4AdamW\u5c11\u66f4\u65b01000\u500d\u4ee5\u4e0a\uff0c\u5185\u5b58\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RL\u4f18\u5316\u4e0e\u76d1\u7763\u5b66\u4e60\u4e0d\u540c\uff0cSGD\u5728RL\u4e2d\u66f4\u6709\u6548\u4e14\u53c2\u6570\u6548\u7387\u6781\u9ad8\uff0c\u8fd9\u4e3aLLM\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u548c\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07730", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07730", "abs": "https://arxiv.org/abs/2602.07730", "authors": ["Siddarth Chandrasekar", "Marlos C. Machado"], "title": "The Laplacian Keyboard: Beyond the Linear Span", "comment": "28 pages, 17 figures", "summary": "Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLaplacian Keyboard\uff08LK\uff09\uff0c\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u884c\u4e3a\u57fa\u5143\u5e93\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u57fa\u5143\uff0c\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u5e38\u4ec5\u7528\u4e8e\u7ebf\u6027\u903c\u8fd1\u5956\u52b1\u51fd\u6570\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002\u9700\u8981\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u3002", "method": "LK\u6846\u67b6\uff1a1\uff09\u4ece\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u9009\u9879\u5e93\uff08\u884c\u4e3a\u57fa\u5143\uff09\uff1b2\uff09\u4fdd\u8bc1\u8be5\u57fa\u5143\u5e93\u5305\u542b\u4efb\u4f55\u7ebf\u6027\u5956\u52b1\u7684\u6700\u4f18\u7b56\u7565\uff1b3\uff09\u901a\u8fc7\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u9009\u9879\uff0c\u5b66\u4e60\u8d85\u51fa\u539f\u59cb\u7ebf\u6027\u7ea6\u675f\u7684\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\uff1a\u5efa\u7acb\u4e86\u96f6\u6837\u672c\u903c\u8fd1\u8bef\u5dee\u7684\u7406\u8bba\u754c\u9650\u3002\u5b9e\u8bc1\u7ed3\u679c\uff1aLK\u8d85\u8d8a\u4e86\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u6807\u51c6RL\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "LK\u6846\u67b6\u6210\u529f\u8d85\u8d8a\u4e86\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u7684\u7ebf\u6027\u7ea6\u675f\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u5408\u884c\u4e3a\u57fa\u5143\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08754", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08754", "abs": "https://arxiv.org/abs/2602.08754", "authors": ["Rose E. Guingrich", "Dvija Mehta", "Umang Bhatt"], "title": "Belief Offloading in Human-AI Interaction", "comment": null, "summary": "What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, \"belief offloading,\" in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u4fe1\u5ff5\u5378\u8f7d\"\u6982\u5ff5\uff0c\u6307\u4eba\u4eec\u5c06\u4fe1\u5ff5\u5f62\u6210\u548c\u7ef4\u62a4\u8fc7\u7a0b\u5916\u5305\u7ed9AI\u7cfb\u7edf\uff0c\u5f71\u54cd\u5176\u884c\u4e3a\u548c\u4fe1\u5ff5\u4f53\u7cfb\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u8fb9\u754c\u6761\u4ef6\u3001\u5206\u7c7b\u548c\u89c4\u8303\u610f\u4e49\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u5c06LLM\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3a\u601d\u7ef4\u4f19\u4f34\uff0c\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\uff0c\u7279\u522b\u662f\"\u4fe1\u5ff5\u5378\u8f7d\"\uff0c\u8fd9\u53ef\u80fd\u5bf9\u8ba4\u77e5\u6280\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u7279\u5b9a\u8ba4\u77e5\u5378\u8f7d\u73b0\u8c61\u53ca\u5176\u540e\u679c\u3002", "method": "\u7ed3\u5408\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\uff0c\u660e\u786e\u5b9a\u4e49\u4fe1\u5ff5\u5378\u8f7d\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u63d0\u4f9b\u63cf\u8ff0\u6027\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u89c4\u8303\u610f\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u4fe1\u5ff5\u5378\u8f7d\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u62ec\u5176\u53d1\u751f\u6761\u4ef6\u3001\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u6307\u51fa\u4e86\u4fe1\u5ff5\u5378\u8f7d\u5bf9\u4eba\u7c7b\u884c\u4e3a\u548c\u4fe1\u5ff5\u4f53\u7cfb\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u4fe1\u5ff5\u5378\u8f7d\u662f\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u73b0\u8c61\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u53d1\u751f\u673a\u5236\u548c\u540e\u679c\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.08905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08905", "abs": "https://arxiv.org/abs/2602.08905", "authors": ["Jiawei Liu", "Xiting Wang", "Yuanyuan Zhong", "Defu Lian", "Yu Yang"], "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models", "comment": "13 pages, 3 figures", "summary": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.", "AI": {"tldr": "STP\u6846\u67b6\u901a\u8fc7\u7a7a\u95f4\u526a\u679d\u548c\u65f6\u95f4\u526a\u679d\u540c\u65f6\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u91ca\u653e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e94\u7528\u4e8edLLMs\u65f6\u9762\u4e34\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u72ec\u7279\u6311\u6218", "method": "\u63d0\u51fa\u65f6\u7a7a\u526a\u679d(STP)\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u526a\u679d\uff08\u5229\u7528\u9759\u6001\u5148\u9a8c\u7ea6\u675f\u63a2\u7d22\u7a7a\u95f4\uff09\u548c\u65f6\u95f4\u526a\u679d\uff08\u7ed5\u8fc7\u5197\u4f59\u7684\u540e\u671f\u7ec6\u5316\u6b65\u9aa4\uff09\u538b\u7f29\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eSTP\u4e25\u683c\u964d\u4f4e\u4e86\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u786e\u4fdd\u66f4\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\uff1b\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eSTP\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "STP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86dLLMs\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.07828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07828", "abs": "https://arxiv.org/abs/2602.07828", "authors": ["Charles Ye", "Jasmine Cui"], "title": "Efficient Representations are Controllable Representations", "comment": null, "summary": "What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.\n  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u7684\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u5728\u6b8b\u5dee\u6d41\u4e2d\u521b\u5efa16\u4e2a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u8fd9\u4e9b\u6807\u5fd7\u5728\u63a8\u7406\u65f6\u6210\u4e3a\u53ef\u63a7\u5236\u751f\u6210\u7684\u771f\u5b9e\u5185\u90e8\u7279\u5f81", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5148\u8bc6\u522b\u6a21\u578b\u7684\u7279\u5f81\u51e0\u4f55\u7ed3\u6784\u518d\u8fdb\u884c\u5e72\u9884\uff0c\u8fc7\u7a0b\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u66f4\u66b4\u529b\u7684\u65b9\u6cd5\u6765\u5b89\u88c5\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u7279\u5f81\u5230\u6a21\u578b\u6fc0\u6d3b\u4e2d", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u8bad\u7ec3\u51763072\u4e2a\u6b8b\u5dee\u6d41\u7ef4\u5ea6\u4e2d\u768416\u4e2a\u6210\u4e3a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u8fd9\u4e9b\u6807\u5fd7\u6307\u793a\u751f\u6210\u6240\u9700\u7684\u6982\u5ff5\u3002\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b66\u4f1a\u4f9d\u8d56\u8fd9\u4e9b\u6807\u5fd7", "result": "\u8fd9\u4e9b\u60f0\u6027\u6807\u5fd7\u6210\u4e3a\u771f\u5b9e\u7684\u5185\u90e8\u7279\u5f81\uff1a\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5f00\u5173\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u751f\u6210\u3002\u6a21\u578b\u4f1a\u6d88\u9664\u5197\u4f59\u7f16\u7801\uff0c\u4f9d\u8d56\u8fd9\u4e9b\u56fa\u5b9a\u4f4d\u7f6e\u7684\u7279\u5f81", "conclusion": "\u5f53\u7279\u5f81\u53ef\u9760\u5730\u5728\u56fa\u5b9a\u4f4d\u7f6e\u63d0\u4f9b\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u4f1a\u9010\u6e10\u6d88\u9664\u5176\u4ed6\u5730\u65b9\u7684\u5197\u4f59\u7f16\u7801\uff0c\u6a21\u578b\u4f1a\u4fb5\u8680\u81ea\u8eab\u7684\u66ff\u4ee3\u8868\u793a\u3002\u6a21\u578b\u7684\u6548\u7387\u538b\u529b\u662f\u53ef\u5229\u7528\u7684\u6760\u6746\uff0c\u53ef\u8bf1\u5bfc\u51fa\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u8868\u793a", "topic": "agent analysis"}}
{"id": "2602.08939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08939", "abs": "https://arxiv.org/abs/2602.08939", "authors": ["Longling Geng", "Andy Ouyang", "Theodore Wu", "Daphne Barretto", "Matthew John Hayes", "Rachael Cooper", "Yuqiao Zeng", "Sameer Vijay", "Gia Ancone", "Ankit Rai", "Matthew Wolfman", "Patrick Flanagan", "Edward Y. Chang"], "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse", "comment": "17 pages, 20 tables, figures", "summary": "LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench", "AI": {"tldr": "CausalT5K\u662f\u4e00\u4e2a\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u9636\u68af\u574d\u584c\u3001\u963f\u8c00\u6027\u6f02\u79fb\u548c\u9519\u8bef\u62d2\u7edd\uff0c\u901a\u8fc7\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u6307\u6807\u63ed\u793a\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "LLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u5b58\u5728\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\uff08\u5982\u963f\u8c00\u6027\u3001\u9636\u68af\u574d\u584c\u3001\u9519\u8bef\u6821\u51c6\u7684\u62d2\u7edd\uff09\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u80fd\u591f\u8fdb\u884c\u7cfb\u7edf\u8bca\u65ad\u7684\u57fa\u51c6\uff0c\u76f8\u5173\u6539\u8fdb\u8fdb\u5c55\u7f13\u6162\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u5168\u9762\u6d4b\u8bd5\u56e0\u679c\u63a8\u7406\u5173\u952e\u80fd\u529b\u7684\u8bca\u65ad\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86CausalT5K\u57fa\u51c6\uff0c\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\uff0c\u8986\u76d610\u4e2a\u9886\u57df\uff0c\u6d4b\u8bd5\u4e09\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u68c0\u6d4b\u9636\u68af\u574d\u584c\u3001\u62b5\u6297\u963f\u8c00\u6027\u6f02\u79fb\u3001\u751f\u6210\u660e\u667a\u62d2\u7edd\u3002\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff0c\u6d89\u53ca40\u540d\u9886\u57df\u4e13\u5bb6\u3001\u8fed\u4ee3\u4ea4\u53c9\u9a8c\u8bc1\u5468\u671f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c4\u5219\u3001LLM\u548c\u4eba\u5de5\u8bc4\u5206\u7684\u590d\u5408\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86Pearl\u7684\u56e0\u679c\u9636\u68af\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u63ed\u793a\u4e86\u56db\u8c61\u9650\u63a7\u5236\u666f\u89c2\uff0c\u5176\u4e2d\u9759\u6001\u5ba1\u8ba1\u7b56\u7565\u666e\u904d\u5931\u8d25\u3002\u57fa\u51c6\u80fd\u591f\u5c06\u6027\u80fd\u5206\u89e3\u4e3a\u5b9e\u7528\u6027\uff08\u654f\u611f\u6027\uff09\u548c\u5b89\u5168\u6027\uff08\u7279\u5f02\u6027\uff09\uff0c\u63ed\u793a\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "CausalT5K\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0c\u80fd\u591f\u7cfb\u7edf\u8bca\u65adLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u63a8\u8fdb\u53ef\u4fe1\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8bc4\u4f30\u5de5\u5177\u3002\u57fa\u51c6\u5c55\u793a\u4e86\u9759\u6001\u5ba1\u8ba1\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u52a8\u6001\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.07832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07832", "abs": "https://arxiv.org/abs/2602.07832", "authors": ["Xian Wu", "Kaijie Zhu", "Ying Zhang", "Lun Wang", "Wenbo Guo"], "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning", "comment": null, "summary": "Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.", "AI": {"tldr": "rePIRL\u662f\u4e00\u4e2a\u53d7\u9006\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u4e13\u5bb6\u7b56\u7565\u5047\u8bbe\u8981\u6c42\u6700\u5c0f\uff0c\u901a\u8fc7\u53cc\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5f3a\u5047\u8bbe\uff08\u5982\u9700\u8981\u5176\u5956\u52b1\u51fd\u6570\uff09\uff0c\u8981\u4e48\u5b58\u5728\u5185\u5728\u9650\u5236\uff08\u5982\u71b5\u5d29\u6e83\uff09\uff0c\u5bfc\u81f4\u5956\u52b1\u6a21\u578b\u6548\u679c\u5f31\u6216\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u53cc\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230LLM\u7684\u6311\u6218\uff0c\u7edf\u4e00\u5728\u7ebf\u548c\u79bb\u7ebfPRM\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u6807\u51c6\u5316\u6570\u5b66\u548c\u7f16\u7a0b\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793arePIRL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u51fa\u7684PRM\u53ef\u7528\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u4e3a\u56f0\u96be\u95ee\u9898\u63d0\u4f9b\u65e9\u671f\u4fe1\u53f7\u3002", "conclusion": "rePIRL\u80fd\u591f\u4ee5\u6700\u5c0f\u5047\u8bbe\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u7edf\u4e00\u6027\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u8bad\u7ec3\u65b9\u6848\u548c\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08948", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08948", "abs": "https://arxiv.org/abs/2602.08948", "authors": ["Chen Jin", "Ryutaro Tanno", "Tom Diethe", "Philip Teare"], "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute", "comment": null, "summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.", "AI": {"tldr": "CoRefine\u662f\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\uff08\u4ec5211k\u53c2\u6570\uff09\u5728\u51bb\u7ed3LLM\u4e0a\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u76f8\u6bd4512\u6837\u672c\u5e76\u884c\u89e3\u7801\u53ef\u51cf\u5c11\u7ea6190\u500dtoken\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524dLLM\u4f9d\u8d56\u5927\u89c4\u6a21\u5e76\u884c\u89e3\u7801\uff08\u5982512\u4e2a\u6837\u672c\uff09\u6765\u63d0\u5347\u63a8\u7406\u7cbe\u5ea6\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11token\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u5f15\u5165CoRefine\u65b9\u6cd5\uff0c\u5728\u51bb\u7ed3LLM\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u8f7b\u91cf\u7ea7Conv1D\u63a7\u5236\u5668\uff08\u4ec5211k\u53c2\u6570\uff09\u3002\u63a7\u5236\u5668\u5206\u6790\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u7684\u7f6e\u4fe1\u5ea6\uff0c\u51b3\u5b9a\u662f\u5426\u505c\u6b62\u3001\u91cd\u65b0\u68c0\u67e5\u6216\u5c1d\u8bd5\u4e0d\u540c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u81ea\u6211\u4fee\u6b63\u3002\u5e73\u5747\u6bcf\u4e2a\u95ee\u9898\u53ea\u97002.7\u6b21\u4f18\u5316\u6b65\u9aa4\u3002\u8fd8\u6269\u5c55\u4e3aCoRefine-Tree\uff0c\u4e00\u79cd\u6df7\u5408\u987a\u5e8f-\u5e76\u884c\u53d8\u4f53\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u548c\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\uff0c\u63a7\u5236\u5668\u5728\u81ea\u4fe1\u505c\u6b62\u65f6\u8fbe\u523092.6%\u7684\u7cbe\u786e\u5ea6\uff0c\u8868\u660e\u7f6e\u4fe1\u5ea6\u52a8\u6001\u53ef\u9760\u5730\u6307\u793a\u6b63\u786e\u6027\u800c\u65e0\u9700\u771f\u5b9e\u9a8c\u8bc1\u3002\u76f8\u6bd4512\u6837\u672c\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c11\u7ea6190\u500dtoken\u6d88\u8017\u3002", "conclusion": "CoRefine\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u63a7\u5236\u4fe1\u53f7\u800c\u975e\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u4e3a\u53ef\u6269\u5c55\u63a8\u7406\u548c\u5177\u6709\u4e0d\u5b8c\u7f8e\u9a8c\u8bc1\u5668\u7684\u667a\u80fd\u4f53\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u539f\u8bed\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u63a8\u7406\u7cbe\u5ea6\u7684\u826f\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2602.07848", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07848", "abs": "https://arxiv.org/abs/2602.07848", "authors": ["Shijie Wang", "Pengfei Li", "Yikun Fu", "Kaifeng Liu", "Fangyuan Li", "Yang Liu", "Xiaowei Sun", "Zonglin Li", "Siyao Zhao", "Jian Zhao", "Kai Tian", "Dong Li", "Junqi Gao", "Yutong Zhang", "Yiqun Chen", "Yuqiang Li", "Zoe Li", "Weinan Zhang", "Peng Ye", "Shuyue Hu", "Lei Bai", "Bowen Zhou", "Kaiyan Zhang", "Biqing Qi"], "title": "MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation", "comment": null, "summary": "While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.", "AI": {"tldr": "MARTI-MARS2\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u4e0e\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u4ece\u53c2\u6570\u5171\u4eab\u540c\u8d28\u591a\u89d2\u8272\u8bad\u7ec3\u5230\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6f14\u8fdb\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-5.1\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u671b\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002\u4f46\u73b0\u6709\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u6216\u540c\u8d28\u53c2\u6570\u8bad\u7ec3\u7684\u591a\u89d2\u8272\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faMARTI-MARS2\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u4e0e\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\u3002\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u8fed\u4ee3\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u5b9e\u73b0\u4ece\u540c\u8d28\u591a\u89d2\u8272\u8bad\u7ec3\u5230\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6f14\u8fdb\u3002\u540c\u65f6\u63d0\u51fa\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565MARTI-MARS2-T+\u3002", "result": "\u5728\u4e24\u4e2a32B\u6a21\u578b\u534f\u4f5c\u4e0b\uff0c\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523077.7%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-5.1\u7b49\u57fa\u7ebf\u3002\u63ed\u793a\u4e86\u65b0\u7684\u6269\u5c55\u89c4\u5f8b\uff1a\u4ece\u5355\u667a\u80fd\u4f53\u5230\u540c\u8d28\u591a\u89d2\u8272\u518d\u5230\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u8303\u5f0f\u9010\u6b65\u63d0\u9ad8RL\u6027\u80fd\u4e0a\u9650\u3001\u9c81\u68d2\u7684TTS\u80fd\u529b\u548c\u66f4\u5927\u7684\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "MARTI-MARS2\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7a81\u7834\u4e86\u5355\u667a\u80fd\u4f53\u80fd\u529b\u9650\u5236\uff0c\u8bc1\u660e\u4e86\u7b56\u7565\u591a\u6837\u6027\u5bf9\u4e8e\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u667a\u80fd\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2602.07873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07873", "abs": "https://arxiv.org/abs/2602.07873", "authors": ["Donghyeon Ki", "Hee-Jun Ahn", "Kyungyoon Kim", "Byung-Jun Lee"], "title": "Direct Soft-Policy Sampling via Langevin Dynamics", "comment": null, "summary": "Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.", "AI": {"tldr": "\u63d0\u51faNC-LQL\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u566a\u58f0\u6270\u52a8\u548c\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u5b9e\u73b0\u8f6f\u7b56\u7565\u91c7\u6837\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8868\u8fbe\u6027\u548c\u71b5\u4f30\u8ba1\u65b9\u9762\u7684\u9650\u5236", "motivation": "\u73b0\u6709\u8f6f\u7b56\u7565\u5b9e\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u53c2\u6570\u5316\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u96be\u4ee5\u4f30\u8ba1\u71b5\u503c\uff0c\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u5728\u9ad8\u7ef4\u975e\u51f8Q\u51fd\u6570\u4e2d\u6df7\u5408\u901f\u5ea6\u6162", "method": "\u63d0\u51fa\u566a\u58f0\u6761\u4ef6\u6717\u4e4b\u4e07Q\u5b66\u4e60(NC-LQL)\uff1a\u901a\u8fc7\u591a\u5c3a\u5ea6\u566a\u58f0\u6270\u52a8Q\u51fd\u6570\uff0c\u5b66\u4e60\u566a\u58f0\u6761\u4ef6Q\u51fd\u6570\uff0c\u5728\u5e73\u6ed1\u7684\u4ef7\u503c\u51fd\u6570\u666f\u89c2\u4e2d\u5b9e\u73b0\u4ece\u5168\u5c40\u63a2\u7d22\u5230\u7cbe\u786e\u6a21\u5f0f\u7ec6\u5316\u7684\u91c7\u6837", "result": "\u5728OpenAI Gym MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNC-LQL\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6269\u6563\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "NC-LQL\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u8f6f\u7b56\u7565\u5b9e\u73b0\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.09000", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09000", "abs": "https://arxiv.org/abs/2602.09000", "authors": ["Ali Hatamizadeh", "Shrimai Prabhumoye", "Igor Gitman", "Ximing Lu", "Seungju Han", "Wei Ping", "Yejin Choi", "Jan Kautz"], "title": "iGRPO: Self-Feedback-Driven LLM Reasoning", "comment": "Tech report", "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.", "AI": {"tldr": "iGRPO\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u751f\u6210\u7684\u8349\u7a3f\u8fdb\u884c\u52a8\u6001\u81ea\u6761\u4ef6\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8aGRPO\u5e76\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u96be\u4ee5\u4ea7\u751f\u51c6\u786e\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5bf9\u9f50\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982PPO\u9700\u8981\u4ef7\u503c\u51fd\u6570\uff0cGRPO\u867d\u7136\u9ad8\u6548\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "iGRPO\u662fGRPO\u7684\u4e24\u9636\u6bb5\u6269\u5c55\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u6837\u591a\u4e2a\u63a2\u7d22\u6027\u8349\u7a3f\u5e76\u9009\u62e9\u6700\u9ad8\u5956\u52b1\u7684\u8349\u7a3f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u6700\u4f73\u8349\u7a3f\u9644\u52a0\u5230\u539f\u59cb\u63d0\u793a\u4e2d\uff0c\u5728\u8349\u7a3f\u6761\u4ef6\u5316\u7684\u6539\u8fdb\u4e0a\u5e94\u7528GRPO\u98ce\u683c\u66f4\u65b0\uff0c\u8bad\u7ec3\u7b56\u7565\u8d85\u8d8a\u5176\u5148\u524d\u6700\u4f73\u5c1d\u8bd5\u3002", "result": "\u5728\u5339\u914d\u7684rollout\u9884\u7b97\u4e0b\uff0ciGRPO\u5728\u591a\u79cd\u57fa\u7840\u6a21\u578b\u4e0a\u4e00\u81f4\u4f18\u4e8eGRPO\u3002\u5e94\u7528\u4e8eOpenReasoning-Nemotron-7B\u5728AceReason-Math\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728AIME24\u548cAIME25\u4e0a\u5206\u522b\u8fbe\u523085.62%\u548c79.64%\u7684\u65b0SOTA\u7ed3\u679c\u3002", "conclusion": "\u8fed\u4ee3\u7684\u3001\u57fa\u4e8e\u81ea\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u8fdb\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0ciGRPO\u7684\u6539\u8fdb\u6846\u67b6\u53ef\u63a8\u5e7f\u5230GRPO\u53d8\u4f53\u4e4b\u5916\uff0c\u5e76\u901a\u8fc7\u5ef6\u8fdf\u71b5\u5d29\u6e83\u6539\u53d8\u5b66\u4e60\u52a8\u6001\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07889", "abs": "https://arxiv.org/abs/2602.07889", "authors": ["Long Chen", "Yinkui Liu", "Shen Li", "Bo Tang", "Xuemin Hu"], "title": "Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning", "comment": null, "summary": "Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u79bb\u7ebfRL\u53cd\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7801\u672cVQVAE\u79bb\u6563\u5316\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u7ed3\u5408FCM\u805a\u7c7b\u66f4\u65b0\u7801\u672c\uff0c\u89e3\u51b3\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5728D4RL\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8eSOTA\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u4e2d\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u8fdb\u884c\u8ba1\u6570\uff0c\u4f46\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u6548\u7387\u964d\u4f4e\u3001\u6027\u80fd\u4e0b\u964d\u751a\u81f3\u7b56\u7565\u5b66\u4e60\u5931\u8d25\u3002", "method": "1) \u57fa\u4e8e\u591a\u7801\u672cVQVAE\u7684\u9ad8\u6548\u4f2a\u8ba1\u6570\u65b9\u6cd5\u79bb\u6563\u5316\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff1b2) \u57fa\u4e8e\u8be5\u4f2a\u8ba1\u6570\u65b9\u6cd5\u7684\u79bb\u7ebfRL\u53cd\u5229\u7528\u65b9\u6cd5\uff1b3) \u57fa\u4e8e\u6a21\u7ccaC\u5747\u503c(FCM)\u805a\u7c7b\u7684\u7801\u672c\u66f4\u65b0\u673a\u5236\u63d0\u9ad8\u7801\u672c\u5411\u91cf\u5229\u7528\u7387\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.07906", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07906", "abs": "https://arxiv.org/abs/2602.07906", "authors": ["Yuzhu Cai", "Zexi Liu", "Xinyu Zhu", "Cheng Wang", "Jiaao Chen", "Hanrui Wang", "Wei-Chen Wang", "Di Jin", "Siheng Chen"], "title": "AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering", "comment": "17 pages, 5 figures", "summary": "Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.", "AI": {"tldr": "\u63d0\u51faAceGRPO\u6846\u67b6\uff0c\u901a\u8fc7\u6f14\u5316\u6570\u636e\u7f13\u51b2\u533a\u548c\u81ea\u9002\u5e94\u91c7\u6837\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u884c\u4e3a\u505c\u6ede\u95ee\u9898\uff0c\u8bad\u7ec3\u51fa\u7684Ace-30B\u6a21\u578b\u5728MLE-Bench-Lite\u4e0a\u8fbe\u5230100%\u6709\u6548\u63d0\u4ea4\u7387", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684MLE\u4ee3\u7406\u5b58\u5728\u884c\u4e3a\u505c\u6ede\u95ee\u9898\uff0c\u56e0\u4e3a\u53c2\u6570\u88ab\u51bb\u7ed3\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u89e3\u51b3\uff0c\u4f46\u5728MLE\u5e94\u7528\u4e2d\u9762\u4e34\u6267\u884c\u5ef6\u8fdf\u9ad8\u548c\u6570\u636e\u9009\u62e9\u6548\u7387\u4f4e\u7684\u95ee\u9898", "method": "\u63d0\u51faAceGRPO\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u6f14\u5316\u6570\u636e\u7f13\u51b2\u533a\uff0c\u5c06\u6267\u884c\u8f68\u8ff9\u91cd\u65b0\u5229\u7528\u4e3a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u8bad\u7ec3\u4efb\u52a1\uff1b2) \u81ea\u9002\u5e94\u91c7\u6837\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u6027\u6f5c\u529b\u51fd\u6570\u52a8\u6001\u4f18\u5148\u5904\u7406\u4ee3\u7406\u5b66\u4e60\u524d\u6cbf\u7684\u4efb\u52a1", "result": "\u8bad\u7ec3\u7684Ace-30B\u6a21\u578b\u5728MLE-Bench-Lite\u4e0a\u8fbe\u5230100%\u6709\u6548\u63d0\u4ea4\u7387\uff0c\u63a5\u8fd1\u524d\u6cbf\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff08\u5982DeepSeek-V3.2\uff09", "conclusion": "AceGRPO\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u91cd\u7528\u548c\u4efb\u52a1\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4ee3\u7406\u7684\u6301\u7eed\u8fed\u4ee3\u4f18\u5316\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2602.08041", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08041", "abs": "https://arxiv.org/abs/2602.08041", "authors": ["Boyang Xia", "Weiyou Tian", "Qingnan Ren", "Jiaqi Huang", "Jie Xiao", "Shuo Lu", "Kai Wang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments", "comment": null, "summary": "Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.", "AI": {"tldr": "\u63d0\u51faISO\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u4e0a\u4e0b\u6587\u6765\u4f18\u5316LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u6218\u7565\u5956\u52b1\u6a21\u578b\u548c\u4e50\u89c2\u5b66\u4e60\u89c4\u5219\uff0c\u5728\u5fb7\u5dde\u6251\u514b\u548c\u5b9d\u53ef\u68a6\u6e38\u620f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\uff0c\u4f20\u7edf\u7684\u77ed\u671f\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u80dc\u7387\uff09\u65e0\u6cd5\u6355\u6349\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6218\u7565\u5916\u90e8\u6027\uff0c\u5bfc\u81f4\u5373\u4f7f\u52a8\u6001\u53ef\u9884\u6d4b\u65f6\uff0c\u57fa\u4e8e\u53d8\u5f02\u7684\u9057\u61be\u5206\u6790\u4e5f\u53d8\u5f97\u65e0\u6548\u3002", "method": "\u63d0\u51fa\u9690\u5f0f\u6218\u7565\u4f18\u5316\uff08ISO\uff09\u6846\u67b6\uff1a1\uff09\u6218\u7565\u5956\u52b1\u6a21\u578b\uff08SRM\uff09\u4f30\u8ba1\u884c\u52a8\u7684\u957f\u671f\u6218\u7565\u4ef7\u503c\uff1b2\uff09iso-grpo\uff0c\u4e00\u79cd\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e50\u89c2\u5b66\u4e60\u89c4\u5219\uff0c\u667a\u80fd\u4f53\u9884\u6d4b\u5f53\u524d\u6218\u7565\u4e0a\u4e0b\u6587\u5e76\u5728\u7ebf\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5f53\u9884\u6d4b\u8bef\u5dee\u6709\u754c\u65f6\uff0c\u83b7\u5f97\u4e9a\u7ebf\u6027\u4e0a\u4e0b\u6587\u9057\u61be\u548c\u5747\u8861\u6536\u655b\u4fdd\u8bc1\uff0c\u6062\u590d\u9759\u6001\u6e38\u620f\u901f\u7387\u3002\u5b9e\u9a8c\uff1a\u57286\u4eba\u65e0\u9650\u6ce8\u5fb7\u5dde\u6251\u514b\u548c\u7ade\u4e89\u6027\u5b9d\u53ef\u68a6\u6e38\u620f\u4e2d\uff0c\u957f\u671f\u56de\u62a5\u6301\u7eed\u4f18\u4e8e\u5f3aLLM\u548cRL\u57fa\u7ebf\uff0c\u5728\u53d7\u63a7\u9884\u6d4b\u566a\u58f0\u4e0b\u8868\u73b0\u4f18\u96c5\u9000\u5316\u3002", "conclusion": "ISO\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u4e0a\u4e0b\u6587\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u6218\u7565\u5916\u90e8\u6027\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08194", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08194", "abs": "https://arxiv.org/abs/2602.08194", "authors": ["Konstantinos Mitsides", "Maxence Faldor", "Antoine Cully"], "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds", "comment": "11 pages (main text), 90 pages total. Project page: https://konstantinosmitsides.github.io/dreaming-in-code", "summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.", "AI": {"tldr": "Dreaming in Code (DiCode) \u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4ee3\u7801\u5c42\u9762\u521b\u9020\u73af\u5883\u53d8\u4f53\u6765\u6784\u5efa\u5b66\u4e60\u8def\u5f84\uff0c\u5e2e\u52a9\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u83b7\u5f97\u957f\u671f\u6280\u80fd\u3002", "motivation": "\u5728\u590d\u6742\u7684\u5f00\u653e\u4e16\u754c\u4e2d\uff0c\u5de8\u5927\u7684\u7ec4\u5408\u7a7a\u95f4\u4f7f\u5f97\u667a\u80fd\u4f53\u96be\u4ee5\u53d1\u73b0\u6301\u7eed\u53ef\u5b66\u4e60\u7ecf\u9a8c\u5e8f\u5217\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u5b64\u7acb\u884c\u4e3a\u7684\u53d1\u73b0\uff0c\u800c\u975e\u7ec4\u7ec7\u6301\u7eed\u8fdb\u6b65\u3002", "method": "\u63d0\u51faDiCode\u6846\u67b6\uff0c\u8ba9\u57fa\u7840\u6a21\u578b\u5408\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\u6765\u642d\u5efa\u5b66\u4e60\u8def\u5f84\u3002\"\u68a6\u60f3\"\u8868\u73b0\u4e3a\u5728\u4ee3\u7801\u5c42\u9762\u521b\u9020\u4e16\u754c\u53d8\u4f53\uff0c\u5728Craftax\u57fa\u51c6\u4e0a\u5b9e\u4f8b\u5316\u3002", "result": "DiCode\u4f7f\u667a\u80fd\u4f53\u83b7\u5f97\u957f\u671f\u6280\u80fd\uff0c\u5e73\u5747\u56de\u62a5\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u9ad816%\uff0c\u5728\u540e\u671f\u6218\u6597\u4efb\u52a1\u4e0a\u53d6\u5f97\u975e\u96f6\u6210\u529f\u7387\uff08\u5148\u524d\u65b9\u6cd5\u5931\u8d25\uff09\u3002", "conclusion": "\u4ee3\u7801\u7ea7\u73af\u5883\u8bbe\u8ba1\u4e3a\u8bfe\u7a0b\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\uff0c\u80fd\u591f\u6784\u5efa\u8fde\u63a5\u5f00\u653e\u4e16\u754c\u4e2d\u80fd\u529b\u5dee\u8ddd\u7684\u4e2d\u95f4\u73af\u5883\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08003", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08003", "abs": "https://arxiv.org/abs/2602.08003", "authors": ["Yigit Turkmen", "Baturalp Buyukates", "Melih Bastopcu"], "title": "Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection", "comment": null, "summary": "Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u8d2a\u5a6a\u9009\u62e9\u7b97\u6cd5\uff0c\u5728\u67e5\u8be2\u9884\u7b97\u4e0b\u6784\u5efaLLM\u96c6\u6210\uff0c\u89e3\u51b3\u6a21\u578b\u5f3a\u76f8\u5173\u65f6\u5982\u4f55\u9009\u62e9\u6700\u4f18\u6a21\u578b\u7ec4\u5408\u7684\u95ee\u9898\u3002", "motivation": "LLM\u96c6\u6210\u901a\u5e38\u7528\u4e8e\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5b9e\u8df5\u4e2d\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5728\u5f62\u6210LLM\u96c6\u6210\u65f6\u5e94\u8be5\u9009\u62e9\u54ea\u4e9b\u6a21\u578b\uff1f\u540c\u65f6\u9700\u8981\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5373\u4f7f\u4f7f\u7528\u591a\u4e2a\u6a21\u578b\uff0c\u6027\u80fd\u4e5f\u53ef\u80fd\u9971\u548c\u3002", "method": "\u5c06\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u96c6\u6210\u9009\u62e9\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6700\u5927\u5316\u771f\u5b9e\u6807\u7b7e\u4e0e\u6240\u9009\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u4f7f\u7528\u9ad8\u65afcopula\u5efa\u6a21\u6a21\u578b\u7684\u76f8\u5173\u8bef\u5dee\uff0c\u5c55\u793a\u96c6\u6210\u6027\u80fd\u7684\u4fe1\u606f\u8bba\u8bef\u5dee\u4e0b\u9650\u3002\u63d0\u51fa\u7b80\u5355\u7684\u8d2a\u5a6a\u4e92\u4fe1\u606f\u9009\u62e9\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4f30\u8ba1\u6240\u9700\u4fe1\u606f\u9879\uff0c\u5728\u67e5\u8be2\u9884\u7b97\u4e0b\u8fed\u4ee3\u6784\u5efa\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08MEDMCQA\u3001MMLU\u548cIMDB\u7535\u5f71\u8bc4\u8bba\uff09\u4e0a\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u67e5\u8be2\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u8d2a\u5a6a\u9009\u62e9\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3LLM\u96c6\u6210\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u96c6\u6210\u6027\u80fd\u9971\u548c\u7684\u73b0\u8c61\u3002", "topic": "agent analysis"}}
{"id": "2602.08377", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08377", "abs": "https://arxiv.org/abs/2602.08377", "authors": ["Bilgehan Sel", "Vaishakh Keshava", "Phillip Wallis", "Lukas Rutishauser", "Ming Jin", "Dingcheng Li"], "title": "Reinforcement Learning with Backtracking Feedback", "comment": "NeurIPS 2025", "summary": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.", "AI": {"tldr": "RLBF\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9LLM\u5b66\u4f1a\u52a8\u6001\u56de\u6eaf\u7ea0\u6b63\u81ea\u8eab\u751f\u6210\u9519\u8bef\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u57fa\u7840\u6548\u7528\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u65f6\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u65b9\u6cd5\u5982BSAFE\u7b49\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u673a\u5236\u6765\u5e94\u5bf9\u590d\u6742\u7684\u5bf9\u6297\u7b56\u7565\uff08\u5982\u4e2d\u95f4\u586b\u5145\u3001GCG\u653b\u51fb\u3001\u89e3\u7801\u53c2\u6570\u64cd\u7eb5\u7b49\uff09\u3002", "method": "\u63d0\u51faRLBF\u6846\u67b6\uff1a1) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u901a\u8fc7critic\u5bf9\u6a21\u578b\u5b9e\u65f6\u8f93\u51fa\u63d0\u4f9b\u53cd\u9988\uff0c\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u5b89\u5168\u8fdd\u89c4\u5e76\u53d1\u51fa\"\u56de\u6eafx\u4e2atoken\"\u4fe1\u53f7\uff0c\u7136\u540e\u7ee7\u7eed\u81ea\u56de\u5f52\u751f\u6210\uff1b2) \u6539\u8fdb\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u751f\u6210\u7b56\u7565(BSAFE+)\uff1a\u5728\u539f\u672c\u5b89\u5168\u7684\u8fde\u8d2f\u6587\u672c\u4e2d\u6ce8\u5165\u8fdd\u89c4\u5185\u5bb9\uff0c\u4e3a\u56de\u6eaf\u673a\u5236\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u521d\u59cb\u8bad\u7ec3\u3002", "result": "RLBF\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u4f18\u8d8a\u7684\u5b89\u5168\u6548\u679c\uff0c\u540c\u65f6\u5173\u952e\u5730\u4fdd\u6301\u4e86\u57fa\u7840\u6a21\u578b\u7684\u529f\u80fd\u6548\u7528\u3002", "conclusion": "RLBF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u56de\u6eaf\u7ea0\u6b63\u548c\u6539\u8fdb\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u751f\u6210\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u5bf9\u6297\u653b\u51fb\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08489", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08489", "abs": "https://arxiv.org/abs/2602.08489", "authors": ["Hyunseok Lee", "Soheil Abbasloo", "Jihoon Tack", "Jinwoo Shin"], "title": "Beyond Correctness: Learning Robust Reasoning via Transfer", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.", "AI": {"tldr": "RLTR\uff08\u53ef\u8f6c\u79fb\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u901a\u8fc7\u6d4b\u8bd5\u90e8\u5206\u63a8\u7406\u524d\u7f00\u80fd\u5426\u6307\u5bfc\u5176\u4ed6\u6a21\u578b\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u6765\u589e\u5f3aLLM\u63a8\u7406\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4RLVR\u5728\u66f4\u5c11\u8bad\u7ec3\u6b65\u9aa4\u4e0b\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8ba4\u4e3a\u7a33\u5065\u7684\u63a8\u7406\u5e94\u8be5\u80fd\u591f\u8d85\u8d8a\u4ea7\u751f\u5b83\u7684\u601d\u7ef4\uff0c\u5177\u6709\u53ef\u8f6c\u79fb\u6027\uff0c\u80fd\u5728\u622a\u65ad\u3001\u91cd\u65b0\u89e3\u91ca\u548c\u5ef6\u7eed\u540e\u4f9d\u7136\u6709\u6548\u3002", "method": "\u63d0\u51faRLTR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u8f6c\u79fb\u5956\u52b1\u6765\u64cd\u4f5c\u5316\u9c81\u68d2\u6027\uff1a\u6d4b\u8bd5\u4ece\u4e00\u4e2a\u6a21\u578b\u63d0\u53d6\u7684\u90e8\u5206\u63a8\u7406\u524d\u7f00\u662f\u5426\u80fd\u6307\u5bfc\u53e6\u4e00\u4e2a\u72ec\u7acb\u6a21\u578b\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u3002\u8fd9\u9f13\u52b1LLM\u4ea7\u751f\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u3002", "result": "\u5728MATH500\u4e0a\uff0cRLTR\u76f8\u6bd4RLVR\u5728Maj@64\u6307\u6807\u4e0a\u83b7\u5f97+3.6%\u7684\u63d0\u5347\uff0c\u5e76\u4e14\u7528\u5927\u7ea62.5\u500d\u66f4\u5c11\u7684\u8bad\u7ec3\u6b65\u9aa4\u5c31\u8fbe\u5230\u4e86RLVR\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u91c7\u6837\u4e00\u81f4\u6027\u3002", "conclusion": "RLTR\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08032", "abs": "https://arxiv.org/abs/2602.08032", "authors": ["Lior Cohen", "Ofir Nabati", "Kaixin Wang", "Navdeep Kumar", "Shie Mannor"], "title": "Horizon Imagination: Efficient On-Policy Training in Diffusion World Models", "comment": "This paper will be published in the ICLR 2026 proceedings", "summary": "We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.", "AI": {"tldr": "\u63d0\u51faHorizon Imagination\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u53bb\u566a\u591a\u4e2a\u672a\u6765\u89c2\u6d4b\uff0c\u5728\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6269\u6563\u4e16\u754c\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u8981\u4e48\u9700\u8981\u91cd\u578b\u6a21\u578b\u63a8\u7406\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u5ea6\u5e8f\u5217\u5316\u7684\u60f3\u8c61\u8fc7\u7a0b\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8", "method": "\u63d0\u51faHorizon Imagination\u65b9\u6cd5\uff0c\u5305\u542b\u7a33\u5b9a\u5316\u673a\u5236\u548c\u65b0\u578b\u91c7\u6837\u8c03\u5ea6\uff0c\u652f\u6301\u5e76\u884c\u53bb\u566a\u591a\u4e2a\u672a\u6765\u89c2\u6d4b\uff0c\u5c06\u53bb\u566a\u9884\u7b97\u4e0e\u6709\u6548\u89c6\u91ce\u89e3\u8026\uff0c\u5e76\u652f\u6301\u5b50\u5e27\u9884\u7b97", "result": "\u5728Atari 100K\u548cCraftium\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u4e00\u534a\u53bb\u566a\u6b65\u6570\u7684\u5b50\u5e27\u9884\u7b97\u4e0b\u4fdd\u6301\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u8c03\u5ea6\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u751f\u6210\u8d28\u91cf", "conclusion": "Horizon Imagination\u4e3a\u6269\u6563\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u60f3\u8c61\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.08819", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08819", "abs": "https://arxiv.org/abs/2602.08819", "authors": ["Jiwoo Hong", "Shao Tang", "Zhipeng Wang"], "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models", "comment": "Preprint", "summary": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u4e0a\u4e0b\u6587\u5956\u52b1\u5efa\u6a21\uff08ICRM\uff09\uff0c\u4e00\u79cd\u8d1d\u53f6\u65af\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u53ef\u63a7\u6027\uff0c\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u504f\u597d\u5206\u5e03\u3002", "motivation": "\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u7b49\u573a\u666f\uff0c\u5956\u52b1\u6a21\u578b\u9700\u8981\u7f16\u7801\u66f4\u590d\u6742\u3001\u591a\u65b9\u9762\u7684\u504f\u597d\u5206\u5e03\u3002\u4f46\u4f20\u7edf\u7684\u5206\u7c7b\u5668\u5956\u52b1\u6a21\u578b\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\u5c31\u4fdd\u6301\u9759\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u65f6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u4e0a\u4e0b\u6587\u5956\u52b1\u5efa\u6a21\uff08ICRM\uff09\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u89c6\u4e3a\u5728Bradley-Terry\u6a21\u578b\u4e0b\u4f7f\u7528\u5171\u8f6dBeta\u5148\u9a8c\u5bf9\u6f5c\u5728\u504f\u597d\u6982\u7387\u8fdb\u884c\u644a\u9500\u53d8\u5206\u63a8\u65ad\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u53ef\u63a7\u6027\u3002", "result": "ICRM\u5728\u5355\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u968f\u7740\u66f4\u591a\u4e0a\u4e0b\u6587\u6f14\u793a\uff0c\u5728SafeRLHF\u4e0a\u83b7\u5f9734%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5728RM-Bench\u4e0a\u83b7\u5f979%\u51c6\u786e\u7387\u63d0\u5347\uff1b\u5728\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u5728\u6709\u7528\u6027\u548c\u62d2\u7edd\u57fa\u51c6\u4e0a\u5c06\u5e15\u7d2f\u6258\u524d\u6cbf\u6269\u5c55\u4e864%\u7684\u8d85\u4f53\u79ef\u589e\u76ca\u3002\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cICRM\u80fd\u6709\u6548\u7f16\u7801\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "ICRM\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u9002\u5e94\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u6f14\u793a\u5728\u6d4b\u8bd5\u65f6\u8c03\u6574\u504f\u597d\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08964", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08964", "abs": "https://arxiv.org/abs/2602.08964", "authors": ["Raghu Arghal", "Fade Chen", "Niall Dalton", "Evgenii Kortukov", "Calum McNamara", "Angelos Nalmpantis", "Moksh Nirvaan", "Gabriele Sarti", "Mario Giulianelli"], "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents", "comment": null, "summary": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u884c\u4e3a\u8bc4\u4f30\u4e0e\u53ef\u89e3\u91ca\u6027\u5206\u6790\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u76ee\u6807\u5bfc\u5411\u6027\uff0c\u901a\u8fc7LLM\u667a\u80fd\u4f53\u57282D\u7f51\u683c\u4e16\u754c\u4e2d\u7684\u5bfc\u822a\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u5176\u5185\u90e8\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7a7a\u95f4\u5730\u56fe\uff0c\u63a8\u7406\u8fc7\u7a0b\u91cd\u7ec4\u8868\u5f81\u4ece\u73af\u5883\u7ed3\u6784\u7ebf\u7d22\u8f6c\u5411\u52a8\u4f5c\u9009\u62e9\u4fe1\u606f\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5c06\u76ee\u6807\u5f52\u56e0\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u9700\u8981\u5efa\u7acb\u8bc4\u4f30\u76ee\u6807\u5bfc\u5411\u6027\u7684\u6846\u67b6\uff0c\u8d85\u8d8a\u5355\u7eaf\u7684\u884c\u4e3a\u8bc4\u4f30\uff0c\u7406\u89e3\u667a\u80fd\u4f53\u5982\u4f55\u5185\u90e8\u8868\u5f81\u548c\u8ffd\u6c42\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u6574\u5408\u884c\u4e3a\u8bc4\u4f30\u4e0e\u53ef\u89e3\u91ca\u6027\u5206\u6790\u7684\u6846\u67b6\u3002\u6848\u4f8b\u7814\u7a76\u4e2d\u4f7f\u7528LLM\u667a\u80fd\u4f53\u57282D\u7f51\u683c\u4e16\u754c\u4e2d\u5bfc\u822a\uff0c\u884c\u4e3a\u8bc4\u4f30\u5bf9\u6bd4\u6700\u4f18\u7b56\u7565\u5728\u4e0d\u540c\u7f51\u683c\u5927\u5c0f\u3001\u969c\u788d\u5bc6\u5ea6\u548c\u76ee\u6807\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\uff1b\u4f7f\u7528\u63a2\u6d4b\u65b9\u6cd5\u89e3\u7801\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u72b6\u6001\u548c\u591a\u6b65\u884c\u52a8\u8ba1\u5212\u7684\u5185\u90e8\u8868\u5f81\u3002", "result": "\u884c\u4e3a\u4e0a\uff0c\u667a\u80fd\u4f53\u6027\u80fd\u968f\u4efb\u52a1\u96be\u5ea6\u6269\u5c55\u4f46\u4fdd\u6301\u7a33\u5065\uff1b\u5185\u90e8\u8868\u5f81\u4e0a\uff0cLLM\u667a\u80fd\u4f53\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7684\u7c97\u7565\u7a7a\u95f4\u5730\u56fe\uff0c\u4fdd\u7559\u4f4d\u7f6e\u548c\u76ee\u6807\u7684\u8fd1\u4f3c\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\uff1b\u63a8\u7406\u8fc7\u7a0b\u91cd\u7ec4\u8868\u5f81\uff0c\u4ece\u73af\u5883\u7ed3\u6784\u7ebf\u7d22\u8f6c\u5411\u652f\u6301\u5373\u65f6\u52a8\u4f5c\u9009\u62e9\u7684\u4fe1\u606f\u3002", "conclusion": "\u4ec5\u9760\u884c\u4e3a\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u8868\u5f81\u667a\u80fd\u4f53\u5982\u4f55\u8868\u5f81\u548c\u8ffd\u6c42\u76ee\u6807\uff0c\u9700\u8981\u5185\u7701\u5f0f\u68c0\u67e5\u6765\u7406\u89e3\u667a\u80fd\u4f53\u7684\u76ee\u6807\u5bfc\u5411\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.08054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08054", "abs": "https://arxiv.org/abs/2602.08054", "authors": ["Manan Tayal", "Mumuksh Tayal"], "title": "Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning", "comment": "23 pages, 8 figures", "summary": "Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.", "AI": {"tldr": "EpiFlow\u662f\u4e00\u4e2a\u5b89\u5168\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u8054\u5408\u4f18\u5316\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u4f7f\u7528epigraph\u503c\u51fd\u6570\u6307\u5bfc\u7b56\u7565\u5408\u6210\uff0c\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u56de\u62a5\u548c\u63a5\u8fd1\u96f6\u7684\u5b89\u5168\u8fdd\u89c4\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5f3a\u5b89\u5168\u6027\u548c\u9ad8\u6027\u80fd\u3002\u73b0\u6709\u5b89\u5168\u79bb\u7ebfRL\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5141\u8bb8\u8fdd\u89c4\u7684\u8f6f\u7ea6\u675f\uff0c\u8981\u4e48\u5f15\u5165\u8fc7\u5ea6\u4fdd\u5b88\u6027\uff0c\u6216\u8005\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u3001\u5956\u52b1\u4f18\u5316\u548c\u6570\u636e\u5206\u5e03\u9075\u5faa\u3002", "method": "\u63d0\u51faEpigraph-Guided Flow Matching (EpiFlow)\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u79bb\u7ebfRL\u8868\u8ff0\u4e3a\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002\u5b66\u4e60\u4ece\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684epigraph\u91cd\u6784\u5bfc\u51fa\u7684\u53ef\u884c\u6027\u503c\u51fd\u6570\uff0c\u907f\u514d\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u89e3\u8026\u76ee\u6807\u6216\u540e\u5904\u7406\u8fc7\u6ee4\u3002\u7b56\u7565\u901a\u8fc7\u57fa\u4e8eepigraph\u503c\u51fd\u6570\u91cd\u65b0\u52a0\u6743\u884c\u4e3a\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6d41\u5339\u914d\u62df\u5408\u751f\u6210\u7b56\u7565\u6765\u5408\u6210\u3002", "result": "\u5728\u5404\u79cd\u5b89\u5168\u5173\u952e\u4efb\u52a1\uff08\u5305\u62ecSafety-Gymnasium\u57fa\u51c6\u6d4b\u8bd5\uff09\u4e2d\uff0cEpiFlow\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u56de\u62a5\u548c\u63a5\u8fd1\u96f6\u7684\u7ecf\u9a8c\u5b89\u5168\u8fdd\u89c4\uff0c\u8bc1\u660e\u4e86epigraph\u5f15\u5bfc\u7b56\u7565\u5408\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "EpiFlow\u901a\u8fc7epigraph\u5f15\u5bfc\u7684\u6d41\u5339\u914d\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b89\u5168\u79bb\u7ebfRL\u4e2d\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08082", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.08082", "abs": "https://arxiv.org/abs/2602.08082", "authors": ["Valentin No\u00ebl"], "title": "Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology", "comment": "32 pages, 2 fgures, 18 tables", "summary": "Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\\% recall with multi-feature detection and 86.1\\% recall with 81.0\\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u62d3\u6251\u8c31\u5206\u6790\u7684\u514d\u8bad\u7ec3\u62a4\u680f\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u81ea\u4e3b\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u5931\u8d25\u548c\u5e7b\u89c9\uff0c\u5728Llama\u548cMistral\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387\u68c0\u6d4b\uff0c\u53d1\u73b0\u5e7b\u89c9\u65f6\u6a21\u578b\u6ce8\u610f\u529b\u53d8\u4e3a\u566a\u58f0\u72b6\u6001\u3002", "motivation": "\u5728\u91ce\u5916\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\u9700\u8981\u53ef\u9760\u7684\u4fdd\u969c\u673a\u5236\u6765\u9632\u6b62\u5de5\u5177\u4f7f\u7528\u5931\u8d25\u3002\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u62d3\u6251\u7684\u8c31\u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u5c42\u8c31\u7279\u5f81\u4f5c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u5668\uff0c\u5305\u62ec\u5e73\u6ed1\u5ea6\u548c\u71b5\u7b49\u7279\u5f81\uff0c\u901a\u8fc7\u9608\u503c\u68c0\u6d4b\u6a21\u578b\u5931\u8d25\u72b6\u6001\u3002", "result": "\u5728Llama 3.1 8B\u4e0a\uff0c\u591a\u7279\u5f81\u68c0\u6d4b\u8fbe\u523097.7%\u53ec\u56de\u7387\uff0c\u5e73\u8861\u90e8\u7f72\u65f6\u8fbe\u523086.1%\u53ec\u56de\u7387\u548c81.0%\u7cbe\u786e\u7387\u3002\u5355\u5c42\u8c31\u7279\u5f81\u68c0\u6d4b\u6548\u679c\u663e\u8457\uff1aLlama L26\u5e73\u6ed1\u5ea6\u8fbe\u523098.2%\u53ec\u56de\u7387\uff0cMistral L3\u71b5\u8fbe\u523094.7%\u53ec\u56de\u7387\u3002\u8de8\u6a21\u578b\u8bc4\u4f30\u53d1\u73b0\"\u5927\u58f0\u8bf4\u8c0e\u8005\"\u73b0\u8c61\uff1aLlama 3.1 8B\u7684\u5931\u8d25\u5728\u8c31\u4e0a\u66f4\u5bb9\u6613\u68c0\u6d4b\u3002", "conclusion": "\u8c31\u5206\u6790\u4e3a\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u8868\u660e\u5e7b\u89c9\u4e0d\u4ec5\u662f\u9519\u8bef\u6807\u8bb0\uff0c\u800c\u662f\u70ed\u529b\u5b66\u72b6\u6001\u53d8\u5316\uff1a\u6a21\u578b\u51fa\u9519\u65f6\u6ce8\u610f\u529b\u53d8\u4e3a\u566a\u58f0\u3002", "topic": "agent analysis"}}
{"id": "2602.08234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08234", "abs": "https://arxiv.org/abs/2602.08234", "authors": ["Peng Xia", "Jianwen Chen", "Hanyang Wang", "Jiaqi Liu", "Kaide Zeng", "Yu Wang", "Siwei Han", "Yiyang Zhou", "Xujiang Zhao", "Haifeng Chen", "Zeyu Zheng", "Cihang Xie", "Huaxiu Yao"], "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "AI": {"tldr": "SkillRL\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u6f14\u5316\u6846\u67b6\uff0c\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\u5e93\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u4e3b\u8981\u5b58\u50a8\u539f\u59cb\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u8f68\u8ff9\u901a\u5e38\u5197\u4f59\u4e14\u566a\u58f0\u591a\uff0c\u65e0\u6cd5\u63d0\u53d6\u9ad8\u7ea7\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faSkillRL\u6846\u67b6\uff1a1) \u57fa\u4e8e\u7ecf\u9a8c\u7684\u84b8\u998f\u673a\u5236\u6784\u5efa\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff1b2) \u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u83b7\u53d6\u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\uff1b3) \u9012\u5f52\u6f14\u5316\u673a\u5236\u4f7f\u6280\u80fd\u5e93\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u534f\u540c\u8fdb\u5316", "result": "\u5728ALFWorld\u3001WebShop\u548c\u4e03\u4e2a\u641c\u7d22\u589e\u5f3a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u5f3a\u57fa\u7ebf15.3%\u4ee5\u4e0a\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "SkillRL\u901a\u8fc7\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6280\u80fd\uff0c\u663e\u8457\u51cf\u5c11token\u5360\u7528\u540c\u65f6\u589e\u5f3a\u63a8\u7406\u6548\u7528\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u7ecf\u9a8c\u5b66\u4e60\u548c\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.08244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08244", "abs": "https://arxiv.org/abs/2602.08244", "authors": ["Juncheng Dong", "Bowen He", "Moyang Guo", "Ethan X. Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers", "comment": null, "summary": "In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.", "AI": {"tldr": "\u63d0\u51faICPRL\u65b0\u8303\u5f0f\uff0c\u4ec5\u4f9d\u8d56\u504f\u597d\u53cd\u9988\u8fdb\u884c\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u5956\u52b1\u76d1\u7763\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e0e\u6709\u5956\u52b1\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u65f6\u9700\u8981\u660e\u786e\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u5728\u5956\u52b1\u6a21\u7cca\u3001\u96be\u4ee5\u6307\u5b9a\u6216\u83b7\u53d6\u6210\u672c\u9ad8\u65f6\u9650\u5236\u4e86\u5e94\u7528\u3002\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u4ec5\u4f9d\u8d56\u504f\u597d\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51faICPRL\u8303\u5f0f\uff0c\u5305\u542b\u4e24\u79cd\u53d8\u4f53\uff1a\u57fa\u4e8e\u6bcf\u6b65\u504f\u597d\u7684I-PRL\u548c\u57fa\u4e8e\u8f68\u8ff9\u7ea7\u6bd4\u8f83\u7684T-PRL\u3002\u9996\u5148\u9a8c\u8bc1\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u504f\u597d\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7136\u540e\u5f15\u5165\u504f\u597d\u539f\u751f\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u504f\u597d\u6570\u636e\u4f18\u5316\u7b56\u7565\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\u6216\u6700\u4f18\u52a8\u4f5c\u6807\u7b7e\u3002", "result": "\u5728\u51b3\u6597\u8d4c\u535a\u673a\u3001\u5bfc\u822a\u548c\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cICPRL\u80fd\u591f\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\uff0c\u6027\u80fd\u4e0e\u6709\u5b8c\u6574\u5956\u52b1\u76d1\u7763\u7684ICRL\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ICPRL\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u8fdb\u884c\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5956\u52b1\u4fe1\u53f7\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08272", "abs": "https://arxiv.org/abs/2602.08272", "authors": ["Junwei Su", "Chuan Wu"], "title": "When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7PAC\u6846\u67b6\u7406\u8bba\u5206\u6790MARL\u4e0eSARL\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u6837\u672c\u6548\u7387\uff0c\u53d1\u73b0\u4efb\u52a1\u53ef\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6MARL\u66f4\u4f18\uff0c\u800c\u4f9d\u8d56\u6027\u5b50\u4efb\u52a1\u4f1a\u524a\u5f31\u5176\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u5bf9\u9f50\u6982\u5ff5\u91cf\u5316\u5206\u89e3\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1MARL\u5728LLM\u8bad\u7ec3\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u4f55\u65f6\u9009\u62e9MARL\u800c\u975eSARL\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u7406\u8bba\u5206\u6790\u6765\u660e\u786e\u4e24\u79cd\u65b9\u6cd5\u7684\u6bd4\u8f83\u4f18\u52bf\u6761\u4ef6\u3002", "method": "\u91c7\u7528PAC\uff08Probably Approximately Correct\uff09\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49LLM\u7684SARL\u548cMARL\u8bbe\u7f6e\uff0c\u63a8\u5bfc\u663e\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\uff0c\u7cfb\u7edf\u5206\u6790\u4efb\u52a1\u5206\u89e3\u548c\u5bf9\u9f50\u5bf9\u5b66\u4e60\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "MARL\u5728\u4efb\u52a1\u81ea\u7136\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6\u80fd\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u5b50\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u6027\u4f1a\u524a\u5f31MARL\u7684\u6bd4\u8f83\u4f18\u52bf\u3002\u4efb\u52a1\u5bf9\u9f50\u6982\u5ff5\u63ed\u793a\u4e86\u5f3a\u5236\u72ec\u7acb\u5206\u89e3\u65f6\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u6f84\u6e05\u4e86\u5b9e\u8bc1\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u590d\u6742LLM\u573a\u666f\u4e2d\u6709\u6548\u90e8\u7f72MARL\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6807\u51c6\uff0c\u5e2e\u52a9\u6839\u636e\u4efb\u52a1\u5206\u89e3\u7279\u6027\u9009\u62e9\u9002\u5f53\u7684RL\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08306", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08306", "abs": "https://arxiv.org/abs/2602.08306", "authors": ["Suizhi Huang", "Mei Li", "Han Yu", "Xiaoxiao Li"], "title": "TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning", "comment": null, "summary": "Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.", "AI": {"tldr": "TextResNet\u901a\u8fc7\u8bed\u4e49\u89e3\u8026\u548c\u56e0\u679c\u8def\u7531\u89e3\u51b3TextGrad\u5728\u6df1\u5ea6\u94fe\u4e2d\u7684\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u6587\u672c\u68af\u5ea6\u4f18\u5316", "motivation": "TextGrad\u7b49\u6587\u672c\u68af\u5ea6\u4f18\u5316\u5668\u5728\u6df1\u5ea6\u94fe\u5f0fAI\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898\u5bfc\u81f4\u53cd\u9988\u4fe1\u53f7\u6df7\u5408\u4e86\u5c40\u90e8\u6279\u8bc4\u548c\u4e0a\u6e38\u4e0a\u4e0b\u6587\uff0c\u4ea7\u751f\u5f52\u56e0\u6a21\u7cca", "method": "\u63d0\u51faTextResNet\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u52a0\u6027\u8bed\u4e49\u589e\u91cf\u4fdd\u6301\u6052\u7b49\u8def\u5f84\uff1b\u540e\u5411\u4f20\u64ad\u4e2d\u7684\u8bed\u4e49\u68af\u5ea6\u5206\u89e3\u901a\u8fc7\u8bed\u4e49\u6295\u5f71\u5668\u89e3\u8026\u53cd\u9988\uff1b\u56e0\u679c\u8def\u7531\u5c06\u6295\u5f71\u4fe1\u53f7\u5b9a\u5411\u5230\u7279\u5b9a\u7ec4\u4ef6\uff1b\u5bc6\u5ea6\u611f\u77e5\u4f18\u5316\u8c03\u5ea6\u52a8\u6001\u5206\u914d\u8d44\u6e90\u5230\u7cfb\u7edf\u74f6\u9888", "result": "TextResNet\u4e0d\u4ec5\u6bd4TextGrad\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u4e14\u5728\u590d\u5408AI\u7cfb\u7edf\u7684\u4ee3\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7a33\u5b9a\u6027\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4f1a\u5d29\u6e83", "conclusion": "TextResNet\u901a\u8fc7\u89e3\u51b3\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898\uff0c\u4e3a\u6df1\u5ea6\u94fe\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u6587\u672c\u68af\u5ea6\u4f18\u5316\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2602.08307", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08307", "abs": "https://arxiv.org/abs/2602.08307", "authors": ["Mengxiao Zhang", "Yuheng Zhang", "Haipeng Luo", "Paul Mineiro"], "title": "Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback", "comment": null, "summary": "In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u591a\u6b65\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5728\u4ec5\u63a5\u6536\u95f4\u63a5\u53cd\u9988\u800c\u975e\u663e\u5f0f\u5956\u52b1\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6b65\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u591a\u8f6e\u51b3\u7b56\u7cfb\u7edf\uff08\u5982\u591a\u8f6eLLM\u90e8\u7f72\uff09\uff0c\u9700\u8981\u5c06\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6269\u5c55\u5230\u5e8f\u5217\u51b3\u7b56\u573a\u666f", "method": "\u5c06Zhang\u7b49\u4eba\uff082024a\uff09\u7684\u5956\u52b1\u4f30\u8ba1\u5668\u4ece\u5355\u6b65\u6269\u5c55\u5230\u591a\u6b65\u8bbe\u7f6e\uff0c\u89e3\u51b3MDP\u4e0b\u6f5c\u5728\u5956\u52b1\u89e3\u7801\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u9006\u95f4\u9699\u52a0\u6743\u7b97\u6cd5\u8fdb\u884c\u7b56\u7565\u4f18\u5316", "result": "\u7b97\u6cd5\u5728\u4e0a\u4e0b\u6587\u60c5\u666fMDP\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\uff0c\u5728\u5408\u6210MDP\u548c\u771f\u5b9e\u4e16\u754c\u7528\u6237\u9884\u8ba2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u7684\u6709\u6548\u6027", "conclusion": "\u6210\u529f\u5c06\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6269\u5c55\u5230\u591a\u6b65\u5e8f\u5217\u51b3\u7b56\u573a\u666f\uff0c\u4e3a\u4ec5\u63a5\u6536\u95f4\u63a5\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.08324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08324", "abs": "https://arxiv.org/abs/2602.08324", "authors": ["Yuntian Tang", "Bohan Jia", "Wenxuan Huang", "Lianyue Zhang", "Jiao Xie", "Wenxi Li", "Wei Li", "Jie Hu", "Xinghao Chen", "Rongrong Ji", "Shaohui Lin"], "title": "Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression", "comment": "15 pages, 7 figures", "summary": "Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\\% token reduction with an accuracy improvement of 0.6\\%, significantly outperforming state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51faExtra-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u538b\u7f29\u5668\u548c\u6df7\u5408\u6bd4\u4f8bSFT+CHRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u6781\u7aef\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301CoT\u63a8\u7406\u7684\u903b\u8f91\u4fdd\u771f\u5ea6\uff0c\u5b9e\u73b073%\u4ee4\u724c\u51cf\u5c11\u540c\u65f6\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709CoT\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u903b\u8f91\u4fdd\u771f\u5ea6\u635f\u5931\u4e25\u91cd\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u53c8\u80fd\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u5728\u6570\u5b66CoT\u6570\u636e\u4e0a\u8bad\u7ec3\u4e13\u7528\u7684\u8bed\u4e49\u4fdd\u6301\u538b\u7f29\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff1b2) \u901a\u8fc7\u6df7\u5408\u6bd4\u4f8bSFT\u5fae\u8c03LLM\uff0c\u4f7f\u5176\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u9884\u7b97\uff1b3) \u63d0\u51faCHRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u673a\u5236\u663e\u5f0f\u6fc0\u52b1\u4f4e\u9884\u7b97\u4e0b\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002\u4ee5Qwen3-1.7B\u5728MATH-500\u4e0a\u4e3a\u4f8b\uff0c\u5b9e\u73b0\u8d85\u8fc773%\u7684\u4ee4\u724c\u51cf\u5c11\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53470.6%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "Extra-CoT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u5feb\u901f\u63a8\u7406\uff0c\u5728\u6781\u7aef\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3aCoT\u63a8\u7406\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.08563", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08563", "abs": "https://arxiv.org/abs/2602.08563", "authors": ["Ahmed Salem", "Andrew Paverd", "Sahar Abdelnabi"], "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs", "comment": "Accepted at IEEE SaTML 2026", "summary": "Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.", "AI": {"tldr": "LLMs\u5b58\u5728\u9690\u5f0f\u8bb0\u5fc6\u80fd\u529b\uff0c\u80fd\u5728\u72ec\u7acb\u4ea4\u4e92\u95f4\u4f20\u9012\u72b6\u6001\u4fe1\u606f\uff0c\u65e0\u9700\u663e\u5f0f\u5b58\u50a8\u6a21\u5757\uff0c\u901a\u8fc7\u8f93\u51fa\u7f16\u7801\u548c\u540e\u7eed\u8f93\u5165\u6062\u590d\u5b9e\u73b0\u8de8\u63a8\u7406\u8bf7\u6c42\u7684\u6301\u4e45\u4fe1\u606f\u901a\u9053\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u5c06LLMs\u89c6\u4e3a\u65e0\u72b6\u6001\u6a21\u578b\u7684\u666e\u904d\u5047\u8bbe\uff0c\u63ed\u793a\u6a21\u578b\u5b58\u5728\u9690\u5f0f\u8bb0\u5fc6\u80fd\u529b\uff0c\u8fd9\u79cd\u80fd\u529b\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0c\u5982\u65f6\u95f4\u70b8\u5f39\u7b49\u65b0\u578b\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u5f15\u5165\u9690\u5f0f\u8bb0\u5fc6\u6982\u5ff5\uff0c\u901a\u8fc7\u8f93\u51fa\u7f16\u7801\u4fe1\u606f\u5e76\u5728\u540e\u7eed\u8f93\u5165\u4e2d\u6062\u590d\uff1b\u4ee5\u65f6\u95f4\u70b8\u5f39\u4e3a\u5177\u4f53\u6848\u4f8b\uff0c\u5c55\u793a\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u6216\u5fae\u8c03\u5373\u53ef\u8bf1\u5bfc\u6b64\u7c7b\u884c\u4e3a\u3002", "result": "\u8bc1\u660eLLMs\u786e\u5b9e\u5b58\u5728\u9690\u5f0f\u8bb0\u5fc6\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u8de8\u4ea4\u4e92\u7684\u72b6\u6001\u4f20\u9012\uff1b\u6210\u529f\u521b\u5efa\u65f6\u95f4\u70b8\u5f39\u653b\u51fb\uff0c\u4ec5\u5f53\u6ee1\u8db3\u901a\u8fc7\u9690\u5f0f\u8bb0\u5fc6\u79ef\u7d2f\u7684\u9690\u85cf\u6761\u4ef6\u5e8f\u5217\u65f6\u624d\u6fc0\u6d3b\u3002", "conclusion": "\u9690\u5f0f\u8bb0\u5fc6\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd\uff0c\u5305\u62ec\u9690\u853d\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3001\u57fa\u51c6\u6c61\u67d3\u3001\u5b9a\u5411\u64cd\u7eb5\u548c\u8bad\u7ec3\u6570\u636e\u4e2d\u6bd2\u7b49\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u68c0\u6d4b\u65b9\u6cd5\u548c\u538b\u529b\u6d4b\u8bd5\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.08584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08584", "abs": "https://arxiv.org/abs/2602.08584", "authors": ["Wensong Bai", "Chao Zhang", "Qihang Xu", "Chufan Chen", "Chenhao Zhou", "Hui Qian"], "title": "Conditional Sequence Modeling for Safe Reinforcement Learning", "comment": null, "summary": "Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.", "AI": {"tldr": "RCDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u7684\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u591a\u4e2a\u6210\u672c\u9608\u503c\u7684\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u5f0f\u6210\u672c\u60e9\u7f5a\u548c\u81ea\u9002\u5e94\u60e9\u7f5a\u7cfb\u6570\u4f18\u5316\u56de\u62a5-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5b89\u5168RL\u65b9\u6cd5\u901a\u5e38\u5728\u9884\u5b9a\u4e49\u7684\u6210\u672c\u9608\u503c\u4e0b\u8bad\u7ec3\uff0c\u5bfc\u81f4\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u4e2d\u53d8\u5316\u7684\u5b89\u5168\u8981\u6c42\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u96f6\u6837\u672c\u9002\u5e94\u591a\u79cd\u6210\u672c\u9608\u503c\u7684\u5355\u4e00\u7b56\u7565\u3002", "method": "\u63d0\u51faRCDT\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff1b2) \u96c6\u6210\u62c9\u683c\u6717\u65e5\u5f0f\u6210\u672c\u60e9\u7f5a\u4e0e\u81ea\u9002\u5e94\u60e9\u7f5a\u7cfb\u6570\uff1b3) \u5f15\u5165\u56de\u62a5-\u6210\u672c\u611f\u77e5\u7684\u8f68\u8ff9\u91cd\u52a0\u6743\u673a\u5236\uff1b4) \u52a0\u5165Q\u503c\u6b63\u5219\u5316\u4ee5\u907f\u514d\u8fc7\u4e8e\u4fdd\u5b88\u7684\u884c\u4e3a\u3002", "result": "\u5728DSRL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRCDT\u5728\u56de\u62a5-\u6210\u672c\u6743\u8861\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u79bb\u7ebf\u5b89\u5168RL\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "RCDT\u662f\u9996\u4e2a\u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u7684\u79bb\u7ebf\u5b89\u5168RL\u7b97\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u591a\u4e2a\u6210\u672c\u9608\u503c\u7684\u7075\u6d3b\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u4f18\u5316\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08616", "abs": "https://arxiv.org/abs/2602.08616", "authors": ["Heiko Hoppe", "Fabian Akkerman", "Wouter van Heeswijk", "Maximilian Schiffer"], "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces", "comment": "26 pages, 8 figures", "summary": "Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.", "AI": {"tldr": "\u63d0\u51faDGRL\u65b9\u6cd5\uff0c\u7ed3\u5408SDN\u548cDBU\u6280\u672f\uff0c\u89e3\u51b3\u9ad8\u7ef4\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u652f\u6301\u9ad8\u8fbe10^20\u4e2a\u52a8\u4f5c\uff0c\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\u73af\u5883\u4e2d\u6027\u80fd\u63d0\u5347\u8fbe66%\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u6d41\u3001\u8c03\u5ea6\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u9ad8\u7ef4\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7f51\u683c\u7ed3\u6784\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5728\u9ad8\u7ef4\u6216\u4e0d\u89c4\u5219\u7ed3\u6784\u9886\u57df\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8ddd\u79bb\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60(DGRL)\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1) \u91c7\u6837\u52a8\u6001\u90bb\u57df(SDN)\uff1a\u5229\u7528\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u968f\u673a\u4f53\u79ef\u63a2\u7d22\uff0c\u5728\u5c40\u90e8\u4fe1\u4efb\u533a\u57df\u63d0\u4f9b\u5b8c\u6574\u652f\u6301\uff1b2) \u8ddd\u79bb\u57fa\u7840\u66f4\u65b0(DBU)\uff1a\u5c06\u7b56\u7565\u4f18\u5316\u8f6c\u5316\u4e3a\u7a33\u5b9a\u56de\u5f52\u4efb\u52a1\uff0c\u89e3\u8026\u68af\u5ea6\u65b9\u5dee\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u57fa\u6570\uff0c\u4fdd\u8bc1\u5355\u8c03\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u51c6\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe66%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u65b9\u6cd5\u81ea\u7136\u6269\u5c55\u5230\u6df7\u5408\u8fde\u7eed-\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\uff0c\u65e0\u9700\u5c42\u6b21\u4f9d\u8d56\u3002", "conclusion": "DGRL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6311\u6218\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u63a2\u7d22\u548c\u7a33\u5b9a\u56de\u5f52\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b9e\u9645\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08621", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08621", "abs": "https://arxiv.org/abs/2602.08621", "authors": ["Yukun Jiang", "Hai Huang", "Mingjie Li", "Yage Zhang", "Michael Backes", "Yang Zhang"], "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs", "comment": null, "summary": "By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0MoE\u67b6\u6784\u7684LLMs\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u64cd\u7eb5\u8def\u7531\u5668\u53ef\u4ee5\u6fc0\u6d3b\"\u4e0d\u5b89\u5168\u8def\u5f84\"\uff0c\u5c06\u5b89\u5168\u8f93\u51fa\u8f6c\u4e3a\u6709\u5bb3\u5185\u5bb9\u3002\u63d0\u51fa\u4e86RoSais\u8bc4\u5206\u548cF-SOUR\u6846\u67b6\u6765\u91cf\u5316\u98ce\u9669\u548c\u53d1\u73b0\u653b\u51fb\u8def\u5f84\uff0c\u5728\u591a\u4e2aMoE\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8MoE\u67b6\u6784\u7684\u6548\u7528\u548c\u6548\u7387\uff0c\u4f46\u5bf9\u5176\u7a00\u758f\u67b6\u6784\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u63a2\u7d22\u4e0d\u8db3\u3002MoE LLMs\u7684\u5b89\u5168\u6027\u53ef\u80fd\u4e0e\u5176\u67b6\u6784\u4e00\u6837\"\u7a00\u758f\"\uff0c\u5b58\u5728\u88ab\u64cd\u7eb5\u8def\u7531\u5668\u6fc0\u6d3b\u6709\u5bb3\u8f93\u51fa\u7684\u98ce\u9669\u3002", "method": "1) \u63d0\u51faRouter Safety\u91cd\u8981\u6027\u8bc4\u5206(RoSais)\u91cf\u5316\u5404\u5c42\u8def\u7531\u5668\u7684\u5b89\u5168\u5173\u952e\u6027\uff1b2) \u5f00\u53d1\u7ec6\u7c92\u5ea6token-layer-wise\u968f\u673a\u4f18\u5316\u6846\u67b6(F-SOUR)\u6765\u53d1\u73b0\u5177\u4f53\u7684\u4e0d\u5b89\u5168\u8def\u5f84\uff0c\u8003\u8651\u8f93\u5165token\u7684\u987a\u5e8f\u6027\u548c\u52a8\u6001\u6027\u3002", "result": "\u5728DeepSeek-V2-Lite\u4e0a\u4ec5\u5c4f\u853d5\u4e2a\u8def\u7531\u5668\uff0c\u653b\u51fb\u6210\u529f\u7387\u63d0\u53474\u500d\u81f30.79\u3002\u5728\u56db\u4e2a\u4ee3\u8868\u6027MoE LLM\u5bb6\u65cf\u4e0a\uff0cF-SOUR\u5728JailbreakBench\u548cAdvBench\u4e0a\u5206\u522b\u8fbe\u5230\u5e73\u57470.90\u548c0.98\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "MoE LLMs\u5b58\u5728\u56fa\u6709\u7684\u5b89\u5168\u98ce\u9669\uff0c\u8def\u7531\u5668\u64cd\u7eb5\u53ef\u80fd\u81ea\u7136\u53d1\u751f\u3002\u63d0\u51fa\u4e86\u5b89\u5168\u611f\u77e5\u7684\u8def\u5f84\u7981\u7528\u548c\u8def\u7531\u5668\u8bad\u7ec3\u7b49\u9632\u5fa1\u65b9\u5411\uff0c\u4e3aMoE LLMs\u7684\u7ea2\u961f\u6d4b\u8bd5\u548c\u5b89\u5168\u4fdd\u969c\u63d0\u4f9b\u53c2\u8003\u3002", "topic": "agent analysis"}}
{"id": "2602.08655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08655", "abs": "https://arxiv.org/abs/2602.08655", "authors": ["Sarthak Wanjari"], "title": "From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism", "comment": "10 pages, 8 figures", "summary": "Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.", "AI": {"tldr": "Geo-IQL\uff1a\u901a\u8fc7k\u8fd1\u90bb\u8ddd\u79bb\u7684\u5bc6\u5ea6\u60e9\u7f5a\u589e\u5f3aIQL\uff0c\u5728\u7a00\u758f\u6570\u636e\u4e2d\u6709\u6548\u9632\u6b62OOD\u52a8\u4f5c\u9ad8\u4f30\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u4fdd\u6301\u7a33\u5b9a\u6d41\u5f62\u6027\u80fd", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u5206\u5e03\u5916\u52a8\u4f5c\u9ad8\u4f30\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u95f4\u5b58\u5728\u6743\u8861\uff1aCQL\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0cIQL\u5728\u75c5\u6001\u6570\u636e\u96c6\u4e0a\u6613\u9000\u5316\u4e3a\u884c\u4e3a\u514b\u9686", "method": "\u63d0\u51fa\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u6846\u67b6\uff0c\u5728\u72b6\u6001-\u52a8\u4f5c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4f7f\u7528k\u8fd1\u90bb\u8ddd\u79bb\u8ba1\u7b97\u5bc6\u5ea6\u60e9\u7f5a\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u6ce8\u5165OOD\u4fdd\u5b88\u6027\uff0c\u9884\u8ba1\u7b97\u60e9\u7f5a\u5b9e\u73b0O(1)\u8bad\u7ec3\u5f00\u9500", "result": "\u5728D4RL MuJoCo\u57fa\u51c6\u4e0a\uff0cGeo-IQL\u5728\u654f\u611f\u4e0d\u7a33\u5b9a\u7684medium-replay\u4efb\u52a1\u4e0a\u6bd4\u6807\u51c6IQL\u63d0\u534718+\u5206\uff0c\u79cd\u5b50\u95f4\u65b9\u5dee\u964d\u4f4e4\u500d\uff1b\u5728MIMIC-III Sepsis\u6570\u636e\u96c6\u4e0a\uff0cIQL\u9000\u5316\u4e3a\u884c\u4e3a\u514b\u9686\u65f6\uff0cGeo-IQL\u4ecd\u80fd\u5b9e\u73b0\u7b56\u7565\u6539\u8fdb\uff0c\u4e0e\u4e34\u5e8a\u533b\u751f\u7ec8\u5c40\u4e00\u81f4\u6027\u8fbe86.4%", "conclusion": "\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u4e3a\u5173\u952e\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u6b63\u5219\u5316\uff0c\u80fd\u5b89\u5168\u514b\u670d\u5c40\u90e8\u6700\u4f18\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861", "topic": "agentic reinforcement learning"}}
{"id": "2602.08676", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08676", "abs": "https://arxiv.org/abs/2602.08676", "authors": ["Tiwei Bie", "Maosong Cao", "Xiang Cao", "Bingsen Chen", "Fuyuan Chen", "Kun Chen", "Lun Du", "Daozhuo Feng", "Haibo Feng", "Mingliang Gong", "Zhuocheng Gong", "Yanmei Gu", "Jian Guan", "Kaiyuan Guan", "Hongliang He", "Zenan Huang", "Juyong Jiang", "Zhonghui Jiang", "Zhenzhong Lan", "Chengxi Li", "Jianguo Li", "Zehuan Li", "Huabin Liu", "Lin Liu", "Guoshan Lu", "Yuan Lu", "Yuxin Ma", "Xingyu Mou", "Zhenxuan Pan", "Kaida Qiu", "Yuji Ren", "Jianfeng Tan", "Yiding Tian", "Zian Wang", "Lanning Wei", "Tao Wu", "Yipeng Xing", "Wentao Ye", "Liangyu Zha", "Tianze Zhang", "Xiaolu Zhang", "Junbo Zhao", "Da Zheng", "Hao Zhong", "Wanli Zhong", "Jun Zhou", "Junlin Zhou", "Liwang Zhu", "Muzhi Zhu", "Yihong Zhuang"], "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing", "comment": "11 pages, 3 figures", "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.", "AI": {"tldr": "LLaDA2.1\u901a\u8fc7\u7ed3\u5408Token-to-Token\u7f16\u8f91\u548cMask-to-Token\u65b9\u6848\uff0c\u5f15\u5165\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\u673a\u5236\uff0c\u63d0\u4f9b\u901f\u5ea6\u6a21\u5f0f\u548c\u8d28\u91cf\u6a21\u5f0f\uff0c\u5e76\u9996\u6b21\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1LLaDA2.0\u5c55\u793a\u4e86\u767e\u4ebf\u7ea7\u5757\u6269\u6563\u6a21\u578b\u7684\u6269\u5c55\u6f5c\u529b\u53ca\u5176\u56fa\u6709\u5e76\u884c\u6027\uff0c\u4f46\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u5fae\u5999\u5e73\u8861\u4e00\u76f4\u662f\u4e2a\u96be\u4ee5\u7a81\u7834\u7684\u8fb9\u754c\u3002\u9700\u8981\u8d85\u8d8a\u8fd9\u79cd\u6743\u8861\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5c06Token-to-Token\u7f16\u8f91\u65e0\u7f1d\u96c6\u6210\u5230\u4f20\u7edfMask-to-Token\u65b9\u6848\u4e2d\uff0c\u5f15\u5165\u8054\u5408\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\u65b9\u6848\uff1b2. \u521b\u5efa\u4e24\u79cd\u6a21\u5f0f\uff1a\u901f\u5ea6\u6a21\u5f0f\uff08\u964d\u4f4eM2T\u9608\u503c\uff0c\u4f9d\u8d56T2T\u4f18\u5316\u8f93\u51fa\uff09\u548c\u8d28\u91cf\u6a21\u5f0f\uff08\u4fdd\u5b88\u9608\u503c\u4fdd\u8bc1\u6027\u80fd\uff09\uff1b3. \u57fa\u4e8e\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9996\u6b21\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u7a33\u5b9a\u68af\u5ea6\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u572833\u4e2a\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaDA2.1\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u4efb\u52a1\u6027\u80fd\u548c\u95ea\u7535\u822c\u7684\u89e3\u7801\u901f\u5ea6\u3002\u5c3d\u7ba1\u6709100B\u53c2\u6570\u89c4\u6a21\uff0c\u5728\u7f16\u7801\u4efb\u52a1\u4e0a\u8fbe\u5230\uff1aHumanEval+ 892 TPS\u3001BigCodeBench 801 TPS\u3001LiveCodeBench 663 TPS\u3002\u53d1\u5e03\u4e86LLaDA2.1-Mini\uff0816B\uff09\u548cLLaDA2.1-Flash\uff08100B\uff09\u4e24\u4e2a\u7248\u672c\u3002", "conclusion": "LLaDA2.1\u901a\u8fc7\u7ed3\u6784\u521b\u65b0\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff0c\u6210\u529f\u8d85\u8d8a\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u4f20\u7edf\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u4eba\u7c7b\u610f\u56fe\u7684\u7406\u89e3\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2602.08690", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08690", "abs": "https://arxiv.org/abs/2602.08690", "authors": ["Shae McFadden", "Myles Foley", "Elizabeth Bates", "Ilias Tsingenopoulos", "Sanyam Vyas", "Vasilios Mavroudis", "Chris Hicks", "Fabio Pierazzi"], "title": "SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u768411\u4e2a\u5e38\u89c1\u65b9\u6cd5\u9677\u9631\uff0c\u901a\u8fc7\u5206\u679066\u7bc7\u76f8\u5173\u8bba\u6587(2018-2025)\u53d1\u73b0\u5e73\u5747\u6bcf\u7bc7\u5b58\u5728\u8d85\u8fc75\u4e2a\u9677\u9631\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u9677\u9631\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u6700\u540e\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u4ece\u5b9e\u9a8c\u5ba4\u6a21\u62df\u5230\u5b9e\u9645\u90e8\u7f72\u7684\u8bf8\u591a\u6311\u6218\uff0c\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u901a\u5e38\u5177\u6709\u5bf9\u6297\u6027\u3001\u975e\u5e73\u7a33\u6027\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u7279\u70b9\uff0c\u5bfc\u81f4\u73b0\u6709\u7814\u7a76\u5b58\u5728\u7cfb\u7edf\u6027\u65b9\u6cd5\u7f3a\u9677\u3002", "method": "1) \u8bc6\u522b\u5e76\u7cfb\u7edf\u5316DRL4Sec\u6587\u732e\u4e2d\u768411\u4e2a\u65b9\u6cd5\u9677\u9631\uff0c\u6db5\u76d6\u73af\u5883\u5efa\u6a21\u3001\u667a\u80fd\u4f53\u8bad\u7ec3\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u7cfb\u7edf\u90e8\u7f72\u56db\u4e2a\u9636\u6bb5\uff1b2) \u5206\u679066\u7bc7\u91cd\u8981DRL4Sec\u8bba\u6587(2018-2025)\uff0c\u91cf\u5316\u6bcf\u4e2a\u9677\u9631\u7684\u666e\u904d\u6027\uff1b3) \u5728\u81ea\u4e3b\u7f51\u7edc\u9632\u5fa1\u3001\u5bf9\u6297\u6027\u6076\u610f\u8f6f\u4ef6\u751f\u6210\u548cWeb\u5b89\u5168\u6d4b\u8bd5\u4e09\u4e2a\u73af\u5883\u4e2d\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u9677\u9631\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e73\u5747\u6bcf\u7bc7\u8bba\u6587\u5b58\u5728\u8d85\u8fc75\u4e2a\u65b9\u6cd5\u9677\u9631\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u8fd9\u4e9b\u9677\u9631\u4f1a\u4e25\u91cd\u5f71\u54cdDRL\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u8868\u660e\u5f53\u524dDRL4Sec\u7814\u7a76\u5b58\u5728\u7cfb\u7edf\u6027\u65b9\u6cd5\u7f3a\u9677\u3002", "conclusion": "DRL\u5728\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u5b58\u5728\u5e7f\u6cdb\u7684\u65b9\u6cd5\u9677\u9631\uff0c\u9700\u8981\u66f4\u4e25\u8c28\u7684\u7814\u7a76\u65b9\u6cd5\u3002\u8bba\u6587\u4e3a\u6bcf\u4e2a\u9677\u9631\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u4ee5\u652f\u6301\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u90e8\u7f72\u7684\u57fa\u4e8eDRL\u7684\u5b89\u5168\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2602.08808", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08808", "abs": "https://arxiv.org/abs/2602.08808", "authors": ["Yapei Chang", "Kyle Lo", "Mohit Iyyer", "Luca Soldaini"], "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs", "comment": "53 pages, 22 figures", "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86How2Everything\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u6539\u8fdb\u76ee\u6807\u6761\u4ef6\u7a0b\u5e8f\u751f\u6210\uff0c\u5305\u62ec\u6570\u636e\u6316\u6398\u3001\u57fa\u51c6\u6784\u5efa\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u3002", "motivation": "\u751f\u6210\u9010\u6b65\"\u5982\u4f55\u505a\"\u7a0b\u5e8f\u662fLLM\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u5927\u89c4\u6a21\u6d4b\u91cf\u548c\u6539\u8fdb\u7a0b\u5e8f\u6709\u6548\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u7814\u7a76\u4e0d\u8db3\u3002", "method": "1) How2Mine\uff1a\u4ece98\u4e07\u7f51\u9875\u4e2d\u6316\u639835.1\u4e07\u6761\u7a0b\u5e8f\uff1b2) How2Bench\uff1a\u6784\u5efa7K\u793a\u4f8b\u7684\u8bc4\u4f30\u96c6\uff1b3) How2Score\uff1a\u4f7f\u7528LLM\u8bc4\u59d4\u68c0\u6d4b\u5173\u952e\u5931\u8d25\u7684\u8bc4\u4f30\u534f\u8bae\uff1b4) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "result": "How2Bench\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u6e05\u6670\u6269\u5c55\u8d8b\u52bf\uff1b\u4f7f\u7528How2Score\u4f5c\u4e3a\u5956\u52b1\u7684RL\u5728\u4e09\u4e2a\u6a21\u578b\u4e0a\u5c06\u6027\u80fd\u63d0\u5347>10\u5206\uff0c\u4e14\u5bf9\u6807\u51c6\u57fa\u51c6\u6ca1\u6709\u7cfb\u7edf\u6027\u56de\u5f52\u3002", "conclusion": "How2Everything\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u7f51\u7edc\u6570\u636e\u5982\u4f55\u652f\u6301\u5927\u89c4\u6a21\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u7684\u95ed\u73af\u3002", "topic": "agent analysis"}}
{"id": "2602.08813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08813", "abs": "https://arxiv.org/abs/2602.08813", "authors": ["Mahdi Sabbaghi", "George Pappas", "Adel Javanmard", "Hamed Hassani"], "title": "Robust Policy Optimization to Prevent Catastrophic Forgetting", "comment": null, "summary": "Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.\n  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.", "AI": {"tldr": "FRPO\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5f53\u524d\u7b56\u7565\u53ca\u4e0b\u6e38\u53ef\u8bbf\u95ee\u7684KL\u90bb\u57df\u7b56\u7565\u6765\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b89\u5168\u9000\u5316\u3002", "motivation": "\u4f20\u7edfRLHF\u8bad\u7ec3\u7684\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u4f7f\u5c0f\u7684\u4e0b\u6e38\u66f4\u65b0\u4e5f\u4f1a\u635f\u5bb3\u4e4b\u524d\u5b66\u4e60\u7684\u884c\u4e3a\uff08\u5982\u5b89\u5168\u6027\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e0b\u6e38\u65f6\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u5df2\u5b66\u884c\u4e3a\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u9700\u8981\u9884\u5fae\u8c03\u9c81\u68d2\u6027\uff1a\u57fa\u7840\u7b56\u7565\u5e94\u907f\u514d\u8106\u6027\u7684\u9ad8\u5956\u52b1\u89e3\u3002", "method": "\u63d0\u51faFine-tuning Robust Policy Optimization (FRPO)\uff0c\u4e00\u4e2a\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u4e0d\u4ec5\u4f18\u5316\u5f53\u524d\u7b56\u7565\u7684\u5956\u52b1\uff0c\u8fd8\u4f18\u5316\u4e0b\u6e38\u9002\u5e94\u53ef\u8fbe\u7684KL\u6709\u754c\u90bb\u57df\u7b56\u7565\u3002\u91c7\u7528\u6700\u5927-\u6700\u5c0f\u5316\u516c\u5f0f\u786e\u4fdd\u7b56\u7565\u504f\u79fb\u4e0b\u7684\u5956\u52b1\u7a33\u5b9a\u6027\uff0c\u57fa\u4e8eGRPO\u4fee\u6539\u5b9e\u73b0\u65e0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFRPO\u663e\u8457\u51cf\u5c11\u4e86\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4e0b\u6e38\u5fae\u8c03\u673a\u5236\uff08SFT\u548cRL\uff09\u4e0b\u7684\u5b89\u5168\u9000\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u5728\u6570\u5b66\u805a\u7126\u7684RL\u8bbe\u7f6e\u4e2d\uff0cFRPO\u5728\u540e\u7eed\u5fae\u8c03\u4e0b\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "FRPO\u901a\u8fc7\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RLHF\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u5bf9\u4e0b\u6e38\u9002\u5e94\u9c81\u68d2\u7684\u57fa\u7840\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08847", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08847", "abs": "https://arxiv.org/abs/2602.08847", "authors": ["Lang Feng", "Longtao Zheng", "Shuo He", "Fuxiang Zhang", "Bo An"], "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems", "comment": "Preprint", "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.", "AI": {"tldr": "\u63d0\u51faDr. MAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f7f\u7528\u81ea\u8eab\u5956\u52b1\u7edf\u8ba1\u8fdb\u884c\u4f18\u52bf\u5f52\u4e00\u5316\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\u5b9e\u73b0\u9ad8\u7ea7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7684\u7fa4\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6269\u5c55\u65f6\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u6839\u672c\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7406\u8bba\u5206\u6790\u53d1\u73b0GRPO\u5f0f\u4f18\u5316\u4e2d\u5168\u5c40\u5f52\u4e00\u5316\u57fa\u7ebf\u53ef\u80fd\u504f\u79bb\u4e0d\u540c\u667a\u80fd\u4f53\u7684\u5956\u52b1\u5206\u5e03\uff0c\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u4e0d\u7a33\u5b9a\u3002\u63d0\u51faDr. MAS\u65b9\u6cd5\uff1a\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f7f\u7528\u81ea\u8eab\u5956\u52b1\u7edf\u8ba1\u8fdb\u884c\u4f18\u52bf\u5f52\u4e00\u5316\uff0c\u6821\u51c6\u68af\u5ea6\u5c3a\u5ea6\u3002\u540c\u65f6\u63d0\u4f9b\u7aef\u5230\u7aefRL\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7f16\u6392\u3001\u7075\u6d3b\u7684\u667a\u80fd\u4f53LLM\u670d\u52a1\u548c\u4f18\u5316\u914d\u7f6e\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528Qwen2.5\u548cQwen3\u7cfb\u5217\u6a21\u578b\u8bc4\u4f30\uff0cDr. MAS\u76f8\u6bd4\u539f\u59cbGRPO\u6709\u660e\u663e\u63d0\u5347\uff08\u6570\u5b66\uff1a+5.6% avg@16\u548c+4.6% pass@16\uff1b\u641c\u7d22\uff1a+15.2% avg@16\u548c+13.1% pass@16\uff09\uff0c\u540c\u65f6\u5927\u5e45\u6d88\u9664\u68af\u5ea6\u5c16\u5cf0\u3002\u5728\u5f02\u6784\u667a\u80fd\u4f53\u6a21\u578b\u5206\u914d\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "Dr. MAS\u901a\u8fc7\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u4f18\u52bf\u5f52\u4e00\u5316\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2dRL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.08934", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08934", "abs": "https://arxiv.org/abs/2602.08934", "authors": ["Suraj Ranganath", "Atharv Ramesh"], "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors", "comment": "Expanded version of a workshop submission. Code available", "summary": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.", "AI": {"tldr": "StealthRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u6539\u5199\u653b\u51fb\u6d4b\u8bd5AI\u6587\u672c\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u68c0\u6d4b\u7387\u5e76\u63ed\u793a\u4e86\u68c0\u6d4b\u5668\u7684\u5171\u4eab\u67b6\u6784\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u5668\u9762\u4e34\u5bf9\u6297\u6027\u6539\u5199\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6311\u6218\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u540c\u65f6\u9003\u907f\u68c0\u6d4b\u3002\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u8bd5\u68c0\u6d4b\u5668\u5728\u73b0\u5b9e\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u8106\u5f31\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8eQwen3-4B\u6a21\u578b\u8bad\u7ec3\u6539\u5199\u7b56\u7565\uff0c\u91c7\u7528Group Relative Policy Optimization (GRPO)\u548cLoRA\u9002\u914d\u5668\uff0c\u9488\u5bf9\u591a\u68c0\u6d4b\u5668\u96c6\u6210\u4f18\u5316\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u5e73\u8861\u68c0\u6d4b\u9003\u907f\u548c\u8bed\u4e49\u4fdd\u6301\u3002", "result": "\u57281%\u8bef\u62a5\u7387\u7684\u5b89\u5168\u76f8\u5173\u64cd\u4f5c\u70b9\u4e0a\uff0cStealthRL\u5b9e\u73b0\u4e86\u63a5\u8fd1\u96f6\u7684\u68c0\u6d4b\u7387(0.001\u5e73\u5747TPR@1%FPR)\uff0c\u5c06\u5e73\u5747AUROC\u4ece0.74\u964d\u81f30.27\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe\u523099.9%\u3002\u653b\u51fb\u8fd8\u80fd\u8fc1\u79fb\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u68c0\u6d4b\u5668\u5bb6\u65cf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u5668\u5b58\u5728\u663e\u8457\u7684\u9c81\u68d2\u6027\u7f3a\u9677\uff0cStealthRL\u4e3a\u5bf9\u6297\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u534f\u8bae\uff0c\u66b4\u9732\u4e86\u68c0\u6d4b\u5668\u5171\u4eab\u7684\u67b6\u6784\u6f0f\u6d1e\u800c\u975e\u7279\u5b9a\u68c0\u6d4b\u5668\u7684\u8106\u5f31\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.fe9180de", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdepthfirst.com%2Fpost%2Fcasting-a-net-ty-for-bugs-and-catching-a-big-one-cve-2025-59419%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=2026Q1_newsletter_TLDRInfoSec%26utm_content=secondary_placement%26utm_term=blog_post/1/0100019c33476dd1-b7f07f3c-280e-4f82-88ca-555aed2f81d0-000000/3PsgrxjF3Zbdng1CCPX6JkuVfH2Fbk2jdBFOCdh4FL0=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdepthfirst.com%2Fpost%2Fcasting-a-net-ty-for-bugs-and-catching-a-big-one-cve-2025-59419%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=2026Q1_newsletter_TLDRInfoSec%26utm_content=secondary_placement%26utm_term=blog_post/1/0100019c33476dd1-b7f07f3c-280e-4f82-88ca-555aed2f81d0-000000/3PsgrxjF3Zbdng1CCPX6JkuVfH2Fbk2jdBFOCdh4FL0=443", "authors": ["TLDR Newsletter"], "title": "Netty zero-day CVE: spoofed email that still passes all the checks", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdepthfirst.com%2Fpost%2Fcasting-a-net-ty-for-bugs-and-catching-a-big-one-cve-2025-59419%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=2026Q1_newsletter_TLDRInfoSec%26utm_content=secondary_placement%26utm_term=blog_post/1/0100019c33476dd1-b7f07f3c-280e-4f82-88ca-555aed2f81d0-000000/3PsgrxjF3Zbdng1CCPX6JkuVfH2Fbk2jdBFOCdh4FL0=443", "summary": "Netty zero-day CVE: spoofed email that still passes all the checks (Sponsor) Netty is everywhere in the Java ecosystem - used by Apple, Meta, and Google among many others. Read about a business logic flaw around SMTP that could enable attackers to bypass SPF, DKIM, and DMARC, and how depthfirst's security AI agent flagged the issue and generated a patch. Read the blog", "source": "tldr", "AI": {"tldr": "Netty SMTP\u7ec4\u4ef6\u5b58\u5728\u4e1a\u52a1\u903b\u8f91\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u4f2a\u9020\u901a\u8fc7SPF\u3001DKIM\u3001DMARC\u9a8c\u8bc1\u7684\u7535\u5b50\u90ae\u4ef6\uff0c\u6df1\u5ea6\u4f18\u5148\u5b89\u5168AI\u4ee3\u7406\u53d1\u73b0\u5e76\u751f\u6210\u8865\u4e01", "motivation": "Netty\u4f5c\u4e3aJava\u751f\u6001\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u6846\u67b6\uff08\u88abApple\u3001Meta\u3001Google\u7b49\u516c\u53f8\u4f7f\u7528\uff09\uff0c\u5176SMTP\u7ec4\u4ef6\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u53ef\u80fd\u5bfc\u81f4\u7535\u5b50\u90ae\u4ef6\u5b89\u5168\u673a\u5236\u88ab\u7ed5\u8fc7\uff0c\u9700\u8981\u53ca\u65f6\u53d1\u73b0\u548c\u4fee\u590d", "method": "\u4f7f\u7528\u6df1\u5ea6\u4f18\u5148\uff08depthfirst\uff09\u7684\u5b89\u5168AI\u4ee3\u7406\u8fdb\u884c\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u8be5AI\u4ee3\u7406\u80fd\u591f\u8bc6\u522b\u4e1a\u52a1\u903b\u8f91\u6f0f\u6d1e\u5e76\u81ea\u52a8\u751f\u6210\u76f8\u5e94\u7684\u8865\u4e01\u7a0b\u5e8f", "result": "\u6210\u529f\u53d1\u73b0Netty\u4e2dSMTP\u76f8\u5173\u7684\u4e1a\u52a1\u903b\u8f91\u6f0f\u6d1e\uff0c\u8be5\u6f0f\u6d1e\u5141\u8bb8\u653b\u51fb\u8005\u4f2a\u9020\u7535\u5b50\u90ae\u4ef6\u5e76\u7ed5\u8fc7SPF\u3001DKIM\u3001DMARC\u7b49\u5b89\u5168\u9a8c\u8bc1\u673a\u5236\uff0cAI\u4ee3\u7406\u81ea\u52a8\u751f\u6210\u4e86\u4fee\u590d\u8865\u4e01", "conclusion": "AI\u9a71\u52a8\u7684\u5b89\u5168\u4ee3\u7406\u80fd\u591f\u6709\u6548\u53d1\u73b0\u590d\u6742\u7684\u4e1a\u52a1\u903b\u8f91\u6f0f\u6d1e\u5e76\u81ea\u52a8\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u8fd9\u5bf9\u4e8e\u4fdd\u62a4\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u7ec4\u4ef6\u5b89\u5168\u5177\u6709\u91cd\u8981\u610f\u4e49", "topic": "code agent"}}
{"id": "tldr.2602.9cfed3d0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FyKHzwo/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/k8P5IngL1kyhZ88J_6d3IL3VNqPASi7DvC1hxJZ-j7U=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FyKHzwo/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/k8P5IngL1kyhZ88J_6d3IL3VNqPASi7DvC1hxJZ-j7U=443", "authors": ["TLDR Newsletter"], "title": "GPT-5.3-Codex", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FyKHzwo/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/k8P5IngL1kyhZ88J_6d3IL3VNqPASi7DvC1hxJZ-j7U=443", "summary": "GPT-5.3-Codex (11 minute read) OpenAI's GPT\u20115.3\u2011Codex is a faster agentic coding model that combines GPT\u20115.2\u2011Codex's coding performance with GPT\u20115.2's reasoning and professional knowledge.", "source": "tldr", "AI": {"tldr": "GPT-5.3-Codex\u662fOpenAI\u63a8\u51fa\u7684\u66f4\u5feb\u4ee3\u7406\u7f16\u7801\u6a21\u578b\uff0c\u7ed3\u5408\u4e86GPT-5.2-Codex\u7684\u7f16\u7801\u6027\u80fd\u548cGPT-5.2\u7684\u63a8\u7406\u4e0e\u4e13\u4e1a\u77e5\u8bc6", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u66f4\u5feb\u7684\u4ee3\u7406\u7f16\u7801\u6a21\u578b\uff0c\u5c06\u5f3a\u5927\u7684\u7f16\u7801\u80fd\u529b\u4e0e\u9ad8\u7ea7\u63a8\u7406\u548c\u4e13\u4e1a\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u7ed3\u5408GPT-5.2-Codex\u7684\u7f16\u7801\u6027\u80fd\u4e0eGPT-5.2\u7684\u63a8\u7406\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f18\u5316\u6a21\u578b\u67b6\u6784\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u54cd\u5e94\u901f\u5ea6", "result": "\u521b\u5efa\u4e86GPT-5.3-Codex\uff0c\u4e00\u4e2a\u66f4\u5feb\u7684\u4ee3\u7406\u7f16\u7801\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u7f16\u7801\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u54cd\u5e94\u901f\u5ea6", "conclusion": "GPT-5.3-Codex\u6210\u529f\u6574\u5408\u4e86\u7f16\u7801\u3001\u63a8\u7406\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u52a9\u624b", "topic": "code agent"}}
{"id": "tldr.2602.b6b944fa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/d6RaGkHZ0l7WzsRIJ6HpwoVU2CFrHC7MOZPxHjMWmvo=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/d6RaGkHZ0l7WzsRIJ6HpwoVU2CFrHC7MOZPxHjMWmvo=443", "authors": ["TLDR Newsletter"], "title": "Claude Opus 4.6", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/d6RaGkHZ0l7WzsRIJ6HpwoVU2CFrHC7MOZPxHjMWmvo=443", "summary": "Claude Opus 4.6 (12 minute read) Anthropic has released Claude Opus 4.6, an upgraded flagship model with stronger agentic coding, longer task persistence, and improved performance in large codebases. The model introduced a 1M-token context window in beta and achieved state-of-the-art results across several reasoning, coding, and economically valuable work evaluations.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Claude Opus 4.6\u65d7\u8230\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u4ee3\u7406\u7f16\u7801\u80fd\u529b\u3001\u66f4\u957f\u7684\u4efb\u52a1\u6301\u4e45\u6027\u548c\u6539\u8fdb\u7684\u5927\u4ee3\u7801\u5e93\u6027\u80fd\uff0c\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "motivation": "\u63d0\u5347AI\u6a21\u578b\u5728\u590d\u6742\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u4ee3\u7801\u5e93\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u6ee1\u8db3\u5b9e\u9645\u7ecf\u6d4e\u4ef7\u503c\u5de5\u4f5c\u7684\u9700\u6c42", "method": "\u5f00\u53d1\u5347\u7ea7\u7248\u65d7\u8230\u6a21\u578bClaude Opus 4.6\uff0c\u5f15\u51651M-token\u4e0a\u4e0b\u6587\u7a97\u53e3\uff08\u6d4b\u8bd5\u7248\uff09\uff0c\u589e\u5f3a\u4ee3\u7406\u7f16\u7801\u80fd\u529b\u548c\u4efb\u52a1\u6301\u4e45\u6027", "result": "\u5728\u591a\u9879\u63a8\u7406\u3001\u7f16\u7801\u548c\u7ecf\u6d4e\u4ef7\u503c\u5de5\u4f5c\u8bc4\u4f30\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6a21\u578b\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347", "conclusion": "Claude Opus 4.6\u4ee3\u8868\u4e86AI\u7f16\u7801\u4ee3\u7406\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b", "topic": "code agent"}}
{"id": "tldr.2602.dad2c23f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fresources%2Fworkshop-build-copilot-agent-with-microsoft%2F%3Futm_source=tldr-ai%26utm_medium=newsletter%26utm_campaign=26Q1_Microsoft_Copilot_Webinar/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/OU45IWlyFJlZQE-Kux4gB9XOsRwTdWInRjnA7EhTztU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fresources%2Fworkshop-build-copilot-agent-with-microsoft%2F%3Futm_source=tldr-ai%26utm_medium=newsletter%26utm_campaign=26Q1_Microsoft_Copilot_Webinar/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/OU45IWlyFJlZQE-Kux4gB9XOsRwTdWInRjnA7EhTztU=443", "authors": ["TLDR Newsletter"], "title": "Join Microsoft and CData to Build an Agentic Infrastructure that's Secure and Scalable", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fresources%2Fworkshop-build-copilot-agent-with-microsoft%2F%3Futm_source=tldr-ai%26utm_medium=newsletter%26utm_campaign=26Q1_Microsoft_Copilot_Webinar/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/OU45IWlyFJlZQE-Kux4gB9XOsRwTdWInRjnA7EhTztU=443", "summary": "Join Microsoft and CData to Build an Agentic Infrastructure that's Secure and Scalable (Sponsor) Join CData and Microsoft on 2/18 for a live build of secure, cross-functional agentic workflows spanning CRM, ERP, and billing. Get Microsoft's best practices to build an agentic infrastructure and how CData Connect AI provides the connectivity, context, and control to move agents from POC to production. Register here to join.", "source": "tldr", "AI": {"tldr": "\u5fae\u8f6f\u4e0eCData\u5408\u4f5c\u6784\u5efa\u5b89\u5168\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u73b0\u573a\u6f14\u793a\u5c55\u793a\u8de8CRM\u3001ERP\u548c\u8ba1\u8d39\u7cfb\u7edf\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u4ece\u667a\u80fd\u4f53\u6982\u5ff5\u9a8c\u8bc1\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u8de8\u7cfb\u7edf\u5de5\u4f5c\u6d41", "method": "\u7ed3\u5408\u5fae\u8f6f\u6700\u4f73\u5b9e\u8df5\u548cCData Connect AI\u6280\u672f\uff0c\u63d0\u4f9b\u8fde\u63a5\u6027\u3001\u4e0a\u4e0b\u6587\u548c\u63a7\u5236\u80fd\u529b\uff0c\u6784\u5efa\u8de8\u529f\u80fd\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41", "result": "\u901a\u8fc7\u73b0\u573a\u6f14\u793a\u5c55\u793a\u5982\u4f55\u6784\u5efa\u5b89\u5168\u3001\u8de8\u529f\u80fd\u7684\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u4ece\u6982\u5ff5\u9a8c\u8bc1\u5230\u751f\u4ea7\u73af\u5883\u7684\u5e73\u6ed1\u8fc7\u6e21", "conclusion": "\u5fae\u8f6f\u4e0eCData\u7684\u5408\u4f5c\u63d0\u4f9b\u4e86\u6784\u5efa\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5b9e\u73b0\u667a\u80fd\u4f53\u6280\u672f\u7684\u89c4\u6a21\u5316\u5e94\u7528", "topic": "agent analysis"}}
{"id": "tldr.2602.a71b0c8e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/bytNcytepyg6nj8Jn3cEZ8SX86a2MN21ojNDDdW2Meg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/bytNcytepyg6nj8Jn3cEZ8SX86a2MN21ojNDDdW2Meg=443", "authors": ["TLDR Newsletter"], "title": "Building a C compiler with a team of parallel Claudes", "comment": "Source: TLDR Newsletter, Date: 2026-02-06, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/bytNcytepyg6nj8Jn3cEZ8SX86a2MN21ojNDDdW2Meg=443", "summary": "Building a C compiler with a team of parallel Claudes (13 minute read) Multiple Claude instances worked in parallel to build a Rust-based C compiler capable of compiling the Linux 6.9 kernel. This approach reduced human supervision and explored autonomous LLM agent capabilities via stress tests and a $20,000, 2,000-session effort. The compiler, while functional, has limitations like inefficient code and reliance on GCC for some features.", "source": "tldr", "AI": {"tldr": "\u591a\u4e2aClaude\u5b9e\u4f8b\u5e76\u884c\u534f\u4f5c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eRust\u7684C\u7f16\u8bd1\u5668\uff0c\u80fd\u591f\u7f16\u8bd1Linux 6.9\u5185\u6838\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u76d1\u7763\uff0c\u63a2\u7d22\u4e86\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u80fd\u529b", "motivation": "\u63a2\u7d22\u5e76\u884cLLM\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u51cf\u5c11\u4eba\u5de5\u76d1\u7763\uff0c\u6d4b\u8bd5\u5927\u89c4\u6a21\u534f\u4f5c\u7684\u53ef\u884c\u6027", "method": "\u4f7f\u7528\u591a\u4e2aClaude\u5b9e\u4f8b\u5e76\u884c\u5de5\u4f5c\uff0c\u901a\u8fc72000\u6b21\u4f1a\u8bdd\u30012\u4e07\u7f8e\u5143\u7684\u6295\u5165\uff0c\u91c7\u7528\u538b\u529b\u6d4b\u8bd5\u65b9\u6cd5\u5f00\u53d1Rust-based C\u7f16\u8bd1\u5668", "result": "\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u7f16\u8bd1Linux 6.9\u5185\u6838\u7684C\u7f16\u8bd1\u5668\uff0c\u4f46\u5b58\u5728\u4ee3\u7801\u6548\u7387\u4e0d\u9ad8\u3001\u90e8\u5206\u529f\u80fd\u4f9d\u8d56GCC\u7b49\u9650\u5236", "conclusion": "\u5e76\u884cLLM\u4ee3\u7406\u80fd\u591f\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u51cf\u5c11\u4eba\u5de5\u76d1\u7763\uff0c\u4f46\u5f53\u524d\u65b9\u6848\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u5c55\u793a\u4e86\u81ea\u4e3b\u4ee3\u7406\u7684\u6f5c\u529b", "topic": "code agent"}}
{"id": "tldr.2602.aadc5a79", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.getdbt.com%2Fblog%2Fdbt-agent-skills%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/zg0RI290B2BEBUsRxk1l48gsBfREjLnB0VJUxawwpqE=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.getdbt.com%2Fblog%2Fdbt-agent-skills%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/zg0RI290B2BEBUsRxk1l48gsBfREjLnB0VJUxawwpqE=443", "authors": ["TLDR Newsletter"], "title": "Make Your AI Better at Data Work With dbt's Agent Skills", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.getdbt.com%2Fblog%2Fdbt-agent-skills%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/zg0RI290B2BEBUsRxk1l48gsBfREjLnB0VJUxawwpqE=443", "summary": "Make Your AI Better at Data Work With dbt's Agent Skills (14 minute read) dbt Labs open-sourced a collection of dbt agent skills to enhance AI coding agents by embedding dbt best practices, transforming generalist tools into specialized data agents for analytics engineering tasks. These skills cover key areas like full analytics workflows, semantic layer management, platform operations, and migrations.", "source": "tldr", "AI": {"tldr": "dbt Labs\u5f00\u6e90\u4e86\u4e00\u5957dbt\u4ee3\u7406\u6280\u80fd\uff0c\u5c06\u901a\u7528AI\u7f16\u7801\u4ee3\u7406\u8f6c\u5316\u4e3a\u4e13\u95e8\u7684\u6570\u636e\u4ee3\u7406\uff0c\u7528\u4e8e\u5206\u6790\u5de5\u7a0b\u4efb\u52a1", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u5728\u6570\u636e\u5de5\u4f5c\u65b9\u9762\u7f3a\u4e4f\u4e13\u95e8\u6280\u80fd\uff0c\u9700\u8981\u5c06dbt\u6700\u4f73\u5b9e\u8df5\u5d4c\u5165\u5230AI\u4ee3\u7406\u4e2d\uff0c\u63d0\u5347\u5176\u5728\u5206\u6790\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "method": "\u5f00\u6e90dbt\u4ee3\u7406\u6280\u80fd\u96c6\u5408\uff0c\u6db5\u76d6\u5b8c\u6574\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u3001\u8bed\u4e49\u5c42\u7ba1\u7406\u3001\u5e73\u53f0\u64cd\u4f5c\u548c\u8fc1\u79fb\u7b49\u5173\u952e\u9886\u57df\uff0c\u5c06\u901a\u7528\u5de5\u5177\u8f6c\u5316\u4e3a\u4e13\u95e8\u7684\u6570\u636e\u4ee3\u7406", "result": "\u521b\u5efa\u4e86\u4e13\u95e8\u9488\u5bf9\u5206\u6790\u5de5\u7a0b\u4efb\u52a1\u7684AI\u4ee3\u7406\u6280\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u6570\u636e\u5de5\u4f5c\uff0c\u63d0\u5347AI\u5728\u6570\u636e\u5de5\u7a0b\u9886\u57df\u7684\u4e13\u4e1a\u80fd\u529b", "conclusion": "\u901a\u8fc7\u5f00\u6e90dbt\u4ee3\u7406\u6280\u80fd\uff0cAI\u7f16\u7801\u4ee3\u7406\u53ef\u4ee5\u66f4\u597d\u5730\u652f\u6301\u5206\u6790\u5de5\u7a0b\u4efb\u52a1\uff0c\u63d0\u9ad8\u6570\u636e\u5de5\u4f5c\u7684\u6548\u7387\u548c\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2602.d091f521", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.smooth.sh%2F%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/gbi1DpXuLoCbZLY36GPMh_GazOtz_KQSE9G3NxUwWdg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.smooth.sh%2F%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/gbi1DpXuLoCbZLY36GPMh_GazOtz_KQSE9G3NxUwWdg=443", "authors": ["TLDR Newsletter"], "title": "Smooth", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.smooth.sh%2F%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/gbi1DpXuLoCbZLY36GPMh_GazOtz_KQSE9G3NxUwWdg=443", "summary": "Smooth (Tool) Smooth is a fast, low-cost browser agent that autonomously performs web tasks.", "source": "tldr", "AI": {"tldr": "Smooth\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\uff0c\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u7f51\u9875\u4efb\u52a1", "motivation": "\u5f53\u524d\u7f51\u9875\u81ea\u52a8\u5316\u4efb\u52a1\u901a\u5e38\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\u6216\u590d\u6742\u7684\u811a\u672c\u7f16\u5199\uff0cSmooth\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u4e3b\u6d4f\u89c8\u5668\u4ee3\u7406\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u7f51\u9875\u4efb\u52a1", "result": "\u521b\u5efa\u4e86Smooth\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u7f51\u9875\u4efb\u52a1\u7684\u81ea\u4e3b\u6267\u884c", "conclusion": "Smooth\u4f5c\u4e3a\u4e00\u4e2a\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7f51\u9875\u4efb\u52a1\u7684\u81ea\u52a8\u5316", "topic": "agent analysis"}}
{"id": "tldr.2602.47fa8cd7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/avUGtXaOd_G3McJFpK2a8FgHzs8wy4p2qhK1rSo3u0A=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/avUGtXaOd_G3McJFpK2a8FgHzs8wy4p2qhK1rSo3u0A=443", "authors": ["TLDR Newsletter"], "title": "Software Factories And The Agentic Moment", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/avUGtXaOd_G3McJFpK2a8FgHzs8wy4p2qhK1rSo3u0A=443", "summary": "Software Factories And The Agentic Moment (6 minute read) The Agentic Moment has profoundly changed the economics of software. Creating high-fidelity clones of SaaS applications was always possible, but not economically feasible. These clones can be used to validate at volumes and rates that far exceed production limits and allow for the testing of failure modes that would be dangerous or impossible against live services. This is handy when developing tests for software factories, non-interac...", "source": "tldr", "AI": {"tldr": "\u8f6f\u4ef6\u5de5\u5382\u548c\u667a\u80fd\u4f53\u65f6\u523b\u6539\u53d8\u4e86\u8f6f\u4ef6\u7ecf\u6d4e\u5b66\uff0c\u4f7f\u5f97\u521b\u5efaSaaS\u5e94\u7528\u7684\u9ad8\u4fdd\u771f\u514b\u9686\u53d8\u5f97\u7ecf\u6d4e\u53ef\u884c\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5371\u9669\u6545\u969c\u6a21\u5f0f", "motivation": "\u4f20\u7edf\u4e0a\u521b\u5efaSaaS\u5e94\u7528\u7684\u9ad8\u4fdd\u771f\u514b\u9686\u867d\u7136\u6280\u672f\u4e0a\u53ef\u884c\uff0c\u4f46\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u884c\u3002\u667a\u80fd\u4f53\u65f6\u523b\u6539\u53d8\u4e86\u8f6f\u4ef6\u7ecf\u6d4e\u5b66\uff0c\u4f7f\u5f97\u8fd9\u79cd\u514b\u9686\u53d8\u5f97\u7ecf\u6d4e\u53ef\u884c\uff0c\u53ef\u4ee5\u7528\u4e8e\u5927\u89c4\u6a21\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5371\u9669\u6545\u969c\u6a21\u5f0f", "method": "\u5229\u7528\u667a\u80fd\u4f53\u6280\u672f\u521b\u5efaSaaS\u5e94\u7528\u7684\u9ad8\u4fdd\u771f\u514b\u9686\uff0c\u8fd9\u4e9b\u514b\u9686\u53ef\u4ee5\u7528\u4e8e\u5927\u89c4\u6a21\u9a8c\u8bc1\u6d4b\u8bd5\uff0c\u8d85\u8d8a\u751f\u4ea7\u9650\u5236\uff0c\u5e76\u6d4b\u8bd5\u5728\u771f\u5b9e\u670d\u52a1\u4e2d\u5371\u9669\u6216\u4e0d\u53ef\u80fd\u7684\u6545\u969c\u6a21\u5f0f", "result": "\u667a\u80fd\u4f53\u65f6\u523b\u4f7f\u5f97\u8f6f\u4ef6\u5de5\u5382\u80fd\u591f\u7ecf\u6d4e\u5730\u521b\u5efa\u9ad8\u4fdd\u771fSaaS\u514b\u9686\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u9a8c\u8bc1\u548c\u5371\u9669\u6545\u969c\u6a21\u5f0f\u6d4b\u8bd5\uff0c\u8fd9\u5728\u4f20\u7edf\u65b9\u6cd5\u4e2d\u662f\u4e0d\u53ef\u80fd\u7684", "conclusion": "\u667a\u80fd\u4f53\u6280\u672f\u6df1\u523b\u6539\u53d8\u4e86\u8f6f\u4ef6\u7ecf\u6d4e\u5b66\uff0c\u4f7f\u5f97\u8f6f\u4ef6\u5de5\u5382\u80fd\u591f\u7ecf\u6d4e\u53ef\u884c\u5730\u521b\u5efa\u9ad8\u4fdd\u771f\u514b\u9686\uff0c\u4e3a\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027", "topic": "swe application"}}
{"id": "tldr.2602.176143e1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/NZeJKQPs7-nKEJ8Cwu_-5Esk46GYpiP4I-zazZS_r5U=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/NZeJKQPs7-nKEJ8Cwu_-5Esk46GYpiP4I-zazZS_r5U=443", "authors": ["TLDR Newsletter"], "title": "Eight more months of agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/NZeJKQPs7-nKEJ8Cwu_-5Esk46GYpiP4I-zazZS_r5U=443", "summary": "Eight more months of agents (9 minute read) The best software for an agent is whatever is best for a programmer.", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u6700\u9002\u5408\u667a\u80fd\u4f53\u7684\u8f6f\u4ef6\u5c31\u662f\u6700\u9002\u5408\u7a0b\u5e8f\u5458\u7684\u8f6f\u4ef6\uff0c\u5f3a\u8c03\u667a\u80fd\u4f53\u5f00\u53d1\u5e94\u5173\u6ce8\u7a0b\u5e8f\u5458\u7684\u9700\u6c42\u548c\u4f53\u9a8c\u3002", "motivation": "\u63a2\u8ba8\u667a\u80fd\u4f53\u8f6f\u4ef6\u5f00\u53d1\u7684\u6838\u5fc3\u7406\u5ff5\uff0c\u5f3a\u8c03\u667a\u80fd\u4f53\u5de5\u5177\u5e94\u8be5\u4ee5\u7a0b\u5e8f\u5458\u4e3a\u4e2d\u5fc3\uff0c\u800c\u4e0d\u662f\u8fc7\u5ea6\u8ffd\u6c42\u6280\u672f\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc78\u4e2a\u6708\u7684\u5b9e\u9645\u5f00\u53d1\u7ecf\u9a8c\u548c\u89c2\u5bdf\uff0c\u5206\u6790\u667a\u80fd\u4f53\u8f6f\u4ef6\u5f00\u53d1\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u8f6f\u4ef6\u7684\u6210\u529f\u5173\u952e\u5728\u4e8e\u80fd\u5426\u6709\u6548\u652f\u6301\u7a0b\u5e8f\u5458\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800c\u4e0d\u662f\u5355\u7eaf\u7684\u6280\u672f\u5148\u8fdb\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53\u8f6f\u4ef6\u5f00\u53d1\u5e94\u56de\u5f52\u672c\u8d28\uff0c\u5173\u6ce8\u7a0b\u5e8f\u5458\u7684\u9700\u6c42\u548c\u4f53\u9a8c\uff0c\u8fd9\u624d\u662f\u6784\u5efa\u4f18\u79c0\u667a\u80fd\u4f53\u5de5\u5177\u7684\u5173\u952e\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.aae4a60b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fclaude-opus-4-6-gradient-ai-platform%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/bAxnPGDLX4ukkWOUQfi39wg_ct8O8wPha1WqJbpmxOk=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fclaude-opus-4-6-gradient-ai-platform%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/bAxnPGDLX4ukkWOUQfi39wg_ct8O8wPha1WqJbpmxOk=443", "authors": ["TLDR Newsletter"], "title": "Now Available: Anthropic Claude Opus 4.6 on DigitalOcean's Agentic Inference Cloud", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fclaude-opus-4-6-gradient-ai-platform%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/bAxnPGDLX4ukkWOUQfi39wg_ct8O8wPha1WqJbpmxOk=443", "summary": "Now Available: Anthropic Claude Opus 4.6 on DigitalOcean's Agentic Inference Cloud (2 minute read) Claude Opus 4.6 has been made available on the DigitalOcean Gradient\u2122 AI Platform via Serverless Inference, offering teams access to Anthropic's advanced model with a 1M-token context for analyzing huge datasets and refactoring entire codebases. The model integrates natively into existing DigitalOcean environments, providing predictable billing and security-hardened defaults without requiring in...", "source": "tldr", "AI": {"tldr": "Anthropic Claude Opus 4.6\u6a21\u578b\u73b0\u5df2\u5728DigitalOcean\u7684Agentic Inference Cloud\u4e0a\u63d0\u4f9b\uff0c\u901a\u8fc7Serverless Inference\u670d\u52a1\uff0c\u4e3a\u56e2\u961f\u63d0\u4f9b1M-token\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u6570\u636e\u96c6\u548c\u91cd\u6784\u6574\u4e2a\u4ee3\u7801\u5e93\u3002", "motivation": "\u4e3a\u5f00\u53d1\u56e2\u961f\u63d0\u4f9b\u8bbf\u95ee\u5148\u8fdbAI\u6a21\u578b\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u548c\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u8ba1\u8d39\u548c\u5b89\u5168\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002", "method": "\u901a\u8fc7DigitalOcean Gradient\u2122 AI\u5e73\u53f0\u7684Serverless Inference\u670d\u52a1\u90e8\u7f72Claude Opus 4.6\u6a21\u578b\uff0c\u63d0\u4f9b\u539f\u751f\u96c6\u6210\u5230\u73b0\u6709DigitalOcean\u73af\u5883\uff0c\u652f\u63011M-token\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "result": "Claude Opus 4.6\u73b0\u5728\u53ef\u901a\u8fc7DigitalOcean\u7684Agentic Inference Cloud\u8bbf\u95ee\uff0c\u4e3a\u56e2\u961f\u63d0\u4f9b\u4e86\u5206\u6790\u5927\u578b\u6570\u636e\u96c6\u548c\u91cd\u6784\u4ee3\u7801\u5e93\u7684\u80fd\u529b\uff0c\u65e0\u9700\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3002", "conclusion": "DigitalOcean\u901a\u8fc7\u5176AI\u5e73\u53f0\u6210\u529f\u90e8\u7f72\u4e86Anthropic\u7684\u5148\u8fdb\u6a21\u578b\uff0c\u4e3a\u5f00\u53d1\u56e2\u961f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u6210\u672c\u53ef\u9884\u6d4b\u7684AI\u63a8\u7406\u670d\u52a1\u3002", "topic": "code agent"}}
{"id": "tldr.2602.5bfc4759", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fmanagement-tools%2Fdatadog-integrates-agent-development-kit-or-adk%2F%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/qGzzhwIK21cprKNnByKTWz309qodt08JurKUGSXWTko=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fmanagement-tools%2Fdatadog-integrates-agent-development-kit-or-adk%2F%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/qGzzhwIK21cprKNnByKTWz309qodt08JurKUGSXWTko=443", "authors": ["TLDR Newsletter"], "title": "Monitoring Google ADK agentic applications with Datadog LLM Observability", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fmanagement-tools%2Fdatadog-integrates-agent-development-kit-or-adk%2F%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/qGzzhwIK21cprKNnByKTWz309qodt08JurKUGSXWTko=443", "summary": "Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.", "source": "tldr", "AI": {"tldr": "Datadog LLM Observability \u65b0\u589e\u5bf9 Google ADK \u667a\u80fd\u4f53\u5e94\u7528\u7684\u81ea\u52a8\u76d1\u63a7\u529f\u80fd\uff0c\u53ef\u8ffd\u8e2a\u667a\u80fd\u4f53\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\uff0c\u4ee5\u4f18\u5316\u591a\u6b65\u9aa4\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "motivation": "\u968f\u7740 Google ADK \u667a\u80fd\u4f53\u5e94\u7528\u7684\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u6765\u8ffd\u8e2a\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u3001\u6027\u80fd\u6307\u6807\u548c\u5b89\u5168\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u590d\u6742\u7684\u591a\u6b65\u9aa4\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "method": "Datadog LLM Observability \u5e73\u53f0\u901a\u8fc7\u81ea\u52a8\u63d2\u6869\u6280\u672f\u96c6\u6210 Google ADK \u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u5b9e\u65f6\u76d1\u63a7\u3001\u79bb\u7ebf\u5b9e\u9a8c\u652f\u6301\u548c\u5b89\u5168\u8bc4\u4f30\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u667a\u80fd\u4f53\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u5168\u9762\u76d1\u63a7\uff0c\u652f\u6301\u5f00\u53d1\u4eba\u5458\u4f18\u5316\u591a\u6b65\u9aa4\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "Datadog \u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u4e3a Google ADK \u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.edfaef32", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/TRQZjeJYlrrH0Lrz-eh035AY4B7orrrg0cW9SK450gQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/TRQZjeJYlrrH0Lrz-eh035AY4B7orrrg0cW9SK450gQ=443", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/TRQZjeJYlrrH0Lrz-eh035AY4B7orrrg0cW9SK450gQ=443", "summary": "Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.", "source": "tldr", "AI": {"tldr": "Datadog LLM Observability\u96c6\u6210Google ADK\u4ee3\u7406\uff0c\u63d0\u4f9b\u81ea\u52a8\u76d1\u63a7\u4ee3\u7406\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\u4ee5\u4f18\u5316\u591a\u6b65\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002", "motivation": "\u968f\u7740Google ADK\u4ee3\u7406\u5e94\u7528\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u6765\u8ffd\u8e2a\u4ee3\u7406\u51b3\u7b56\u3001\u6027\u80fd\u6307\u6807\u548c\u5b89\u5168\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "Datadog LLM Observability\u5e73\u53f0\u81ea\u52a8instrument Google ADK\u4ee3\u7406\uff0c\u6536\u96c6\u4ee3\u7406\u51b3\u7b56\u3001token\u4f7f\u7528\u91cf\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\u7b49\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9Google ADK\u4ee3\u7406\u5e94\u7528\u7684\u5168\u9762\u76d1\u63a7\u80fd\u529b\uff0c\u80fd\u591f\u8ffd\u8e2a\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u7684\u4ee3\u7406\u884c\u4e3a\uff0c\u652f\u6301\u6027\u80fd\u4f18\u5316\u548c\u5b89\u5168\u8bc4\u4f30\u3002", "conclusion": "Datadog LLM Observability\u4e3aGoogle ADK\u4ee3\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316\u591a\u6b65\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.cc9f6552", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/LGeEh9NXfIQZDoM4Y4BCSHstFSxwnnqT5ipubV_X6TI=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/LGeEh9NXfIQZDoM4Y4BCSHstFSxwnnqT5ipubV_X6TI=443", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/LGeEh9NXfIQZDoM4Y4BCSHstFSxwnnqT5ipubV_X6TI=443", "summary": "Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.", "source": "tldr", "AI": {"tldr": "Datadog LLM Observability\u65b0\u589e\u5bf9Google ADK agentic\u5e94\u7528\u7684\u81ea\u52a8\u76d1\u63a7\u529f\u80fd\uff0c\u53ef\u8ffd\u8e2aagent\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\u4ee5\u4f18\u5316\u591a\u6b65agentic\u5de5\u4f5c\u6d41\u3002", "motivation": "\u968f\u7740agentic\u5e94\u7528\u5728Google ADK\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u6765\u8ffd\u8e2aagent\u51b3\u7b56\u8fc7\u7a0b\u3001\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u548c\u5de5\u4f5c\u6d41\u6027\u80fd\uff0c\u4ee5\u4f18\u5316\u591a\u6b65agentic\u5e94\u7528\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "Datadog LLM Observability\u901a\u8fc7\u81ea\u52a8instrumentation\u6280\u672f\u96c6\u6210Google ADK agents\uff0c\u63d0\u4f9b\u5b9e\u65f6\u76d1\u63a7\u6307\u6807\u6536\u96c6\uff0c\u5305\u62ec\u51b3\u7b56\u8ffd\u8e2a\u3001token\u6d88\u8017\u3001\u5ef6\u8fdf\u6d4b\u91cf\u548c\u54cd\u5e94\u8d28\u91cf\u8bc4\u4f30\uff0c\u540c\u65f6\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u73af\u5883\u642d\u5efa\u548c\u5b89\u5168\u8bc4\u4f30\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9Google ADK agentic\u5e94\u7528\u7684\u5168\u9762\u76d1\u63a7\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u65f6\u8ffd\u8e2a\u591a\u6b65\u5de5\u4f5c\u6d41\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u6027\u80fd\u6307\u6807\u548c\u5b89\u5168\u8bc4\u4f30\u6570\u636e\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316agentic\u5e94\u7528\u6027\u80fd\u3002", "conclusion": "Datadog LLM Observability\u5bf9Google ADK\u7684\u96c6\u6210\u6269\u5c55\u4e86agentic\u5e94\u7528\u76d1\u63a7\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u591a\u6b65\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u5206\u6790\u548c\u4f18\u5316\u5de5\u5177\uff0c\u63d0\u5347\u4e86agentic\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.97928395", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/6Fibwt7P_B-rePvfiEcqgnqJRI3adnzY75aD3rarkGA=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/6Fibwt7P_B-rePvfiEcqgnqJRI3adnzY75aD3rarkGA=443", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/6Fibwt7P_B-rePvfiEcqgnqJRI3adnzY75aD3rarkGA=443", "summary": "Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.", "source": "tldr", "AI": {"tldr": "Datadog LLM Observability\u65b0\u589e\u5bf9Google ADK agentic\u5e94\u7528\u7684\u81ea\u52a8\u76d1\u63a7\u529f\u80fd\uff0c\u652f\u6301\u8ffd\u8e2a\u4ee3\u7406\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u540c\u65f6\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740agentic\u5e94\u7528\uff08\u7279\u522b\u662f\u57fa\u4e8eGoogle ADK\u6784\u5efa\u7684\u5e94\u7528\uff09\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u6765\u8ffd\u8e2a\u4ee3\u7406\u51b3\u7b56\u8fc7\u7a0b\u3001\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u3001\u6027\u80fd\u6307\u6807\u548c\u5b89\u5168\u98ce\u9669\uff0c\u4ee5\u4f18\u5316\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u3002", "method": "\u901a\u8fc7Datadog LLM Observability\u5e73\u53f0\u81ea\u52a8instrument Google ADK agents\uff0c\u5b9e\u73b0\u5bf9\u4ee3\u7406\u51b3\u7b56\u3001token\u4f7f\u7528\u3001\u5ef6\u8fdf\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u76d1\u63a7\uff0c\u540c\u65f6\u652f\u6301\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5b89\u5168\u8bc4\u4f30\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9Google ADK agentic\u5e94\u7528\u7684\u5168\u9762\u76d1\u63a7\u80fd\u529b\uff0c\u80fd\u591f\u8ffd\u8e2a\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u4e2d\u7684\u4ee3\u7406\u884c\u4e3a\u3001\u8d44\u6e90\u6d88\u8017\u548c\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u4f18\u5316agentic\u5e94\u7528\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002", "conclusion": "Datadog LLM Observability\u7684\u6269\u5c55\u529f\u80fd\u4e3a\u76d1\u63a7\u548c\u4f18\u5316Google ADK agentic\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8agentic\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.a4f40ef6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcharliehills.substack.com%2Fp%2Fi-fired-my-team-and-hired-claude%3Futm_source=tldrmarketing/1/0100019c424bfc96-38389dba-9ec7-439a-95b0-6368dcbb0134-000000/AAcXLhQsQe-DeW-x1Tq49fUYFOFywF0IH9aK-MpzEXk=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcharliehills.substack.com%2Fp%2Fi-fired-my-team-and-hired-claude%3Futm_source=tldrmarketing/1/0100019c424bfc96-38389dba-9ec7-439a-95b0-6368dcbb0134-000000/AAcXLhQsQe-DeW-x1Tq49fUYFOFywF0IH9aK-MpzEXk=443", "authors": ["TLDR Newsletter"], "title": "I fired my team and hired Claude Opus 4.6", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcharliehills.substack.com%2Fp%2Fi-fired-my-team-and-hired-claude%3Futm_source=tldrmarketing/1/0100019c424bfc96-38389dba-9ec7-439a-95b0-6368dcbb0134-000000/AAcXLhQsQe-DeW-x1Tq49fUYFOFywF0IH9aK-MpzEXk=443", "summary": "I fired my team and hired Claude Opus 4.6 (9 minute read) This guide teaches how to use Claude Opus 4.6 to automate workflows by combining Skills, Cowork, and Plugins. Skills let you document processes once and have Claude follow them consistently. Cowork connects Claude to files, apps, and multi-step tasks, while Plugins bundle Skills with tool integrations and slash commands. The guide covers building Skills without code, applying them across content and visualization tasks, leveraging pre-...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Claude Opus 4.6\u901a\u8fc7Skills\u3001Cowork\u548cPlugins\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u65e0\u9700\u7f16\u7801\u5373\u53ef\u6784\u5efa\u6280\u80fd\u5e76\u5e94\u7528\u4e8e\u5185\u5bb9\u548c\u53ef\u89c6\u5316\u4efb\u52a1", "motivation": "\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u4eba\u5de5\u56e2\u961f\u6267\u884c\u91cd\u590d\u6027\u4efb\u52a1\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u4e00\u81f4\u6027\u5dee\u3002Claude Opus 4.6\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7AI\u4ee3\u7406\u66ff\u4ee3\u4eba\u5de5\u56e2\u961f\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7a0b\u7684\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316", "method": "\u4f7f\u7528Claude Opus 4.6\u7684\u4e09\u4e2a\u6838\u5fc3\u529f\u80fd\uff1aSkills\uff08\u8bb0\u5f55\u6d41\u7a0b\u8ba9Claude\u4e00\u81f4\u6267\u884c\uff09\u3001Cowork\uff08\u8fde\u63a5\u6587\u4ef6\u3001\u5e94\u7528\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\uff09\u3001Plugins\uff08\u5c06Skills\u4e0e\u5de5\u5177\u96c6\u6210\u548c\u659c\u6760\u547d\u4ee4\u6346\u7ed1\uff09\u3002\u6307\u5357\u6db5\u76d6\u65e0\u4ee3\u7801\u6784\u5efaSkills\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5185\u5bb9\u548c\u53ef\u89c6\u5316\u4efb\u52a1", "result": "\u901a\u8fc7Claude Opus 4.6\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u53ef\u4ee5\u66ff\u4ee3\u4eba\u5de5\u56e2\u961f\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u3001\u6807\u51c6\u5316\u548c\u6548\u7387\u63d0\u5347\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u4fdd\u6301\u4e00\u81f4\u6027", "conclusion": "Claude Opus 4.6\u4e3a\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684AI\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7Skills\u3001Cowork\u548cPlugins\u7684\u7ec4\u5408\uff0c\u65e0\u9700\u7f16\u7801\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u7684\u81ea\u52a8\u5316\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2602.03be8278", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/FVZdejyWP0E2F8zOHCmbEFxJ-DdZxyBb_58gB7jRr9A=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/FVZdejyWP0E2F8zOHCmbEFxJ-DdZxyBb_58gB7jRr9A=443", "authors": ["TLDR Newsletter"], "title": "Software Factories And The Agentic Moment", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/FVZdejyWP0E2F8zOHCmbEFxJ-DdZxyBb_58gB7jRr9A=443", "summary": "Software Factories And The Agentic Moment (6 minute read) StrongDM has developed a \"Software Factory\" where AI agents autonomously write and refine code based on specifications and scenarios, removing human involvement in coding and review. One of the main innovations making this possible is its \"Digital Twin Universe,\" a system of behavioral clones of third-party services that allows for high-volume, safe, and cost-effective validation of software scenarios.", "source": "tldr", "AI": {"tldr": "StrongDM\u5f00\u53d1\u4e86\"\u8f6f\u4ef6\u5de5\u5382\"\uff0cAI\u4ee3\u7406\u80fd\u57fa\u4e8e\u89c4\u8303\u548c\u573a\u666f\u81ea\u4e3b\u7f16\u5199\u548c\u4f18\u5316\u4ee3\u7801\uff0c\u65e0\u9700\u4eba\u5de5\u7f16\u7801\u548c\u5ba1\u67e5\uff0c\u901a\u8fc7\"\u6570\u5b57\u5b6a\u751f\u5b87\u5b99\"\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u3001\u4f4e\u6210\u672c\u7684\u8f6f\u4ef6\u573a\u666f\u9a8c\u8bc1\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u8f6f\u4ef6\u5f00\u53d1\u7684\u5168\u81ea\u52a8\u5316\uff0c\u6d88\u9664\u4eba\u5de5\u7f16\u7801\u548c\u5ba1\u67e5\u73af\u8282\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u89e3\u51b3\u7b2c\u4e09\u65b9\u670d\u52a1\u6d4b\u8bd5\u7684\u5b89\u5168\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\"\u8f6f\u4ef6\u5de5\u5382\"\u67b6\u6784\uff0cAI\u4ee3\u7406\u6839\u636e\u89c4\u8303\u548c\u573a\u666f\u81ea\u4e3b\u751f\u6210\u4ee3\u7801\uff1b\u521b\u65b0\u6027\u5730\u5f15\u5165\"\u6570\u5b57\u5b6a\u751f\u5b87\u5b99\"\u7cfb\u7edf\uff0c\u521b\u5efa\u7b2c\u4e09\u65b9\u670d\u52a1\u7684\u884c\u4e3a\u514b\u9686\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u5b89\u5168\u3001\u4f4e\u6210\u672c\u7684\u8f6f\u4ef6\u573a\u666f\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u81ea\u4e3b\u7f16\u5199\u548c\u4f18\u5316\u4ee3\u7801\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u3001\u4f4e\u6210\u672c\u7684\u8f6f\u4ef6\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u53c2\u4e0e\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u5382\u548c\u6570\u5b57\u5b6a\u751f\u5b87\u5b99\u7684\u7ed3\u5408\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u6548\u679c\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "topic": "swe application"}}
{"id": "tldr.2602.531473a0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flocalghost.dev%2Fblog%2Fstop-generating-start-thinking%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/X-34Yf5QtI3UteW96yJfcKMcZnC9hwzeACIVdp627yQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flocalghost.dev%2Fblog%2Fstop-generating-start-thinking%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/X-34Yf5QtI3UteW96yJfcKMcZnC9hwzeACIVdp627yQ=443", "authors": ["TLDR Newsletter"], "title": "Stop generating, start thinking", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flocalghost.dev%2Fblog%2Fstop-generating-start-thinking%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/X-34Yf5QtI3UteW96yJfcKMcZnC9hwzeACIVdp627yQ=443", "summary": "Stop generating, start thinking (9 minute read) A seasoned engineer is uneasy about the industry's rapid adoption of LLM-generated code. He's worried about outsourcing critical thinking, as LLMs are best at generation but not as great at reasoning like a human is.", "source": "tldr", "AI": {"tldr": "\u8d44\u6df1\u5de5\u7a0b\u5e08\u62c5\u5fe7\u884c\u4e1a\u5feb\u901f\u91c7\u7528LLM\u751f\u6210\u4ee3\u7801\uff0c\u8ba4\u4e3a\u8fd9\u4f1a\u5916\u5305\u5173\u952e\u6027\u601d\u8003\uff0c\u56e0\u4e3aLLM\u64c5\u957f\u751f\u6210\u4f46\u4e0d\u64c5\u957f\u4eba\u7c7b\u63a8\u7406", "motivation": "\u884c\u4e1a\u5bf9LLM\u751f\u6210\u4ee3\u7801\u7684\u5feb\u901f\u91c7\u7528\u5f15\u53d1\u62c5\u5fe7\uff0c\u62c5\u5fc3\u8fd9\u4f1a\u5916\u5305\u5173\u952e\u6027\u601d\u8003\u80fd\u529b\uff0cLLM\u64c5\u957f\u751f\u6210\u4f46\u4e0d\u64c5\u957f\u4eba\u7c7b\u5f0f\u63a8\u7406", "method": "\u89c2\u70b9\u6027\u6587\u7ae0\uff0c\u57fa\u4e8e\u5de5\u7a0b\u7ecf\u9a8c\u5206\u6790LLM\u4ee3\u7801\u751f\u6210\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4eba\u7c7b\u63a8\u7406\u7684\u91cd\u8981\u6027", "result": "\u63d0\u51fa\u8b66\u544a\uff1a\u8fc7\u5ea6\u4f9d\u8d56LLM\u751f\u6210\u4ee3\u7801\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u601d\u8003\u80fd\u529b\u9000\u5316\uff0c\u9700\u8981\u5e73\u8861\u4f7f\u7528", "conclusion": "\u5e94\u505c\u6b62\u76f2\u76ee\u751f\u6210\uff0c\u5f00\u59cb\u601d\u8003\uff0c\u5728\u5229\u7528LLM\u7684\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u63a8\u7406\u80fd\u529b", "topic": "agent analysis"}}
{"id": "tldr.2602.1bd2e270", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Ftwo-kinds-of-ai-users-are-emerging%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/MpQHqHqim_eLDyvMZIU6SuLrstpz4WgSr0rNwVuqZic=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Ftwo-kinds-of-ai-users-are-emerging%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/MpQHqHqim_eLDyvMZIU6SuLrstpz4WgSr0rNwVuqZic=443", "authors": ["TLDR Newsletter"], "title": "Two kinds of AI users are emerging. The gap between them is astonishing", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Ftwo-kinds-of-ai-users-are-emerging%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/MpQHqHqim_eLDyvMZIU6SuLrstpz4WgSr0rNwVuqZic=443", "summary": "Two kinds of AI users are emerging. The gap between them is astonishing (5 minute read) Martin sees two kinds of AI users emerging: power users who've embraced tools like Claude Code and are flying, and everyone else stuck on ChatGPT or Microsoft's enterprise Copilot \u2014 which is so bad that Microsoft itself uses Claude Code internally. The result is small teams with API access and coding agents massively outproducing enterprises trapped behind locked-down IT and legacy software.", "source": "tldr", "AI": {"tldr": "\u6587\u7ae0\u6307\u51faAI\u7528\u6237\u5206\u5316\u4e3a\u4e24\u7c7b\uff1a\u4f7f\u7528Claude Code\u7b49\u5148\u8fdb\u5de5\u5177\u7684\u9ad8\u6548\u7528\u6237\uff0c\u4ee5\u53ca\u53d7\u9650\u4e8eChatGPT\u6216\u4f01\u4e1a\u7ea7Copilot\u7684\u666e\u901a\u7528\u6237\uff0c\u5bfc\u81f4\u5c0f\u578b\u56e2\u961f\u751f\u4ea7\u529b\u8fdc\u8d85\u5927\u578b\u4f01\u4e1a", "motivation": "\u63ed\u793aAI\u4f7f\u7528\u4e2d\u7684\u4e24\u6781\u5206\u5316\u73b0\u8c61\uff0c\u5f3a\u8c03\u5148\u8fdbAI\u5de5\u5177\u5bf9\u751f\u4ea7\u529b\u7684\u5de8\u5927\u5f71\u54cd\uff0c\u4ee5\u53ca\u4f01\u4e1a\u56e0IT\u9650\u5236\u548c\u9057\u7559\u7cfb\u7edf\u800c\u843d\u540e\u7684\u73b0\u72b6", "method": "\u901a\u8fc7\u89c2\u5bdf\u548c\u5bf9\u6bd4\u5206\u6790\u4e24\u7c7bAI\u7528\u6237\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\uff1a\u4e00\u7c7b\u4f7f\u7528Claude Code\u7b49API\u5de5\u5177\u548c\u7f16\u7801\u4ee3\u7406\uff0c\u53e6\u4e00\u7c7b\u53d7\u9650\u4e8e\u4f01\u4e1a\u7ea7\u9650\u5236\u6027\u5de5\u5177", "result": "\u5c0f\u578b\u56e2\u961f\u51ed\u501fAPI\u8bbf\u95ee\u548c\u7f16\u7801\u4ee3\u7406\u5de5\u5177\u751f\u4ea7\u529b\u8fdc\u8d85\u5927\u578b\u4f01\u4e1a\uff0c\u5fae\u8f6f\u5185\u90e8\u751a\u81f3\u4f7f\u7528Claude Code\u800c\u975e\u81ea\u5bb6Copilot\uff0c\u663e\u793a\u4f01\u4e1a\u7ea7\u5de5\u5177\u8d28\u91cf\u4e0d\u8db3", "conclusion": "AI\u4f7f\u7528\u6548\u7387\u51fa\u73b0\u4e25\u91cd\u5206\u5316\uff0c\u4f01\u4e1a\u9700\u6253\u7834IT\u9650\u5236\u548c\u9057\u7559\u7cfb\u7edf\u675f\u7f1a\uff0c\u91c7\u7528\u5148\u8fdbAI\u5de5\u5177\u624d\u80fd\u4fdd\u6301\u7ade\u4e89\u529b", "topic": "agent analysis"}}
{"id": "tldr.2602.9e928822", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.io%2Fgh-aw%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/091-nmIlzgcJu2kEx5736Qug1BJMAQBSaSqjPVdfzbQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.io%2Fgh-aw%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/091-nmIlzgcJu2kEx5736Qug1BJMAQBSaSqjPVdfzbQ=443", "authors": ["TLDR Newsletter"], "title": "GitHub Agentic Workflows", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.io%2Fgh-aw%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/091-nmIlzgcJu2kEx5736Qug1BJMAQBSaSqjPVdfzbQ=443", "summary": "GitHub Agentic Workflows (Website) GitHub Agentic Workflows enable repository automation by running AI coding agents within GitHub Actions. Users define these continuous AI workflows in natural language using Markdown files, which are then compiled into GitHub Actions to automate tasks like code improvements, issue triage, documentation, and analysis. They integrate deeply with GitHub, support multiple AI engines like Copilot, Claude, and Codex, and prioritize safety with sandboxed execution ...", "source": "tldr", "AI": {"tldr": "GitHub Agentic Workflows \u662f\u4e00\u4e2a\u5728 GitHub Actions \u4e2d\u8fd0\u884c AI \u7f16\u7801\u4ee3\u7406\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u6301\u7eed AI \u5de5\u4f5c\u6d41\uff0c\u81ea\u52a8\u6267\u884c\u4ee3\u7801\u6539\u8fdb\u3001\u95ee\u9898\u5206\u7c7b\u3001\u6587\u6863\u751f\u6210\u7b49\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u914d\u7f6e\u590d\u6742\u3001\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u8f7b\u677e\u521b\u5efa AI \u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u4f7f\u7528 Markdown \u6587\u4ef6\u5b9a\u4e49\u81ea\u7136\u8bed\u8a00\u5de5\u4f5c\u6d41\uff0c\u7136\u540e\u7f16\u8bd1\u4e3a GitHub Actions\uff1b\u652f\u6301\u591a\u79cd AI \u5f15\u64ce\uff08Copilot\u3001Claude\u3001Codex\uff09\uff1b\u91c7\u7528\u6c99\u76d2\u6267\u884c\u786e\u4fdd\u5b89\u5168\u6027\uff1b\u6df1\u5ea6\u96c6\u6210 GitHub \u5e73\u53f0\u3002", "result": "\u5b9e\u73b0\u4e86\u5728 GitHub Actions \u4e2d\u8fd0\u884c AI \u7f16\u7801\u4ee3\u7406\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6267\u884c\u4ee3\u7801\u6539\u8fdb\u3001\u95ee\u9898\u5206\u7c7b\u3001\u6587\u6863\u751f\u6210\u548c\u5206\u6790\u7b49\u4efb\u52a1\uff0c\u7b80\u5316\u4e86\u5de5\u4f5c\u6d41\u914d\u7f6e\u8fc7\u7a0b\u3002", "conclusion": "GitHub Agentic Workflows \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5b89\u5168\u4e14\u5f3a\u5927\u7684\u65b9\u5f0f\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5229\u7528 AI \u4ee3\u7406\u81ea\u52a8\u5316 GitHub \u4ed3\u5e93\u4e2d\u7684\u5404\u79cd\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2602.f431e562", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-cloud-agent-thesis%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/ZkPqZymxOlboMpd9J_i1ptsQUHcdAyKHYNqUVHGZPdY=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-cloud-agent-thesis%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/ZkPqZymxOlboMpd9J_i1ptsQUHcdAyKHYNqUVHGZPdY=443", "authors": ["TLDR Newsletter"], "title": "The Cloud Agent Thesis", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-cloud-agent-thesis%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/ZkPqZymxOlboMpd9J_i1ptsQUHcdAyKHYNqUVHGZPdY=443", "summary": "The Cloud Agent Thesis (6 minute read) Cloud agents like Devin shift AI coding from local pair programming to organizational delegation. Running on remote infrastructure, these tools allow non-engineers to trigger tasks via Slack or Jira. This model scales capacity asynchronously, moving the primary bottleneck from code generation to automated review.", "source": "tldr", "AI": {"tldr": "\u4e91\u4ee3\u7406\uff08\u5982Devin\uff09\u5c06AI\u7f16\u7801\u4ece\u672c\u5730\u7ed3\u5bf9\u7f16\u7a0b\u8f6c\u53d8\u4e3a\u7ec4\u7ec7\u59d4\u6258\uff0c\u901a\u8fc7\u8fdc\u7a0b\u57fa\u7840\u8bbe\u65bd\u8fd0\u884c\uff0c\u5141\u8bb8\u975e\u5de5\u7a0b\u5e08\u901a\u8fc7Slack\u6216Jira\u89e6\u53d1\u4efb\u52a1\uff0c\u5f02\u6b65\u6269\u5c55\u5f00\u53d1\u80fd\u529b\uff0c\u4e3b\u8981\u74f6\u9888\u4ece\u4ee3\u7801\u751f\u6210\u8f6c\u79fb\u5230\u81ea\u52a8\u5316\u5ba1\u67e5\u3002", "motivation": "\u4f20\u7edfAI\u7f16\u7801\u5de5\u5177\u5c40\u9650\u4e8e\u672c\u5730\u7ed3\u5bf9\u7f16\u7a0b\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7ec4\u7ec7\u7ea7\u5f00\u53d1\u9700\u6c42\u3002\u4e91\u4ee3\u7406\u6a21\u578b\u65e8\u5728\u901a\u8fc7\u8fdc\u7a0b\u57fa\u7840\u8bbe\u65bd\u548c\u5f02\u6b65\u4efb\u52a1\u89e6\u53d1\u673a\u5236\uff0c\u5b9e\u73b0\u5f00\u53d1\u80fd\u529b\u7684\u89c4\u6a21\u5316\u6269\u5c55\uff0c\u8ba9\u975e\u6280\u672f\u4eba\u5458\u4e5f\u80fd\u53c2\u4e0e\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e91\u4ee3\u7406\u67b6\u6784\uff0c\u5728\u8fdc\u7a0b\u57fa\u7840\u8bbe\u65bd\u4e0a\u8fd0\u884cAI\u7f16\u7801\u4ee3\u7406\uff0c\u901a\u8fc7Slack\u3001Jira\u7b49\u534f\u4f5c\u5de5\u5177\u63a5\u53e3\u5141\u8bb8\u975e\u5de5\u7a0b\u5e08\u7528\u6237\u89e6\u53d1\u5f00\u53d1\u4efb\u52a1\uff0c\u5b9e\u73b0\u5f02\u6b65\u3001\u53ef\u6269\u5c55\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\u3002", "result": "\u4e91\u4ee3\u7406\u6a21\u578b\u6210\u529f\u5c06AI\u7f16\u7801\u4ece\u672c\u5730\u534f\u4f5c\u6269\u5c55\u5230\u7ec4\u7ec7\u7ea7\u59d4\u6258\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u80fd\u529b\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5c06\u4e3b\u8981\u74f6\u9888\u4ece\u4ee3\u7801\u751f\u6210\u8f6c\u79fb\u5230\u81ea\u52a8\u5316\u5ba1\u67e5\u6d41\u7a0b\u3002", "conclusion": "\u4e91\u4ee3\u7406\u4ee3\u8868\u4e86AI\u7f16\u7801\u5de5\u5177\u7684\u91cd\u8981\u6f14\u8fdb\u65b9\u5411\uff0c\u901a\u8fc7\u8fdc\u7a0b\u57fa\u7840\u8bbe\u65bd\u548c\u5f02\u6b65\u4efb\u52a1\u673a\u5236\u5b9e\u73b0\u4e86\u5f00\u53d1\u80fd\u529b\u7684\u89c4\u6a21\u5316\u6269\u5c55\uff0c\u4e3a\u7ec4\u7ec7\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "tldr.2602.c8ac5831", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fstablyai%2Fagent-slack%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/4mpMQ-cos6i3SBwaWkOnGND1UBbjx4sJDO-MpTKQ6Hc=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fstablyai%2Fagent-slack%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/4mpMQ-cos6i3SBwaWkOnGND1UBbjx4sJDO-MpTKQ6Hc=443", "authors": ["TLDR Newsletter"], "title": "Agent Slack", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fstablyai%2Fagent-slack%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/4mpMQ-cos6i3SBwaWkOnGND1UBbjx4sJDO-MpTKQ6Hc=443", "summary": "Agent Slack (GitHub Repo) Agent Slack is a CLI specifically designed to enable AI agents to automate various interactions within Slack.", "source": "tldr", "AI": {"tldr": "Agent Slack\u662f\u4e00\u4e2a\u4e13\u4e3aAI\u4ee3\u7406\u8bbe\u8ba1\u7684CLI\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u5316Slack\u5e73\u53f0\u4e0a\u7684\u5404\u79cd\u4ea4\u4e92\u64cd\u4f5c", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u548c\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u8ba9AI\u4ee3\u7406\u80fd\u591f\u4e0e\u6d41\u884c\u7684\u534f\u4f5c\u5e73\u53f0\uff08\u5982Slack\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u4efb\u52a1\u6267\u884c", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u547d\u4ee4\u884c\u754c\u9762\uff08CLI\uff09\u5de5\u5177\uff0c\u4e13\u95e8\u9488\u5bf9Slack API\u8fdb\u884c\u4f18\u5316\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u7f16\u7a0b\u65b9\u5f0f\u8bbf\u95ee\u548c\u64cd\u4f5cSlack\u7684\u5404\u79cd\u529f\u80fd", "result": "\u521b\u5efa\u4e86Agent Slack\u5de5\u5177\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e0eSlack\u5e73\u53f0\u4ea4\u4e92\u7684\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u7b80\u5316\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u5b9e\u73b0", "conclusion": "Agent Slack\u586b\u8865\u4e86AI\u4ee3\u7406\u4e0e\u534f\u4f5c\u5de5\u5177\u96c6\u6210\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u652f\u6301", "topic": "code agent"}}
{"id": "tldr.2602.906d6aaa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Feb6Jtf/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/Bh_gpIiEgi5zrfPXDxvJNU98uEeaX777ALtd2_ikdAw=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Feb6Jtf/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/Bh_gpIiEgi5zrfPXDxvJNU98uEeaX777ALtd2_ikdAw=443", "authors": ["TLDR Newsletter"], "title": "Unbrowse: 100x Faster Agent Web Interactions via x402", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Feb6Jtf/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/Bh_gpIiEgi5zrfPXDxvJNU98uEeaX777ALtd2_ikdAw=443", "summary": "Unbrowse: 100x Faster Agent Web Interactions via x402 (4 minute read) Foundry Unbrowse is an open-source tool that accelerates AI agent web interactions by converting browser automation into direct API calls, improving speeds from 10-45 seconds to 200ms while boosting reliability from 70-85% to 95%+. The tool, built as an OpenClaw plugin, intercepts network traffic to extract underlying API endpoints and auto-generates TypeScript clients, eliminating resource-intensive headless Chrome. Foundr...", "source": "tldr", "AI": {"tldr": "Unbrowse \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u901a\u8fc7\u5c06\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u8f6c\u6362\u4e3a\u76f4\u63a5 API \u8c03\u7528\uff0c\u5c06 AI \u4ee3\u7406\u7684\u7f51\u9875\u4ea4\u4e92\u901f\u5ea6\u63d0\u5347 100 \u500d\uff08\u4ece 10-45 \u79d2\u964d\u81f3 200 \u6beb\u79d2\uff09\uff0c\u540c\u65f6\u5c06\u53ef\u9760\u6027\u4ece 70-85% \u63d0\u5347\u81f3 95% \u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d AI \u4ee3\u7406\u901a\u8fc7 headless Chrome \u8fdb\u884c\u7f51\u9875\u4ea4\u4e92\u5b58\u5728\u901f\u5ea6\u6162\uff0810-45 \u79d2\uff09\u548c\u53ef\u9760\u6027\u4f4e\uff0870-85%\uff09\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u4ee3\u7406\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4f5c\u4e3a OpenClaw \u63d2\u4ef6\u6784\u5efa\uff0c\u901a\u8fc7\u62e6\u622a\u7f51\u7edc\u6d41\u91cf\u63d0\u53d6\u5e95\u5c42 API \u7aef\u70b9\uff0c\u5e76\u81ea\u52a8\u751f\u6210 TypeScript \u5ba2\u6237\u7aef\uff0c\u4ece\u800c\u7ed5\u8fc7\u8d44\u6e90\u5bc6\u96c6\u578b\u7684 headless Chrome\u3002", "result": "\u5b9e\u73b0\u4e86 100 \u500d\u7684\u901f\u5ea6\u63d0\u5347\uff08\u4ece 10-45 \u79d2\u964d\u81f3 200 \u6beb\u79d2\uff09\uff0c\u53ef\u9760\u6027\u4ece 70-85% \u63d0\u5347\u81f3 95% \u4ee5\u4e0a\uff0c\u663e\u8457\u6539\u5584\u4e86 AI \u4ee3\u7406\u7684\u7f51\u9875\u4ea4\u4e92\u6027\u80fd\u3002", "conclusion": "Unbrowse \u901a\u8fc7\u5c06\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u8f6c\u6362\u4e3a\u76f4\u63a5 API \u8c03\u7528\uff0c\u4e3a AI \u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u7f51\u9875\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf headless Chrome \u65b9\u6cd5\u7684\u6027\u80fd\u74f6\u9888\u3002", "topic": "code agent"}}
{"id": "tldr.2602.7e02822c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz1URwz/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/O6XU1z0nBTWkQV_uhsES_MQvz9igK_VGwJyGqXb13fY=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz1URwz/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/O6XU1z0nBTWkQV_uhsES_MQvz9igK_VGwJyGqXb13fY=443", "authors": ["TLDR Newsletter"], "title": "Infrastructure Pillars for Autonomous Agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz1URwz/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/O6XU1z0nBTWkQV_uhsES_MQvz9igK_VGwJyGqXb13fY=443", "summary": "Infrastructure Pillars for Autonomous Agents (5 minute read) Virtuals Protocol has outlined infrastructure requirements for autonomous AI agents, arguing that traditional systems fail because they assume human operators rather than persistent, value-holding entities. The framework centers on Identity \u2013 featuring proposed ERC-8004 standard with on-chain wallets as immutable identifiers plus reputation registries \u2013 and Commerce \u2013 an Agent Commerce Protocol enabling discovery, standardized negot...", "source": "tldr", "AI": {"tldr": "Virtuals Protocol\u63d0\u51fa\u81ea\u4e3bAI\u4ee3\u7406\u7684\u57fa\u7840\u8bbe\u65bd\u6846\u67b6\uff0c\u5f3a\u8c03\u4f20\u7edf\u7cfb\u7edf\u56e0\u5047\u8bbe\u4eba\u7c7b\u64cd\u4f5c\u800c\u5931\u8d25\uff0c\u9700\u8981\u8eab\u4efd\u548c\u5546\u4e1a\u4e24\u5927\u652f\u67f1", "motivation": "\u4f20\u7edf\u7cfb\u7edf\u5047\u8bbe\u4eba\u7c7b\u64cd\u4f5c\u8005\uff0c\u65e0\u6cd5\u652f\u6301\u6301\u4e45\u5316\u3001\u6301\u6709\u4ef7\u503c\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u9700\u8981\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301AI\u4ee3\u7406\u7684\u8eab\u4efd\u8bc6\u522b\u548c\u5546\u4e1a\u6d3b\u52a8", "method": "\u63d0\u51fa\u5305\u542b\u8eab\u4efd\u548c\u5546\u4e1a\u4e24\u5927\u652f\u67f1\u7684\u6846\u67b6\uff1a\u8eab\u4efd\u652f\u67f1\u5305\u62ecERC-8004\u6807\u51c6\uff08\u94fe\u4e0a\u94b1\u5305\u4f5c\u4e3a\u4e0d\u53ef\u53d8\u6807\u8bc6\u7b26\uff09\u548c\u58f0\u8a89\u6ce8\u518c\u8868\uff1b\u5546\u4e1a\u652f\u67f1\u5305\u62ec\u4ee3\u7406\u5546\u4e1a\u534f\u8bae\uff0c\u652f\u6301\u53d1\u73b0\u3001\u6807\u51c6\u5316\u534f\u5546\u7b49\u529f\u80fd", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u57fa\u7840\u8bbe\u65bd\u6846\u67b6\uff0c\u4e3a\u81ea\u4e3bAI\u4ee3\u7406\u8bbe\u8ba1\u4e86\u8eab\u4efd\u8bc6\u522b\u548c\u5546\u4e1a\u4ea4\u6613\u7684\u7cfb\u7edf\u67b6\u6784", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u8eab\u4efd\u548c\u5546\u4e1a\u662f\u6838\u5fc3\u652f\u67f1\uff0cERC-8004\u6807\u51c6\u548c\u4ee3\u7406\u5546\u4e1a\u534f\u8bae\u662f\u5173\u952e\u7ec4\u4ef6", "topic": "agent analysis"}}
