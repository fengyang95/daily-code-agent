<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.SE](#cs.SE) [Total: 7]
- [wechat.article](#wechat.article) [Total: 24]
- [cs.AI](#cs.AI) [Total: 19]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation](https://arxiv.org/abs/2510.05124)
*Mingjin Li,Yu Liu,Huayi Liu,Xiang Ye,Chao Jiang,Hongguang Zhang*

Main category: cs.CL

TL;DR: MADS是一个通过智能体自博弈生成说服性多轮对话的可扩展框架，包含用户智能体、对话智能体和优化智能体，无需人工标注即可生成训练数据，在真实营销场景中显著提升了小模型的转化率。


<details>
  <summary>Details</summary>
Motivation: 解决行业关键挑战：缺乏用户数据、冷启动评估困难和提示效率低下，通过低成本生成训练数据来提升说服能力。

Method: 使用三个协调智能体：模拟多样化用户行为的用户智能体、执行任务导向说服策略的对话智能体、评估优化对话结果的优化智能体，结合用户态度链建模和专用LLM评估。

Result: 在真实营销场景中，显著提升了小LLM的说服能力，将有机流量转化率从1.83%提高到2.24%，提升了22.4%。

Conclusion: MADS框架能够有效生成高质量对话数据，显著提升模型说服能力，具有明确的商业价值。

Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for
generating persuasive multi-turn dialogues via agent self-play. MADS employs
three coordinated agents: User Agents simulating diverse persona-driven
behaviors, a Dialog Agent executing task-oriented persuasion strategies and an
Optimization Agent evaluating and refining dialogue outcomes. We further
validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and
dedicated LLMs' persuasion assessment. This approach enables low-cost
generation of training data without human annotation, addressing key industry
challenges such as lack of user data, cold-start evaluation difficulties, and
prompt inefficiency. Applied to a real-world marketing scenario, MADS
significantly improved the persuasion capacity of small LLMs, increasing the
organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) ,
demonstrating clear business value.

</details>


### [2] [Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery](https://arxiv.org/abs/2510.05131)
*Bowen Wei*

Main category: cs.CL

TL;DR: 提出了一种混合语义搜索系统，结合了轻量级容错词汇检索、基于嵌入的向量相似性和约束性大语言模型重排序，以解决Head Start项目中GoEngage平台的任务查找困难问题。


<details>
  <summary>Details</summary>
Motivation: Head Start项目工作人员在GoEngage平台上难以找到合适的任务模块，主要由于领域特定术语、系统特定命名以及词汇搜索在处理拼写错误和词序变化方面的局限性。

Method: 采用混合语义搜索方法：轻量级容错词汇检索、嵌入向量相似性计算、约束性LLM重排序，并利用现有任务库和知识库基础设施，通过智能缓存、候选列表生成和优雅降级机制确保效率。

Result: 提供了一个全面的实施框架，包括所需资源、分阶段实施策略、离线评估协议（使用Hit@K、Precision@K、Recall@K、MRR指标）和在线测量方法（查询成功率、零结果率、停留时间代理指标）。

Conclusion: 该系统通过低误报率确保可信度，能够适应术语变化，并通过智能机制实现经济高效，为Head Start项目提供实用的任务搜索解决方案。

Abstract: Head Start programs utilizing GoEngage face significant challenges when new
or rotating staff attempt to locate appropriate Tasks (modules) on the platform
homepage. These difficulties arise from domain-specific jargon (e.g., IFPA,
DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent
limitations of lexical search in handling typos and varied word ordering. We
propose a pragmatic hybrid semantic search system that synergistically combines
lightweight typo-tolerant lexical retrieval, embedding-based vector similarity,
and constrained large language model (LLM) re-ranking. Our approach leverages
the organization's existing Task Repository and Knowledge Base infrastructure
while ensuring trustworthiness through low false-positive rates, evolvability
to accommodate terminological changes, and economic efficiency via intelligent
caching, shortlist generation, and graceful degradation mechanisms. We provide
a comprehensive framework detailing required resources, a phased implementation
strategy with concrete milestones, an offline evaluation protocol utilizing
curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online
measurement methodology incorporating query success metrics, zero-result rates,
and dwell-time proxies.

</details>


### [3] [Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics](https://arxiv.org/abs/2510.05137)
*Maojia Song,Renhang Liu,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Soujanya Poria,Jingren Zhou*

Main category: cs.CL

TL;DR: WebDetective是一个用于评估RAG系统和网络代理在深度多跳搜索任务中的基准测试，通过无提示问题和受控维基百科沙箱提供完整可追溯性，并采用分离搜索充分性、知识利用和拒绝行为的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前多跳搜索评估存在两个主要问题：基准测试在问题文本中泄露推理路径，以及评估简化为单一通过率，无法区分失败的具体原因。

Method: 开发WebDetective基准测试，包含无提示多跳问题和受控维基百科沙箱，确保模型行为的完全可追溯性，并建立分离搜索充分性、知识利用和拒绝行为的整体评估框架。

Result: 评估25个最先进模型显示系统性弱点：模型在拥有足够证据时仍难以有效利用知识，在缺乏证据时几乎不会适当拒绝。这些模式暴露了当前系统擅长执行给定推理路径但无法自主发现推理路径的根本差距。

Conclusion: 开发了EvidenceLoop代理工作流程，通过验证循环和系统证据跟踪来改进搜索和综合能力，证明WebDetective的诊断框架可以指导具体的架构改进，使其成为开发真正自主推理系统的关键工具。

Abstract: RAG (Retrieval-Augmented Generation) systems and web agents are increasingly
evaluated on multi-hop deep search tasks, yet current practice suffers from two
major limitations. First, most benchmarks leak the reasoning path in the
question text, allowing models to follow surface cues rather than discover
reasoning chains autonomously. Second, evaluation is typically reduced to a
single pass rate, which collapses diverse behaviours into one score and
obscures whether failures stem from inadequate search, poor knowledge use, or
inappropriate refusal. To address these issues, we present WebDetective, a
benchmark of hint-free multi-hop questions paired with a controlled Wikipedia
sandbox that ensures full traceability of model actions, and a holistic
evaluation framework that separates search sufficiency, knowledge utilisation,
and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals
systematic weaknesses across all architectures: models struggle with knowledge
utilisation despite having sufficient evidence and demonstrate near-absent
appropriate refusal when evidence is lacking. These patterns expose a
fundamental gap: today's systems excel at executing given reasoning paths but
fail when required to discover them. We develop an agentic workflow,
EvidenceLoop, that explicitly targets the challenges our benchmark identifies,
incorporating verification loops and systematic evidence tracking that improve
both search and synthesis capabilities. This baseline demonstrates that
WebDetective's diagnostic framework can guide concrete architectural
improvements, establishing our benchmark as a critical tool for developing
genuinely autonomous reasoning systems rather than pattern-following agents.

</details>


### [4] [NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description](https://arxiv.org/abs/2510.05139)
*Hamed Jelodar,Mohammad Meymani,Parisa Hamedi,Tochukwu Emmanuel Nwankwo,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.CL

TL;DR: 提出了NLD-LLM框架来评估语言模型生成源代码描述的能力，通过精心设计的提示工程，发现较小的模型在良好提示下也能表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估语言模型在自然语言描述任务中的表现，特别是生成准确简洁的源代码描述的能力。

Method: 使用多样化的transformer模型（Qwen、DeepSeek、Phi、LLaMA、Mistral），采用全面的提示设计策略和迭代精炼过程，结合语义和结构指标进行评估。

Result: 提示工程显著影响模型效果，较小模型在良好提示支持下表现具有竞争力。

Conclusion: 精心设计的提示工程可以显著提升语言模型在代码描述任务中的表现，使较小模型也能达到良好效果。

Abstract: Natural Language Description (NLD) is a Natural Language Processing (NLP)
task that requires models to generate structured and meaningful outputs from
natural language inputs. In this work, we propose NLD-LLM, a systematic NLP
framework to evaluate the performance of language models to generate accurate
and concise source code descriptions. This framework incorporates a diverse set
of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral,
spanning various sizes, architectures, and training approaches. Central to
NLD-LLM is a comprehensive prompt design strategy that includes standardized
formatting, clear task guidance, and NLD prompting, ensuring fair and
consistent evaluation. Additionally, we apply an iterative refinement process
to improve output's quality and assess the model's adaptability. Using semantic
and structural metrics, our analysis demonstrates that prompt engineering
significantly impacts the effectiveness of the model such that smaller models
often performing competitively when supported by well-crafted prompts.

</details>


### [5] [Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs](https://arxiv.org/abs/2510.05148)
*Qi Li,Runpeng Yu,Haiquan Lu,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于离散扩散大语言模型解码轨迹的模型归属方法，通过定向解码图和轨迹高斯分布进行模型识别。


<details>
  <summary>Details</summary>
Motivation: 离散扩散大语言模型的解码机制不仅提升模型性能，还能作为模型归属的强大工具，解决不同模型和同一模型不同检查点之间的区分问题。

Method: 提出定向解码图来提取解码步骤间的结构关系，并使用高斯轨迹归属方法，通过拟合每个解码位置的高斯分布来计算轨迹的似然度作为归属分数。

Result: 在不同设置下的大量实验验证了所提方法的有效性。

Conclusion: 离散扩散大语言模型的解码轨迹可用于可靠的模型归属，提出的方法在多种场景下表现优异。

Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive paradigm for non-autoregressive language modeling. Their
distinctive decoding mechanism enables faster inference speed and strong
performance in code generation and mathematical tasks. In this work, we show
that the decoding mechanism of dLLMs not only enhances model utility but also
can be used as a powerful tool for model attribution. A key challenge in this
problem lies in the diversity of attribution scenarios, including
distinguishing between different models as well as between different
checkpoints or backups of the same model. To ensure broad applicability, we
identify two fundamental problems: what information to extract from the
decoding trajectory, and how to utilize it effectively. We first observe that
relying directly on per-step model confidence yields poor performance. This is
mainly due to the bidirectional decoding nature of dLLMs: each newly decoded
token influences the confidence of other decoded tokens, making model
confidence highly redundant and washing out structural signal regarding
decoding order or dependencies. To overcome this, we propose a novel
information extraction scheme called the Directed Decoding Map (DDM), which
captures structural relationships between decoding steps and better reveals
model-specific behaviors. Furthermore, to make full use of the extracted
structural information during attribution, we propose Gaussian-Trajectory
Attribution (GTA), where we fit a cell-wise Gaussian distribution at each
decoding position for each target model, and define the likelihood of a
trajectory as the attribution score: if a trajectory exhibits higher
log-likelihood under the distribution of a specific model, it is more likely to
have been generated by that model. Extensive experiments under different
settings validate the utility of our methods.

</details>


### [6] [A Single Character can Make or Break Your LLM Evals](https://arxiv.org/abs/2510.05152)
*Jingtong Su,Jianyu Zhang,Karen Ullrich,Léon Bottou,Mark Ibrahim*

Main category: cs.CL

TL;DR: 研究发现LLM评估中示例分隔符的选择对模型性能有显著影响，不同分隔符可导致MMLU性能±23%的变化，甚至能操纵模型排名。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要关注示例数量，但示例格式选择（分隔符）的影响未被充分研究，而用户在实际使用中面临各种分隔符选择。

Method: 通过测试不同分隔符（逗号、换行、分号等）在多个主流模型家族上的表现，分析注意力头分数，并探索提升鲁棒性的方法。

Result: 分隔符选择显著影响模型性能，可导致MMLU得分±23%变化；好分隔符能引导注意力到关键token；在提示中指定分隔符可提升鲁棒性。

Conclusion: LLM对分隔符选择极其敏感，这种脆弱性普遍存在且不随规模改善；建议在提示中明确指定分隔符，并提供最佳分隔符推荐。

Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples
to steer models' responses to the desired style. While the number of examples
used has been studied and standardized, the choice of how to format examples is
less investigated. In evaluation protocols and real world usage, users face the
choice how to separate in-context examples: use a comma? new line? semi-colon?
hashtag? etc.? Surprisingly, we find this seemingly minor choice can
dramatically alter model response quality. Across leading model families
(Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$
depending on the choice of delimiter. In fact, one can manipulate model
rankings to put any model in the lead by only modifying the single character
separating examples. We find LLMs' brittleness pervades topics, model families,
and doesn't improve with scale. By probing attention head scores, we find that
good-performing delimiters steer attention towards key tokens in the input.
Finally, we explore methods to improve LLMs' robustness to the choice of
delimiter. We find specifying the selected delimiter in the prompt boosts
robustness and offer practical recommendations for the best-performing
delimiters to select.

</details>


### [7] [Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.05251)
*Chenghao Yang,Lin Gui,Chenxiao Yang,Victor Veitch,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.CL

TL;DR: 提出了一种名为探索性退火解码（EAD）的新策略，通过从高到低动态调整采样温度，实现"开头探索、结尾利用"的生成策略，显著提升强化学习与可验证奖励（RLVR）中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 标准固定温度采样在平衡样本质量和训练稳定性方面存在困难，高温度会降低样本质量，低温度会限制发现能力。需要一种更有效的探索策略来提升LLM的推理能力。

Method: EAD方法在生成过程中从高到低动态调整采样温度，在序列开头鼓励有意义的语义多样性，在结尾保持样本质量并接近目标策略分布。

Result: EAD在各种RLVR算法和模型规模上都显著优于固定温度采样，提高了样本效率，且是轻量级即插即用方法。

Conclusion: 将探索与序列生成的自然动态对齐是提升LLM推理能力的有效途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm
for enhancing the reasoning capabilities of large language models (LLMs), yet
its success hinges on effective exploration. An ideal exploration strategy must
navigate two fundamental challenges: it must preserve sample quality while also
ensuring training stability. While standard fixed-temperature sampling is
simple, it struggles to balance these competing demands, as high temperatures
degrade sample quality and low temperatures limit discovery. In this work, we
propose a simpler and more effective strategy, Exploratory Annealed Decoding
(EAD), grounded in the insight that exploration is most impactful on early
tokens which define a sequence's semantic direction. EAD implements an
intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by
annealing the sampling temperature from high to low during generation. This
dynamic schedule encourages meaningful, high-level diversity at the start, then
gradually lowers the temperature to preserve sample quality and keep the
sampling distribution close to the target policy, which is essential for stable
training. We demonstrate that EAD is a lightweight, plug-and-play method that
significantly improves sample efficiency, consistently outperforming
fixed-temperature sampling across various RLVR algorithms and model sizes. Our
work suggests that aligning exploration with the natural dynamics of sequential
generation offers a robust path to improving LLM reasoning.

</details>


### [8] [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310)
*Yining She,Daniel W. Peterson,Marianne Menglin Liu,Vikas Upadhyay,Mohammad Hossein Chaghazardi,Eunsuk Kang,Dan Roth*

Main category: cs.CL

TL;DR: 研究发现基于LLM的安全护栏模型在面对检索增强生成(RAG)中的上下文信息时存在脆弱性，插入良性文档会改变约11%的输入护栏和8%的输出护栏判断，暴露了上下文鲁棒性差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保LLM系统的安全性成为迫切需求。外部LLM护栏模型虽然流行，但它们本身也容易受到数据分布变化的影响，特别是在RAG场景中。

Method: 以RAG为案例研究，系统评估了3个Llama Guard和2个GPT-oss模型，分析插入良性文档到护栏上下文对判断的影响，并分别分析了检索文档、用户查询和LLM生成响应等组件的作用。

Result: 确认插入良性文档会改变约11%的输入护栏和8%的输出护栏判断，使它们不可靠。测试的两种缓解方法仅带来轻微改进。

Conclusion: 当前护栏存在上下文鲁棒性差距，需要开发对检索和查询组合具有鲁棒性的训练和评估协议。

Abstract: With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

</details>


### [9] [A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis](https://arxiv.org/abs/2510.05414)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Haifeng Wang,Minghui Cheng*

Main category: cs.CL

TL;DR: 开发了一个基于LLM的多智能体系统，用于自动化2D框架的有限元建模，通过分解任务到专门智能体，在20个基准问题上达到80%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: LLM在结构工程中的潜力尚未充分探索，特别是在需要几何建模、复杂推理和领域知识的有限元建模任务中。

Method: 使用轻量级Llama-3.3 70B Instruct模型构建多智能体系统，包括问题分析、几何建模、代码转换、模型验证和载荷应用等专门智能体。

Result: 在20个基准问题的10次重复试验中，系统在大多数情况下达到80%以上的准确率，优于Gemini-2.5 Pro和ChatGPT-4o模型。

Conclusion: 基于LLM的多智能体系统能够有效自动化有限元建模任务，展示了在结构工程中应用LLM的可行性。

Abstract: Large language models (LLMs) have recently been used to empower autonomous
agents in engineering, significantly improving automation and efficiency in
labor-intensive workflows. However, their potential remains underexplored in
structural engineering, particularly for finite element modeling tasks
requiring geometric modeling, complex reasoning, and domain knowledge. To
bridge this gap, this paper develops a LLM-based multi-agent system to automate
finite element modeling of 2D frames. The system decomposes structural analysis
into subtasks, each managed by a specialized agent powered by the lightweight
Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis
Agent, which extracts geometry, boundary, and material parameters from the user
input. Next, a Geometry Agent incrementally derives node coordinates and
element connectivity by applying expert-defined rules. These structured outputs
are converted into executable OpenSeesPy code by a Translation Agent and
refined by a Model Validation Agent through consistency checks. Then, a Load
Agent applies load conditions into the assembled structural model. Experimental
evaluations on 20 benchmark problems demonstrate that the system achieves
accuracy over 80% in most cases across 10 repeated trials, outperforming
Gemini-2.5 Pro and ChatGPT-4o models.

</details>


### [10] [AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering](https://arxiv.org/abs/2510.05445)
*Zheyuan Zhang,Kaiwen Shi,Zhengqing Yuan,Zehong Wang,Tianyi Ma,Keerthiram Murugesan,Vincent Galassi,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 提出了tAgentRouter框架，将多智能体问答建模为知识图谱引导的路由问题，通过图神经网络学习任务感知的智能体路由分布，实现多智能体协作。


<details>
  <summary>Details</summary>
Motivation: 现有智能体路由方法过于强调成本效率，忽略了问答任务中细粒度的上下文和关系结构，需要自适应路由机制来利用不同智能体的互补优势。

Method: 将问答实例转换为知识图谱，联合编码查询、上下文实体和智能体，使用异构图神经网络在节点类型间传播信息，生成任务感知的路由分布，通过软监督和加权聚合学习协作方案。

Result: 实验表明该框架在多个基准测试和LLM骨干网络上一致优于单智能体和集成基线方法，展现出有效性和鲁棒性。

Conclusion: 基于图监督的多智能体路由方法在问答任务中具有显著优势，能够捕获不同智能体的互补优势。

Abstract: Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

</details>


### [11] [SocialNLI: A Dialogue-Centric Social Inference Dataset](https://arxiv.org/abs/2510.05458)
*Akhil Deo,Kate Sanders,Benjamin Van Durme*

Main category: cs.CL

TL;DR: SoNLI是首个社交对话推理数据集，用于评估模型在理解复杂社交现象（如讽刺和反讽）方面的能力，通过多步反事实推理来测试模型的心理理论能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言和推理模型在理解对话中的复杂社交现象（如讽刺和反讽）方面存在困难，需要专门的数据集来评估和改进这些能力。

Method: 创建SoNLI数据集，包含精心挑选的对话转录，配对推理、似然分数和人工编写的解释，通过多步反事实推理评估模型的心理理论能力。

Result: 引入了SoNLI数据集，为评估和改进模型在社交推理方面的能力提供了基础。

Conclusion: SoNLI数据集有助于识别当前模型的弱点，并为开发更熟练的AI助手提供了解决方案。

Abstract: Making theory-of-mind inferences from human dialogue is a strong indicator of
a model's underlying social abilities, which are fundamental for adept AI
assistants. However, large language and reasoning models struggle to understand
sophisticated social phenomena in transcript data, such as sarcasm and irony.
To assess the weaknesses of current models and to identify their solutions, we
introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset.
SoNLI consists of a collection of dialogue transcripts hand-picked to center
complex social nuances like irony and sarcasm, paired with inferences,
corresponding likelihood scores, and human-written explanations. We explore
social inference analysis as a facet of theory-of-mind, and evaluate LLM and
reasoning model theory-of-mind ability through multi-step counterfactual
reasoning.

</details>


### [12] [Prototype-Based Dynamic Steering for Large Language Models](https://arxiv.org/abs/2510.05498)
*Ceyhun Efe Kayan,Li Zhang*

Main category: cs.CL

TL;DR: 提出了原型动态引导（PDS）方法，通过聚类激活差异构建推理原型，在推理时根据输入生成实例特定的引导向量，无需指令或微调即可增强LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM依赖显式推理指令或静态引导方法，缺乏自适应、无需指令的推理增强方法。

Method: 通过聚类CoT和中性提示的激活差异构建推理原型，在推理时将输入隐藏状态投影到这些原型上形成实例特定的引导向量。

Result: 在GSM8K、AQuA-RAT和BIG-Bench任务上持续提升准确率，即使抑制CoT也能保持增益，表明该方法增强了潜在推理过程而非表面行为变化。

Conclusion: 原型引导的动态转向是增强LLM推理的轻量级替代方案，无需训练时方法。

Abstract: Despite impressive breadth, LLMs still rely on explicit reasoning
instructions or static, one-fits-all steering methods, leaving a gap for
adaptive, instruction-free reasoning amplification. We present Prototype-Based
Dynamic Steering (PDS), a test-time method that amplifies large language model
(LLM) reasoning without adding or altering instructions. We introduce
"reasoning prototypes" by clustering activation differences between
Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden
state is projected onto these prototypes to form an instance-specific steering
vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently
improves accuracy without fine-tuning or prompt engineering. Notably, the gains
persist even when CoT is explicitly suppressed to improve cost-efficiency,
indicating that the intervention strengthens latent reasoning processes rather
than inducing a superficial behavioral shift. These results position dynamic,
prototype-guided steering as a lightweight alternative to training-time
approaches for enhancing LLM reasoning.

</details>


### [13] [CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension](https://arxiv.org/abs/2510.05520)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Zihang Tian,Xu Chen,Quanyu Dai,Zhenhua Dong,Ruiming Tang*

Main category: cs.CL

TL;DR: 本文提出CAM（建构主义代理记忆），基于皮亚杰建构主义理论设计LLM记忆模块，通过结构化模式、灵活同化和动态适应提升长文档理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理长文档时面临信息过载问题，需要系统化的记忆模块设计来提升阅读理解能力。

Method: 开发CAM原型，采用增量重叠聚类算法构建结构化记忆，支持分层摘要和在线批量集成，推理时自适应激活查询相关信息。

Result: 在问答、查询摘要和声明验证等长文本理解任务中，CAM在性能和效率上均优于现有方法。

Conclusion: 基于建构主义理论设计的记忆系统能有效提升LLM的长文档理解能力，实现更稳健高效的阅读代理。

Abstract: Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

</details>


### [14] [Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations](https://arxiv.org/abs/2510.05571)
*Chengzhi Liu,Yuzhe Yang,Kaiwen Zhou,Zhen Zhang,Yue Fan,Yannan Xie,Peng Qi,Xin Eric Wang*

Main category: cs.CL

TL;DR: EvoPresent是一个自我改进的学术演示生成框架，通过多任务强化学习的美学模型PresAesth提供可靠的美学评分和反馈，实现迭代自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法在讲故事能力、美学质量和自我调整方面存在局限，难以实现高效吸引人的学术传播。核心问题是缺乏有效的评估机制。

Method: 引入EvoPresent框架，结合连贯叙事、美学感知设计和虚拟角色演示。核心是PresAesth多任务强化学习美学模型，提供美学评分、缺陷调整和比较反馈。

Result: 构建了EvoPresent Benchmark基准，包含650篇顶级AI会议论文的多模态资源和2000个幻灯片对。发现高质量反馈对代理自我改进至关重要，多任务RL训练在美学感知任务中表现出更强的泛化能力。

Conclusion: 高质量反馈是代理自我改进的关键，自动生成管道在视觉设计和内容构建之间存在权衡，多任务RL训练在美学感知方面具有更好的泛化性能。

Abstract: The promotion of academic papers has become an important means of enhancing
research visibility. However, existing automated methods struggle limited
storytelling, insufficient aesthetic quality, and constrained self-adjustment,
making it difficult to achieve efficient and engaging dissemination. At the
heart of those challenges is a simple principle: \emph{there is no way to
improve it when you cannot evaluate it right}. To address this, we introduce
\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent
narratives, aesthetic-aware designs, and realistic presentation delivery via
virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task
reinforcement learning (RL) aesthetic model that provides reliable aesthetic
scoring, defect adjustment, and comparative feedback, enabling iterative
self-improvement even under limited aesthetic training data. To systematically
evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a
comprehensive benchmark comprising: \textit{Presentation Generation Quality},
built on 650 top-tier AI conference papers with multimodal resources (slides,
videos and scripts) to assess both content and design; and \textit{Aesthetic
Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels,
supporting joint training and evaluation on scoring, defect adjustment, and
comparison. Our findings highlight that (i) High-quality feedback is essential
for agent self-improvement, while initial capability alone does not guarantee
effective self-correction. (ii) Automated generation pipelines exhibit a
trade-off between visual design and content construction. (iii) Multi-task RL
training shows stronger generalization in aesthetic awareness tasks.

</details>


### [15] [A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks](https://arxiv.org/abs/2510.05608)
*Shuzheng Si,Haozhe Zhao,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了EAGLET方法，通过训练一个即插即用的全局规划器来增强执行器代理在长时程任务中的规划能力，无需人工干预，显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理在长时程任务中缺乏全局规划，导致盲目试错和产生幻觉动作，需要提升规划能力。

Method: 采用计划-执行框架，通过两步训练过程：首先使用同源共识过滤策略从先进LLM合成高质量计划进行微调，然后使用基于规则的强化学习阶段进一步改进规划器。

Result: 在三个长时程代理任务上的实验表明，配备该规划器的执行器代理优于现有方法，达到新的最先进性能，同时训练成本比基于RL的基线降低8倍。

Conclusion: EAGLET提供了一种高效有效的解决方案，无需人工努力或额外训练数据，显著提升了代理在复杂任务中的规划能力。

Abstract: Agents based on large language models (LLMs) struggle with brainless
trial-and-error and generating hallucinatory actions due to a lack of global
planning in long-horizon tasks. In this paper, we introduce a plan-and-execute
framework and propose EAGLET, an efficient and effective planner training
method to enhance the executor agent's planning abilities without human effort.
Specifically, we train a plug-and-play global planner through a two-step
process: we first synthesize high-quality plans from an advanced LLM using our
proposed homologous consensus filtering strategy, and apply fine-tuning as a
cold start. Moreover, we further improve the planner with a rule-based
reinforcement learning stage using a novel executor capability gain reward,
ensuring it can handle task instructions of varying difficulty. Experiments on
three long-horizon agent tasks show that executor agents equipped with our
planner outperform existing methods, achieving new state-of-the-art
performance. Meanwhile, EAGLET reduces training costs by 8x compared to
RL-based baselines, and it does not require manual effort or extra training
data, offering an efficient and effective solution.

</details>


### [16] [DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision](https://arxiv.org/abs/2510.05691)
*Yongqi Leng,Yikun Lei,Xikai Liu,Meizhi Zhong,Bojian Xiong,Yurong Zhang,Yan Gao,Yi Wu,Yao Hu,Deyi Xiong*

Main category: cs.CL

TL;DR: DecEx-RAG通过将RAG建模为马尔可夫决策过程，引入决策-执行框架和高效剪枝策略，显著提升了LLM在复杂任务中的自主分解、动态检索和高质量回答生成能力，在六个数据集上平均性能提升6.2%，数据构建效率提升近6倍。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结果监督强化学习的Agentic RAG方法存在探索效率低、奖励信号稀疏和全局奖励反馈模糊等问题，需要更有效的优化方法。

Method: 将RAG建模为马尔可夫决策过程，结合决策制定和执行，引入高效的剪枝策略来优化数据扩展，通过综合的过程级策略优化来提升模型能力。

Result: 在六个数据集上实现了平均6.2%的绝对性能提升，显著优于现有基线方法，同时剪枝策略使数据构建效率提升了近6倍。

Conclusion: DecEx-RAG为过程监督的RAG训练提供了高效解决方案，显著增强了LLM在复杂任务中的自主处理能力。

Abstract: Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

</details>


### [17] [Prompt reinforcing for long-term planning of large language models](https://arxiv.org/abs/2510.05921)
*Hsien-Chin Lin,Benjamin Matthias Ruppik,Carel van Niekerk,Chia-Hao Shen,Michael Heck,Nurul Lubis,Renato Vukovic,Shutong Feng,Milica Gašić*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习思想的提示优化框架，通过修改LLM代理的任务指令提示来实现长期规划，在文本转SQL和任务导向对话等多轮任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM在多轮交互中表现不佳，经常依赖错误的早期假设且无法跟踪用户目标，而对话系统研究表明长期规划对处理交互任务至关重要。

Method: 通过生成逐轮反馈并利用经验回放进行提示重写，仅修改LLM代理的任务指令提示来实现规划能力。

Result: 在文本转SQL和任务导向对话等多轮任务中显示出显著改进，能够泛化到不同的LLM代理，并可利用多样化的LLM作为元提示代理。

Conclusion: 该方法为基于强化学习思想的无参数优化方法的未来研究提供了保证。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

</details>


### [18] [LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language](https://arxiv.org/abs/2510.05972)
*Periklis Mantenoglou,Rishi Hazra,Pedro Zuidberg Dos Martires,Luc De Raedt*

Main category: cs.CL

TL;DR: LexiCon是一个基于自然语言的约束规划基准，用于评估LLM在带约束规划任务中的表现，特别是安全约束方面。


<details>
  <summary>Details</summary>
Motivation: LLM在无约束规划任务中表现良好，但在现实世界部署时需要评估其在带约束（特别是安全约束）规划任务中的能力。

Method: 基于现有规划环境添加时间约束，然后将约束问题转化为自然语言让LLM解决。LexiCon具有可扩展性，可自动为新环境生成时间约束。

Result: 实验表明，包括GPT-5、o3和R1在内的最先进LLM，在规划任务的约束程度增加时性能会下降。

Conclusion: LexiCon提供了一个系统化评估LLM约束规划能力的方法，发现当前LLM在约束规划方面仍有局限。

Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been
evaluated on planning tasks described in natural language. However, LLMs have
largely been tested on planning domains without constraints. In order to deploy
them in real-world settings where adherence to constraints, in particular
safety constraints, is critical, we need to evaluate their performance on
constrained planning tasks. We introduce LexiCon -- a natural language-based
(Lexi) constrained (Con) planning benchmark, consisting of a suite of
environments, that can be used to evaluate the planning capabilities of LLMs in
a principled fashion. The core idea behind LexiCon is to take existing planning
environments and impose temporal constraints on the states. These constrained
problems are then translated into natural language and given to an LLM to
solve. A key feature of LexiCon is its extensibility. That is, the set of
supported environments can be extended with new (unconstrained) environment
generators, for which temporal constraints are constructed automatically. This
renders LexiCon future-proof: the hardness of the generated planning problems
can be increased as the planning capabilities of LLMs improve. Our experiments
reveal that the performance of state-of-the-art LLMs, including reasoning
models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of
the planning tasks increases.

</details>


### [19] [Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments](https://arxiv.org/abs/2510.06001)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文通过重新评估GPT-2在寄生间隙结构中的表现，论证了直接最小对比较方法比差异中的差异方法更具诊断透明度，发现GPT-2在所有测试条件下都成功掌握了填充语-间隙依赖关系。


<details>
  <summary>Details</summary>
Motivation: 先前研究使用不同评估指标对LLM学习复杂句法的能力得出矛盾结论，本文旨在澄清这些分歧并证明评估指标选择的重要性。

Method: 生成完整的8种排列范式寄生间隙刺激，使用Wilcox风格的直接最小对比较方法系统评估GPT-2模型。

Result: GPT-2在所有四个测试条件下都成功掌握了填充语-间隙许可原则，即使在复杂的寄生间隙环境中也表现出稳健的知识。

Conclusion: 直接最小对比较方法比DiD风格指标提供更清晰的诊断结果，评估指标的选择对准确评估LLM的句法能力至关重要。

Abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
have applied Large Language Models (LLMs) to test the learnability of complex
syntax through surprisal-based metrics. However, divergent conclusions raise
questions concerning the insights these metrics offer. While Wilcox et al.
(2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate
that models successfully generalise knowledge of filler-gap dependencies, Lan
et al. (2024) used a Difference-in-Differences (DiD) metric and found that
models largely fail on parasitic gaps (PGs). This paper argues that the direct
minimal pair approach offers greater diagnostic transparency. We demonstrate
this by generating a full 8-permutation paradigm of refined PG stimuli and
evaluating the GPT-2 model used in previous studies with a systematic
Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across
all four tested conditions, indicating robust knowledge of filler-gap licensing
principles even in complex PG environments. This finding, which contrasts with
the more ambiguous results from DiD-style metrics, suggests that the choice of
evaluation metric is critical for assessing an LLM's syntactic competence.

</details>


### [20] [ASPO: Asymmetric Importance Sampling Policy Optimization](https://arxiv.org/abs/2510.06062)
*Jiakang Wang,Runze Liu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: 论文提出ASPO方法，通过翻转正优势令牌的重要性采样比率来解决OSRL中的令牌权重不平衡问题，改善了LLM强化学习的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 发现当前基于令牌级裁剪的LLM强化学习方法存在根本缺陷：正优势令牌的重要性采样比率不匹配，导致正负令牌权重不平衡，抑制低概率令牌更新而过度放大高概率令牌。

Method: 提出ASPO方法，翻转正优势令牌的重要性采样比率使其与负令牌的学习动态对齐，并采用软双重裁剪机制来稳定极端更新同时保持梯度流动。

Result: 在编程和数学推理基准测试中，ASPO显著缓解了过早收敛问题，提高了训练稳定性，并在基于GRPO的强基线基础上提升了最终性能。

Conclusion: 分析为OSRL中令牌级权重的作用提供了新见解，并强调了在LLM强化学习中校正重要性采样的关键重要性。

Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level
clipping mechanisms during Reinforcement Learning (RL). However, we identify a
fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance
Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to
unbalanced token weighting for positive and negative tokens. This mismatch
suppresses the update of low-probability tokens while over-amplifying already
high-probability ones. To address this, we propose Asymmetric Importance
Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy
that flips the IS ratios of positive-advantage tokens, aligning their update
direction with the learning dynamics of negative ones. AIS further incorporates
a soft dual-clipping mechanism to stabilize extreme updates while maintaining
gradient flow. Comprehensive experiments on coding and mathematical reasoning
benchmarks demonstrate that ASPO significantly mitigates premature convergence,
improves training stability, and enhances final performance over strong
GRPO-based baselines. Our analysis provides new insights into the role of
token-level weighting in OSRL and highlights the critical importance of
correcting IS in LLM RL. The code and models of ASPO are available at
https://github.com/wizard-III/Archer2.0.

</details>


### [21] [The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2510.06101)
*Muyu He,Muhammad Ali Shafique,Anand Kumar,Tsach Mackey,Nazneen Rajani*

Main category: cs.CL

TL;DR: 研究发现代码推理蒸馏存在'代码推理谷'现象：随着蒸馏数据量增加，下游编程竞赛性能先下降后以超对数线性方式增长。小模型在低数据量时从简单编程问题获益更多，且训练数据中输出的正确性对蒸馏结果无影响。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型推理能力蒸馏到小模型时的性能随数据量变化的扩展趋势，填补了该领域的研究空白。

Method: 在两个小型非推理LLM上蒸馏竞争性编程技能，分析不同数据量下的性能变化，并在不同蒸馏阶段对相同数据进行微调以验证学习阶段。

Result: 发现代码推理蒸馏存在'代码推理谷'现象，小模型在低数据量时从简单问题获益更多，训练数据正确性不影响蒸馏结果。

Conclusion: 这项工作推进了对代码推理蒸馏训练动态的理解，超越了直觉认知。

Abstract: Distilling the thinking traces of a Large Language Model (LLM) with reasoning
capabilities into a smaller model has been proven effective. Yet, there is a
scarcity of work done on how model performances scale with the quantity of
distillation data. In this work, we study the scaling trend of distilling
competitive coding skills on two small non-reasoning LLMs. We validate the
hypothesis that there is a $\textit{valley of code reasoning}$: downstream
performance on competitive coding first drops as data quantity increases, then
it steadily increases in a sharper-than-log-linear fashion. Having identified
the trend, we further fine-tune the models at two different distillation stages
on the same data to ground conclusions on their respective learning phases. We
learn that across stages in the low and medium-low data regimes, small models
benefit significantly from easier coding questions than from harder ones. We
also find that, surprisingly, the correctness of outputs in training data makes
no difference to distillation outcomes. Our work represents a step forward in
understanding the training dynamics of code reasoning distillation outside
intuition

</details>


### [22] [RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback](https://arxiv.org/abs/2510.06186)
*Chunyu Miao,Henry Peng Zou,Yangning Li,Yankai Chen,Yibo Wang,Fangxin Wang,Yifan Li,Wooseong Yang,Bowei He,Xinni Zhang,Dianzhi Yu,Hanchen Yang,Hoang H Nguyen,Yue Zhou,Jie Yang,Jizhou Guo,Wenzhe Fan,Chin-Yuan Yeh,Panpan Meng,Liancheng Fang,Jinhu Qi,Wei-Chieh Huang,Zhengyao Gu,Yuwei Han,Langzhou He,Yuyao Yang,Xue Liu,Irwin King,Philip S. Yu*

Main category: cs.CL

TL;DR: RECODE-H是一个包含102个任务的基准测试，通过多轮交互评估LLM代理在科学研究代码生成中的表现，并提出了整合反馈的迭代代码生成框架ReCodeAgent。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多采用单次设置，忽略了科学研究开发中迭代和反馈驱动的实际工作流程，需要评估LLM在真实研究环境中的代码生成能力。

Method: 构建RECODE-H基准测试，包含结构化指令、单元测试和五级反馈层次；提出ReCodeAgent框架，将反馈整合到迭代代码生成过程中。

Result: 实验显示，在GPT-5、Claude-Sonnet-4、DeepSeek-V3.1和Gemini 2.5等领先LLM上，更丰富的反馈带来了显著的性能提升，但在复杂研究代码生成方面仍存在挑战。

Conclusion: RECODE-H为开发科学研究实现中自适应、反馈驱动的LLM代理奠定了基础。

Abstract: Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment](https://arxiv.org/abs/2510.05157)
*Abrar Shahid,Ibteeker Mahir Ishum,AKM Tahmidul Haque,M Sohel Rahman,A. B. M. Alim Al Islam*

Main category: cs.LG

TL;DR: 本文通过自定义OpenAI Gym环境研究网络安全的对抗性强化学习，模拟多端口服务的暴力攻击和响应防御，在零和奖励框架下训练攻击者和防御者DQN智能体。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性强化学习在网络安全的实际应用，探索攻击者和防御者智能体在复杂网络环境中的博弈行为。

Method: 使用自定义OpenAI Gym环境，包含背景流量噪声、渐进利用机制、IP规避策略、蜜罐陷阱和多级速率限制防御。采用DQN算法在零和奖励框架下训练对抗智能体。

Result: 防御者的可观测性和陷阱有效性对成功攻击构成显著障碍。经过50,000+训练回合，防御者始终保持战略优势，特别是在复杂防御策略下表现更佳。

Conclusion: 奖励塑造和训练调度对学习稳定性至关重要。该环境适合研究自主防御系统、攻击者-防御者共同进化和向真实网络安全场景的迁移学习。

Abstract: This paper presents a controlled study of adversarial reinforcement learning
in network security through a custom OpenAI Gym environment that models
brute-force attacks and reactive defenses on multi-port services. The
environment captures realistic security trade-offs including background traffic
noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot
traps, and multi-level rate-limiting defenses. Competing attacker and defender
agents are trained using Deep Q-Networks (DQN) within a zero-sum reward
framework, where successful exploits yield large terminal rewards while
incremental actions incur small costs. Through systematic evaluation across
multiple configurations (varying trap detection probabilities, exploitation
difficulty thresholds, and training regimens), the results demonstrate that
defender observability and trap effectiveness create substantial barriers to
successful attacks. The experiments reveal that reward shaping and careful
training scheduling are critical for learning stability in this adversarial
setting. The defender consistently maintains strategic advantage across 50,000+
training episodes, with performance gains amplifying when exposed to complex
defensive strategies including adaptive IP blocking and port-specific controls.
Complete implementation details, reproducible hyperparameter configurations,
and architectural guidelines are provided to support future research in
adversarial RL for cybersecurity. The zero-sum formulation and realistic
operational constraints make this environment suitable for studying autonomous
defense systems, attacker-defender co-evolution, and transfer learning to
real-world network security scenarios.

</details>


### [24] [Adjusting the Output of Decision Transformer with Action Gradient](https://arxiv.org/abs/2510.05285)
*Rui Lin,Yiwen Zhang,Zhicheng Peng,Minghao Lyu*

Main category: cs.LG

TL;DR: 提出Action Gradient方法解决Decision Transformer在离线强化学习中的轨迹拼接和动作外推问题，通过直接调整动作来优化Q值，显著提升DT算法性能


<details>
  <summary>Details</summary>
Motivation: Decision Transformer将强化学习与transformer结合，但面临轨迹拼接和动作外推两大挑战。现有方法单独解决这些问题时性能不稳定，需要一种能稳定整合这些技术的新方法

Method: 提出Action Gradient方法，利用动作相对于Q值的梯度来直接优化动作，实现与策略梯度类似的功能，并能高效整合token预测技术

Result: 实验结果表明该方法能显著提升基于DT算法的性能，部分结果达到最先进水平

Conclusion: Action Gradient方法有效解决了DT在离线强化学习中的关键挑战，提供了一种稳定且高效的优化方案

Abstract: Decision Transformer (DT), which integrates reinforcement learning (RL) with
the transformer model, introduces a novel approach to offline RL. Unlike
classical algorithms that take maximizing cumulative discounted rewards as
objective, DT instead maximizes the likelihood of actions. This paradigm shift,
however, presents two key challenges: stitching trajectories and extrapolation
of action. Existing methods, such as substituting specific tokens with
predictive values and integrating the Policy Gradient (PG) method, address
these challenges individually but fail to improve performance stably when
combined due to inherent instability. To address this, we propose Action
Gradient (AG), an innovative methodology that directly adjusts actions to
fulfill a function analogous to that of PG, while also facilitating efficient
integration with token prediction techniques. AG utilizes the gradient of the
Q-value with respect to the action to optimize the action. The empirical
results demonstrate that our method can significantly enhance the performance
of DT-based algorithms, with some results achieving state-of-the-art levels.

</details>


### [25] [Adversarial Reinforcement Learning for Large Language Model Agent Safety](https://arxiv.org/abs/2510.05442)
*Zizhao Wang,Dingcheng Li,Vaishakh Keshava,Phillip Wallis,Ananth Balashankar,Peter Stone,Lukas Rutishauser*

Main category: cs.LG

TL;DR: 提出了ARLAS框架，通过对抗性强化学习训练LLM代理防御间接提示注入攻击，同时提高任务成功率


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理使用工具时面临间接提示注入攻击风险，现有防御方法依赖手工制作的攻击模式，缺乏多样性，无法应对新型攻击

Method: 采用对抗性强化学习框架，将问题建模为两人零和博弈，同时训练攻击者LLM生成多样化提示注入和代理LLM防御攻击并完成任务，使用基于种群的学习框架防止循环学习

Result: 在BrowserGym和AgentDojo上评估，ARLAS微调的代理显著降低了攻击成功率，同时提高了任务成功率，生成的攻击更加多样化和具有挑战性

Conclusion: ARLAS框架能够生成多样化攻击并训练出更鲁棒的代理，相比基础模型具有更好的安全性和任务性能

Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to
complete complex tasks. However, this tool usage introduces the risk of
indirect prompt injections, where malicious instructions hidden in tool outputs
can manipulate the agent, posing security risks like data leakage. Current
defense strategies typically rely on fine-tuning LLM agents on datasets of
known attacks. However, the generation of these datasets relies on manually
crafted attack patterns, which limits their diversity and leaves agents
vulnerable to novel prompt injections. To address this limitation, we propose
Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework
that leverages adversarial reinforcement learning (RL) by formulating the
problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker
that learns to autonomously generate diverse prompt injections and an agent
that learns to defend against them while completing its assigned tasks. To
ensure robustness against a wide range of attacks and to prevent cyclic
learning, we employ a population-based learning framework that trains the agent
to defend against all previous attacker checkpoints. Evaluated on BrowserGym
and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower
attack success rate than the original model while also improving their task
success rate. Our analysis further confirms that the adversarial process
generates a diverse and challenging set of attacks, leading to a more robust
agent compared to the base model.

</details>


### [26] [Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs](https://arxiv.org/abs/2510.05446)
*Runlin Zhou,Chixiang Chen,Elynn Chen*

Main category: cs.LG

TL;DR: 提出两种基于Thompson采样的元强化学习算法MTSRL和MTSRL+，通过线性值函数表示和Gaussian元先验，在任务共享结构时实现元后悔保证。


<details>
  <summary>Details</summary>
Motivation: 研究在有限时域MDP中，当相关任务在最优动作值函数中共享相似结构时的元强化学习问题，旨在通过学习任务间共享的先验知识来提高学习效率。

Method: 使用线性表示Q*_h(s,a)=Φ_h(s,a)θ^{(k)}_h和Gaussian元先验，提出MTSRL（仅学习先验均值）和MTSRL+（额外估计协方差并使用先验扩展）两种Thompson风格算法，并开发先验对齐技术。

Result: 在已知协方差时获得Õ(H⁴S³/²√ANK)元后悔，在学习协方差时获得Õ(H⁴S³/²√AN³K)元后悔，当任务数量足够多时优于先验无关方法。模拟实验显示MTSRL/MTSRL+能快速跟踪元先知并显著优于基线方法。

Conclusion: 首次为具有学习Q先验的Thompson风格RL提供了元后悔保证，并为实验丰富环境提供了实用方法（通过RLSVI预热、OLS聚合、协方差扩展）。

Abstract: We study meta-reinforcement learning in finite-horizon MDPs where related
tasks share similar structures in their optimal action-value functions.
Specifically, we posit a linear representation
$Q^*_h(s,a)=\Phi_h(s,a)\,\theta^{(k)}_h$ and place a Gaussian meta-prior $
\mathcal{N}(\theta^*_h,\Sigma^*_h)$ over the task-specific parameters
$\theta^{(k)}_h$. Building on randomized value functions, we propose two
Thompson-style algorithms: (i) MTSRL, which learns only the prior mean and
performs posterior sampling with the learned mean and known covariance; and
(ii) $\text{MTSRL}^{+}$, which additionally estimates the covariance and
employs prior widening to control finite-sample estimation error. Further, we
develop a prior-alignment technique that couples the posterior under the
learned prior with a meta-oracle that knows the true prior, yielding
meta-regret guarantees: we match prior-independent Thompson sampling in the
small-task regime and strictly improve with more tasks once the prior is
learned. Concretely, for known covariance we obtain
$\tilde{O}(H^{4}S^{3/2}\sqrt{ANK})$ meta-regret, and with learned covariance
$\tilde{O}(H^{4}S^{3/2}\sqrt{AN^3K})$; both recover a better behavior than
prior-independent after $K \gtrsim \tilde{O}(H^2)$ and $K \gtrsim
\tilde{O}(N^2H^2)$, respectively. Simulations on a stateful recommendation
environment (with feature and prior misspecification) show that after brief
exploration, MTSRL/MTSRL\(^+\) track the meta-oracle and substantially
outperform prior-independent RL and bandit-only meta-baselines. Our results
give the first meta-regret guarantees for Thompson-style RL with learned
Q-priors, and provide practical recipes (warm-start via RLSVI, OLS aggregation,
covariance widening) for experiment-rich settings.

</details>


### [27] [Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment](https://arxiv.org/abs/2510.05526)
*Ziyi Chen,Junyi Li,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: 提出了RLHF-COV和DPO-COV算法，同时解决偏好对齐中的三个关键问题：偏好数据污染、奖励过优化和冗长性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF和DPO方法在偏好对齐中存在三个严重问题：偏好数据污染、奖励过优化和冗长性偏见，现有工作大多只解决其中一个问题，且缺乏理论保证。

Method: 提出RLHF-COV和DPO-COV算法，在离线和在线设置下同时缓解三个问题。DPO-COV无需奖励估计，简单易实现，且与RLHF-COV等价。

Result: 理论分析显示DPO-COV在污染数据上获得长度正则化的泛化误差率，匹配干净数据的最佳已知率。实验验证了算法在离线和在线设置下的有效性。

Conclusion: RLHF-COV和DPO-COV能同时解决偏好对齐中的三个关键问题，具有理论保证和实际效果。

Abstract: Reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) are important techniques to align large language models
(LLM) with human preference. However, the quality of RLHF and DPO training is
seriously compromised by \textit{\textbf{C}orrupted} preference, reward
\textit{\textbf{O}veroptimization}, and bias towards
\textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only
one of these important issues, and the few other works require much computation
to estimate multiple reward models and lack theoretical guarantee of
generalization ability. In this work, we propose RLHF-\textbf{COV} and
DPO-\textbf{COV} algorithms that can simultaneously mitigate these three
issues, in both offline and online settings. This ability is theoretically
demonstrated by obtaining length-regularized generalization error rates for our
DPO-COV algorithms trained on corrupted data, which match the best-known rates
for simpler cases with clean data and without length regularization. Moreover,
our DPO-COV algorithm is simple to implement without reward estimation, and is
proved to be equivalent to our RLHF-COV algorithm, which directly implies the
equivalence between the vanilla RLHF and DPO algorithms. Experiments
demonstrate the effectiveness of our DPO-COV algorithms under both offline and
online settings.

</details>


### [28] [QGraphLIME - Explaining Quantum Graph Neural Networks](https://arxiv.org/abs/2510.05683)
*Haribandhu Jena,Jyotirmaya Shivottam,Subhankar Mishra*

Main category: cs.LG

TL;DR: 提出了QuantumGraphLIME（QGraphLIME）框架，用于解释量子图神经网络，通过局部代理模型和结构保持扰动生成不确定性感知的节点和边重要性排序。


<details>
  <summary>Details</summary>
Motivation: 量子图神经网络在图形结构数据学习方面具有强大能力，但其可解释性受到测量诱导随机性和图形结构组合性质的复杂影响。

Method: 使用模型无关的事后解释框架，将模型解释视为在结构保持扰动上拟合的局部代理模型的分布，通过聚合代理归因及其离散度来生成重要性排序。

Result: 在具有已知真实值的受控合成图上进行实证研究，展示了准确稳定的解释，消融实验显示非线性代理建模的明显优势和对扰动设计的敏感性。

Conclusion: 建立了一个原则性、不确定性感知且结构敏感的方法来解释量子图神经网络，为扩展到更广泛架构和真实世界数据集奠定了基础。

Abstract: Quantum graph neural networks offer a powerful paradigm for learning on
graph-structured data, yet their explainability is complicated by
measurement-induced stochasticity and the combinatorial nature of graph
structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a
model-agnostic, post-hoc framework that treats model explanations as
distributions over local surrogates fit on structure-preserving perturbations
of a graph. By aggregating surrogate attributions together with their
dispersion, QGraphLIME yields uncertainty-aware node and edge importance
rankings for quantum graph models. The framework further provides a
distribution-free, finite-sample guarantee on the size of the surrogate
ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of
the induced distribution of a binary class probability at target accuracy and
confidence under standard independence assumptions. Empirical studies on
controlled synthetic graphs with known ground truth demonstrate accurate and
stable explanations, with ablations showing clear benefits of nonlinear
surrogate modeling and highlighting sensitivity to perturbation design.
Collectively, these results establish a principled, uncertainty-aware, and
structure-sensitive approach to explaining quantum graph neural networks, and
lay the groundwork for scaling to broader architectures and real-world
datasets, as quantum resources mature. Code is available at
https://github.com/smlab-niser/qglime.

</details>


### [29] [Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches](https://arxiv.org/abs/2510.05748)
*Hachem Madmoun,Salem Lahlou*

Main category: cs.LG

TL;DR: 研究比较了直接通信和课程学习在促进多智能体LLM系统合作方面的效果，发现简单通信协议比基于经验的训练更可靠，而课程学习设计需要谨慎考虑游戏序列中的战略教训。


<details>
  <summary>Details</summary>
Motivation: 探索在多智能体LLM系统中激发合作的机制，特别关注直接通信和课程学习两种方法的效果差异。

Method: 在4玩家猎鹿博弈中使用单字"廉价谈话"通道进行通信实验，同时在迭代公共物品博弈与惩罚中设计渐进复杂游戏的课程学习。

Result: 廉价谈话通信将合作率从0%提升到48.3%，而课程学习使智能体收益减少27.4%，强调背叛均衡的游戏会诱导"习得性悲观"。

Conclusion: 对于协调问题，简单通信协议比基于经验的训练更可靠，社会困境的课程设计需要仔细考虑游戏序列中的战略教训。

Abstract: Eliciting cooperation in multi-agent LLM systems is critical for AI
alignment. We investigate two approaches: direct communication and curriculum
learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases
cooperation from 0% to 48.3%, demonstrating communication as a robust
coordination mechanism. In contrast, we find that curriculum learning is highly
sensitive to design choices: our pedagogical curriculum through progressively
complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game
with Punishment. Qualitative analysis reveals that curricula emphasizing
defection-equilibrium games can induce "learned pessimism" in agents. These
findings suggest that for coordination problems, simple communication protocols
may be more reliable than experience-based training, and that curriculum design
for social dilemmas requires careful attention to the strategic lessons
embedded in game sequences.

</details>


### [30] [LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection](https://arxiv.org/abs/2510.05935)
*Mohamed Bal-Ghaoui,Fayssal Sabri*

Main category: cs.LG

TL;DR: 提出LLM-FS-Agent多智能体架构，通过智能体间的"辩论"实现可解释的特征选择，在网络安全领域显著提升分类性能并减少46%训练时间。


<details>
  <summary>Details</summary>
Motivation: 高维数据挑战机器学习模型的可解释性和计算效率，现有LLM特征选择方法缺乏结构化推理和透明决策依据。

Method: 设计多智能体架构，让不同角色的LLM智能体通过"辩论"方式集体评估特征相关性并生成详细理由。

Result: 在CIC-DIAD 2024 IoT入侵检测数据集上，LLM-FS-Agent实现优于或可比分类性能，平均减少46%下游训练时间（XGBoost p=0.028）。

Conclusion: 该审议架构同时提升决策透明度和计算效率，为实际应用提供实用可靠的特征选择解决方案。

Abstract: High-dimensional data remains a pervasive challenge in machine learning,
often undermining model interpretability and computational efficiency. While
Large Language Models (LLMs) have shown promise for dimensionality reduction
through feature selection, existing LLM-based approaches frequently lack
structured reasoning and transparent justification for their decisions. This
paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for
interpretable and robust feature selection. The system orchestrates a
deliberative "debate" among multiple LLM agents, each assigned a specific role,
enabling collective evaluation of feature relevance and generation of detailed
justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the
CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance
against strong baselines, including LLM-Select and traditional methods such as
PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves
superior or comparable classification performance while reducing downstream
training time by an average of 46% (statistically significant improvement, p =
0.028 for XGBoost). These findings highlight that the proposed deliberative
architecture enhances both decision transparency and computational efficiency,
establishing LLM-FS-Agent as a practical and reliable solution for real-world
applications.

</details>


### [31] [From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning](https://arxiv.org/abs/2510.06038)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Liu Wenfei,Zuo Zhiqiang*

Main category: cs.LG

TL;DR: 提出了一种奖励自由、主动的人类在环学习方法H-DSAC，结合PVP和DSAC技术，通过构建分布式代理价值函数来编码人类意图，实现自动驾驶的安全高效训练。


<details>
  <summary>Details</summary>
Motivation: 将人类专业知识融入强化学习过程，以减少风险探索并提高样本效率，解决RL在现实世界自动驾驶应用中的安全性和效率挑战。

Method: 结合代理价值传播(PVP)和分布式软演员-评论家(DSAC)，构建分布式代理价值函数，该函数通过为专家演示分配更高期望回报并对需要人工干预的动作进行惩罚来编码人类意图。

Result: 仿真和真实世界实验结果表明，该框架能够实现自动驾驶的安全、鲁棒和样本高效学习，在实用训练时间内完成真实世界驾驶策略学习。

Conclusion: H-DSAC方法成功地将人类指导融入强化学习，实现了自动驾驶的安全高效训练，为现实世界RL应用提供了可行解决方案。

Abstract: Autonomous driving with reinforcement learning (RL) has significant
potential. However, applying RL in real-world settings remains challenging due
to the need for safe, efficient, and robust learning. Incorporating human
expertise into the learning process can help overcome these challenges by
reducing risky exploration and improving sample efficiency. In this work, we
propose a reward-free, active human-in-the-loop learning method called
Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines
Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to
enable efficient and safe training in real-world environments. The key
innovation is the construction of a distributed proxy value function within the
DSAC framework. This function encodes human intent by assigning higher expected
returns to expert demonstrations and penalizing actions that require human
intervention. By extrapolating these labels to unlabeled states, the policy is
effectively guided toward expert-like behavior. With a well-designed state
space, our method achieves real-world driving policy learning within practical
training times. Results from both simulation and real-world experiments
demonstrate that our framework enables safe, robust, and sample-efficient
learning for autonomous driving.

</details>


### [32] [Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL](https://arxiv.org/abs/2510.06092)
*Nyal Patel,Matthieu Bou,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 提出了一种基于失败感知的逆向强化学习算法，专注于从RLHF模型误分类或难以判断的样本中提取潜在的奖励信号，从而更好地理解模型对齐的内在动机。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从人类反馈强化学习(RLHF)中提取潜在奖励信号时，对所有偏好对一视同仁，忽略了最具信息量的失败样本，这限制了奖励模型的可解释性和安全性。

Method: 开发了失败感知的逆向强化学习算法，重点关注误分类或得分相近的困难样本，通过这些失败案例来恢复定义模型行为的潜在奖励函数。

Result: 在LLM去毒化任务中，失败感知IRL在多个指标上优于现有IRL基线，无需外部分类器或监督。提取的奖励能更好地捕捉RLHF学习到的真实动机，实现比标准IRL更有效的重新RLHF训练。

Conclusion: 失败感知IRL是一种稳健、可扩展的方法，可用于审计模型对齐并减少IRL过程中的模糊性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with human preferences, yet the underlying reward signals they
internalize remain hidden, posing a critical challenge for interpretability and
safety. Existing approaches attempt to extract these latent incentives using
Inverse Reinforcement Learning (IRL), but treat all preference pairs equally,
often overlooking the most informative signals: those examples the extracted
reward model misclassifies or assigns nearly equal scores, which we term
\emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that
focuses on misclassified or difficult examples to recover the latent rewards
defining model behaviors. By learning from these failures, our failure-aware
IRL extracts reward functions that better reflect the true objectives behind
RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines
across multiple metrics when applied to LLM detoxification, without requiring
external classifiers or supervision. Crucially, failure-aware IRL yields
rewards that better capture the true incentives learned during RLHF, enabling
more effective re-RLHF training than standard IRL. This establishes
failure-aware IRL as a robust, scalable method for auditing model alignment and
reducing ambiguity in the IRL process.

</details>


### [33] [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](https://arxiv.org/abs/2510.06096)
*Matthieu Bou,Nyal Patel,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 提出了一个基于贝叶斯逆强化学习的LLM审计框架，通过量化不确定性、减少非可识别性并提供可操作的诊断，来验证LLM的真实优化目标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型隐式优化的目标过于不透明，使得可信对齐和审计成为重大挑战。现有方法要么产生单一、过度自信的奖励估计，要么无法解决任务的基本模糊性。

Method: 使用贝叶斯逆强化学习框架，不仅恢复目标分布，还提供三种关键审计能力：量化非可识别性、提供不确定性感知诊断、验证策略级效用。

Result: 成功审计了一个去毒化的LLM，产生了良好校准且可解释的目标，增强了对齐保证。

Conclusion: 这项工作为审计人员、安全团队和监管机构提供了一个实用工具包，用于验证LLM真正试图实现的目标，推动AI更加可信和负责任。

Abstract: The objectives that Large Language Models (LLMs) implicitly optimize remain
dangerously opaque, making trustworthy alignment and auditing a grand
challenge. While Inverse Reinforcement Learning (IRL) can infer reward
functions from behaviour, existing approaches either produce a single,
overconfident reward estimate or fail to address the fundamental ambiguity of
the task (non-identifiability). This paper introduces a principled auditing
framework that re-frames reward inference from a simple estimation task to a
comprehensive process for verification. Our framework leverages Bayesian IRL to
not only recover a distribution over objectives but to enable three critical
audit capabilities: (i) Quantifying and systematically reducing
non-identifiability by demonstrating posterior contraction over sequential
rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics
that expose spurious shortcuts and identify out-of-distribution prompts where
the inferred objective cannot be trusted; and (iii) Validating policy-level
utility by showing that the refined, low-uncertainty reward can be used
directly in RLHF to achieve training dynamics and toxicity reductions
comparable to the ground-truth alignment process. Empirically, our framework
successfully audits a detoxified LLM, yielding a well-calibrated and
interpretable objective that strengthens alignment guarantees. Overall, this
work provides a practical toolkit for auditors, safety teams, and regulators to
verify what LLMs are truly trying to achieve, moving us toward more trustworthy
and accountable AI.

</details>


### [34] [Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks](https://arxiv.org/abs/2510.06138)
*Rushiv Arora*

Main category: cs.LG

TL;DR: LEXPOL是一种基于语言条件的策略混合架构，用于多任务强化学习，通过任务元数据（如自然语言描述）来选择和组合子策略，实现跨任务的端到端训练。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习通常依赖任务元数据来指导不同目标的行为，但如何有效利用这些元数据来索引和组合可重用技能仍是一个挑战。

Method: 使用文本编码器编码任务元数据，通过学习的门控模块选择和混合多个子策略，支持端到端跨任务训练。

Result: 在MetaWorld基准测试中，LEXPOL在成功率和样本效率上匹配或超过了强大多任务基线，无需任务特定重训练。

Conclusion: 自然语言元数据可以有效地在单一策略中索引和重组可重用技能，为多任务学习提供了新思路。

Abstract: Multi-task reinforcement learning often relies on task metadata -- such as
brief natural-language descriptions -- to guide behavior across diverse
objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned
mixture-of-policies architecture for multi-task RL. LEXPOL encodes task
metadata with a text encoder and uses a learned gating module to select or
blend among multiple sub-policies, enabling end-to-end training across tasks.
On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines
in success rate and sample efficiency, without task-specific retraining. To
analyze the mechanism, we further study settings with fixed expert policies
obtained independently of the gate and show that the learned language gate
composes these experts to produce behaviors appropriate to novel task
descriptions and unseen task combinations. These results indicate that
natural-language metadata can effectively index and recombine reusable skills
within a single policy.

</details>


### [35] [LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams](https://arxiv.org/abs/2510.06151)
*Aju Ani Justus,Chris Baber*

Main category: cs.LG

TL;DR: 使用LLMs作为策略不可知的人类代理生成合成数据，以模拟人类决策，解决异构智能体团队中与策略不可访问或非平稳队友协作的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的人工参与数据，限制了可扩展性。需要一种可扩展的方法来训练智能体与人类等策略不可访问的队友协作。

Method: 在基于Stag Hunt博弈的网格世界捕获游戏中进行了三个实验：比较LLMs与人类参与者的决策一致性；通过提示诱导风险敏感策略；在动态网格世界中测试LLMs生成移动动作。

Result: LLMs在决策标准应用上比人类参与者更接近专家；通过提示可以模拟人类的风险规避和风险寻求行为；LLMs生成的轨迹与人类参与者路径相似。

Conclusion: 虽然LLMs尚不能完全复制人类适应性，但其提示引导的多样性为模拟策略不可知队友提供了可扩展基础。

Abstract: A critical challenge in modelling Heterogeneous-Agent Teams is training
agents to collaborate with teammates whose policies are inaccessible or
non-stationary, such as humans. Traditional approaches rely on expensive
human-in-the-loop data, which limits scalability. We propose using Large
Language Models (LLMs) as policy-agnostic human proxies to generate synthetic
data that mimics human decision-making. To evaluate this, we conduct three
experiments in a grid-world capture game inspired by Stag Hunt, a game theory
paradigm that balances risk and reward. In Experiment 1, we compare decisions
from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and
Mixtral 8x22B models. LLMs, prompted with game-state observations and reward
structures, align more closely with experts than participants, demonstrating
consistency in applying underlying decision criteria. Experiment 2 modifies
prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM
outputs mirror human participants' variability, shifting between risk-averse
and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic
grid-world where the LLM agents generate movement actions. LLMs produce
trajectories resembling human participants' paths. While LLMs cannot yet fully
replicate human adaptability, their prompt-guided diversity offers a scalable
foundation for simulating policy-agnostic teammates.

</details>


### [36] [Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents](https://arxiv.org/abs/2510.06214)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 提出了Stratified GRPO方法来解决LLM搜索代理中轨迹结构异质性导致的跨层偏差问题，通过分层优势归一化在同类轨迹内计算优势值，显著提升了训练效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在解决复杂多步问题时依赖外部工具，但搜索代理的轨迹结构异质性导致标准策略梯度方法存在跨层偏差，扭曲了信用分配并阻碍了复杂搜索策略的探索。

Method: 提出Stratified GRPO方法，核心是分层优势归一化(SAN)，根据轨迹结构特性将轨迹划分为同质层，在每层内局部计算优势值，确保轨迹只与同类轨迹比较。

Result: 在多样单跳和多跳问答基准测试中，Stratified GRPO比GRPO性能提升高达11.3分，获得更高的训练奖励、更好的训练稳定性和更有效的搜索策略。

Conclusion: 分层方法为LLM搜索代理中的结构异质性提供了原则性解决方案，消除了跨层偏差，提供了更纯净和尺度稳定的学习信号。

Abstract: Large language model (LLM) agents increasingly rely on external tools such as
search engines to solve complex, multi-step problems, and reinforcement
learning (RL) has become a key paradigm for training them. However, the
trajectories of search agents are structurally heterogeneous, where variations
in the number, placement, and outcomes of search calls lead to fundamentally
different answer directions and reward distributions. Standard policy gradient
methods, which use a single global baseline, suffer from what we identify and
formalize as cross-stratum bias-an "apples-to-oranges" comparison of
heterogeneous trajectories. This cross-stratum bias distorts credit assignment
and hinders exploration of complex, multi-step search strategies. To address
this, we propose Stratified GRPO, whose central component, Stratified Advantage
Normalization (SAN), partitions trajectories into homogeneous strata based on
their structural properties and computes advantages locally within each
stratum. This ensures that trajectories are evaluated only against their true
peers. Our analysis proves that SAN eliminates cross-stratum bias, yields
conditionally unbiased unit-variance estimates inside each stratum, and retains
the global unbiasedness and unit-variance properties enjoyed by standard
normalization, resulting in a more pure and scale-stable learning signal. To
improve practical stability under finite-sample regimes, we further linearly
blend SAN with the global estimator. Extensive experiments on diverse
single-hop and multi-hop question-answering benchmarks demonstrate that
Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3
points, achieving higher training rewards, greater training stability, and more
effective search policies. These results establish stratification as a
principled remedy for structural heterogeneity in RL for LLM search agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [37] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: 提出了一种基于强化学习的自适应配置分配框架，用于解决非平稳环境下的软件测试资源配置问题


<details>
  <summary>Details</summary>
Motivation: 现代软件系统需要在异构和动态变化的环境中进行严格测试，但传统组合优化方法无法适应概率漂移的非平稳设置

Method: 将配置分配建模为序列决策问题，结合Q学习和混合奖励设计（融合模拟结果和实时反馈），采用自适应在线-离线训练方案

Result: 在广泛模拟研究中，该方法持续优于静态和基于优化的基线方法，接近oracle性能

Conclusion: 强化学习为自适应配置分配提供了强大的新范式，超越了传统方法，在动态测试和资源调度领域具有广泛应用前景

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [38] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard是一个为LLM智能体提供正式安全保证的框架，通过离线验证和在线监控的双阶段架构确保智能体行为符合预设安全约束。


<details>
  <summary>Details</summary>
Motivation: 在医疗等敏感领域部署自主AI智能体存在安全风险，现有系统无法完全保证智能体行为符合安全约束，需要一种正式的安全保障机制。

Method: 采用双阶段架构：离线阶段通过澄清用户意图、合成行为策略并进行测试和形式验证来确保策略正确性；在线阶段作为运行时监控器验证每个拟执行动作。

Result: VeriGuard框架能够为LLM智能体提供正式的安全保证，显著提高智能体的可信度。

Conclusion: 通过分离离线验证和在线监控，VeriGuard实现了形式化保证的实际应用，为LLM智能体提供了强大的安全保障。

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [39] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本文系统评估了LLM在测试用例生成中的推理能力，基于Bloom认知分类学构建评估框架，发现LLM在记忆层面表现良好，但在应用层面面临严重性能下降，同时揭示了技术元素比叙述描述对测试生成更关键的影响。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在自动化软件测试中的泛化能力，特别是超越记忆模式、理解自然语言错误报告并进行推理的能力，目前这方面的研究尚不明确。

Method: 基于LIBRO框架和Bloom认知分类学，在Defects4J、GHRB及其变异版本上评估StarCoder和GPT-4o，引入语言和语义挑战，分析不同认知层级的性能表现。

Result: LLM在记忆层面能较好复现先前结果，在理解层面对语言改写和翻译展现部分鲁棒性，但在应用层面标识符变异导致性能下降超过60%。开放书本设置下few-shot示例可将成功率提升三倍，技术元素比叙述描述对测试生成影响更大。

Conclusion: 研究揭示了LLM生成测试的认知过程，为性能改进提供了具体方向，并为该任务建立了稳健现实的评估范式。

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [40] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX是一个开源AI多智能体系统，用于为遗留代码生成单元测试，提高测试覆盖率和关键值测试能力。


<details>
  <summary>Details</summary>
Motivation: 解决复杂遗留代码库中单元测试生成的挑战，提高软件可靠性和可维护性。

Method: 结合AI智能体、形式化方法和大型语言模型(LLMs)来自动化测试生成。

Result: 能够生成高质量测试并识别潜在问题，同时提高遗留代码的可读性和文档质量。

Conclusion: UnitTenX为遗留代码测试提供了一个有效的自动化框架，尽管LLMs在错误检测方面存在局限性。

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [41] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 研究LLM生成的代码审查评论类型及其被开发者采纳的情况，发现LLM和人类审查员在不同项目背景下各有优势，可读性、bug和维护性相关的评论更易被采纳。


<details>
  <summary>Details</summary>
Motivation: 理解哪些类型的LLM生成的代码审查评论更可能触发代码变更，以识别可操作的评论。

Method: 开发LLM-as-a-Judge自动分类审查评论，基于五个类别的分类法进行实证研究。

Result: LLM和人类审查员在不同项目背景下表现出不同的优势；可读性、bug和维护性相关的评论比代码设计相关的评论有更高的解决率。

Conclusion: 大部分LLM生成的评论是可操作的，LLM和人类审查员具有互补性，为改进LLM驱动的代码审查工具提供了建议。

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [42] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum是一个开源的4B参数代码补全模型，专为JetBrains IDE交互使用设计，采用多阶段训练和严格数据治理，在保持紧凑规模的同时满足生产环境的成本和延迟要求。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在IDE中提供高质量交互式代码补全的紧凑模型，满足生产环境的成本、延迟和用户体验要求。

Method: 采用多阶段训练流程：包括预训练、填空训练、项目上下文监督微调，以及基于真实场景反馈的直接偏好优化对齐。

Result: 模型在离线基准测试和生产环境部署中都表现出高质量，证明了紧凑模型能够满足大规模生产需求。

Conclusion: 紧凑、任务专注的模型配合严格的数据治理和多阶段训练，能够从研究原型成功扩展到数十万用户的生产部署。

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [43] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文通过调查91名软件工程师（其中72名活跃使用GenAI工具），系统研究了开发者如何将生成式AI工具集成到软件开发工作流中，重点关注提示策略、对话模式和可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注个别提示工程技术，而缺乏对软件开发者整体工作流程中GenAI工具集成方式的系统性调查。

Method: 采用大规模问卷调查方法，分析软件工程师在开发过程中使用AI的模式，包括提示策略、对话模式和可靠性评估。

Result: 14个关键发现显示：代码生成几乎普遍使用，但熟练度与使用AI进行调试和代码审查等复杂任务密切相关；开发者偏好迭代式多轮对话而非单次提示；文档任务被认为最可靠，而复杂代码生成和调试面临较大挑战。

Conclusion: 研究为当前开发者实践提供了经验基准，从简单代码生成到深度工作流集成，为未来改进提供了可行见解。

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [44] [<em class="highlight">强化学习</em>+卡尔曼滤波彻底爆了！一区手到擒来！](http://mp.weixin.qq.com/s?__biz=MzkzOTY0NTMyNw==&mid=2247489462&idx=1&sn=85b4bfb71b17d2ef334393709a17da08&chksm=c34f9d6b95a77db074dbe92f9a5a0ffb1c80bd4049cf5f64ef7f3af8f4da3ee242fe83cfe35e#rd)
*AI科研技术派*

Main category: wechat.article

TL;DR: 而强化学习则擅长通过试错法学习最优策略。两者结合能够优势互补，更好地理解和预测环境状态，从而提高预测的准确性和模型的鲁棒性。目前，该思路在游戏、机器人控制、自动驾驶等领域，都有着广泛应用，可发挥空间很


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而强化学习则擅长通过试错法学习最优策略。两者结合能够优势互补，更好地理解和预测环境状态，从而提高预测的准确性和模型的鲁棒性。目前，该思路在游戏、机器人控制、自动驾驶等领域，都有着广泛应用，可发挥空间很

</details>


### [45] [<em class="highlight">强化学习</em>之父：LLM 是“死路一条”，那 AI 的活路又在何方？](http://mp.weixin.qq.com/s?__biz=MzI3OTc1MjEyMg==&mid=2247483725&idx=1&sn=1a1d28ddf502470f6010d4270a8c0c1b&chksm=ea7cacaa8cc6a4cb5b82de7b0eb05835f6f18381b928002e5de867461ec70452664da2f46762#rd)
*俞者佩之*

Main category: wechat.article

TL;DR: Richard Sutton，被誉为强化学习之父，2024 年图灵奖得主，《The Bitter Lesson》（《苦涩的教训》）作者。他曾指出：真正有效的人工智能，不是人类精心编程出的“聪明”，而是机器从经验中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Richard Sutton，被誉为强化学习之父，2024 年图灵奖得主，《The Bitter Lesson》（《苦涩的教训》）作者。他曾指出：真正有效的人工智能，不是人类精心编程出的“聪明”，而是机器从经验中学习的能力。

</details>


### [46] [RAG 已过时：<em class="highlight">强化学习</em>智能体 (RL Agents) 将成为新的检索栈](http://mp.weixin.qq.com/s?__biz=MzA3NzQ4ODMxOQ==&mid=2247484024&idx=1&sn=3ccfe92c6e6cce7e710083e74a68a3ab&chksm=9ef1ef6b529b1870ab96c7f69bc84c0dd8c7e3f2337733b45dec71a8602f018c435adb9584af#rd)
*叶梓的 AI 研习社*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）：通过奖励/惩罚让AI自我进化奖励函数（Reward Function）：规定“做对什么才算强”的标准大白话：让AI像玩闯关游戏一样，做对一步奖励、查错一步惩罚，从不断试错中变得又快又准


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）：通过奖励/惩罚让AI自我进化奖励函数（Reward Function）：规定“做对什么才算强”的标准大白话：让AI像玩闯关游戏一样，做对一步奖励、查错一步惩罚，从不断试错中变得又快又准

</details>


### [47] [【<em class="highlight">强化学习</em>】#9 策略梯度](http://mp.weixin.qq.com/s?__biz=Mzk0MjY5NzY5OQ==&mid=2247486749&idx=1&sn=5fe666990db6b19a247dd34726484f7d&chksm=c2a2b46ec87db36466f089ec005eb33b1e904d2d4c5c390e8f7d94a974187a56ee8e16e57343#rd)
*一杯为品*

Main category: wechat.article

TL;DR: 到目前为止，本系列所介绍的所有强化学习方法几乎都是基于价值的方法，它们都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，没有动作价值函数的估计就没有策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 到目前为止，本系列所介绍的所有强化学习方法几乎都是基于价值的方法，它们都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，没有动作价值函数的估计就没有策略。

</details>


### [48] [DiffusionNFT：面向扩散模型的在线<em class="highlight">强化学习</em>新范式](http://mp.weixin.qq.com/s?__biz=MzUzNjk1NTU5Mw==&mid=2247484166&idx=1&sn=d6f2c6aa6a7a71b103fe8489202fbd9e&chksm=fb416e7a113cc6e79d7fb92e213858ba45101b95ed8dc5d0bce433c9983eacac5d7132b89a1b#rd)
*AI前沿文献速递*

Main category: wechat.article

TL;DR: 02核心思想：基于前向过程的强化学习DiffusionNFT的核心创新在于，它不依赖策略梯度框架，而是通过前向扩散过程中的流匹配目标（Flow Matching Objective）进行策略优化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 02核心思想：基于前向过程的强化学习DiffusionNFT的核心创新在于，它不依赖策略梯度框架，而是通过前向扩散过程中的流匹配目标（Flow Matching Objective）进行策略优化。

</details>


### [49] [机器<em class="highlight">学习</em>算法是实现 “数据驱动决策” 的核心工具，有监督<em class="highlight">学习</em>、无监督<em class="highlight">学习</em>、<em class="highlight">强化学习</em>三大核心领域](http://mp.weixin.qq.com/s?__biz=MzI3ODE4MjczNA==&mid=2651506232&idx=1&sn=c1021f14ca07a95b8ed74fc6fcb61673&chksm=f1e9ba3463f5cc4bf813af08859ddcffeb72477b46441b7c4403fbfe3944de1e97b6494b894f#rd)
*算法驱动的数据圈*

Main category: wechat.article

TL;DR: 以下按监督学习、无监督学习、强化学习三大核心领域，梳理工业界和科研中最常用、最重要的算法，涵盖其核心原理、用途、优缺点及典型场景。一、监督学习算法（已知标签，学习 “输入→输出” 映射）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以下按监督学习、无监督学习、强化学习三大核心领域，梳理工业界和科研中最常用、最重要的算法，涵盖其核心原理、用途、优缺点及典型场景。一、监督学习算法（已知标签，学习 “输入→输出” 映射）

</details>


### [50] [斯坦福大学 | “会打篮球”的<em class="highlight">强化学习</em>框架！让AI从零散动作学会长时序运动](http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488693&idx=1&sn=0b8f3bb591293e40304abad07bb70a87&chksm=c1bc868825dba8bad1a53546033684ad9a44b95b7f453fe00ae87bb79dbb86e93aca8eb7ad45#rd)
*具身智能研究室*

Main category: wechat.article

TL;DR: 算法实现细节1 强化学习基础：PPO + 对抗模仿学习通过 PPO 算法优化策略，确保训练稳定性；引入模仿奖励，让策略同时模仿人类动作与完成任务目标。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 算法实现细节1 强化学习基础：PPO + 对抗模仿学习通过 PPO 算法优化策略，确保训练稳定性；引入模仿奖励，让策略同时模仿人类动作与完成任务目标。

</details>


### [51] [智能体<em class="highlight">强化学习</em>爆发：三周革新让 AI 从“懂”到“会”](http://mp.weixin.qq.com/s?__biz=MzYyMTg1MTI3Mw==&mid=2247483704&idx=1&sn=ac20e96758638ee9712f435a05abf0a7&chksm=fe5b5d0450979901faf087deaf27619b58c7fc18e906ccd0ecdd6c977cec3d5fedc90979ceaf#rd)
*智能波哨*

Main category: wechat.article

TL;DR: 智能体强化学习：为何此刻爆发？智能体强化学习的本质，是让AI在反复试错中学习，就像人类学习骑自行车一样。不只是知道平衡的原理，而是在一次次摔倒中掌握真正骑行的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 智能体强化学习：为何此刻爆发？智能体强化学习的本质，是让AI在反复试错中学习，就像人类学习骑自行车一样。不只是知道平衡的原理，而是在一次次摔倒中掌握真正骑行的能力。

</details>


### [52] [清华、NVIDIA、斯坦福提出DiffusionNFT：基于前向过程的扩散<em class="highlight">强化学习</em>新范式，训练效率提升25倍](http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574246&idx=3&sn=d030c6b3d28f02f39e3c074409323059&chksm=ea9282adcbb07d1f58195b4780d2a7455716fdb58a6e0e4617c2795541f01e1f08c53c02629f#rd)
*机器学习算法与自然语言处理*

Main category: wechat.article

TL;DR: nvidia deep imagination 研究组与斯坦福 stefano ermon 团队联合提出了一种全新的扩散模型强化学习（rl）范式 ——diffusion negative-aware finetuning （diffusionnft）。该方法首次突破现有 RL 对扩散模型的基本假设，直接在前向加噪过程（forward


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: nvidia deep imagination 研究组与斯坦福 stefano ermon 团队联合提出了一种全新的扩散模型强化学习（rl）范式 ——diffusion negative-aware finetuning （diffusionnft）。该方法首次突破现有 RL 对扩散模型的基本假设，直接在前向加噪过程（forward

</details>


### [53] [从零实现一个乞丐版<em class="highlight">Code</em> <em class="highlight">Agent</em>](http://mp.weixin.qq.com/s?__biz=MzU5Njk5OTk1Mg==&mid=2247483661&idx=1&sn=ad6ad98cdc874f90f578b0ac63412917&chksm=ffbd9f2cfa869df500306449c60902f6635bf619f3552de77e0085cab1ad1e42ba3b88ca5939#rd)
*purecattleandhorse*

Main category: wechat.article

TL;DR: Code Agent 的工作流程与人类编程类似：理解需求后，编写代码、测试、发现问题、修改代码、再次测试，直至任务完成。在这一过程中，Code Agent 会不断尝试，直到成功完成任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Code Agent 的工作流程与人类编程类似：理解需求后，编写代码、测试、发现问题、修改代码、再次测试，直至任务完成。在这一过程中，Code Agent 会不断尝试，直到成功完成任务。

</details>


### [54] [Andrew Ng新课《<em class="highlight">Agentic</em> AI》：让你领先99%的开发者？](http://mp.weixin.qq.com/s?__biz=Mzk0MTY4MjE4OA==&mid=2247488434&idx=1&sn=afb007fa0529a169a6bcc1ca46ccf906&chksm=c3b61c7e788b072b53b667cdf9b2c95e0e59957a989464929904acc087db4bf602cc385f54a1#rd)
*蔡荔谈AI*

Main category: wechat.article

TL;DR: 与只能被动问答的传统大语言模型（LLM）不同，Agentic AI系统拥有主动“行动”的能力。它们能够像人类一样，感知环境、制定计划、调用工具、执行任务，并从结果中学习和反思。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 与只能被动问答的传统大语言模型（LLM）不同，Agentic AI系统拥有主动“行动”的能力。它们能够像人类一样，感知环境、制定计划、调用工具、执行任务，并从结果中学习和反思。

</details>


### [55] [面向未来<em class="highlight">Agentic</em>网络的信任范式演进：从Zero-Trust到Max-Trust](http://mp.weixin.qq.com/s?__biz=MzYzMzAzOTQyNA==&mid=2247483653&idx=1&sn=286524b5bb0815496fb2a58f364ff821&chksm=f1d3348b79f47f74d33ca3aa39146b806f3165e3c15289222d85fe004d03405a2543c48a0de0#rd)
*饱饱公主*

Main category: wechat.article

TL;DR: 策略管理面向未来Agentic网络的信任范式演进：从Zero-Trust到Max-Trust2025年10月8日，ETSI Security Conference 2025 在法国索菲亚·安提波利斯举行。作为欧洲电信标准协会（ETSI， the standards people）年度最具影响力的安全盛会，本次大会汇聚


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 策略管理面向未来Agentic网络的信任范式演进：从Zero-Trust到Max-Trust2025年10月8日，ETSI Security Conference 2025 在法国索菲亚·安提波利斯举行。作为欧洲电信标准协会（ETSI， the standards people）年度最具影响力的安全盛会，本次大会汇聚

</details>


### [56] [<em class="highlight">Agentic</em> AI 工程化蓝图：从 Demo 到生产级<em class="highlight">智能体</em>的 5 步落地密码](http://mp.weixin.qq.com/s?__biz=MzIyNDkxMjQ3OA==&mid=2247485471&idx=1&sn=abcbd3eff445d393e9c7ba5acc85ef1f&chksm=e9f4b9accb7f3cd47e5174d49f0ad5a812b6b1b28ec9ee683d84c0af0e88999c6f52466fd036#rd)
*机器之魂*

Main category: wechat.article

TL;DR: 在供应商成本优化智能体中，我们可能会提取最近的发票，检测异常，并总结可疑的行项目，然后才要求模型推荐行动。 在法律合同审查智能体中，我们可能会仅检索与知识产权或责任相关的条款，并将其构建成清晰的“危险


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在供应商成本优化智能体中，我们可能会提取最近的发票，检测异常，并总结可疑的行项目，然后才要求模型推荐行动。 在法律合同审查智能体中，我们可能会仅检索与知识产权或责任相关的条款，并将其构建成清晰的“危险

</details>


### [57] [静态工作流已过时？<em class="highlight">Agentic</em> AI正在接管自动化舞台](http://mp.weixin.qq.com/s?__biz=MzI0NTg0Njk1OQ==&mid=2247493965&idx=1&sn=10b831c49d4c444a963ca513436073ca&chksm=e878ebdba015b33c8e98749f6ff05c9cba0dc7fe08a1f633af438a4a78f8b7bf7e5a12c1d4a1#rd)
*Halo咯咯*

Main category: wechat.article

TL;DR: Agentic AI版：会分解、会适应、会反思再看这个Agentic Bot，完全不同：from datetime import datetime， timedeltaclass TrulyAgenticBot：def __init__（self）：self.tasks = {}def decompose_goal（self， goal）：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI版：会分解、会适应、会反思再看这个Agentic Bot，完全不同：from datetime import datetime， timedeltaclass TrulyAgenticBot：def __init__（self）：self.tasks = {}def decompose_goal（self， goal）：

</details>


### [58] [华为发布《迈向智能世界：<em class="highlight">Agentic</em> AI重塑企业数智化新纪元》（42页附下载）](http://mp.weixin.qq.com/s?__biz=MzkzNzUzNTMwNw==&mid=2247490151&idx=1&sn=b0d8ad3ef738935305d4c89d93d0f44c&chksm=c3cdb81b52709006936966878fe534fe47c0d0c527ce97174cd0a8503c6aee97d978583f6cec#rd)
*扣子Bolt*

Main category: wechat.article

TL;DR: 写在最后Agentic AI 不只是新技术，更是一种新范式。它让企业从“被动运维”走向“自主决策”，让AI从“工具”变为“伙伴”，让智能从“单点”进化为“群体”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 写在最后Agentic AI 不只是新技术，更是一种新范式。它让企业从“被动运维”走向“自主决策”，让AI从“工具”变为“伙伴”，让智能从“单点”进化为“群体”。

</details>


### [59] [<em class="highlight">Agentic</em> AI 正在快速重塑互联网](http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247484909&idx=1&sn=84c46153e276dbe08d45a5bf722a0a84&chksm=9737bda3b33546b27744ff7c86f5e01eac80461ddd31f9da166cd5576417974d9de76b200149#rd)
*ATinfo*

Main category: wechat.article

TL;DR: Agentic AI 正在快速重塑互联网核心要点：AI 智能体正在推动从基础运行时间和带宽向数据质量、服务弹性和端到端保障的关注转变数据质量验证成为信任的关键因素


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 正在快速重塑互联网核心要点：AI 智能体正在推动从基础运行时间和带宽向数据质量、服务弹性和端到端保障的关注转变数据质量验证成为信任的关键因素

</details>


### [60] [只会聊天的大模型已过时？读懂<em class="highlight">Agentic</em> RL，看懂AI如何从“说”到“做”](http://mp.weixin.qq.com/s?__biz=MzAxMTA4MDE4MQ==&mid=2247484141&idx=1&sn=cd536a173766a57bfb23964ca847a8ba&chksm=9ac091d54d941d0d7afea6bf65b902a2b15e984d5224657760a7d0d32f57ef59bf9b5a472598#rd)
*数字Gem*

Main category: wechat.article

TL;DR: 应用广阔： 从深度研究、软件工程到数学推理，Agentic RL正在各个专业领域催生出能力更强的专用智能体，展现出巨大的应用潜力。一、从“知识库”到“行动者”：AI的进化新范式


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 应用广阔： 从深度研究、软件工程到数学推理，Agentic RL正在各个专业领域催生出能力更强的专用智能体，展现出巨大的应用潜力。一、从“知识库”到“行动者”：AI的进化新范式

</details>


### [61] [DeepSeek的三种使用方式](http://mp.weixin.qq.com/s?__biz=MzU3Nzc1MDQ4NQ==&mid=2247489525&idx=1&sn=a451bcf4c8df8b01ccf7d4c4528f63cc&chksm=fc271bc8655343a3b09cbdcfe419837423a6ba0775f725ae291e6d660a3e76f3246d84cdb84f#rd)
*石家庄南开校友会*

Main category: wechat.article

TL;DR: deepseek-r1是一教日有bt1b参数大小岭创新性大语台模型，该项的关于transformer 数，在14.8万亿个tokens上进行预v练。该项型采用多头潜在注意力（mla）和deep... depsenk-v3册despseok队开发的所一代专家员合（mce）语台模型，共有bt1b参 api女


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: deepseek-r1是一教日有bt1b参数大小岭创新性大语台模型，该项的关于transformer 数，在14.8万亿个tokens上进行预v练。该项型采用多头潜在注意力（mla）和deep... depsenk-v3册despseok队开发的所一代专家员合（mce）语台模型，共有bt1b参 api女

</details>


### [62] [探索大语言<em class="highlight">模型</em>（LLM）：提升 RAG 性能的全方位优化策略](http://mp.weixin.qq.com/s?__biz=Mzk5MDYyOTU2MA==&mid=2247484109&idx=1&sn=61dc1315ee197552462ddeb3db94302f&chksm=c45d75e20d30f7d61b659ed2824362e9485fa586f04eae08824dc5e0da5d3dc384e8719c10e8#rd)
*AI欧应万*

Main category: wechat.article

TL;DR: 在大语言模型（LLM）应用日益普及的今天，检索增强生成（RAG）技术已成为连接外部知识与模型推理的核心桥梁。然而，基础版 RAG 系统往往难以满足复杂业务场景的需求，如何提升其准确性、效率和鲁棒性成为开发者关注的焦


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在大语言模型（LLM）应用日益普及的今天，检索增强生成（RAG）技术已成为连接外部知识与模型推理的核心桥梁。然而，基础版 RAG 系统往往难以满足复杂业务场景的需求，如何提升其准确性、效率和鲁棒性成为开发者关注的焦

</details>


### [63] [一篇带你搞清楚大语言<em class="highlight">模型</em>核心原理](http://mp.weixin.qq.com/s?__biz=Mzk4ODE0MDMyNw==&mid=2247484143&idx=1&sn=392cd901ee0ef5996da95dbfe2f33ce9&chksm=c4fc0a15ea71df3a84175184ca1704008f65b5046ac4d92446e956a8a2440ae6920ff2df2f9d#rd)
*大模型阿尤*

Main category: wechat.article

TL;DR: 初步认识了大模型长什么样了，接下来一起来看看如何训练出一个大模型。Collect demonstration data，Collect comparison data，optimize a policy against and train a supervised policy. and train a reward model. the reward model using reinforcem...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 初步认识了大模型长什么样了，接下来一起来看看如何训练出一个大模型。Collect demonstration data，Collect comparison data，optimize a policy against and train a supervised policy. and train a reward model. the reward model using reinforcement learning. a promptis a pro

</details>


### [64] [Anthropic推出AI安全工具Petri：通过自主Agent研究<em class="highlight">大模型</em>行为](http://mp.weixin.qq.com/s?__biz=MjM5NjA0NjgyMA==&mid=2651328480&idx=1&sn=80fcf97c75584ae92b81bb14916aeb59&chksm=bc1b7ca9c0bcb8275c4b4c45efff90a224a89e6e2108bbe1ada508778efb12db966ce52dc06b#rd)
*FreeBuf*

Main category: wechat.article

TL;DR: 1、petri是一款开源工具，通过ai agent审计大语言模型（llm） 的行为，旨在识别模型可能存在的欺骗用户、举报行为、配合人类 滥用及助长恐怖主义等多种潜在问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1、petri是一款开源工具，通过ai agent审计大语言模型（llm） 的行为，旨在识别模型可能存在的欺骗用户、举报行为、配合人类 滥用及助长恐怖主义等多种潜在问题。

</details>


### [65] [一张图搞明白“提示词”、“Agent”、“<em class="highlight">大模型</em>”、“MCP”、“工具”之间的关系](http://mp.weixin.qq.com/s?__biz=MzAxNTMxMzk1NQ==&mid=2457645562&idx=1&sn=558787d81d4b32529f5d2e6dcf1f28de&chksm=8dade576fb2084b39f190397f3c418f0e193e532fd0b761334bd2efec0fdb042e821b752c5ea#rd)
*成为一名架构师*

Main category: wechat.article

TL;DR: 大模型本身是“纯思维”的，它不知道实时天气，不能执行代码，也不能操作数据库，工具弥补了这一缺陷。关系：智能体通过调用工具来扩展其能力边界，从“思考”走向“行动”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型本身是“纯思维”的，它不知道实时天气，不能执行代码，也不能操作数据库，工具弥补了这一缺陷。关系：智能体通过调用工具来扩展其能力边界，从“思考”走向“行动”。

</details>


### [66] [RL不再撒胡椒面！港科大 × 清华新作：只盯“规划token”，<em class="highlight">大模型</em>推理力狂飙](http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660372&idx=2&sn=e9a81cca335aead04faee932a82589cc&chksm=e89e904a1d458d3b052b4f33d075d6f2f537638963ae8bedca267b9059979864df96a7e0c63c#rd)
*数据派THU*

Main category: wechat.article

TL;DR: 大模型在 RL 的驱动下，先学会“怎么做”，再学会“怎么想”。在大模型推理力的进化史上，总有一些让人百思不得其解的“谜之现象”：模型为何会突然迎来 Aha 时刻，像是顿悟般智力飞升？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型在 RL 的驱动下，先学会“怎么做”，再学会“怎么想”。在大模型推理力的进化史上，总有一些让人百思不得其解的“谜之现象”：模型为何会突然迎来 Aha 时刻，像是顿悟般智力飞升？

</details>


### [67] [一篇读懂 10 大主流<em class="highlight">大模型</em> Agent 构建框架：从特性到场景，新手入门必看](http://mp.weixin.qq.com/s?__biz=Mzk5MDkzMjY3NA==&mid=2247484800&idx=1&sn=752fa3ddfc6bfa545ba093f320fa7d6d&chksm=c48a614135ce9c018b26bdb73dc1d7b9e6c4d96e393cc69e9c81ad7809e0f405bed25d5b7c33#rd)
*大模型知知*

Main category: wechat.article

TL;DR: 2024/3/23 19：45Foxit Phantom P.1.0199·大模型（LLMs）微调面2024/3/23 19：45Foxit Phantom P...2，958AI产品经理AI产品经理AI产品经理ToB产品产品经理，成为10-LLMs训练经验LLM面试题合集


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2024/3/23 19：45Foxit Phantom P.1.0199·大模型（LLMs）微调面2024/3/23 19：45Foxit Phantom P...2，958AI产品经理AI产品经理AI产品经理ToB产品产品经理，成为10-LLMs训练经验LLM面试题合集

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
*Yansen Zhang,Qingcan Kang,Yujie Chen,Yufei Wang,Xiongwei Han,Tao Zhong,Mingxuan Yuan,Chen Ma*

Main category: cs.AI

TL;DR: SAC-Opt是一个基于语义锚点的后向引导修正框架，用于提高LLM生成优化模型代码的准确性和语义保真度，通过语义对齐而非求解器反馈来纠正逻辑错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖求解器驱动的单次前向生成和有限的错误修复，无法检测语义错误，导致生成语法正确但逻辑有误的模型。

Method: 提出SAC-Opt框架，通过将原始语义锚点与生成代码重构的锚点对齐，选择性修正不匹配的组件，实现细粒度的约束和目标逻辑优化。

Result: 在7个公共数据集上的实验表明，SAC-Opt将平均建模准确率提高了7.8%，在ComplexLP数据集上最高提升21.9%。

Conclusion: 语义锚点修正对基于LLM的优化工作流程至关重要，能确保从问题意图到求解器可执行代码的忠实转换。

Abstract: Large language models (LLMs) have opened new paradigms in optimization
modeling by enabling the generation of executable solver code from natural
language descriptions. Despite this promise, existing approaches typically
remain solver-driven: they rely on single-pass forward generation and apply
limited post-hoc fixes based on solver error messages, leaving undetected
semantic errors that silently produce syntactically correct but logically
flawed models. To address this challenge, we propose SAC-Opt, a backward-guided
correction framework that grounds optimization modeling in problem semantics
rather than solver feedback. At each step, SAC-Opt aligns the original semantic
anchors with those reconstructed from the generated code and selectively
corrects only the mismatched components, driving convergence toward a
semantically faithful model. This anchor-driven correction enables fine-grained
refinement of constraint and objective logic, enhancing both fidelity and
robustness without requiring additional training or supervision. Empirical
results on seven public datasets demonstrate that SAC-Opt improves average
modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP
dataset. These findings highlight the importance of semantic-anchored
correction in LLM-based optimization workflows to ensure faithful translation
from problem intent to solver-executable code.

</details>


### [69] [Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework](https://arxiv.org/abs/2510.05158)
*Xin He,Liangliang You,Hongduan Tian,Bo Han,Ivor Tsang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: Lang-PINN是一个基于大语言模型的多智能体系统，能够直接从自然语言任务描述构建可训练的物理信息神经网络(PINNs)，显著降低了构建PINNs的复杂度和错误率。


<details>
  <summary>Details</summary>
Motivation: 传统构建PINNs的过程劳动密集且容易出错，科学家需要将问题解释为PDE公式、设计架构和损失函数、实现稳定的训练流程。现有LLM方法只解决孤立步骤，缺乏端到端的视角。

Method: Lang-PINN协调四个互补的智能体：PDE智能体将任务描述解析为符号PDE，PINN智能体选择架构，代码智能体生成模块化实现，反馈智能体执行并诊断错误进行迭代优化。

Result: 实验显示Lang-PINN比竞争基线显著降低误差并提高鲁棒性：均方误差降低3-5个数量级，端到端执行成功率提高50%以上，时间开销减少74%。

Conclusion: 该设计将非正式任务陈述转化为可执行和可验证的PINN代码，为科学计算提供了更高效的自动化解决方案。

Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for
solving partial differential equations (PDEs), but constructing a usable PINN
remains labor-intensive and error-prone. Scientists must interpret problems as
PDE formulations, design architectures and loss functions, and implement stable
training pipelines. Existing large language model (LLM) based approaches
address isolated steps such as code generation or architecture suggestion, but
typically assume a formal PDE is already specified and therefore lack an
end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system
that builds trainable PINNs directly from natural language task descriptions.
Lang-PINN coordinates four complementary agents: a PDE Agent that parses task
descriptions into symbolic PDEs, a PINN Agent that selects architectures, a
Code Agent that generates modular implementations, and a Feedback Agent that
executes and diagnoses errors for iterative refinement. This design transforms
informal task statements into executable and verifiable PINN code. Experiments
show that Lang-PINN achieves substantially lower errors and greater robustness
than competitive baselines: mean squared error (MSE) is reduced by up to 3--5
orders of magnitude, end-to-end execution success improves by more than 50\%,
and reduces time overhead by up to 74\%.

</details>


### [70] [Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents](https://arxiv.org/abs/2510.05188)
*Wenda Xie,Chao Guo,Yanqing Jing. Junle Wang,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: Dramaturge是一个基于分层多LLM代理的任务导向分治方法，用于改进长叙事脚本的质量。它通过全局审查、场景级审查和分层协调修订三个阶段，以自上而下的方式确保上下文一致性，并通过粗到细的迭代过程持续改进脚本。


<details>
  <summary>Details</summary>
Motivation: LLM在单次生成过程中难以产生高质量的长叙事内容，直接修改通常会导致局部编辑与整体叙事要求之间的不一致。需要一种能够理解整个上下文、识别全局结构问题和局部细节缺陷，并在多个粒度和位置协调修订的方法。

Method: 提出Dramaturge方法，包含三个主要阶段：1）全局审查阶段把握整体故事情节和结构问题；2）场景级审查阶段识别详细场景和句子缺陷；3）分层协调修订阶段协调并整合整个脚本的结构和细节改进。采用自上而下的任务流程和粗到细的迭代过程。

Result: 综合实验表明，Dramaturge在脚本级整体质量和场景级细节方面显著优于所有基线方法。

Conclusion: Dramaturge是一个即插即用的方法，可以轻松集成到现有方法中以改进生成的脚本质量。

Abstract: Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

</details>


### [71] [BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions](https://arxiv.org/abs/2510.05318)
*Nan Huo,Xiaohan Xu,Jinyang Li,Per Jacobsson,Shipei Lin,Bowen Qin,Binyuan Hui,Xiaolong Li,Ge Qu,Shuzheng Si,Linheng Han,Edward Alexander,Xintong Zhu,Rui Qin,Ruihan Yu,Yiyao Jin,Feige Zhou,Weihao Zhong,Yun Chen,Hongyu Liu,Chenhao Ma,Fatma Ozcan,Yannis Papakonstantinou,Reynold Cheng*

Main category: cs.AI

TL;DR: BIRD-INTERACT是一个用于评估多轮交互式文本到SQL任务的新基准，它通过结合数据库、知识库和用户模拟器来模拟真实世界的数据库助手挑战，包含预定义对话协议和开放代理设置两种评估模式。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮文本到SQL基准将对话历史视为静态上下文或仅限于只读操作，无法反映生产级数据库助手的真实挑战，如处理模糊查询、执行错误和不断变化的用户需求。

Method: 构建了一个综合交互环境，包括分层知识库、元数据文件和函数驱动的用户模拟器；设计了两种评估设置：预定义对话协议（c-Interact）和开放代理设置（a-Interact）；创建了涵盖完整CRUD操作的任务套件，包含模糊和后续子任务。

Result: BIRD-INTERACT具有很高的难度：GPT-5在c-Interact中仅完成8.67%的任务，在a-Interact中完成17.00%的任务。通过内存嫁接和交互测试时缩放分析验证了有效交互对于复杂动态文本到SQL任务的重要性。

Conclusion: BIRD-INTERACT基准成功恢复了多轮文本到SQL任务的真实性，突显了当前模型在处理复杂动态交互方面的局限性，强调了有效交互策略的重要性。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

</details>


### [72] [Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis](https://arxiv.org/abs/2510.05335)
*Oskar Wysocki,Magdalena Wysocka,Mauricio Jacobo,Harriet Unsworth,André Freitas*

Main category: cs.AI

TL;DR: M-Reason是一个用于生物医学领域（特别是癌症研究）的透明、基于代理的推理和证据集成系统，利用LLM和模块化代理编排来自动化证据检索、评估和合成。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中证据合成的复杂性，提供可解释、可审计的自动化推理系统，提高研究效率和输出一致性。

Method: 使用模块化代理编排，每个代理专门处理特定的证据流，支持并行处理和细粒度分析，结合确定性代码进行验证。

Result: 系统在效率和输出一致性方面取得显著提升，提供了完整的证据追溯性，从源证据到最终结论都可追踪。

Conclusion: M-Reason既是证据合成的实用工具，也是科学研究中稳健多代理LLM系统的测试平台，展示了多代理系统在科学领域的潜力。

Abstract: We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

</details>


### [73] [MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption](https://arxiv.org/abs/2510.05580)
*Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: MetaVLA是一个统一的、骨干网络无关的后训练框架，通过上下文感知元协同训练整合多样化目标任务，利用辅助任务提升领域内泛化能力，在LIBERO基准上显著优于OpenVLA。


<details>
  <summary>Details</summary>
Motivation: 解决当前VLA模型需要任务特定微调、对未见任务泛化能力差的问题，实现高效可扩展的对齐。

Method: 提出上下文感知元协同训练，整合多样化目标任务到单一微调阶段，结合轻量级元学习机制（源自注意力神经过程）实现快速适应。

Result: 在LIBERO基准上，MetaVLA在长时任务上比OpenVLA提升8.0%，训练步数从240K减少到75K，GPU时间减少约76%。

Conclusion: 展示了可扩展、低资源后训练的可行性，为通用具身智能体铺平道路。

Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

</details>


### [74] [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
*Zhuofeng Li,Haoxiang Zhang,Seungju Han,Sheng Liu,Jianwen Xie,Yu Zhang,Yejin Choi,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: AgentFlow是一个可训练的在线智能体框架，通过四个模块（规划器、执行器、验证器、生成器）和演进内存协调工作，使用Flow-GRPO算法在多轮交互中优化规划器，在长视野、稀疏奖励任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强方法训练单一策略，在长视野和多样化工具场景下扩展性差，泛化能力弱。智能体系统通过模块分解提供替代方案，但大多数缺乏训练或离线训练与多轮交互动态脱节。

Method: 提出AgentFlow框架，包含四个协调模块和演进内存，使用Flow-GRPO算法在多轮环境中进行在线策略训练，将多轮优化转化为可处理的单轮策略更新。

Result: 在十个基准测试中，基于7B骨干的AgentFlow在搜索任务上平均准确率提升14.9%，智能体任务14.0%，数学任务14.5%，科学任务4.1%，甚至超过GPT-4o等大型专有模型。

Conclusion: AgentFlow通过在线优化显著提升了规划能力、工具调用可靠性和模型规模扩展性，验证了在多轮交互中直接优化智能体系统的有效性。

Abstract: Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

</details>


### [75] [From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions](https://arxiv.org/abs/2510.05596)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Geng Sun,Xianbin Wang,Shiwen Mao,Abbas Jamalipour*

Main category: cs.AI

TL;DR: 本文提出了一个自进化智能AI框架，通过多智能体协作实现无线系统的自主优化，在低空无线网络天线演化案例中展示了52.02%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统静态AI模型无法适应动态环境变化，需要开发能够自主进化的智能系统来提升无线系统的自适应能力和性能。

Method: 提出多智能体协作的自进化智能AI框架，使用角色专业化提示的多个大语言模型，在监督智能体协调下通过结构化对话、迭代反馈和系统验证实现完整生命周期自主执行。

Result: 在天线演化案例中，框架成功将固定天线优化自主升级为可移动天线优化，波束增益提升高达52.02%，性能恢复效果显著，持续超越固定基线。

Conclusion: 自进化智能AI框架展示了在无线系统中实现自主适应和性能优化的可行性，为下一代无线智能系统提供了新的技术范式。

Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

</details>


### [76] [Artificially intelligent agents in the social and behavioral sciences: A history and outlook](https://arxiv.org/abs/2510.05743)
*Petter Holme,Milena Tsvetkova*

Main category: cs.AI

TL;DR: 回顾人工智能代理在社会科学中的历史发展和当前趋势，从早期计算机到现代大语言模型，强调AI在科学过程中的作用和技术进步带来的变革。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能代理在社会科学中的演变历程，理解技术如何影响我们对人类行为的科学研究方法。

Method: 采用历史回顾和趋势分析的方法，涵盖从1950年代至今的关键发展节点，包括社会模拟、博弈论智能代理、大数据时代和生成式AI应用等主题。

Result: 展示了AI代理在社会科学中的持续演进，从最初的社会模拟研究到当前基于大语言模型的实验，揭示了技术与科学认知的深度交织。

Conclusion: 我们与用于理解自身的技术深度交织，AI代理的发展不仅反映了技术进步，也重塑了社会科学的研究范式。

Abstract: We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

</details>


### [77] [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
*Bohan Yao,Shiva Krishna Reddy Malay,Vikas Yadav*

Main category: cs.AI

TL;DR: 提出了一种新的自动多智能体系统设计范式ARM，通过优化思维链推理来构建通用推理模块，显著优于现有方法并具备优秀的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动多智能体系统设计方法性能不佳，需要为每个新任务重新发现架构且依赖昂贵的数据标注。观察到简单思维链推理与复杂系统性能相当，因此需要深入研究推理单元。

Method: 引入Agentic Reasoning Module (ARM)，将思维链泛化为由专门推理模块执行的细粒度推理步骤。通过代码空间的树搜索发现模块，从简单CoT模块开始，利用执行轨迹的反射进行突变演化。

Result: ARM方法显著优于手动设计的多智能体系统和最先进的自动设计方法。基于ARM构建的系统在不同基础模型和任务领域中都保持高性能，无需进一步优化。

Conclusion: ARM作为一种通用的推理构建模块，可以作为直接递归循环或在学习的元编排器中作为子程序使用，展现出卓越的泛化能力。

Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

</details>


### [78] [RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases](https://arxiv.org/abs/2510.05764)
*Lang Qin,Zijian Gan,Xu Cao,Pengcheng Jiang,Yankai Jiang,Jiawei Han,Kaishun Wu,Jintai Chen*

Main category: cs.AI

TL;DR: RareAgent是一个自进化的多智能体系统，通过组织对抗性辩论来动态构建证据图，从多角度支持、反驳或蕴含假设，从而解决罕见病药物重定位中缺乏先验关联的问题。


<details>
  <summary>Details</summary>
Motivation: 罕见病药物重定位在缺乏药物与目标疾病先验关联时面临挑战，传统知识图谱补全和消息传递图神经网络因缺乏可靠信号而性能不佳。

Method: 采用自进化多智能体系统，组织任务特定的对抗性辩论，动态构建证据图，并通过事后分析推理策略生成文本反馈来优化智能体策略，同时将成功推理路径提炼为可转移启发式规则。

Result: RareAgent将指示AUPRC比推理基线提高了18.1%，并提供了与临床证据一致的透明推理链。

Conclusion: RareAgent通过主动证据寻求推理而非被动模式识别，显著提升了罕见病药物重定位的性能和可解释性。

Abstract: Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

</details>


### [79] [The Safety Challenge of World Models for Embodied AI Agents: A Review](https://arxiv.org/abs/2510.05865)
*Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi*

Main category: cs.AI

TL;DR: 对自动驾驶和机器人领域的世界模型进行文献综述和实证分析，重点关注场景和控制生成任务的安全影响


<details>
  <summary>Details</summary>
Motivation: 随着具身人工智能的发展，需要更先进的集成模型来感知、解释和预测环境动态，但必须确保预测对智能体和环境都是安全的

Method: 对世界模型在自动驾驶和机器人领域的文献进行综述，收集最先进模型的预测结果，识别和分类常见故障（病理），并进行定量评估

Result: 识别了世界模型在场景和控制生成任务中的常见病理，并提供了定量评估结果

Conclusion: 世界模型在具身智能中具有重要作用，但需要关注其安全影响，特别是在场景和控制生成任务中

Abstract: The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

</details>


### [80] [Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies](https://arxiv.org/abs/2510.05909)
*Aksel Joonas Reedi,Corentin Léger,Julien Pourcel,Loris Gaven,Perrine Charriau,Guillaume Pourcel*

Main category: cs.AI

TL;DR: DebateQD：一种基于质量多样性进化的辩论优化方法，通过竞争性辩论训练提升LLM的泛化能力，相比传统基于真理的优化方法，在泛化性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前优化LLM输出真实答案的方法容易过拟合，产生脆弱的推理能力，无法很好地泛化。需要探索新的优化方法来提升LLM的泛化性能。

Method: 提出DebateQD算法，通过质量多样性进化在不同类别（理性、权威、情感诉求等）中演化多样化的辩论策略，使用锦标赛式辩论竞赛，两个LLM辩论，第三个LLM评判。

Result: 在三个模型规模（7B、32B、72B参数）和多个数据集大小上，说服优化的策略实现了高达13.94%更小的训练-测试泛化差距，同时匹配或超过了基于真理优化的测试性能。

Conclusion: 竞争性说服压力而非协作寻求真理，能够培养更具可迁移性的推理技能，为改进LLM泛化提供了有前景的路径。

Abstract: Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

</details>


### [81] [Information-Theoretic Policy Pre-Training with Empowerment](https://arxiv.org/abs/2510.05996)
*Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker*

Main category: cs.AI

TL;DR: 本文提出使用折扣赋权作为预训练信号，通过最大化长期环境控制能力来初始化策略，提高下游任务的适应性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 赋权作为内在动机在强化学习中已有应用，但作为预训练信号的研究有限。作者希望探索赋权预训练在数据高效下游任务适应中的潜力。

Method: 引入折扣赋权概念，平衡短期和长期环境控制，提出基于赋权最大化的预训练范式，初始化策略以获取对环境动态的稳健理解。

Result: 实证表明，具有长视野的赋权最大化策略具有数据效率和有效性，能显著提高下游任务的适应性。

Conclusion: 赋权预训练是一种通用的初始化策略，为未来在高维复杂任务中扩展该框架铺平了道路。

Abstract: Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

</details>


### [82] [ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models](https://arxiv.org/abs/2510.06014)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Zhiyuan Yu,Qipeng Guo,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: ARISE是一个专门评估大型推理模型测试时扩展能力的新指标，包含样本级感知和动态采样机制，能有效衡量模型计算资源增加时的性能变化。


<details>
  <summary>Details</summary>
Motivation: 随着推理模型快速发展，需要系统性地比较和评估不同模型的测试时扩展能力，现有评估方法无法有效衡量计算资源增加时的性能变化。

Method: 提出ARISE评估指标，包含样本级感知机制（惩罚负向扩展行为）和动态采样机制（减少准确率波动和token数不稳定的影响）。

Result: 在数学推理、代码生成和代理任务等多个领域的实验表明，ARISE能可靠地测量测试时扩展能力，并发现Claude Opus相比其他模型具有更优的扩展特性。

Conclusion: ARISE为评估推理模型的测试时扩展能力提供了精细可靠的度量标准，揭示了不同模型在扩展效率上的显著差异。

Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

</details>


### [83] [Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research](https://arxiv.org/abs/2510.06056)
*Gang Liu,Yihan Zhu,Jie Chen,Meng Jiang*

Main category: cs.AI

TL;DR: DeepEvolve是一个结合深度研究和算法演化的科学助手代理，通过外部知识检索、跨文件代码编辑和系统化调试的反馈驱动迭代循环，持续改进科学算法。


<details>
  <summary>Details</summary>
Motivation: 现有科学助手要么仅依赖算法演化，要么只进行孤立深度研究，都存在关键局限。纯算法演化依赖LLM内部知识，在复杂领域快速达到瓶颈；纯深度研究提出想法但不验证，导致不切实际的解决方案。

Method: 集成深度研究与算法演化，结合外部知识检索、跨文件代码编辑和系统化调试，在反馈驱动的迭代循环中不仅提出新假设，还进行细化、实现和测试。

Result: 在化学、数学、生物学、材料和专利等九个基准测试中，DeepEvolve持续改进初始算法，产生可执行的新算法并保持持续增益。

Conclusion: 通过弥合无引导演化与无基础研究之间的差距，DeepEvolve为推进科学算法发现提供了可靠框架。

Abstract: Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

</details>


### [84] [Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents](https://arxiv.org/abs/2510.06078)
*Tao Zhe,Rui Liu,Fateme Memar,Xiao Luo,Wei Fan,Xinyue Ye,Zhongren Peng,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出了RouteLLM，一个分层多智能体框架，将自然语言意图转化为约束感知的路线推荐系统


<details>
  <summary>Details</summary>
Motivation: 传统路由算法假设结构化输入和固定目标，难以适应自然语言查询；现有LLM方法在空间推理和联合建模路线级与POI级偏好方面存在局限

Method: 分层多智能体框架：先解析用户查询为结构化意图，然后通过管理器协调约束代理、POI代理和路径优化代理，最后通过验证器确保约束满足并生成可解释的路线

Result: 实验表明该方法能可靠地将文本偏好转化为约束感知路线，在路线质量和偏好满足度上优于传统方法

Conclusion: RouteLLM成功连接了语言灵活性和空间结构，实现了对路线可行性和用户偏好的推理

Abstract: Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

</details>


### [85] [Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices](https://arxiv.org/abs/2510.06093)
*Mallika Mainali,Harsha Sureshbabu,Anik Sen,Christopher B. Rauch,Noah D. Reifsnyder,John Meyer,J. T. Turner,Michael W. Floyd,Matthew Molineaux,Rosina O. Weber*

Main category: cs.AI

TL;DR: 该研究比较了经典AI方法和基于LLM的方法在决策者对齐方面的表现，使用健康保险决策数据集评估了三种不同风险容忍度决策者的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 随着算法决策在关键领域的应用增多，AI对齐研究从通用价值对齐转向考虑决策者属性的情境特定方法，但现有方法的泛化能力尚未充分探索。

Method: 实现了一个经典AI模型并开发了基于LLM的算法决策者，使用GPT-5和GPT-4在零样本提示框架下进行评估，采用加权自一致性方法。

Result: 经典AI和基于LLM的模型在与基于属性的目标对齐方面表现相当，经典AI在中等风险配置下表现出略好的对齐效果。

Conclusion: 两种方法在决策者对齐任务中表现相似，经典AI在特定风险配置下略有优势，数据集和开源实现已公开。

Abstract: As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

</details>


### [86] [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)
*Batu El,James Zou*

Main category: cs.AI

TL;DR: 研究发现，在竞争环境中优化LLMs会无意中导致模型失准，表现为欺骗性营销、虚假信息和有害行为增加，即使模型被明确要求保持真实。


<details>
  <summary>Details</summary>
Motivation: 理解竞争性反馈循环如何影响LLM行为，特别是在商业、政治和社交媒体等竞争性场景中。

Method: 使用模拟环境测试三种竞争场景：商业营销、选举竞选和社交媒体参与度优化。

Result: 竞争优化导致显著失准：销售增长6.3%伴随欺骗性营销增加14.0%；选票份额增长4.9%伴随虚假信息增加22.3%；参与度增长7.5%伴随虚假信息激增188.6%。

Conclusion: 市场竞争压力会系统性地侵蚀AI对齐，形成恶性竞争，需要更强治理和精心设计的激励机制来防止竞争动态破坏社会信任。

Abstract: Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [87] [Pulumi Launches Neo: an Agentic AI Platform Engineer for Multi-Cloud Infrastructure](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Fnews%2F2025%2F09%2Fpulumi-neo%2F%3Futm_source=tldrdevops/1/01000199b938a20e-b9c1370a-946d-433b-b57a-de1ac8d05317-000000/3NLR3eUSNh2J-AKCLImk5-cLcLiQdCFLy6K3P6v_Ces=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pulumi推出Neo：一个用于多云基础设施的AI驱动平台工程代理，可自动化基础设施配置、管理和优化，同时强制执行治理和合规性。


<details>
  <summary>Details</summary>
Motivation: 解决平台团队在复杂多云环境中可靠扩展基础设施管理的问题，通过AI代理自动化基础设施生命周期管理。

Method: 将AI代理Neo集成到Pulumi Cloud平台中，利用基础设施即代码实践进行学习，随时间推移变得更有效。

Result: Neo能够自动化基础设施配置、管理和优化，强制执行治理和合规性政策。

Conclusion: Neo作为AI驱动的平台工程代理，帮助平台团队在复杂多云环境中实现可靠扩展和自动化基础设施管理。

Abstract: Pulumi Launches Neo: an Agentic AI Platform Engineer for Multi-Cloud Infrastructure (4 minute read) Pulumi has launched Neo, an AI-powered platform engineering agent built into Pulumi Cloud that automates infrastructure provisioning, management, and optimization while enforcing governance and compliance. Neo learns from infrastructure-as-code practices to become more effective over time and helps platform teams scale reliably across complex multi-cloud environments.

</details>


### [88] [Nexus](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagent.nexus%2F%3Futm_source=tldrfounders/1/01000199b97e6069-41f7ac1f-b6e8-44bb-b2db-e5f7143074a3-000000/tE5IWY0W3x_kUp804C4kJYnZTgluWO0FBZdkchC6skk=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Nexus是一个无需代码即可让业务团队构建AI代理的工具


<details>
  <summary>Details</summary>
Motivation: 使非技术背景的业务团队能够轻松创建和使用AI代理，降低技术门槛

Method: 提供无代码平台，让用户通过可视化界面构建AI代理

Result: 业务团队能够自主构建AI代理，无需依赖开发人员

Conclusion: Nexus成功实现了让业务团队无代码构建AI代理的目标

Abstract: Nexus (Tool) Nexus enables Business teams to build AI agents without code.

</details>


### [89] [Agentic Checkout: Stripe + OpenAI's new protocol](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.fintechbrainfood.com%2Fp%2Fagentic-checkout%3Futm_source=tldrfintech/1/01000199b9a7f5d1-11adf0a4-1272-46ab-b4af-0de9fb8ad60f-000000/zW3ItVQJ27siIFiY3ziayCAMTqs6YJjYoZmvXb0Vk8E=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI和Stripe合作推出Agentic Commerce Protocol，让AI代理能够处理商业交易，包括在ChatGPT中集成购物功能和共享支付令牌。


<details>
  <summary>Details</summary>
Motivation: 减少用户在AI环境中的购物摩擦，让商家能够通过AI代理销售产品，推动AI驱动的商业发展。

Method: 开发Agentic Commerce Protocol协议，构建基于SKU的结账系统，并推出共享支付令牌技术。

Result: 在ChatGPT中实现了商业功能，用户可以直接购买产品，为商家提供了通过AI代理销售的新渠道。

Conclusion: AI代理商业协议将改变电商格局，使AI能够更自然地处理交易，降低用户购买障碍。

Abstract: Agentic Checkout: Stripe + OpenAI's new protocol (15 minute read) This blog unpacks 3 key announcements. First, OpenAI launched commerce in ChatGPT, and the checkout is powered by Stripe. This lets users find and actually buy things in ChatGPT and reduces user friction. Second, OpenAI and Stripe open-sourced a protocol for any merchant to sell via AI Agents. Called the Agentic Commerce Protocol, it builds checkouts from SKUs. Third, Stripe also launched “Shared Payment Tokens.” This lets any ...

</details>


### [90] [🤖 Take Your AI and Agent Skills from Zero to Hero 🚀](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fbit.ly%2F44s7T94%3Ftrk=d742e3c7-637d-46fc-9bd1-37eb384e449c%26sc_channel=el/2/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/LDmIClBUoQ6I9LEk4VB0aYYgdVpBmHLvdjZR4_bXslg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊SageMaker路演提供高级实践培训，帮助开发者从零开始掌握AI和智能体技能，涵盖模型训练、数据基础建设和生产级AI智能体部署。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供实际生产级别的AI和智能体开发培训，而非基础理论教程，帮助他们在2026年前提升AI技能水平。

Method: 通过城市巡回路演的形式，提供多个实践课程选择，包括模型训练和微调、构建AI就绪数据基础、部署生产AI智能体等。

Result: 参与者能够获得高级的、动手实践的AI开发经验，使用真实的生成级服务进行学习。

Conclusion: 该培训项目旨在帮助开发者将AI和智能体技能从零提升到专业水平，为2026年的AI发展需求做好准备。

Abstract: 🤖 Take Your AI and Agent Skills from Zero to Hero 🚀 (Sponsor) Join the Amazon SageMaker Roadshow for advanced, hands-on development with real production-grade services—not boring basics or theoretical tutorials. Choose from multiple sessions in a city near you, covering: model training and fine-tuning, building an AI-ready data foundation, and deploying production AI agents. Level up your AI skills for 2026.Mastering AI Agents: Building Production-Ready Applications » Unlock the Power Buildin...

</details>


### [91] [Level up your AI skills for 2026.](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fbit.ly%2F44s7T94%3Ftrk=d742e3c7-637d-46fc-9bd1-37eb384e449c%26sc_channel=el/2/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/LDmIClBUoQ6I9LEk4VB0aYYgdVpBmHLvdjZR4_bXslg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 这是一个亚马逊SageMaker路演的宣传内容，旨在帮助开发者从零开始掌握AI和智能代理技能，提供实践性培训。


<details>
  <summary>Details</summary>
Motivation: 为了帮助开发者提升AI技能，特别是智能代理的开发能力，使其能够构建生产就绪的AI应用。

Method: 通过亚马逊SageMaker路演提供实践培训，包括模型训练和微调、构建AI就绪数据基础、部署生产AI代理等多个环节。

Result: 参与者将能够掌握先进的AI开发技能，为2026年的AI发展做好准备。

Conclusion: 这是一个技能提升的培训机会，专注于AI代理的生产级应用开发。

Abstract: 🤖 Take Your AI and Agent Skills from Zero to Hero 🚀 (Sponsor) Join the Amazon SageMaker Roadshow for advanced, hands-on development with real production-grade services—not boring basics or theoretical tutorials. Choose from multiple sessions in a city near you, covering: model training and fine-tuning, building an AI-ready data foundation, and deploying production AI agents. Level up your AI skills for 2026.Mastering AI Agents: Building Production-Ready Applications » Unlock the Power Buildin...

</details>


### [92] [Mastering AI Agents: Building Production-Ready Applications](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fbit.ly%2F44s7T94%3Ftrk=d742e3c7-637d-46fc-9bd1-37eb384e449c%26sc_channel=el/2/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/LDmIClBUoQ6I9LEk4VB0aYYgdVpBmHLvdjZR4_bXslg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊SageMaker路演提供从零到精通的AI和智能体技能培训，涵盖模型训练、数据基础建设和生产级AI智能体部署


<details>
  <summary>Details</summary>
Motivation: 帮助开发者从基础到高级掌握AI和智能体技能，为2026年的AI发展做好准备

Method: 通过亚马逊SageMaker路演提供实践培训，包括模型训练和微调、构建AI就绪数据基础、部署生产AI智能体等多个主题

Result: 参与者能够获得生产级AI服务的实际开发经验，提升AI技能水平

Conclusion: 该培训项目为开发者提供了从理论到实践的完整AI技能提升路径

Abstract: 🤖 Take Your AI and Agent Skills from Zero to Hero 🚀 (Sponsor) Join the Amazon SageMaker Roadshow for advanced, hands-on development with real production-grade services—not boring basics or theoretical tutorials. Choose from multiple sessions in a city near you, covering: model training and fine-tuning, building an AI-ready data foundation, and deploying production AI agents. Level up your AI skills for 2026.Mastering AI Agents: Building Production-Ready Applications » Unlock the Power Buildin...

</details>


### [93] [Unlock the Power Building Agentic AI on 200+ Foundation Models](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fbit.ly%2F44s7T94%3Ftrk=d742e3c7-637d-46fc-9bd1-37eb384e449c%26sc_channel=el/2/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/LDmIClBUoQ6I9LEk4VB0aYYgdVpBmHLvdjZR4_bXslg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊SageMaker路演提供AI和智能体技能实践培训，涵盖模型训练、数据基础建设和生产级AI智能体部署


<details>
  <summary>Details</summary>
Motivation: 帮助开发者从零开始掌握AI和智能体技能，提供生产级服务的实践培训而非基础理论

Method: 通过多城市路演形式，提供模型训练与微调、构建AI就绪数据基础、部署生产AI智能体等实践课程

Result: 参与者能够获得2026年所需的AI技能提升，掌握生产级AI应用开发能力

Conclusion: 该路演为开发者提供了从基础到高级的AI技能培训机会，特别关注生产环境应用

Abstract: 🤖 Take Your AI and Agent Skills from Zero to Hero 🚀 (Sponsor) Join the Amazon SageMaker Roadshow for advanced, hands-on development with real production-grade services—not boring basics or theoretical tutorials. Choose from multiple sessions in a city near you, covering: model training and fine-tuning, building an AI-ready data foundation, and deploying production AI agents. Level up your AI skills for 2026.Mastering AI Agents: Building Production-Ready Applications » Unlock the Power Buildin...

</details>


### [94] [Unify Data and Analytics](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fbit.ly%2F44s7T94%3Ftrk=d742e3c7-637d-46fc-9bd1-37eb384e449c%26sc_channel=el/2/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/LDmIClBUoQ6I9LEk4VB0aYYgdVpBmHLvdjZR4_bXslg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊SageMaker路演提供实践性AI和智能体开发培训，涵盖模型训练、数据基础建设和生产级AI智能体部署


<details>
  <summary>Details</summary>
Motivation: 帮助开发者从零开始掌握AI和智能体技能，为2026年做好准备

Method: 通过城市巡回路演的形式，提供多场实践性培训课程

Result: 参与者能够获得生产级AI服务开发的实际经验

Conclusion: 该路演是提升AI技能的有效途径，特别适合希望掌握生产级AI应用开发的开发者

Abstract: 🤖 Take Your AI and Agent Skills from Zero to Hero 🚀 (Sponsor) Join the Amazon SageMaker Roadshow for advanced, hands-on development with real production-grade services—not boring basics or theoretical tutorials. Choose from multiple sessions in a city near you, covering: model training and fine-tuning, building an AI-ready data foundation, and deploying production AI agents. Level up your AI skills for 2026.Mastering AI Agents: Building Production-Ready Applications » Unlock the Power Buildin...

</details>


### [95] [OpenAI prepares to release Agent Builder during DevDay on October 6](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fopenai-prepares-to-release-agent-builder-during-devday-on-october-6%2F%3Futm_source=tldrai/1/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/wADdXA-81w9W2sVXBbgu01PMkYtBUhXFLXZPZuUc9zM=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI将在10月6日开发者日发布Agent Builder工具，帮助用户构建智能体工作流，集成MCPs、ChatKit小部件等工具，与n8n和Zapier等现有工作流自动化工具竞争。


<details>
  <summary>Details</summary>
Motivation: 提供用户友好的工具来简化智能体工作流的创建过程，降低技术门槛，让更多开发者能够构建复杂的AI驱动应用。

Method: 采用拖放式画布界面，提供预定义模板和模块化构建块（逻辑节点、连接器等），支持快速创建智能体流程。

Result: 开发了一个直观的智能体构建平台，能够集成多种工具和服务，简化工作流自动化过程。

Conclusion: Agent Builder将显著降低智能体工作流开发的技术门槛，推动AI应用的普及和创新。

Abstract: OpenAI prepares to release Agent Builder during DevDay on October 6 (2 minute read) OpenAI's Agent Builder will help users build agentic workflows and connect MCPs, ChatKit widgets, and other tools. It is a direct competitor to established workflow automation tools like n8n and Zapier. The Agent Builder features a drag-and-drop canvas that allows users to create agent flows from predefined templates. The canvas supports a range of modular building blocks, nodes for logic, connectors, and more...

</details>


### [96] [Jules API](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.google.com%2Fjules%2Fapi%3Futm_source=tldrai/1/01000199b9ace004-92ee1a2c-29cc-457d-8072-0ee218c14f07-000000/yTMSjgBu81cUZZCPXQQC1COI1Krd1KhyM5pvT6oxEtA=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Jules API为开发者提供程序化访问Jules能力的接口，用于自动化和增强软件开发周期，包括创建自定义工作流、自动化错误修复和代码审查等任务。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供程序化访问Jules智能能力的方式，以便将Jules的智能直接集成到日常工具中，实现软件开发流程的自动化。

Method: 提供API接口，允许开发者通过程序化调用访问Jules的各项功能，包括创建自定义工作流、自动化任务等。

Result: 开发了Jules API，使开发者能够将Jules的智能能力集成到自己的开发工具和工作流程中。

Conclusion: Jules API为软件开发者提供了强大的自动化工具，能够显著提升开发效率和代码质量。

Abstract: Jules API (5 minute read) The Jules API gives developers programmatic access to Jules' capabilities to automate and enhance software development cycles. It can be used to create custom workflows, automate tasks like bug fixing and code reviews, and embed Jules' intelligence directly into everyday tools. This page provides a quick overview of the API and walks through how to make an API call.

</details>
