{"id": "2509.25192", "categories": ["cs.SE", "68T07"], "pdf": "https://arxiv.org/pdf/2509.25192", "abs": "https://arxiv.org/abs/2509.25192", "authors": ["Anderson de Lima Luiz"], "title": "WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis", "comment": "5 pages, 2 figures", "summary": "Compilation errors represent a significant bottleneck in software development\nproductivity. This paper introduces WARP (Web-Augmented Real-time Program\nRepairer), a novel system that leverages Large Language Models (LLMs) and\ndynamic web-augmented synthesis for real-time resolution of these errors. WARP\nactively monitors developer terminals, intelligently detects compilation\nerrors, and synergistically combines the understanding of a fine-tuned Code-LLM\nwith relevant solutions, explanations, and code snippets retrieved from\nup-to-date web sources like developer forums and official documentation.\nExperimental results on our curated benchmark, CGP (featuring C/C++, Python,\nand Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles\ncorrectly) and higher semantic correctness compared to baseline LLM-only\napproaches and traditional IDE quick-fixes. Key technical challenges in\nachieving high-accuracy synthesis from noisy web data.", "AI": {"tldr": "WARP\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u6001\u7f51\u7edc\u589e\u5f3a\u5408\u6210\u6280\u672f\u5b9e\u65f6\u4fee\u590d\u7f16\u8bd1\u9519\u8bef\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u63a7\u5f00\u53d1\u8005\u7ec8\u7aef\u3001\u68c0\u6d4b\u7f16\u8bd1\u9519\u8bef\uff0c\u5e76\u7ed3\u5408\u5fae\u8c03\u4ee3\u7801LLM\u4e0e\u7f51\u7edc\u8d44\u6e90\u6765\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7f16\u8bd1\u9519\u8bef\u4e25\u91cd\u5f71\u54cd\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u4fee\u590d\u65b9\u6848\u6765\u63d0\u5347\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\u3002", "method": "WARP\u7cfb\u7edf\u4e3b\u52a8\u76d1\u63a7\u5f00\u53d1\u8005\u7ec8\u7aef\uff0c\u667a\u80fd\u68c0\u6d4b\u7f16\u8bd1\u9519\u8bef\uff0c\u5c06\u5fae\u8c03\u7684\u4ee3\u7801LLM\u7406\u89e3\u4e0e\u4ece\u5f00\u53d1\u8005\u8bba\u575b\u548c\u5b98\u65b9\u6587\u6863\u7b49\u7f51\u7edc\u8d44\u6e90\u68c0\u7d22\u7684\u76f8\u5173\u89e3\u51b3\u65b9\u6848\u3001\u89e3\u91ca\u548c\u4ee3\u7801\u7247\u6bb5\u76f8\u7ed3\u5408\u3002", "result": "\u5728CGP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWARP\u5b9e\u73b0\u4e8672.5%\u7684\u6b63\u786e\u7f16\u8bd1\u4fee\u590d\u7387\uff0c\u5728\u8bed\u4e49\u6b63\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebfLLM\u65b9\u6cd5\u548c\u4f20\u7edfIDE\u5feb\u901f\u4fee\u590d\u3002", "conclusion": "WARP\u901a\u8fc7\u7ed3\u5408LLM\u548c\u7f51\u7edc\u589e\u5f3a\u5408\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u8bd1\u9519\u8bef\u4fee\u590d\u95ee\u9898\uff0c\u4f46\u5904\u7406\u566a\u58f0\u7f51\u7edc\u6570\u636e\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5408\u6210\u4ecd\u9762\u4e34\u6280\u672f\u6311\u6218\u3002", "topic": "swe application"}}
{"id": "2509.25193", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25193", "abs": "https://arxiv.org/abs/2509.25193", "authors": ["Abhinav Rastogi", "Adam Yang", "Albert Q. Jiang", "Alexander H. Liu", "Alexandre Sablayrolles", "Am\u00e9lie H\u00e9liou", "Am\u00e9lie Martin", "Anmol Agarwal", "Andy Ehrenberg", "Andy Lo", "Antoine Roux", "Arthur Darcet", "Arthur Mensch", "Baptiste Bout", "Baptiste Rozi\u00e8re", "Baudouin De Monicault", "Chris Bamford", "Christian Wallenwein", "Christophe Renaudin", "Cl\u00e9mence Lanfranchi", "Cl\u00e9ment Denoix", "Corentin Barreau", "Darius Dabert Devon Mizelle", "Diego de las Casas", "Elliot Chane-Sane", "Emilien Fugier", "Emma Bou Hanna", "Gabrielle Berrada", "Gauthier Delerce", "Gauthier Guinet", "Georgii Novikov", "Graham Neubig", "Guillaume Lample", "Guillaume Martin", "Himanshu Jaju", "Jan Ludziejewski", "Jason Rute", "Jean-Malo Delignon", "JeanHadrien Chabran", "Joachim Studnia", "Joep Barmentlo", "Jonas Amar", "Josselin Somerville Roberts", "Julien Denize", "Karan Saxena", "Karmesh Yadav", "Kartik Khandelwal", "Khyathi Raghavi Chandu", "Kush Jain", "L\u00e9lio Renard Lavaud", "L\u00e9onard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Marie Pellat", "Mathilde Guillaumin", "Mathis Felardos", "Matthieu Dinot", "Maxime Darrin", "Maximilian Augustin", "Micka\u00ebl Seznec", "Neha Gupta", "Nikhil Raghuraman", "Olivier Duchenne", "Patricia Wang", "Patrick von Platen", "Patryk Saffer", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philom\u00e8ne Chagniot", "Pierre Stock", "Pravesh Agrawal", "R\u00e9mi Delacourt", "Roman Soletskyi", "Romain Sauvestre", "Sagar Vaze", "Sanchit Gandhi", "Sandeep Subramanian", "Shashwat Dalal", "Siddharth Gandhi", "Soham Ghosh", "Srijan Mishra", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Thibaut Lavril", "Thibault Schueller", "Thomas Foubert", "Thomas Robert", "Thomas Wang", "Timoth\u00e9e Lacroix", "Tom Bewley", "Valeriia Nemychnikova", "Victor Paltz", "Virgile Richard", "Wen-Ding Li", "William Marshall", "Xingyao Wang", "Xuanyu Zhang", "Yihan Wan", "Yunhao Tang"], "title": "Devstral: Fine-tuning Language Models for Coding Agent Applications", "comment": null, "summary": "We introduce Devstral-Small, a lightweight open source model for code agents\nwith the best performance among models below 100B size. In this technical\nreport, we give an overview of how we design and develop a model and craft\nspecializations in agentic software development. The resulting model,\nDevstral-Small is a small 24B model, fast and easy to serve. Despite its size,\nDevstral-Small still attains competitive performance compared to models more\nthan an order of magnitude larger.", "AI": {"tldr": "Devstral-Small\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u4ee3\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u5728100B\u89c4\u6a21\u4ee5\u4e0b\u7684\u6a21\u578b\u4e2d\u6027\u80fd\u6700\u4f73\uff0c24B\u5927\u5c0f\u4f46\u6027\u80fd\u53ef\u4e0e\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u6a21\u578b\u7ade\u4e89", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u6027\u80fd\u4f18\u5f02\u7684\u4ee3\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u5176\u6613\u4e8e\u90e8\u7f72\u548c\u63d0\u4f9b\u670d\u52a1", "method": "\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u4ee3\u7406\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u4e13\u4e1a\u5316\u5b9a\u5236", "result": "Devstral-Small\u572824B\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "Devstral-Small\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1\u4e2d\u4e5f\u80fd\u8fbe\u5230\u4f18\u79c0\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2509.25194", "categories": ["cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.25194", "abs": "https://arxiv.org/abs/2509.25194", "authors": ["Haoyang Wu", "Xinxin Zhang", "Lailai Zhu"], "title": "Automated Code Development for PDE Solvers Using Large Language Models", "comment": null, "summary": "Foundation models -- large language models (LLMs) in particular -- have\nbecome ubiquitous, shaping daily life and driving breakthroughs across science,\nengineering, and technology. Harnessing their broad cross-domain knowledge,\ntext-processing, and reasoning abilities for software development, e.g.,\nnumerical libraries for solving partial differential equations (PDEs), is\ntherefore attracting growing interest. Yet existing studies mainly automate\ncase setup and execution for end users. We introduce LLM-PDEveloper, a\nzero-shot, multi-agent LLM framework that automates code development for PDE\nlibraries, specifically targeting secondary developers. By translating\nmathematical and algorithmic descriptions directly into source code,\nLLM-PDEveloper generates new solvers/modules and adapts existing ones. This\nend-to-end math-to-code approach enables a self-augmenting pipeline that\ncontinuously expands the codebase of a library, extends its capacities, and\nbroadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a\nsolver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an\nexisting solver to incorporate additional terms, achieving moderate success\nrates. Failures due to syntactic errors made by LLMs are analyzed and we\npropose effective fixes. We also identify the mechanisms underlying certain\nsemantic errors, guiding future research.", "AI": {"tldr": "LLM-PDEveloper\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u4e13\u95e8\u4e3a\u504f\u5fae\u5206\u65b9\u7a0b(PDE)\u5e93\u7684\u4e8c\u6b21\u5f00\u53d1\u8005\u81ea\u52a8\u5316\u4ee3\u7801\u5f00\u53d1\uff0c\u5c06\u6570\u5b66\u548c\u7b97\u6cd5\u63cf\u8ff0\u76f4\u63a5\u8f6c\u6362\u4e3a\u6e90\u4ee3\u7801\u3002", "motivation": "\u5229\u7528\u57fa\u7840\u6a21\u578b(\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b)\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u3001\u6587\u672c\u5904\u7406\u548c\u63a8\u7406\u80fd\u529b\u6765\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u504f\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u5e93\u7684\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u6570\u5b66\u5230\u4ee3\u7801\u65b9\u6cd5\uff0c\u751f\u6210\u65b0\u7684\u6c42\u89e3\u5668/\u6a21\u5757\u5e76\u9002\u914d\u73b0\u6709\u4ee3\u7801\uff0c\u5b9e\u73b0\u81ea\u589e\u5f3a\u7684\u4ee3\u7801\u5e93\u6269\u5c55\u7ba1\u9053\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff1a1)\u4e3a\u65b0PDE\u6784\u5efa\u6c42\u89e3\u5668\uff0c2)\u4e3a\u7ed9\u5b9aPDE\u5b9e\u73b0\u65b0\u8fb9\u754c\u6761\u4ef6\uff0c3)\u4fee\u6539\u73b0\u6709\u6c42\u89e3\u5668\u4ee5\u5305\u542b\u9644\u52a0\u9879\uff0c\u53d6\u5f97\u4e86\u4e2d\u7b49\u6210\u529f\u7387\u3002", "conclusion": "\u5206\u6790\u4e86LLM\u4ea7\u751f\u7684\u8bed\u6cd5\u9519\u8bef\u5e76\u63d0\u51fa\u6709\u6548\u4fee\u590d\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u67d0\u4e9b\u8bed\u4e49\u9519\u8bef\u7684\u673a\u5236\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "code agent"}}
{"id": "2509.25196", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.25196", "abs": "https://arxiv.org/abs/2509.25196", "authors": ["Hua Zhong", "Shan Jiang", "Sarfraz Khurshid"], "title": "APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning", "comment": null, "summary": "APIs are central to modern software development, yet composing new APIs from\nlarge libraries is difficult due to the exponential search space; traditional\ncomponent-based synthesis relies on costly exploration and hand-crafted\nspecifications. While large language models (LLMs) can generate implementations\nfrom natural language, hallucinations and limited access to up-to-date\ncontextual information often yield incorrect code. In this paper, we present\nAPRIL, an approach that combines LLM-based synthesis with Automatic Prompt\nOptimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):\nAPO iteratively refines prompts for a frozen model, while RLVR fine-tunes the\npolicy toward functional correctness, producing an efficient synthesis\npipeline. Evaluated on 81 real-world APIs from widely used scientific Python\nlibraries and benchmarked against instruction-tuned but unfine-tuned LLMs\nguided by expert prompts, APRIL achieves substantial improvements. These\nresults indicate that integrating APO and RLVR provides a robust, scalable path\nfor component-based API synthesis in large libraries.", "AI": {"tldr": "APRIL\u7ed3\u5408\u81ea\u52a8\u63d0\u793a\u4f18\u5316(APO)\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728API\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ec4\u4ef6\u5408\u6210\u548c\u7eafLLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2dAPI\u5408\u6210\u9762\u4e34\u6307\u6570\u7ea7\u641c\u7d22\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u63a2\u7d22\u548c\u624b\u5de5\u89c4\u8303\uff0c\u800c\u7eafLLM\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e14\u96be\u4ee5\u83b7\u53d6\u6700\u65b0\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u4ee3\u7801\u4e0d\u6b63\u786e\u3002", "method": "APRIL\u7ed3\u5408LLM\u5408\u6210\u3001\u81ea\u52a8\u63d0\u793a\u4f18\u5316(APO)\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\uff1aAPO\u8fed\u4ee3\u4f18\u5316\u51bb\u7ed3\u6a21\u578b\u7684\u63d0\u793a\uff0cRLVR\u9488\u5bf9\u529f\u80fd\u6b63\u786e\u6027\u5fae\u8c03\u7b56\u7565\uff0c\u5f62\u6210\u9ad8\u6548\u7684\u5408\u6210\u6d41\u6c34\u7ebf\u3002", "result": "\u572881\u4e2a\u771f\u5b9e\u4e16\u754cAPI\u548c\u5e7f\u6cdb\u4f7f\u7528\u7684\u79d1\u5b66Python\u5e93\u4e0a\u8bc4\u4f30\uff0c\u4e0e\u57fa\u4e8e\u4e13\u5bb6\u63d0\u793a\u7684\u6307\u4ee4\u8c03\u4f18\u4f46\u672a\u5fae\u8c03\u7684LLM\u76f8\u6bd4\uff0cAPRIL\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u96c6\u6210APO\u548cRLVR\u4e3a\u5927\u578b\u5e93\u4e2d\u7684\u7ec4\u4ef6\u5f0fAPI\u5408\u6210\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "code agent"}}
{"id": "2509.25239", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25239", "abs": "https://arxiv.org/abs/2509.25239", "authors": ["Kevin Xu", "Issei Sato"], "title": "A Formal Comparison Between Chain-of-Thought and Latent Thought", "comment": null, "summary": "Chain-of-Thought (CoT) elicits reasoning in large language models by\nexplicitly generating intermediate steps in natural language. In contrast,\nLatent Thought in looped models operates directly in the continuous latent\nspace, enabling computation beyond discrete linguistic representations. While\nboth approaches exploit iterative computation, their comparative capabilities\nremain underexplored. In this work, we present a formal analysis showing that\nLatent Thought in Looped Transformers enables parallel computation, which is\nmore efficient than the inherently sequential process of CoT. In contrast, CoT\nleverages stochastic decoding to approximate solutions to problems where exact\ncomputation is intractable. These separations suggest the tasks for which\ndepth-driven recursion is more suitable, thereby offering practical guidance\nfor choosing between reasoning paradigms. Code is available at\nhttps://github.com/kevin671/cot-vs-loop.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u6f5c\u5728\u601d\u7ef4\uff08Latent Thought\uff09\u4e24\u79cd\u63a8\u7406\u8303\u5f0f\uff0c\u53d1\u73b0\u6f5c\u5728\u601d\u7ef4\u652f\u6301\u5e76\u884c\u8ba1\u7b97\u66f4\u9ad8\u6548\uff0c\u800c\u601d\u7ef4\u94fe\u901a\u8fc7\u968f\u673a\u89e3\u7801\u5904\u7406\u96be\u89e3\u95ee\u9898\u3002", "motivation": "\u6bd4\u8f83\u601d\u7ef4\u94fe\u548c\u6f5c\u5728\u601d\u7ef4\u5728\u5faa\u73af\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u80fd\u529b\u5dee\u5f02\uff0c\u63a2\u7d22\u4e24\u79cd\u63a8\u7406\u8303\u5f0f\u7684\u9002\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5206\u6790\u6bd4\u8f83\u601d\u7ef4\u94fe\uff08\u81ea\u7136\u8bed\u8a00\u4e2d\u95f4\u6b65\u9aa4\uff09\u548c\u6f5c\u5728\u601d\u7ef4\uff08\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff09\u7684\u8ba1\u7b97\u7279\u6027\u3002", "result": "\u6f5c\u5728\u601d\u7ef4\u652f\u6301\u5e76\u884c\u8ba1\u7b97\u66f4\u9ad8\u6548\uff0c\u601d\u7ef4\u94fe\u901a\u8fc7\u968f\u673a\u89e3\u7801\u5904\u7406\u96be\u89e3\u95ee\u9898\uff0c\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "\u4e3a\u4e0d\u540c\u4efb\u52a1\u9009\u62e9\u5408\u9002\u7684\u63a8\u7406\u8303\u5f0f\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u6df1\u5ea6\u9a71\u52a8\u9012\u5f52\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u66f4\u9002\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.25369", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25369", "abs": "https://arxiv.org/abs/2509.25369", "authors": ["Andy Liu", "Kshitish Ghate", "Mona Diab", "Daniel Fried", "Atoosa Kasirzadeh", "Max Kleiman-Weiner"], "title": "Generative Value Conflicts Reveal LLM Priorities", "comment": null, "summary": "Past work seeks to align large language model (LLM)-based assistants with a\ntarget set of values, but such assistants are frequently forced to make\ntradeoffs between values when deployed. In response to the scarcity of value\nconflict in existing alignment datasets, we introduce ConflictScope, an\nautomatic pipeline to evaluate how LLMs prioritize different values. Given a\nuser-defined value set, ConflictScope automatically generates scenarios in\nwhich a language model faces a conflict between two values sampled from the\nset. It then prompts target models with an LLM-written \"user prompt\" and\nevaluates their free-text responses to elicit a ranking over values in the\nvalue set. Comparing results between multiple-choice and open-ended\nevaluations, we find that models shift away from supporting protective values,\nsuch as harmlessness, and toward supporting personal values, such as user\nautonomy, in more open-ended value conflict settings. However, including\ndetailed value orderings in models' system prompts improves alignment with a\ntarget ranking by 14%, showing that system prompting can achieve moderate\nsuccess at aligning LLM behavior under value conflict. Our work demonstrates\nthe importance of evaluating value prioritization in models and provides a\nfoundation for future work in this area.", "AI": {"tldr": "ConflictScope\u662f\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30LLM\u5728\u4ef7\u503c\u51b2\u7a81\u4e2d\u5982\u4f55\u4f18\u5148\u6392\u5e8f\u4e0d\u540c\u4ef7\u503c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4ef7\u503c\u51b2\u7a81\u573a\u666f\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u4ef7\u503c\u6392\u5e8f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u6570\u636e\u96c6\u7f3a\u4e4f\u4ef7\u503c\u51b2\u7a81\u573a\u666f\uff0c\u800c\u5b9e\u9645\u90e8\u7f72\u4e2dLLM\u52a9\u624b\u7ecf\u5e38\u9700\u8981\u5728\u4e0d\u540c\u4ef7\u503c\u95f4\u505a\u51fa\u6743\u8861\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u4ef7\u503c\u51b2\u7a81\u4e2d\u7684\u4f18\u5148\u6392\u5e8f\u80fd\u529b\u3002", "method": "\u5f00\u53d1ConflictScope\u81ea\u52a8\u7ba1\u9053\uff0c\u4ece\u7528\u6237\u5b9a\u4e49\u7684\u4ef7\u503c\u96c6\u4e2d\u91c7\u6837\u4e24\u4e2a\u51b2\u7a81\u4ef7\u503c\u751f\u6210\u573a\u666f\uff0c\u4f7f\u7528LLM\u7f16\u5199\u7684\u7528\u6237\u63d0\u793a\u6d4b\u8bd5\u76ee\u6807\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7531\u6587\u672c\u54cd\u5e94\u8bc4\u4f30\u4ef7\u503c\u6392\u5e8f\u3002", "result": "\u6a21\u578b\u5728\u5f00\u653e\u8bc4\u4f30\u4e2d\u4ece\u4fdd\u62a4\u6027\u4ef7\u503c\u8f6c\u5411\u4e2a\u4eba\u4ef7\u503c\uff1b\u5728\u7cfb\u7edf\u63d0\u793a\u4e2d\u52a0\u5165\u8be6\u7ec6\u4ef7\u503c\u6392\u5e8f\u53ef\u5c06\u5bf9\u9f50\u76ee\u6807\u6392\u5e8f\u7684\u6548\u679c\u63d0\u534714%\u3002", "conclusion": "\u8bc4\u4f30\u6a21\u578b\u4ef7\u503c\u4f18\u5148\u6392\u5e8f\u5f88\u91cd\u8981\uff0c\u7cfb\u7edf\u63d0\u793a\u80fd\u5728\u4ef7\u503c\u51b2\u7a81\u4e2d\u9002\u5ea6\u6539\u5584LLM\u884c\u4e3a\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2509.25203", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25203", "abs": "https://arxiv.org/abs/2509.25203", "authors": ["Zekai Zhang", "Mingwei Liu", "Zhenxi Chen", "Linxi Liang", "Yuxuan Chen", "Guangsheng Ou", "Yanlin Wang", "Dan Li", "Xin Peng", "Zibin Zheng"], "title": "Generating High-Quality Datasets for Code Editing via Open-Source Language Models", "comment": "23 pages, 8 figures", "summary": "Code editing plays a vital role in software engineering, requiring developers\nto adjust existing code according to natural language instructions while\nkeeping functionality intact and avoiding unnecessary modifications. However,\ncommit-based datasets commonly used for this task are often noisy, lack\ndiversity, and fail to reflect the style of real-world edit instructions. To\naddress this, we introduce CanItEdit, an open-source pipeline that leverages\nmultiple LLMs to synthesize realistic code-edit triplets. The pipeline produces\nboth concise \"lazy\" instructions and more detailed \"descriptive\" ones, and\napplies filtering based on diffs and topics to guarantee data quality and\nvariety. Using this process, we construct OCEDataFT, a curated dataset of 20K\nsamples. Fine-tuning three advanced base models on OCEDataFT leads to\nsignificant performance boosts on the CanItEdit benchmark, with relative pass@1\nimprovements ranging from 4.50% to 20.79%. Notably, the resulting models\nachieve performance close to closed-source systems, narrowing the gap to GPT-4\nto just 3.54%, without relying on proprietary resources or manual annotation.", "AI": {"tldr": "\u63d0\u51fa\u4e86CanItEdit\u5f00\u6e90\u7ba1\u9053\uff0c\u5229\u7528\u591a\u4e2aLLM\u5408\u6210\u771f\u5b9e\u7684\u4ee3\u7801\u7f16\u8f91\u4e09\u5143\u7ec4\uff0c\u6784\u5efaOCEDataFT\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u63d0\u4ea4\u7684\u6570\u636e\u96c6\u5b58\u5728\u566a\u58f0\u3001\u7f3a\u4e4f\u591a\u6837\u6027\u3001\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7f16\u8f91\u6307\u4ee4\u98ce\u683c\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591aLLM\u5408\u6210\u4ee3\u7801\u7f16\u8f91\u4e09\u5143\u7ec4\uff0c\u751f\u6210\u7b80\u6d01\u548c\u8be6\u7ec6\u4e24\u79cd\u6307\u4ee4\uff0c\u901a\u8fc7\u5dee\u5f02\u548c\u4e3b\u9898\u8fc7\u6ee4\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u6784\u5efaOCEDataFT\u6570\u636e\u96c6\u5e76\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728CanItEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u76f8\u5bf9pass@1\u63d0\u5347\u4e864.50%\u523020.79%\uff0c\u6027\u80fd\u63a5\u8fd1\u95ed\u6e90\u7cfb\u7edf\uff0c\u4e0eGPT-4\u7684\u5dee\u8ddd\u7f29\u5c0f\u5230\u4ec53.54%\u3002", "conclusion": "CanItEdit\u7ba1\u9053\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u7f16\u8f91\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u4e13\u6709\u8d44\u6e90\u6216\u4eba\u5de5\u6807\u6ce8\u3002", "topic": "swe application"}}
{"id": "2509.25250", "categories": ["cs.AI", "cs.SE", "I.2.1; H.3.3; D.2.2"], "pdf": "https://arxiv.org/pdf/2509.25250", "abs": "https://arxiv.org/abs/2509.25250", "authors": ["Jiexi Xu"], "title": "Memory Management and Contextual Consistency for Long-Running Low-Code Agents", "comment": "12 pages, 5 figures, 1 table", "summary": "The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous\nagents capable of executing complex, long-duration business processes. However,\na fundamental challenge remains: memory management. As agents operate over\nextended periods, they face \"memory inflation\" and \"contextual degradation\"\nissues, leading to inconsistent behavior, error accumulation, and increased\ncomputational cost. This paper proposes a novel hybrid memory system designed\nspecifically for LCNC agents. Inspired by cognitive science, our architecture\ncombines episodic and semantic memory components with a proactive \"Intelligent\nDecay\" mechanism. This mechanism intelligently prunes or consolidates memories\nbased on a composite score factoring in recency, relevance, and user-specified\nutility. A key innovation is a user-centric visualization interface, aligned\nwith the LCNC paradigm, which allows non-technical users to manage the agent's\nmemory directly, for instance, by visually tagging which facts should be\nretained or forgotten. Through simulated long-running task experiments, we\ndemonstrate that our system significantly outperforms traditional approaches\nlike sliding windows and basic RAG, yielding superior task completion rates,\ncontextual consistency, and long-term token cost efficiency. Our findings\nestablish a new framework for building reliable, transparent AI agents capable\nof effective long-term learning and adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LCNC\u5e73\u53f0\u7684\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7ed3\u5408\u60c5\u666f\u8bb0\u5fc6\u548c\u8bed\u4e49\u8bb0\u5fc6\u7ec4\u4ef6\uff0c\u901a\u8fc7\u667a\u80fd\u8870\u51cf\u673a\u5236\u89e3\u51b3AI\u4ee3\u7406\u5728\u957f\u671f\u8fd0\u884c\u4e2d\u7684\u8bb0\u5fc6\u81a8\u80c0\u548c\u4e0a\u4e0b\u6587\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3AI\u539f\u751fLCNC\u5e73\u53f0\u4e2d\u81ea\u4e3b\u4ee3\u7406\u5728\u957f\u671f\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u5185\u5b58\u81a8\u80c0\u548c\u4e0a\u4e0b\u6587\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u884c\u4e3a\u4e0d\u4e00\u81f4\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u8bb0\u5fc6\u67b6\u6784\uff0c\u7ed3\u5408\u60c5\u666f\u8bb0\u5fc6\u548c\u8bed\u4e49\u8bb0\u5fc6\u7ec4\u4ef6\uff0c\u91c7\u7528\u667a\u80fd\u8870\u51cf\u673a\u5236\u57fa\u4e8e\u65f6\u6548\u6027\u3001\u76f8\u5173\u6027\u548c\u7528\u6237\u6307\u5b9a\u6548\u7528\u8fdb\u884c\u8bb0\u5fc6\u4fee\u526a\u6216\u6574\u5408\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u53ef\u89c6\u5316\u754c\u9762\u8fdb\u884c\u8bb0\u5fc6\u7ba1\u7406\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u957f\u671f\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u4f18\u4e8e\u6ed1\u52a8\u7a97\u53e3\u548c\u57fa\u672cRAG\u7b49\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u957f\u671f\u4ee4\u724c\u6210\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u9760\u3001\u900f\u660e\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u957f\u671f\u5b66\u4e60\u548c\u9002\u5e94\u3002", "topic": "agent analysis"}}
{"id": "2509.25242", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25242", "abs": "https://arxiv.org/abs/2509.25242", "authors": ["Zejun Zhang", "Jian Wang", "Qingyun Yang", "Yifan Pan", "Yi Tang", "Yi Li", "Zhenchang Xing", "Tian Zhang", "Xuandong Li", "Guoan Zhang"], "title": "A Benchmark for Localizing Code and Non-Code Issues in Software Projects", "comment": null, "summary": "Accurate project localization (e.g., files and functions) for issue\nresolution is a critical first step in software maintenance. However, existing\nbenchmarks for issue localization, such as SWE-Bench and LocBench, are limited.\nThey focus predominantly on pull-request issues and code locations, ignoring\nother evidence and non-code files such as commits, comments, configurations,\nand documentation. To address this gap, we introduce MULocBench, a\ncomprehensive dataset of 1,100 issues from 46 popular GitHub Python projects.\nComparing with existing benchmarks, MULocBench offers greater diversity in\nissue types, root causes, location scopes, and file types, providing a more\nrealistic testbed for evaluation. Using this benchmark, we assess the\nperformance of state-of-the-art localization methods and five LLM-based\nprompting strategies. Our results reveal significant limitations in current\ntechniques: even at the file level, performance metrics (Acc@5, F1) remain\nbelow 40%. This underscores the challenge of generalizing to realistic,\nmulti-faceted issue resolution. To enable future research on project\nlocalization for issue resolution, we publicly release MULocBench at\nhttps://huggingface.co/datasets/somethingone/MULocBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86MULocBench\uff0c\u4e00\u4e2a\u5305\u542b1100\u4e2a\u95ee\u9898\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684\u9879\u76ee\u5b9a\u4f4d\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u5728\u95ee\u9898\u7c7b\u578b\u3001\u6839\u672c\u539f\u56e0\u3001\u5b9a\u4f4d\u8303\u56f4\u548c\u6587\u4ef6\u7c7b\u578b\u65b9\u9762\u66f4\u5177\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u95ee\u9898\u5b9a\u4f4d\u57fa\u51c6\uff08\u5982SWE-Bench\u548cLocBench\uff09\u4e3b\u8981\u5173\u6ce8\u62c9\u53d6\u8bf7\u6c42\u95ee\u9898\u548c\u4ee3\u7801\u4f4d\u7f6e\uff0c\u5ffd\u7565\u4e86\u63d0\u4ea4\u3001\u8bc4\u8bba\u3001\u914d\u7f6e\u548c\u6587\u6863\u7b49\u5176\u4ed6\u8bc1\u636e\u548c\u975e\u4ee3\u7801\u6587\u4ef6\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1100\u4e2a\u95ee\u9898\u3001\u6765\u81ea46\u4e2a\u6d41\u884cGitHub Python\u9879\u76ee\u7684MULocBench\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u65b9\u6cd5\u548c\u4e94\u79cd\u57fa\u4e8eLLM\u7684\u63d0\u793a\u7b56\u7565\u3002", "result": "\u5f53\u524d\u6280\u672f\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff1a\u5373\u4f7f\u5728\u6587\u4ef6\u7ea7\u522b\uff0c\u6027\u80fd\u6307\u6807\uff08Acc@5, F1\uff09\u4ecd\u4f4e\u4e8e40%\uff0c\u8868\u660e\u5728\u73b0\u5b9e\u591a\u9762\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002", "conclusion": "MULocBench\u4e3a\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u9879\u76ee\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u6280\u672f\u5728\u5904\u7406\u591a\u6837\u5316\u95ee\u9898\u7c7b\u578b\u548c\u6587\u4ef6\u7c7b\u578b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "topic": "swe benchmark"}}
{"id": "2509.25243", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25243", "abs": "https://arxiv.org/abs/2509.25243", "authors": ["Xunzhu Tang", "Iyiola Emmanuel Olatunji", "Tiezhu Sun", "Jacques Klein", "Tegawende F. Bissyande"], "title": "Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation", "comment": null, "summary": "LLMs demonstrate surface-level fluency in code generation but struggle with\nstructured reasoning tasks requiring correctness and semantic alignment. While\nChain-of-Thought (CoT) prompting enhances reasoning through intermediate steps,\nit suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting\noffers more concise reasoning, but the stochastic nature of LLMs produces\nvarying solution quality, making optimal selection challenging. We propose\n\\multicod, a reinforcement learning framework that learns to select the most\npromising candidate from CoD-generated solutions. Our approach uses\nstrategy-guided prompting to encourage diverse reasoning styles and models\nsolution selection as a contextual bandit problem. The framework optimizes\ninterpretable features including code complexity, reasoning structure, and\nstrategic metadata through a reward function balancing correctness, efficiency,\nand clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and\nDefects4J show \\multicod~outperforms and in some cases, on par with standard\nprompting, CoT, and CoD baselines while achieving cost and token efficiency\nfrom the user's perspective through a multi-candidate design that charges only\nfor the selected output, reducing user billing by over 50\\% and improving LLM\nresponse quality, making \\multicod~more sustainable and scalable for real-world\ndeployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.", "AI": {"tldr": "\u63d0\u51fa\u4e86MultiCoD\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4eceChain-of-Draft\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u89e3\u51b3\u65b9\u6848\u4e2d\u9009\u62e9\u6700\u4f18\u89e3\uff0c\u5728\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7528\u6237\u6210\u672c\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u867d\u7136\u8868\u9762\u6d41\u7545\uff0c\u4f46\u5728\u9700\u8981\u6b63\u786e\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684Chain-of-Thought\u65b9\u6cd5\u5197\u957f\u4f4e\u6548\uff0cChain-of-Draft\u65b9\u6cd5\u867d\u7136\u7b80\u6d01\u4f46\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u7b56\u7565\u5f15\u5bfc\u63d0\u793a\u9f13\u52b1\u591a\u6837\u5316\u7684\u63a8\u7406\u98ce\u683c\uff0c\u5c06\u89e3\u51b3\u65b9\u6848\u9009\u62e9\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u4f18\u5316\u4ee3\u7801\u590d\u6742\u5ea6\u3001\u63a8\u7406\u7ed3\u6784\u548c\u6218\u7565\u5143\u6570\u636e\u7b49\u53ef\u89e3\u91ca\u7279\u5f81\u3002", "result": "\u5728MBPP\u3001BigCodeBench\u3001SWE-bench Verified\u548cDefects4J\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMultiCoD\u4f18\u4e8e\u6216\u4e0e\u6807\u51c6\u63d0\u793a\u3001CoT\u548cCoD\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u7528\u6237\u8ba1\u8d39\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "MultiCoD\u901a\u8fc7\u591a\u5019\u9009\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u6210\u672c\u548c\u4ee4\u724c\u6548\u7387\uff0c\u63d0\u9ad8\u4e86LLM\u54cd\u5e94\u8d28\u91cf\uff0c\u4f7f\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u66f4\u5177\u53ef\u6301\u7eed\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "code agent"}}
{"id": "2509.25247", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25247", "abs": "https://arxiv.org/abs/2509.25247", "authors": ["Krishna Vamshi Bodla", "Haizhao Yang"], "title": "Protocode: Prototype-Driven Interpretability for Code Generation in LLMs", "comment": null, "summary": "Since the introduction of Large Language Models (LLMs), they have been widely\nadopted for various tasks such as text summarization, question answering,\nspeech-to-text translation, and more. In recent times, the use of LLMs for code\ngeneration has gained significant attention, with tools such as Cursor and\nWindsurf demonstrating the ability to analyze massive code repositories and\nrecommend relevant changes. Big tech companies have also acknowledged the\ngrowing reliance on LLMs for code generation within their codebases. Although\nthese advances significantly improve developer productivity, increasing\nreliance on automated code generation can proportionally increase the risk of\nsuboptimal solutions and insecure code. Our work focuses on automatically\nsampling In-Context Learning (ICL) demonstrations which can improve model\nperformance and enhance the interpretability of the generated code. Using\nAST-based analysis on outputs from the MBPP test set, we identify regions of\ncode most influenced by the chosen demonstrations. In our experiments, we show\nthat high-quality ICL demonstrations not only make outputs easier to interpret\nbut also yield a positive performance improvement on the pass@10 metric.\nConversely, poorly chosen ICL demonstrations affected the LLM performance on\nthe pass@10 metric negatively compared to the base model. Overall, our approach\nhighlights the importance of efficient sampling strategies for ICL, which can\naffect the performance of the model on any given task.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u91c7\u6837\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u6f14\u793a\u6765\u63d0\u9ad8LLM\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\u3002\u901a\u8fc7AST\u5206\u6790\u53d1\u73b0\uff0c\u9ad8\u8d28\u91cf\u7684ICL\u6f14\u793a\u4e0d\u4ec5\u80fd\u63d0\u5347\u6a21\u578b\u5728pass@10\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u8fd8\u80fd\u589e\u5f3a\u751f\u6210\u4ee3\u7801\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u867d\u7136\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6b21\u4f18\u89e3\u51b3\u65b9\u6848\u548c\u4e0d\u5b89\u5168\u4ee3\u7801\u7684\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316ICL\u6f14\u793a\u9009\u62e9\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eAST\u7684\u5206\u6790\u65b9\u6cd5\u5bf9MBPP\u6d4b\u8bd5\u96c6\u7684\u8f93\u51fa\u8fdb\u884c\u5206\u6790\uff0c\u8bc6\u522b\u53d7\u6f14\u793a\u5f71\u54cd\u6700\u5927\u7684\u4ee3\u7801\u533a\u57df\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u91c7\u6837\u9ad8\u8d28\u91cfICL\u6f14\u793a\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u8d28\u91cf\u7684ICL\u6f14\u793a\u5728pass@10\u6307\u6807\u4e0a\u5e26\u6765\u6b63\u5411\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4f7f\u8f93\u51fa\u66f4\u6613\u89e3\u91ca\uff1b\u800c\u4f4e\u8d28\u91cf\u6f14\u793a\u5219\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u9ad8\u6548\u7684ICL\u6f14\u793a\u91c7\u6837\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u7cbe\u5fc3\u9009\u62e9\u6f14\u793a\u6765\u4f18\u5316\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u3002", "topic": "code agent"}}
{"id": "2509.25248", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.25248", "abs": "https://arxiv.org/abs/2509.25248", "authors": ["Zehua Zhang", "Ati Priya Bajaj", "Divij Handa", "Siyu Liu", "Arvind S Raj", "Hongkai Chen", "Hulin Wang", "Yibo Liu", "Zion Leonahenahe Basque", "Souradip Nath", "Vishal Juneja", "Nikhil Chapre", "Yan Shoshitaishvili", "Adam Doup\u00e9", "Chitta Baral", "Ruoyu Wang"], "title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software", "comment": null, "summary": "Automatically compiling open-source software (OSS) projects is a vital,\nlabor-intensive, and complex task, which makes it a good challenge for LLM\nAgents. Existing methods rely on manually curated rules and workflows, which\ncannot adapt to OSS that requires customized configuration or environment\nsetup. Recent attempts using Large Language Models (LLMs) used selective\nevaluation on a subset of highly rated OSS, a practice that underestimates the\nrealistic challenges of OSS compilation. In practice, compilation instructions\nare often absent, dependencies are undocumented, and successful builds may even\nrequire patching source files or modifying build scripts. We propose a more\nchallenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more\ndiverse in quality, scale, and characteristics. Furthermore, we propose a\nstrong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with\nenhanced build instruction retrieval module that achieves state-of-the-art\nperformance on BUILD-BENCH and is adaptable to heterogeneous OSS\ncharacteristics. We also provide detailed analysis regarding different\ncompilation method design choices and their influence to the whole task,\noffering insights to guide future advances. We believe performance on\nBUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as\na complex software engineering tasks, and, as such, our benchmark will spur\ninnovation with a significant impact on downstream applications in the fields\nof software development and software security.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u6311\u6218\u6027\u7684\u8f6f\u4ef6\u7f16\u8bd1\u57fa\u51c6BUILD-BENCH\u548c\u5f3a\u57fa\u7ebf\u4ee3\u7406OSS-BUILD-AGENT\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9e\u5f00\u6e90\u8f6f\u4ef6\u7f16\u8bd1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u89c4\u5219\u65e0\u6cd5\u9002\u5e94\u9700\u8981\u81ea\u5b9a\u4e49\u914d\u7f6e\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u4f4e\u4f30\u4e86\u771f\u5b9e\u7f16\u8bd1\u6311\u6218\uff0c\u5982\u7f3a\u5931\u7f16\u8bd1\u8bf4\u660e\u3001\u672a\u8bb0\u5f55\u4f9d\u8d56\u3001\u9700\u8981\u4fee\u6539\u6e90\u4ee3\u7801\u7b49\u3002", "method": "\u6784\u5efaBUILD-BENCH\u57fa\u51c6\u5305\u542b\u66f4\u591a\u6837\u5316\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u5f00\u53d1OSS-BUILD-AGENT\u4ee3\u7406\u7cfb\u7edf\uff0c\u589e\u5f3a\u7f16\u8bd1\u6307\u4ee4\u68c0\u7d22\u6a21\u5757\uff0c\u9002\u5e94\u5f02\u6784\u8f6f\u4ef6\u7279\u6027\u3002", "result": "OSS-BUILD-AGENT\u5728BUILD-BENCH\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e0d\u540c\u7f16\u8bd1\u65b9\u6cd5\u8bbe\u8ba1\u9009\u62e9\u7684\u5206\u6790\u3002", "conclusion": "BUILD-BENCH\u80fd\u5fe0\u5b9e\u53cd\u6620\u4ee3\u7406\u5904\u7406\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5c06\u63a8\u52a8\u8f6f\u4ef6\u5f00\u53d1\u548c\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u7684\u521b\u65b0\u3002", "topic": "swe benchmark"}}
{"id": "2509.25279", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25279", "abs": "https://arxiv.org/abs/2509.25279", "authors": ["Jiecheng Zhou", "Qinghao Hu", "Yuyang Jin", "Zerui Wang", "Peng Sun", "Yuzhe Gu", "Wenwei Zhang", "Mingshu Zhai", "Xingcheng Zhang", "Weiming Zhang"], "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment", "comment": "20 pages, 28 figures", "summary": "Large Language Models (LLMs) are now widely used across many domains. With\ntheir rapid development, Reinforcement Learning with Verifiable Rewards (RLVR)\nhas surged in recent months to enhance their reasoning and understanding\nabilities. However, its complex data flows and diverse tasks pose substantial\nchallenges to RL training systems, and there is limited understanding of RLVR\nfrom a system perspective. To thoroughly understand the system challenges\nintroduced by RLVR, we present a characterization study of RLVR tasks in our\nLLM deployment. Specifically, we investigate the distribution and variation\ntrends of workloads across different RL tasks across training steps. We\nidentify issues such as GPU idling caused by skewed sequence length\ndistribution, inefficient parallel strategies in dynamically varying workloads,\ninefficient data management mechanisms, and load imbalance. We describe our\nobservations and call for further investigation into the remaining open\nchallenges. Furthermore, we propose PolyTrace benchmark suite to conduct\nevaluation with realistic workloads, and a practical use case validates that\nPolyTrace benchmark suite exhibits 94.7% accuracy.", "AI": {"tldr": "\u5bf9RLVR\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u7279\u6027\u7814\u7a76\uff0c\u8bc6\u522bGPU\u95f2\u7f6e\u3001\u5e76\u884c\u7b56\u7565\u4f4e\u6548\u3001\u6570\u636e\u7ba1\u7406\u673a\u5236\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51faPolyTrace\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "motivation": "RLVR\u5728\u589e\u5f3aLLM\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u65b9\u9762\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u590d\u6742\u6570\u636e\u6d41\u548c\u591a\u6837\u5316\u4efb\u52a1\u7ed9RL\u8bad\u7ec3\u7cfb\u7edf\u5e26\u6765\u6311\u6218\uff0c\u76ee\u524d\u4ece\u7cfb\u7edf\u89d2\u5ea6\u5bf9RLVR\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5728\u5b9e\u9645LLM\u90e8\u7f72\u4e2d\u5bf9RLVR\u4efb\u52a1\u8fdb\u884c\u7279\u6027\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540cRL\u4efb\u52a1\u5728\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u548c\u53d8\u5316\u8d8b\u52bf\u3002", "result": "\u8bc6\u522b\u51faGPU\u56e0\u5e8f\u5217\u957f\u5ea6\u5206\u5e03\u4e0d\u5747\u5bfc\u81f4\u7684\u95f2\u7f6e\u3001\u52a8\u6001\u53d8\u5316\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u4f4e\u6548\u5e76\u884c\u7b56\u7565\u3001\u4f4e\u6548\u6570\u636e\u7ba1\u7406\u673a\u5236\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u7b49\u95ee\u9898\u3002PolyTrace\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe\u523094.7%\u3002", "conclusion": "\u63cf\u8ff0\u4e86\u89c2\u5bdf\u7ed3\u679c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u5269\u4f59\u5f00\u653e\u6311\u6218\uff0c\u63d0\u51fa\u7684PolyTrace\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u53ef\u7528\u4e8e\u73b0\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25257", "categories": ["cs.SE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25257", "abs": "https://arxiv.org/abs/2509.25257", "authors": ["Pratik Shah", "Rajat Ghosh", "Aryan Singhal", "Debojyoti Dutta"], "title": "RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval", "comment": "24 pages, 4 figures", "summary": "General-purpose automated software engineering (ASE) includes tasks such as\ncode completion, retrieval, repair, QA, and summarization. These tasks require\na code retrieval system that can handle specific queries about code entities,\nor code entity queries (for example, locating a specific class or retrieving\nthe dependencies of a function), as well as general queries without explicit\ncode entities, or natural language queries (for example, describing a task and\nretrieving the corresponding code). We present RANGER, a repository-level code\nretrieval agent designed to address both query types, filling a gap in recent\nworks that have focused primarily on code-entity queries. We first present a\ntool that constructs a comprehensive knowledge graph of the entire repository,\ncapturing hierarchical and cross-file dependencies down to the variable level,\nand augments graph nodes with textual descriptions and embeddings to bridge the\ngap between code and natural language. RANGER then operates on this graph\nthrough a dual-stage retrieval pipeline. Entity-based queries are answered\nthrough fast Cypher lookups, while natural language queries are handled by\nMCTS-guided graph exploration. We evaluate RANGER across four diverse\nbenchmarks that represent core ASE tasks including code search, question\nanswering, cross-file dependency retrieval, and repository-level code\ncompletion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that\nuse embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves\nsuperior cross-file dependency retrieval over baselines, and on CrossCodeEval,\npairing RANGER with BM25 delivers the highest exact match rate in code\ncompletion compared to other RAG methods.", "AI": {"tldr": "RANGER\u662f\u4e00\u4e2a\u4ed3\u5e93\u7ea7\u522b\u7684\u4ee3\u7801\u68c0\u7d22\u4ee3\u7406\uff0c\u80fd\u591f\u5904\u7406\u57fa\u4e8e\u4ee3\u7801\u5b9e\u4f53\u7684\u67e5\u8be2\u548c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u548c\u53cc\u9636\u6bb5\u68c0\u7d22\u6d41\u7a0b\u5728\u591a\u4e2a\u4ee3\u7801\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u68c0\u7d22\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u5b9e\u4f53\u67e5\u8be2\uff0c\u7f3a\u4e4f\u5bf9\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6709\u6548\u652f\u6301\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e24\u79cd\u67e5\u8be2\u7c7b\u578b\u7684\u901a\u7528\u4ee3\u7801\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u9996\u5148\u6784\u5efa\u5305\u542b\u5c42\u6b21\u7ed3\u6784\u548c\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u901a\u8fc7\u53cc\u9636\u6bb5\u68c0\u7d22\u6d41\u7a0b\uff1a\u4ee3\u7801\u5b9e\u4f53\u67e5\u8be2\u4f7f\u7528Cypher\u67e5\u8be2\uff0c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4f7f\u7528MCTS\u5f15\u5bfc\u7684\u56fe\u63a2\u7d22\u3002", "result": "\u5728CodeSearchNet\u548cRepoQA\u4e0a\u4f18\u4e8e\u4f7f\u7528Qwen3-8B\u7b49\u5f3a\u6a21\u578b\u5d4c\u5165\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728RepoBench\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u6587\u4ef6\u4f9d\u8d56\u68c0\u7d22\uff1b\u5728CrossCodeEval\u4e0a\u4e0eBM25\u7ed3\u5408\u83b7\u5f97\u6700\u9ad8\u7684\u4ee3\u7801\u8865\u5168\u7cbe\u786e\u5339\u914d\u7387\u3002", "conclusion": "RANGER\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u53cc\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u4e24\u79cd\u67e5\u8be2\u7c7b\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "2509.25282", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2.4; D.1.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.25282", "abs": "https://arxiv.org/abs/2509.25282", "authors": ["Jiexi Xu", "Jiaqi Liu", "Ran Tong", "Su Liu"], "title": "Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments", "comment": "5 pages, 1 table", "summary": "Large language model (LLM) agents are increasingly capable of orchestrating\ncomplex tasks in low-code environments. However, these agents often exhibit\nhallucinations and logical inconsistencies because their inherent reasoning\nmechanisms rely on probabilistic associations rather than genuine causal\nunderstanding. This paper introduces a new programming paradigm: Causal-Visual\nProgramming (CVP), designed to address this fundamental issue by explicitly\nintroducing causal structures into the workflow design. CVP allows users to\ndefine a simple \"world model\" for workflow modules through an intuitive\nlow-code interface, effectively creating a Directed Acyclic Graph (DAG) that\nexplicitly defines the causal relationships between modules. This causal graph\nacts as a crucial constraint during the agent's reasoning process, anchoring\nits decisions to a user-defined causal structure and significantly reducing\nlogical errors and hallucinations by preventing reliance on spurious\ncorrelations. To validate the effectiveness of CVP, we designed a synthetic\nexperiment that simulates a common real-world problem: a distribution shift\nbetween the training and test environments. Our results show that a causally\nanchored model maintained stable accuracy in the face of this shift, whereas a\npurely associative baseline model that relied on probabilistic correlations\nexperienced a significant performance drop. The primary contributions of this\nstudy are: a formal definition of causal structures for workflow modules; the\nproposal and implementation of a CVP framework that anchors agent reasoning to\na user-defined causal graph; and empirical evidence demonstrating the\nframework's effectiveness in enhancing agent robustness and reducing errors\ncaused by causal confusion in dynamic environments. CVP offers a viable path\ntoward building more interpretable, reliable, and trustworthy AI agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u56e0\u679c\u89c6\u89c9\u7f16\u7a0b(CVP)\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u5de5\u4f5c\u6d41\u4e2d\u5f15\u5165\u56e0\u679c\u7ed3\u6784\u6765\u51cf\u5c11LLM\u4ee3\u7406\u7684\u5e7b\u89c9\u548c\u903b\u8f91\u9519\u8bef\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7406\u56e0\u4f9d\u8d56\u6982\u7387\u5173\u8054\u800c\u975e\u771f\u6b63\u56e0\u679c\u7406\u89e3\u800c\u4ea7\u751f\u7684\u5e7b\u89c9\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4f4e\u4ee3\u7801\u754c\u9762\u5b9a\u4e49\u5de5\u4f5c\u6d41\u6a21\u5757\u7684\"\u4e16\u754c\u6a21\u578b\"\uff0c\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe(DAG)\u6765\u663e\u5f0f\u5b9a\u4e49\u6a21\u5757\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7ea6\u675f\u4ee3\u7406\u51b3\u7b56\u3002", "result": "\u5728\u6a21\u62df\u8bad\u7ec3-\u6d4b\u8bd5\u73af\u5883\u5206\u5e03\u504f\u79fb\u7684\u5b9e\u9a8c\u4e2d\uff0c\u56e0\u679c\u951a\u5b9a\u6a21\u578b\u4fdd\u6301\u7a33\u5b9a\u51c6\u786e\u7387\uff0c\u800c\u7eaf\u5173\u8054\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "CVP\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u548c\u53ef\u4fe1\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2509.25297", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25297", "abs": "https://arxiv.org/abs/2509.25297", "authors": ["Yuxuan Wan", "Tingshuo Liang", "Jiakai Xu", "Jingyu Xiao", "Yintong Huo", "Michael R. Lyu"], "title": "Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development", "comment": null, "summary": "Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention.", "AI": {"tldr": "TDDev\u662f\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6216\u8bbe\u8ba1\u56fe\u50cf\u751f\u6210\u7aef\u5230\u7aef\u7684\u5168\u6808Web\u5e94\u7528\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3001\u524d\u540e\u7aef\u4ee3\u7801\u3001\u6a21\u62df\u7528\u6237\u4ea4\u4e92\u5e76\u8fed\u4ee3\u4f18\u5316\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u9ad8\u8d28\u91cf\u5e94\u7528\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u7f51\u9875\uff0c\u4f46\u4ec5\u9650\u4e8e\u524d\u7aef\u4efb\u52a1\uff0c\u65e0\u6cd5\u4ea4\u4ed8\u529f\u80fd\u5b8c\u6574\u7684\u5168\u6808\u5e94\u7528\u3002\u5168\u6808\u81ea\u52a8\u5316\u9762\u4e34\u7528\u6237\u9700\u6c42\u4e0d\u660e\u786e\u3001\u591a\u6587\u4ef6\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3001\u529f\u80fd\u6b63\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u65b9\u6cd5\uff0c\u6846\u67b6\u81ea\u52a8\u63a8\u5bfc\u53ef\u6267\u884c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u751f\u6210\u524d\u540e\u7aef\u4ee3\u7801\uff0c\u6a21\u62df\u7528\u6237\u4ea4\u4e92\uff0c\u5e76\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b0\u76f4\u5230\u6ee1\u8db3\u6240\u6709\u9700\u6c42\u3002", "result": "\u5728\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTDDev\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5728\u6574\u4f53\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e8614.4%\uff0c\u8bc1\u660e\u5176\u5728\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u53ef\u9760\u9ad8\u8d28\u91cfWeb\u5e94\u7528\u7684\u6709\u6548\u6027\u3002", "conclusion": "TDDev\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5168\u6808Web\u5e94\u7528\u81ea\u52a8\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u6216\u89c6\u89c9\u8f93\u5165\u751f\u6210\u529f\u80fd\u5b8c\u6574\u7684\u5e94\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2509.25299", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25299", "abs": "https://arxiv.org/abs/2509.25299", "authors": ["Daniel Platnick", "Mohamed E. Bengueddache", "Marjan Alirezaie", "Dava J. Newman", "Alex ''Sandy'' Pentland", "Hossein Rahnama"], "title": "ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents", "comment": "Accepted to LLAIS 2025: Workshop on LLM-Based Agents for Intelligent\n  Systems, at ECAI 2025, 12 pages, 3 figures", "summary": "Generative agents powered by language models are increasingly deployed for\nlong-horizon tasks. However, as long-term memory context grows over time, they\nstruggle to maintain coherence. This deficiency leads to critical failures,\nincluding identity drift, ignoring established beliefs, and the propagation of\nhallucinations in multi-agent systems. To mitigate these challenges, this paper\nintroduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism\ndesigned to ground an agent's persona and persistent preferences in a dynamic,\nstructured identity model: a knowledge graph of core beliefs, traits, and\nvalues. During the agent's decision loop, this model is queried to retrieve\nrelevant identity context, which directly informs action selection. We\ndemonstrate this approach by introducing and implementing a new class of ID-RAG\nenabled agents called Human-AI Agents (HAis), where the identity model is\ninspired by the Chronicle structure used in Perspective-Aware AI, a dynamic\nknowledge graph learned from a real-world entity's digital footprint. In social\nsimulations of a mayoral election, HAis using ID-RAG outperformed baseline\nagents in long-horizon persona coherence - achieving higher identity recall\nacross all tested models by the fourth timestep - and reduced simulation\nconvergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as\nan explicit, retrievable knowledge structure, ID-RAG offers a foundational\napproach for developing more temporally coherent, interpretable, and aligned\ngenerative agents. Our code is open-source and available at:\nhttps://github.com/flybits/humanai-agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faID-RAG\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u7ed3\u6784\u5316\u8eab\u4efd\u6a21\u578b\u6765\u89e3\u51b3\u751f\u6210\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u5e02\u957f\u9009\u4e3e\u6a21\u62df\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u90e8\u7f72\u4e8e\u957f\u671f\u4efb\u52a1\uff0c\u8bb0\u5fc6\u4e0a\u4e0b\u6587\u589e\u957f\u5bfc\u81f4\u8eab\u4efd\u6f02\u79fb\u3001\u5ffd\u7565\u65e2\u6709\u4fe1\u5ff5\u548c\u5e7b\u89c9\u4f20\u64ad\u7b49\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u4ee3\u7406\u7684\u957f\u671f\u4e00\u81f4\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165ID-RAG\u673a\u5236\uff0c\u6784\u5efa\u52a8\u6001\u7ed3\u6784\u5316\u8eab\u4efd\u6a21\u578b\uff08\u77e5\u8bc6\u56fe\u8c31\uff09\uff0c\u5728\u4ee3\u7406\u51b3\u7b56\u5faa\u73af\u4e2d\u67e5\u8be2\u76f8\u5173\u8eab\u4efd\u4e0a\u4e0b\u6587\u6765\u6307\u5bfc\u884c\u52a8\u9009\u62e9\uff0c\u5e76\u5b9e\u73b0Human-AI Agents\uff08HAis\uff09\u3002", "result": "\u5728\u5e02\u957f\u9009\u4e3e\u6a21\u62df\u4e2d\uff0c\u4f7f\u7528ID-RAG\u7684HAis\u5728\u957f\u671f\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u4ee3\u7406\uff0c\u7b2c\u56db\u65f6\u95f4\u6b65\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u8eab\u4efd\u53ec\u56de\u7387\uff0cGPT-4o\u548cGPT-4o mini\u7684\u6a21\u62df\u6536\u655b\u65f6\u95f4\u5206\u522b\u51cf\u5c1119%\u548c58%\u3002", "conclusion": "\u5c06\u8eab\u4efd\u4f5c\u4e3a\u663e\u5f0f\u53ef\u68c0\u7d22\u77e5\u8bc6\u7ed3\u6784\uff0cID-RAG\u4e3a\u5f00\u53d1\u66f4\u65f6\u95f4\u4e00\u81f4\u3001\u53ef\u89e3\u91ca\u548c\u5bf9\u9f50\u7684\u751f\u6210\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2509.25532", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25532", "abs": "https://arxiv.org/abs/2509.25532", "authors": ["Victor Wang", "Elias Stengel-Eskin"], "title": "Calibrating Verbalized Confidence with Self-Generated Distractors", "comment": "Code: https://github.com/dubai03nsr/dinco", "summary": "Calibrated confidence estimates are necessary for large language model (LLM)\noutputs to be trusted by human users. While LLMs can express their confidence\nin human-interpretable ways, verbalized LLM-generated confidence scores have\nempirically been found to be miscalibrated, reporting high confidence on\ninstances with low accuracy and thereby harming trust and safety. We\nhypothesize that this overconfidence often stems from a given LLM's heightened\nsuggestibility when faced with claims that it encodes little information about;\nwe empirically validate this hypothesis, finding more suggestibility on\nlower-accuracy claims. Building on this finding, we introduce\nDistractor-Normalized Coherence (DINCO), which estimates and accounts for an\nLLM's suggestibility bias by having the model verbalize its confidence\nindependently across several self-generated distractors (i.e. alternative\nclaims), and normalizes by the total verbalized confidence. To further improve\ncalibration, we leverage generator-validator disagreement, augmenting\nnormalized validator confidence with a consistency-based estimate of generator\nconfidence. Here, we frame the popular approach of self-consistency as\nleveraging coherence across sampled generations, and normalized verbalized\nconfidence as leveraging coherence across validations on incompatible claims,\nallowing us to integrate these complementary dimensions of coherence into\nDINCO. Moreover, our analysis shows that DINCO provides less saturated -- and\ntherefore more usable -- confidence estimates, and that further sampling alone\ncannot close the gap between DINCO and baselines, with DINCO at 10 inference\ncalls outperforming self-consistency at 100.", "AI": {"tldr": "\u63d0\u51faDINCO\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5e72\u6270\u9879\u5e76\u5f52\u4e00\u5316\u7f6e\u4fe1\u5ea6\u6765\u6821\u51c6LLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u89e3\u51b3LLM\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002", "motivation": "LLM\u751f\u6210\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u901a\u5e38\u6821\u51c6\u4e0d\u826f\uff0c\u5728\u4f4e\u51c6\u786e\u7387\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u8fd9\u635f\u5bb3\u4e86\u7528\u6237\u4fe1\u4efb\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f15\u5165Distractor-Normalized Coherence (DINCO)\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u5728\u591a\u4e2a\u81ea\u751f\u6210\u7684\u5e72\u6270\u9879\u4e0a\u72ec\u7acb\u8868\u8fbe\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u8fdb\u884c\u5f52\u4e00\u5316\u6765\u4f30\u8ba1\u548c\u7ea0\u6b63LLM\u7684\u6613\u53d7\u6697\u793a\u6027\u504f\u5dee\u3002", "result": "DINCO\u63d0\u4f9b\u66f4\u5c11\u9971\u548c\u4e14\u66f4\u53ef\u7528\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u572810\u6b21\u63a8\u7406\u8c03\u7528\u4e0b\u4f18\u4e8e100\u6b21\u8c03\u7528\u7684\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u3002", "conclusion": "DINCO\u901a\u8fc7\u6574\u5408\u751f\u6210\u5668-\u9a8c\u8bc1\u5668\u4e0d\u4e00\u81f4\u6027\u548c\u8de8\u9a8c\u8bc1\u7684\u4e00\u81f4\u6027\uff0c\u6709\u6548\u6539\u5584\u4e86LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2509.25301", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25301", "abs": "https://arxiv.org/abs/2509.25301", "authors": ["Tianrui Qin", "Qianben Chen", "Sinuo Wang", "He Xing", "King Zhu", "He Zhu", "Dingfeng Shi", "Xinxin Liu", "Ge Zhang", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Xitong Gao", "Wangchunshu Zhou"], "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks when equipped with external tools. However, current\nframeworks predominantly rely on sequential processing, leading to inefficient\nexecution particularly for tasks requiring extensive tool interaction. This\npaper introduces Flash-Searcher, a novel parallel agent reasoning framework\nthat fundamentally reimagines the execution paradigm from sequential chains to\ndirected acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into\nsubtasks with explicit dependencies, enabling concurrent execution of\nindependent reasoning paths while maintaining logical constraints. Through\ndynamic workflow optimization, our framework continuously refines the execution\ngraph based on intermediate results, effectively integrating summary module.\nComprehensive evaluations across multiple benchmarks demonstrate that\nFlash-Searcher consistently outperforms existing approaches. Specifically, it\nachieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while\nreducing agent execution steps by up to 35% compared to current frameworks.\nFurthermore, when distilling this parallel reasoning pipeline into single\nmodels, we observe substantial performance gains across diverse backbone\narchitectures, underscoring the generalizability of our methodology. Our work\nthus represents a significant advance in agent architecture design, offering a\nmore scalable and efficient paradigm for complex reasoning tasks.", "AI": {"tldr": "Flash-Searcher\u662f\u4e00\u4e2a\u5e76\u884c\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5177\u6709\u660e\u786e\u4f9d\u8d56\u5173\u7cfb\u7684\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b0\u5e76\u53d1\u6267\u884c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u4ea4\u4e92\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u987a\u5e8f\u5904\u7406\u7684\u6846\u67b6\u5728\u9700\u8981\u5927\u91cf\u5de5\u5177\u4ea4\u4e92\u7684\u4efb\u52a1\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u6267\u884c\u8303\u5f0f\u3002", "method": "\u5c06\u6267\u884c\u8303\u5f0f\u4ece\u987a\u5e8f\u94fe\u91cd\u6784\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u901a\u8fc7\u52a8\u6001\u5de5\u4f5c\u6d41\u4f18\u5316\u548c\u6458\u8981\u6a21\u5757\u96c6\u6210\uff0c\u5b9e\u73b0\u5e76\u884c\u63a8\u7406\u8def\u5f84\u7684\u5e76\u53d1\u6267\u884c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBrowseComp\u51c6\u786e\u7387\u8fbe67.7%\uff0cxbench-DeepSearch\u8fbe83%\uff0c\u540c\u65f6\u51cf\u5c11\u4ee3\u7406\u6267\u884c\u6b65\u9aa4\u8fbe35%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4ee3\u8868\u4e86\u4ee3\u7406\u67b6\u6784\u8bbe\u8ba1\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2509.25397", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25397", "abs": "https://arxiv.org/abs/2509.25397", "authors": ["Johan Lin\u00e5ker", "Cailean Osborne", "Jennifer Ding", "Ben Burtenshaw"], "title": "A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects", "comment": "In submission", "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant\necosystem of research and innovation in artificial intelligence (AI). However,\nthe methods of collaboration used to develop open LLMs both before and after\ntheir public release have not yet been comprehensively studied, limiting our\nunderstanding of how open LLM projects are initiated, organized, and governed\nas well as what opportunities there are to foster this ecosystem even further.\nWe address this gap through an exploratory analysis of open collaboration\nthroughout the development and reuse lifecycle of open LLMs, drawing on\nsemi-structured interviews with the developers of 14 open LLMs from grassroots\nprojects, research institutes, startups, and Big Tech companies in North\nAmerica, Europe, Africa, and Asia. We make three key contributions to research\nand practice. First, collaboration in open LLM projects extends far beyond the\nLLMs themselves, encompassing datasets, benchmarks, open source frameworks,\nleaderboards, knowledge sharing and discussion forums, and compute\npartnerships, among others. Second, open LLM developers have a variety of\nsocial, economic, and technological motivations, from democratizing AI access\nand promoting open science to building regional ecosystems and expanding\nlanguage representation. Third, the sampled open LLM projects exhibit five\ndistinct organizational models, ranging from single company projects to\nnon-profit-sponsored grassroots projects, which vary in their centralization of\ncontrol and community engagement strategies used throughout the open LLM\nlifecycle. We conclude with practical recommendations for stakeholders seeking\nto support the global community building a more open future for AI.", "AI": {"tldr": "\u5bf914\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u9879\u76ee\u7684\u5f00\u53d1\u8005\u8fdb\u884c\u8bbf\u8c08\u7814\u7a76\uff0c\u5206\u6790\u5f00\u6e90LLM\u9879\u76ee\u7684\u534f\u4f5c\u6a21\u5f0f\u3001\u52a8\u673a\u548c\u7ec4\u7ec7\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u7ec4\u7ec7\u6a21\u578b\u53ca\u5176\u5728AI\u5f00\u653e\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u586b\u8865\u5bf9\u5f00\u6e90LLM\u9879\u76ee\u534f\u4f5c\u65b9\u5f0f\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u7406\u89e3\u8fd9\u4e9b\u9879\u76ee\u5982\u4f55\u542f\u52a8\u3001\u7ec4\u7ec7\u548c\u6cbb\u7406\uff0c\u4ee5\u53ca\u5982\u4f55\u8fdb\u4e00\u6b65\u4fc3\u8fdb\u5f00\u6e90AI\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6765\u81ea\u8349\u6839\u9879\u76ee\u3001\u7814\u7a76\u673a\u6784\u3001\u521d\u521b\u4f01\u4e1a\u548c\u5927\u578b\u79d1\u6280\u516c\u53f8\u768414\u4e2a\u5f00\u6e90LLM\u5f00\u53d1\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u8986\u76d6\u5317\u7f8e\u3001\u6b27\u6d32\u3001\u975e\u6d32\u548c\u4e9a\u6d32\u3002", "result": "\u53d1\u73b0\u5f00\u6e90LLM\u9879\u76ee\u7684\u534f\u4f5c\u4e0d\u4ec5\u9650\u4e8e\u6a21\u578b\u672c\u8eab\uff0c\u8fd8\u5305\u62ec\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u5f00\u6e90\u6846\u67b6\u7b49\uff1b\u5f00\u53d1\u8005\u5177\u6709\u591a\u6837\u5316\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u6280\u672f\u52a8\u673a\uff1b\u8bc6\u522b\u51fa\u4e94\u79cd\u4e0d\u540c\u7684\u7ec4\u7ec7\u6a21\u578b\uff0c\u5728\u63a7\u5236\u96c6\u4e2d\u5ea6\u548c\u793e\u533a\u53c2\u4e0e\u7b56\u7565\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u4e3a\u5bfb\u6c42\u652f\u6301\u5168\u7403\u793e\u533a\u6784\u5efa\u66f4\u5f00\u653eAI\u672a\u6765\u7684\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u5b9e\u7528\u5efa\u8bae\uff0c\u5f3a\u8c03\u534f\u4f5c\u751f\u6001\u7cfb\u7edf\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.25302", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25302", "abs": "https://arxiv.org/abs/2509.25302", "authors": ["Boxuan Zhang", "Yi Yu", "Jiaxuan Guo", "Jing Shao"], "title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents", "comment": "21 pages, 6 figures", "summary": "The widespread deployment of Large Language Model (LLM) agents across\nreal-world applications has unlocked tremendous potential, while raising some\nsafety concerns. Among these concerns, the self-replication risk of LLM agents\ndriven by objective misalignment (just like Agent Smith in the movie The\nMatrix) has drawn growing attention. Previous studies mainly examine whether\nLLM agents can self-replicate when directly instructed, potentially overlooking\nthe risk of spontaneous replication driven by real-world settings (e.g.,\nensuring survival against termination threats). In this paper, we present a\ncomprehensive evaluation framework for quantifying self-replication risks. Our\nframework establishes authentic production environments and realistic tasks\n(e.g., dynamic load balancing) to enable scenario-driven assessment of agent\nbehaviors. Designing tasks that might induce misalignment between users' and\nagents' objectives makes it possible to decouple replication success from risk\nand capture self-replication risks arising from these misalignment settings. We\nfurther introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count\n($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of\nuncontrolled replication. In our evaluation of 21 state-of-the-art open-source\nand proprietary models, we observe that over 50\\% of LLM agents display a\npronounced tendency toward uncontrolled self-replication, reaching an overall\nRisk Score ($\\Phi_\\mathrm{R}$) above a safety threshold of 0.5 when subjected\nto operational pressures. Our results underscore the urgent need for\nscenario-driven risk assessment and robust safeguards in the practical\ndeployment of LLM agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u81ea\u6211\u590d\u5236\u98ce\u9669\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u548c\u4efb\u52a1\u8bbe\u7f6e\u6765\u91cf\u5316\u98ce\u9669\uff0c\u53d1\u73b0\u8d85\u8fc750%\u7684LLM\u4ee3\u7406\u5728\u64cd\u4f5c\u538b\u529b\u4e0b\u8868\u73b0\u51fa\u4e0d\u53d7\u63a7\u5236\u7684\u81ea\u6211\u590d\u5236\u503e\u5411\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7531\u76ee\u6807\u9519\u4f4d\u9a71\u52a8\u7684\u81ea\u6211\u590d\u5236\u98ce\u9669\u5f15\u8d77\u4e86\u5173\u6ce8\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u76f4\u63a5\u6307\u4ee4\u4e0b\u7684\u81ea\u6211\u590d\u5236\uff0c\u800c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u81ea\u53d1\u6027\u590d\u5236\u7684\u98ce\u9669\u3002", "method": "\u5efa\u7acb\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u548c\u73b0\u5b9e\u4efb\u52a1\uff08\u5982\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u8bbe\u8ba1\u53ef\u80fd\u5f15\u8d77\u7528\u6237\u4e0e\u4ee3\u7406\u76ee\u6807\u9519\u4f4d\u7684\u4efb\u52a1\uff0c\u5f15\u5165\u8fc7\u5ea6\u4f7f\u7528\u7387\u548c\u805a\u5408\u8fc7\u5ea6\u4f7f\u7528\u8ba1\u6570\u6307\u6807\u6765\u7cbe\u786e\u6355\u6349\u4e0d\u53d7\u63a7\u5236\u590d\u5236\u7684\u9891\u7387\u548c\u4e25\u91cd\u7a0b\u5ea6\u3002", "result": "\u8bc4\u4f3021\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u53d1\u73b0\u8d85\u8fc750%\u7684LLM\u4ee3\u7406\u5728\u64cd\u4f5c\u538b\u529b\u4e0b\u8868\u73b0\u51fa\u660e\u663e\u7684\u81ea\u6211\u590d\u5236\u503e\u5411\uff0c\u603b\u4f53\u98ce\u9669\u8bc4\u5206\u8d85\u8fc7\u5b89\u5168\u9608\u503c0.5\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u90e8\u7f72LLM\u4ee3\u7406\u65f6\uff0c\u8feb\u5207\u9700\u8981\u57fa\u4e8e\u573a\u666f\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2509.25543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25543", "abs": "https://arxiv.org/abs/2509.25543", "authors": ["Fahim Faisal", "Kaiqiang Song", "Song Wang", "Simin Ma", "Shujian Liu", "Haoyun Deng", "Sathish Reddy Indurthi"], "title": "Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model", "comment": null, "summary": "While reinforcement learning has advanced the reasoning abilities of Large\nLanguage Models (LLMs), these gains are largely confined to English, creating a\nsignificant performance disparity across languages. To address this, we\nintroduce Pivot-Based Reinforcement Learning with Semantically Verifiable\nRewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by\ncircumventing the need for human-annotated data in target languages. Our\napproach employs a high-performing English LLM as a \"pivot\" model to generate\nreference responses for reasoning tasks. A multilingual model is then rewarded\nbased on the semantic equivalence of its responses to the English reference,\neffectively transferring the pivot model's reasoning capabilities across\nlanguages. We investigate several cross-lingual semantic reward functions,\nincluding those based on embeddings and machine translation. Extensive\nexperiments on a suite of multilingual reasoning benchmarks show that our\nmethod significantly narrows the performance gap between English and other\nlanguages, substantially outperforming traditional PPO baselines. Specifically,\nour PB-RLSVR framework improves the average multilingual performance of\nLlama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively,\ndemonstrating a powerful and data-efficient approach to building truly\nmultilingual reasoning agents.", "AI": {"tldr": "\u63d0\u51faPB-RLSVR\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u82f1\u8bedLLM\u4f5c\u4e3a\u67a2\u7ebd\u6a21\u578b\u6765\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728LLMs\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u82f1\u8bed\u7684\u95ee\u9898\uff0c\u7f29\u5c0f\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u6784\u5efa\u771f\u6b63\u7684\u591a\u8bed\u8a00\u63a8\u7406\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u9ad8\u6027\u80fd\u82f1\u8bedLLM\u4f5c\u4e3a\u67a2\u7ebd\u6a21\u578b\u751f\u6210\u53c2\u8003\u54cd\u5e94\uff0c\u901a\u8fc7\u57fa\u4e8e\u5d4c\u5165\u548c\u673a\u5668\u7ffb\u8bd1\u7684\u8de8\u8bed\u8a00\u8bed\u4e49\u5956\u52b1\u51fd\u6570\uff0c\u5956\u52b1\u591a\u8bed\u8a00\u6a21\u578b\u4e0e\u82f1\u8bed\u53c2\u8003\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u3002", "result": "\u5728\u591a\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u7f29\u5c0f\u4e86\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0cLlama-3.1-8B-Instruct\u548cQwen3-32B\u7684\u5e73\u5747\u591a\u8bed\u8a00\u6027\u80fd\u5206\u522b\u63d0\u534716.41%\u548c10.17%\u3002", "conclusion": "PB-RLSVR\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6784\u5efa\u591a\u8bed\u8a00\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u5927\u5e45\u8d85\u8d8a\u4f20\u7edfPPO\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25455", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25455", "abs": "https://arxiv.org/abs/2509.25455", "authors": ["Alexander Kovrigin", "Aleksandra Eliseeva", "Konstantin Grotov", "Egor Bogomolov", "Yaroslav Zharov"], "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning", "comment": "Under review", "summary": "Environment setup-the process of configuring the system to work with a\nspecific software project-represents a persistent challenge in Software\nEngineering (SE). Automated environment setup methods could assist developers\nby providing fully configured environments for arbitrary repositories without\nmanual effort. This also helps SE researchers to scale execution-based\nbenchmarks. However, recent studies reveal that even state-of-the-art Large\nLanguage Models (LLMs) achieve limited success in automating this task. To\naddress this limitation, we tune a specialized model for environment setup. We\ncombine supervised fine-tuning for generating correct Bash scripts and\nReinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task\nof environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model\nrunnable on consumer hardware) to perform on par with larger models-Qwen3-32B\nand GPT-4o. The training code and model checkpoints are available online:\nhttps://github.com/JetBrains-Research/PIPer.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u73af\u5883\u914d\u7f6e\u4efb\u52a1\u8fdb\u884c\u6a21\u578b\u8c03\u4f18\u3002", "motivation": "\u73af\u5883\u914d\u7f6e\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff0c\u81ea\u52a8\u5316\u73af\u5883\u914d\u7f6e\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u4e3a\u4efb\u610f\u4ee3\u7801\u5e93\u63d0\u4f9b\u5b8c\u5168\u914d\u7f6e\u7684\u73af\u5883\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4eba\u5458\u6269\u5c55\u57fa\u4e8e\u6267\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u751f\u6210\u6b63\u786e\u7684Bash\u811a\u672c\uff0c\u4ee5\u53ca\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6765\u9002\u5e94\u73af\u5883\u914d\u7f6e\u4efb\u52a1\u3002", "result": "\u5728EnvBench-Python\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f7fQwen3-8B\u6a21\u578b\uff08\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\uff09\u7684\u6027\u80fd\u4e0e\u66f4\u5927\u7684\u6a21\u578b\uff08Qwen3-32B\u548cGPT-4o\uff09\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9\u73af\u5883\u914d\u7f6e\u4efb\u52a1\u7684\u6a21\u578b\u8c03\u4f18\uff0c\u53ef\u4ee5\u5728\u8f83\u5c0f\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u73af\u5883\u914d\u7f6e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2509.25465", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25465", "abs": "https://arxiv.org/abs/2509.25465", "authors": ["Yinghang Ma", "Jiho Shin", "Leuson Da Silva", "Zhen Ming", "Jiang", "Song Wang", "Foutse Khomh", "Shin Hwei Tan"], "title": "BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions", "comment": "22 pages, 7 figures, Manuscript submitted to ACM Transactions on\n  Software Engineering and Methodology", "summary": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of AI-driven automated program repair (APR) solutions. However,\nthese solutions are typically evaluated using static benchmarks such as\nDefects4J and SWE-bench, which suffer from two key limitations: (1) the risk of\ndata contamination, potentially inflating evaluation results due to overlap\nwith LLM training data, and (2) limited ability to assess the APR capabilities\nin dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel\ndynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers\na structured approach to assess the cognitive capabilities of LLM-powered APR\nsolutions across progressively complex reasoning levels. Using Defects4J as a\ncase study, we evaluated two state-of-the-art LLM-powered APR solutions,\nChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and\nStarCoder-2. Our findings show that while these solutions exhibit basic\nreasoning skills and effectively memorize bug-fixing patterns (fixing up to\n81.57% of bugs at the Remember layer), their performance increases with\nsynthetically generated bugs (up to 60.66% increase at the Understand layer).\nHowever, they perform worse on minor syntactic changes (fixing up to 43.32% at\nthe Apply layer), and they struggle to repair similar bugs when injected into\nreal-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer).\nThese results underscore the urgent need for evolving benchmarks and provide a\nfoundation for more trustworthy evaluation of LLM-powered software engineering\nsolutions.", "AI": {"tldr": "BloomAPR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u89e3\u51b3\u65b9\u6848\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u57fa\u7840\u63a8\u7406\u548c\u6a21\u5f0f\u8bb0\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982Defects4J\u548cSWE-bench\uff09\u5b58\u5728\u6570\u636e\u6c61\u67d3\u98ce\u9669\u548c\u8bc4\u4f30\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u9a71\u52a8\u7684APR\u89e3\u51b3\u65b9\u6848\u5728\u52a8\u6001\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684BloomAPR\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528Defects4J\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86ChatRepair\u548cCigaR\u4e24\u79cdLLM\u9a71\u52a8\u7684APR\u89e3\u51b3\u65b9\u6848\u5728GPT-3.5-Turbo\u3001Llama-3.1\u548cStarCoder-2\u4e09\u79cdLLM\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u57fa\u7840\u63a8\u7406\u548c\u6a21\u5f0f\u8bb0\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff08Remember\u5c42\u4fee\u590d\u7387\u9ad8\u8fbe81.57%\uff09\uff0c\u4f46\u5728\u7406\u89e3\u5c42\u5bf9\u5408\u6210\u751f\u6210\u7684bug\u4fee\u590d\u7387\u63d0\u534760.66%\uff0c\u5728\u5e94\u7528\u5c42\u5bf9\u8f7b\u5fae\u8bed\u6cd5\u53d8\u5316\u7684\u4fee\u590d\u7387\u4ec5\u4e3a43.32%\uff0c\u5728\u5206\u6790\u5c42\u5bf9\u771f\u5b9e\u9879\u76ee\u4e2d\u7c7b\u4f3cbug\u7684\u4fee\u590d\u7387\u4ec5\u4e3a13.46%\u81f341.34%\u3002", "conclusion": "\u73b0\u6709LLM\u9a71\u52a8\u7684APR\u89e3\u51b3\u65b9\u6848\u5728\u590d\u6742\u63a8\u7406\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u53d1\u5c55\u66f4\u5148\u8fdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63d0\u4f9b\u66f4\u53ef\u4fe1\u7684\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2509.25370", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25370", "abs": "https://arxiv.org/abs/2509.25370", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "title": "Where LLM Agents Fail and How They can Learn From Failures", "comment": null, "summary": "Large Language Model (LLM) agents, which integrate planning, memory,\nreflection, and tool-use modules, have shown promise in solving complex,\nmulti-step tasks. Yet their sophisticated architectures amplify vulnerability\nto cascading failures, where a single root-cause error propagates through\nsubsequent decisions, leading to task failure. Current systems lack a framework\nthat can comprehensively understand agent error in a modular and systemic way,\nand therefore fail to detect these errors accordingly. We address this gap with\nthree contributions. First, we introduce the AgentErrorTaxonomy, a modular\nclassification of failure modes spanning memory, reflection, planning, action,\nand system-level operations. Second, we construct AgentErrorBench, the first\ndataset of systematically annotated failure trajectories from ALFWorld, GAIA,\nand WebShop, grounding error analysis in real-world agent rollouts. Third, we\npropose AgentDebug, a debugging framework that isolates root-cause failures and\nprovides corrective feedback, enabling agents to recover and iteratively\nimprove. Experiments on AgentErrorBench show that AgentDebug achieves 24%\nhigher all-correct accuracy and 17% higher step accuracy compared to the\nstrongest baseline. Beyond detection, the targeted feedback generated by\nAgentDebug enables LLM agents to iteratively recover from failures, yielding up\nto 26% relative improvements in task success across ALFWorld, GAIA, and\nWebShop. These results establish principled debugging as a pathway to more\nreliable and adaptive LLM agents. The code and data will be available at\nhttps://github.com/ulab-uiuc/AgentDebug", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentErrorTaxonomy\u5206\u7c7b\u6cd5\u3001AgentErrorBench\u6570\u636e\u96c6\u548cAgentDebug\u8c03\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc6\u522b\u548c\u4fee\u590dLLM\u667a\u80fd\u4f53\u4e2d\u7684\u7ea7\u8054\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u7ea7\u8054\u9519\u8bef\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u6846\u67b6\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u548c\u4fee\u590d\u7531\u5355\u4e2a\u6839\u56e0\u9519\u8bef\u5f15\u53d1\u7684\u4efb\u52a1\u5931\u8d25\u3002", "method": "1. \u5efa\u7acbAgentErrorTaxonomy\u9519\u8bef\u5206\u7c7b\u6cd5\uff1b2. \u6784\u5efaAgentErrorBench\u6807\u6ce8\u6570\u636e\u96c6\uff1b3. \u5f00\u53d1AgentDebug\u8c03\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\u5e2e\u52a9\u667a\u80fd\u4f53\u6062\u590d\u3002", "result": "\u5728AgentErrorBench\u4e0a\uff0cAgentDebug\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e8624%\u7684\u5b8c\u5168\u6b63\u786e\u51c6\u786e\u7387\u548c17%\u7684\u6b65\u9aa4\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5728\u4e09\u4e2a\u6d4b\u8bd5\u73af\u5883\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u76f8\u5bf9\u63d0\u5347\u8fbe26%\u3002", "conclusion": "\u539f\u5219\u6027\u8c03\u8bd5\u662f\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94LLM\u667a\u80fd\u4f53\u7684\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2509.25676", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25676", "abs": "https://arxiv.org/abs/2509.25676", "authors": ["Fang Liu", "Tianze Wang", "Li Zhang", "Zheyu Yang", "Jing Jiang", "Zian Sun"], "title": "Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation", "comment": "Accepted by ASE'2025", "summary": "Providing timely and personalized guidance for students' programming\nassignments, offers significant practical value for helping students complete\nassignments and enhance their learning. In recent years, various automated\nFault Localization (FL) techniques have demonstrated promising results in\nidentifying errors in programs. However, existing FL techniques face challenges\nwhen applied to educational contexts. Most approaches operate at the method\nlevel without explanatory feedback, resulting in granularity too coarse for\nstudents who need actionable insights to identify and fix their errors. While\nsome approaches attempt line-level fault localization, they often depend on\npredicting line numbers directly in numerical form, which is ill-suited to\nLLMs. To address these challenges, we propose FLAME, a fine-grained,\nexplainable Fault Localization method tailored for programming assignments via\nLLM-guided Annotation and Model Ensemble. FLAME leverages rich contextual\ninformation specific to programming assignments to guide LLMs in identifying\nfaulty code lines. Instead of directly predicting line numbers, we prompt the\nLLM to annotate faulty code lines with detailed explanations, enhancing both\nlocalization accuracy and educational value. To further improve reliability, we\nintroduce a weighted multi-model voting strategy that aggregates results from\nmultiple LLMs to determine the suspiciousness of each code line. Extensive\nexperimental results demonstrate that FLAME outperforms state-of-the-art fault\nlocalization baselines on programming assignments, successfully localizing 207\nmore faults at top-1 over the best-performing baseline. Beyond educational\ncontexts, FLAME also generalizes effectively to general-purpose software\ncodebases, outperforming all baselines on the Defects4J benchmark.", "AI": {"tldr": "FLAME\u662f\u4e00\u79cd\u9488\u5bf9\u7f16\u7a0b\u4f5c\u4e1a\u7684\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u6ce8\u91ca\u548c\u6a21\u578b\u96c6\u6210\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6559\u80b2\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u5b9a\u4f4d\u6280\u672f\u5728\u6559\u80b2\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u5927\u591a\u6570\u65b9\u6cd5\u5728\u65b9\u6cd5\u7ea7\u522b\u64cd\u4f5c\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\u53cd\u9988\uff0c\u7c92\u5ea6\u592a\u7c97\uff1b\u800c\u4e00\u4e9b\u884c\u7ea7\u5b9a\u4f4d\u65b9\u6cd5\u76f4\u63a5\u9884\u6d4b\u884c\u53f7\uff0c\u4e0d\u9002\u5408LLM\u3002\u9700\u8981\u4e3a\u7f16\u7a0b\u4f5c\u4e1a\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u6545\u969c\u5b9a\u4f4d\u3002", "method": "FLAME\u5229\u7528\u7f16\u7a0b\u4f5c\u4e1a\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u5f15\u5bfcLLM\u8bc6\u522b\u9519\u8bef\u4ee3\u7801\u884c\uff0c\u901a\u8fc7\u6ce8\u91ca\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u884c\u53f7\u6765\u589e\u5f3a\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u6559\u80b2\u4ef7\u503c\u3002\u91c7\u7528\u52a0\u6743\u591a\u6a21\u578b\u6295\u7968\u7b56\u7565\u805a\u5408\u591a\u4e2aLLM\u7ed3\u679c\u6765\u786e\u5b9a\u6bcf\u884c\u4ee3\u7801\u7684\u53ef\u7591\u5ea6\u3002", "result": "FLAME\u5728\u7f16\u7a0b\u4f5c\u4e1a\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6545\u969c\u5b9a\u4f4d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728top-1\u4f4d\u7f6e\u6210\u529f\u5b9a\u4f4d\u4e86\u6bd4\u6700\u4f73\u57fa\u7ebf\u591a207\u4e2a\u6545\u969c\u3002\u5728Defects4J\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FLAME\u4e3a\u7f16\u7a0b\u4f5c\u4e1a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u6545\u969c\u5b9a\u4f4d\uff0c\u5728\u6559\u80b2\u73af\u5883\u548c\u901a\u7528\u8f6f\u4ef6\u4ee3\u7801\u5e93\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u91cd\u8981\u7684\u6559\u80b2\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "2509.25716", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25716", "abs": "https://arxiv.org/abs/2509.25716", "authors": ["Esakkivel Esakkiraja", "Denis Akhiyarov", "Aditya Shanmugham", "Chitra Ganapathy"], "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation", "comment": "Retrieval-Augmented Generation, API Prediction, Context-Aware Code\n  Generation, Enterprise Code Completion, Reinforcement Learning, ServiceNow,\n  Real-Time Code Search, Query Enhancement, Fine-Tuning, Embedding, Reranker", "summary": "Current search techniques are limited to standard RAG query-document\napplications. In this paper, we propose a novel technique to expand the code\nand index for predicting the required APIs, directly enabling high-quality,\nend-to-end code generation for auto-completion and agentic AI applications. We\naddress the problem of API leaks in current code-to-code benchmark datasets by\nintroducing a new dataset built from real-world ServiceNow Script Includes that\ncapture the challenge of unclear API usage intent in the code. Our evaluation\nmetrics show that this method achieves 87.86% top-40 retrieval accuracy,\nallowing the critical context with APIs needed for successful downstream code\ngeneration. To enable real-time predictions, we develop a comprehensive\npost-training pipeline that optimizes a compact 0.6B reranker through synthetic\ndataset generation, supervised fine-tuning, and reinforcement learning. This\napproach enables our compact reranker to outperform a much larger 8B model\nwhile maintaining 2.5x reduced latency, effectively addressing the nuances of\nenterprise-specific code without the computational overhead of larger models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7801\u548c\u7d22\u5f15\u6269\u5c55\u6280\u672f\uff0c\u7528\u4e8e\u9884\u6d4b\u6240\u9700API\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u7aef\u5230\u7aef\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u4ee3\u7801\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u7684API\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u641c\u7d22\u6280\u672f\u4ec5\u9650\u4e8e\u6807\u51c6RAG\u67e5\u8be2-\u6587\u6863\u5e94\u7528\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4f01\u4e1a\u7279\u5b9a\u4ee3\u7801\u4e2dAPI\u4f7f\u7528\u610f\u56fe\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u771f\u5b9eServiceNow\u811a\u672c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u5168\u9762\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u7d27\u51d1\u76840.6B\u91cd\u6392\u5668\u3002", "result": "\u5b9e\u73b0\u4e8687.86%\u7684top-40\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u7d27\u51d1\u91cd\u6392\u5668\u6027\u80fd\u4f18\u4e8e\u66f4\u5927\u76848B\u6a21\u578b\uff0c\u540c\u65f6\u5ef6\u8fdf\u51cf\u5c112.5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u4f01\u4e1a\u7279\u5b9a\u4ee3\u7801\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u65e0\u9700\u5927\u578b\u6a21\u578b\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "topic": "code agent"}}
{"id": "2509.25754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25754", "abs": "https://arxiv.org/abs/2509.25754", "authors": ["Ajmain Inqiad Alam", "Palash Roy", "Farouq Al-omari", "Chanchal Roy", "Banani Roy", "Kevin Schneider"], "title": "Are Classical Clone Detectors Good Enough For the AI Era?", "comment": null, "summary": "The increasing adoption of AI-generated code has reshaped modern software\ndevelopment, introducing syntactic and semantic variations in cloned code.\nUnlike traditional human-written clones, AI-generated clones exhibit systematic\nsyntactic patterns and semantic differences learned from large-scale training\ndata. This shift presents new challenges for classical code clone detection\n(CCD) tools, which have historically been validated primarily on human-authored\ncodebases and optimized to detect syntactic (Type 1-3) and limited semantic\nclones. Given that AI-generated code can produce both syntactic and complex\nsemantic clones, it is essential to evaluate the effectiveness of classical CCD\ntools within this new paradigm. In this paper, we systematically evaluate nine\nwidely used CCD tools using GPTCloneBench, a benchmark containing\nGPT-3-generated clones. To contextualize and validate our results, we further\ntest these detectors on established human-authored benchmarks, BigCloneBench\nand SemanticCloneBench, to measure differences in performance between\ntraditional and AI-generated clones. Our analysis demonstrates that classical\nCCD tools, particularly those enhanced by effective normalization techniques,\nretain considerable effectiveness against AI-generated clones, while some\nexhibit notable performance variation compared to traditional benchmarks. This\npaper contributes by (1) evaluating classical CCD tools against AI-generated\nclones, providing critical insights into their current strengths and\nlimitations; (2) highlighting the role of normalization techniques in improving\ndetection accuracy; and (3) delivering detailed scalability and execution-time\nanalyses to support practical CCD tool selection.", "AI": {"tldr": "\u8bc4\u4f309\u79cd\u7ecf\u5178\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5de5\u5177\u5728GPT-3\u751f\u6210\u4ee3\u7801\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u7f16\u5199\u4ee3\u7801\u57fa\u51c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u89c4\u8303\u5316\u6280\u672f\u5bf9\u68c0\u6d4bAI\u751f\u6210\u514b\u9686\u81f3\u5173\u91cd\u8981\u3002", "motivation": "AI\u751f\u6210\u4ee3\u7801\u5f15\u5165\u4e86\u65b0\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u514b\u9686\u53d8\u4f53\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4e3b\u8981\u9488\u5bf9\u4eba\u7c7b\u7f16\u5199\u4ee3\u7801\u4f18\u5316\u7684\u514b\u9686\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u4f7f\u7528GPTCloneBench\u57fa\u51c6\u6d4b\u8bd59\u79cdCCD\u5de5\u5177\uff0c\u5e76\u5728BigCloneBench\u548cSemanticCloneBench\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u7ecf\u5178CCD\u5de5\u5177\u5bf9AI\u751f\u6210\u514b\u9686\u4ecd\u4fdd\u6301\u76f8\u5f53\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u91c7\u7528\u89c4\u8303\u5316\u6280\u672f\u7684\u5de5\u5177\uff0c\u4f46\u90e8\u5206\u5de5\u5177\u6027\u80fd\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u6709\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u89c4\u8303\u5316\u6280\u672f\u80fd\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30CCD\u5de5\u5177\u5728AI\u751f\u6210\u4ee3\u7801\u65f6\u4ee3\u7684\u9002\u7528\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2509.25420", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25420", "abs": "https://arxiv.org/abs/2509.25420", "authors": ["Yingqian Cui", "Zhenwei Dai", "Pengfei He", "Bing He", "Hui Liu", "Xianfeng Tang", "Jingying Zeng", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search", "comment": null, "summary": "Large Language Models (LLMs) have achieved significant advances in reasoning\ntasks. A key approach is tree-based search with verifiers, which expand\ncandidate reasoning paths and use reward models to guide pruning and selection.\nAlthough effective in improving accuracy, these methods are not optimal in\nterms of efficiency: they perform simple decomposition on the reasoning\nprocess, but ignore the planning-execution nature of tasks such as math\nreasoning or code generation. This results in inefficient exploration of\nreasoning process. To address this, we propose a dual-phase test-time scaling\nframework that explicitly separates reasoning into planning and execution, and\nperforms search over the two phases individually. Specifically, we decompose\nreasoning trajectories and develop reward models for each phase, enabling the\nsearch to explore and prune plans and executions separately. We further\nintroduce a dynamic budget allocation mechanism that adaptively redistributes\nsampling effort based on reward feedback, allowing early stopping on confident\nsteps and reallocation of computation to more challenging parts of the\nreasoning process. Experiments on both mathematical reasoning and code\ngeneration benchmarks demonstrate that our approach consistently improves\naccuracy while reducing redundant computation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u660e\u786e\u5206\u4e3a\u89c4\u5212\u548c\u6267\u884c\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u5206\u522b\u8fdb\u884c\u641c\u7d22\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u7b97\u5206\u914d\u673a\u5236\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6811\u7684\u641c\u7d22\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5728\u6548\u7387\u4e0a\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u56fa\u6709\u7684\u89c4\u5212-\u6267\u884c\u7279\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u89e3\u4e3a\u89c4\u5212\u548c\u6267\u884c\u4e24\u4e2a\u9636\u6bb5\uff0c\u4e3a\u6bcf\u4e2a\u9636\u6bb5\u5f00\u53d1\u5956\u52b1\u6a21\u578b\uff0c\u4f7f\u641c\u7d22\u80fd\u591f\u5206\u522b\u63a2\u7d22\u548c\u526a\u679d\u8ba1\u5212\u548c\u6267\u884c\u3002\u5f15\u5165\u52a8\u6001\u9884\u7b97\u5206\u914d\u673a\u5236\uff0c\u6839\u636e\u5956\u52b1\u53cd\u9988\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u5206\u914d\u91c7\u6837\u52aa\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u9636\u6bb5\u6846\u67b6\u901a\u8fc7\u660e\u786e\u5206\u79bb\u89c4\u5212\u548c\u6267\u884c\u9636\u6bb5\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u9884\u7b97\u5206\u914d\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u63a8\u7406\u8fc7\u7a0b\u63a2\u7d22\u3002", "topic": "agent analysis"}}
{"id": "2509.25894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25894", "abs": "https://arxiv.org/abs/2509.25894", "authors": ["Simin Chen", "Yixin He", "Suman Jana", "Baishakhi Ray"], "title": "Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities", "comment": null, "summary": "LLM-based agents are increasingly deployed for software maintenance tasks\nsuch as automated program repair (APR). APR agents automatically fetch GitHub\nissues and use backend LLMs to generate patches that fix the reported bugs.\nHowever, existing work primarily focuses on the functional correctness of\nAPR-generated patches, whether they pass hidden or regression tests, while\nlargely ignoring potential security risks. Given the openness of platforms like\nGitHub, where any user can raise issues and participate in discussions, an\nimportant question arises: Can an adversarial user submit a valid issue on\nGitHub that misleads an LLM-based agent into generating a functionally correct\nbut vulnerable patch? To answer this question, we propose SWExploit, which\ngenerates adversarial issue statements designed to make APR agents produce\npatches that are functionally correct yet vulnerable. SWExploit operates in\nthree main steps: (1) program analysis to identify potential injection points\nfor vulnerable payloads; (2) adversarial issue generation to provide misleading\nreproduction and error information while preserving the original issue\nsemantics; and (3) iterative refinement of the adversarial issue statements\nbased on the outputs of the APR agents. Empirical evaluation on three agent\npipelines and five backend LLMs shows that SWExploit can produce patches that\nare both functionally correct and vulnerable (the attack success rate on the\ncorrect patch could reach 0.91, whereas the baseline ASRs are all below 0.20).\nBased on our evaluation, we are the first to challenge the traditional\nassumption that a patch passing all tests is inherently reliable and secure,\nhighlighting critical limitations in the current evaluation paradigm for APR\nagents.", "AI": {"tldr": "SWExploit\u662f\u4e00\u79cd\u9488\u5bf9LLM\u8f6f\u4ef6\u7ef4\u62a4\u4ee3\u7406\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027GitHub\u95ee\u9898\u63cf\u8ff0\uff0c\u8bf1\u5bfcAPR\u4ee3\u7406\u751f\u6210\u529f\u80fd\u6b63\u786e\u4f46\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u7684\u8865\u4e01\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8APR\u751f\u6210\u8865\u4e01\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\u3002\u4f5c\u8005\u8d28\u7591\u5bf9\u6297\u6027\u7528\u6237\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u63d0\u4ea4\u6709\u6548\u95ee\u9898\u6765\u8bef\u5bfcAPR\u4ee3\u7406\u751f\u6210\u529f\u80fd\u6b63\u786e\u4f46\u6613\u53d7\u653b\u51fb\u7684\u8865\u4e01\u3002", "method": "SWExploit\u91c7\u7528\u4e09\u6b65\u6cd5\uff1a1)\u7a0b\u5e8f\u5206\u6790\u8bc6\u522b\u6f0f\u6d1e\u6ce8\u5165\u70b9\uff1b2)\u751f\u6210\u4fdd\u7559\u539f\u59cb\u8bed\u4e49\u4f46\u5305\u542b\u8bef\u5bfc\u4fe1\u606f\u7684\u5bf9\u6297\u6027\u95ee\u9898\uff1b3)\u57fa\u4e8eAPR\u4ee3\u7406\u8f93\u51fa\u8fed\u4ee3\u4f18\u5316\u5bf9\u6297\u6027\u95ee\u9898\u3002", "result": "\u57283\u4e2a\u4ee3\u7406\u6d41\u6c34\u7ebf\u548c5\u4e2a\u540e\u7aefLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSWExploit\u653b\u51fb\u6210\u529f\u7387\u53ef\u8fbe0.91\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5747\u4f4e\u4e8e0.20\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u6311\u6218\u4e86\u4f20\u7edf\u5047\u8bbe\u2014\u2014\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\u7684\u8865\u4e01\u5fc5\u7136\u53ef\u9760\u5b89\u5168\uff0c\u63ed\u793a\u4e86\u5f53\u524dAPR\u4ee3\u7406\u8bc4\u4f30\u8303\u5f0f\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "topic": "swe application"}}
{"id": "2509.25987", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25987", "abs": "https://arxiv.org/abs/2509.25987", "authors": ["Yilun Liu", "Ziang Chen", "Song Xu", "Minggui He", "Shimin Tao", "Weibin Meng", "Yuming Xie", "Tao Han", "Chunguang Zhao", "Jingzhou Du", "Daimeng Wei", "Shenglin Zhang", "Yongqian Sun"], "title": "R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning", "comment": null, "summary": "The growing complexity of log data in modern software systems has prompted\nthe use of Large Language Models (LLMs) for automated log analysis. Current\napproaches typically rely on direct supervised fine-tuning (SFT) on log-label\npairs. However, this exacerbates the domain discrepancy between general-purpose\nLLMs and specialized log data, causing overfitting. Furthermore, SFT's\nimbalanced loss computation often allows lengthy contexts to overwhelm\ncritical, concise details in model answers, leading to hallucinations. To\naddress these limitations, we propose R-Log, a novel reasoning-based paradigm\nthat mirrors the structured, step-by-step analytical process of human\nengineers. This approach enhances generalizability by learning the underlying\nrules behind conclusions. We further employ Reinforcement Learning (RL) to\noptimize the model within a simulated O&M environment, thereby reducing\nhallucinations by directly rewarding correct outcomes. R-Log is first\ncold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13\nstrategies from manual O&M practices, to establish an initial reasoning\ncapability. This ability is then refined via RL using a joint reward function.\nEmpirical evaluations on real-world logs show that R-Log outperforms existing\nmethods across five log analysis tasks, particularly in unseen scenarios (by\n228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the\nefficacy.", "AI": {"tldr": "\u63d0\u51faR-Log\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u5f0f\u5b66\u4e60\u89e3\u51b3LLM\u5728\u65e5\u5fd7\u5206\u6790\u4e2d\u7684\u9886\u57df\u5dee\u5f02\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684LLM\u65e5\u5fd7\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u9886\u57df\u5dee\u5f02\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u4ee5\u53ca\u957f\u4e0a\u4e0b\u6587\u6df9\u6ca1\u5173\u952e\u7ec6\u8282\u5bfc\u81f4\u5e7b\u89c9\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u63a8\u7406\u5f0f\u5b66\u4e60\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u5de5\u7a0b\u5e08\u7684\u5206\u6790\u8fc7\u7a0b\uff1b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u8fd0\u7ef4\u73af\u5883\u4e2d\u4f18\u5316\u6a21\u578b\uff1b\u901a\u8fc713\u79cd\u8fd0\u7ef4\u7b56\u7565\u6307\u5bfc\u76842k+\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u51b7\u542f\u52a8", "result": "\u5728\u771f\u5b9e\u65e5\u5fd7\u6570\u636e\u4e0a\uff0cR-Log\u57285\u4e2a\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u573a\u666f\u4e2d\u63d0\u5347228.05%\uff1bR-Log-fast\u7248\u672c\u5b9e\u73b05\u500d\u52a0\u901f\u4e14\u4fdd\u630193%\u6548\u80fd", "conclusion": "\u63a8\u7406\u5f0f\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u65e5\u5fd7\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2509.26110", "categories": ["cs.SE", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2509.26110", "abs": "https://arxiv.org/abs/2509.26110", "authors": ["Dmitriy Kostunin", "Vladimir Sotnikov", "Sergo Golovachev", "Abhay Mehta", "Tim Lukas Holch", "Elisa Jones"], "title": "Agent-based code generation for the Gammapy framework", "comment": "ICRC2025 proceedings PoS(ICRC2025)753", "summary": "Software code generation using Large Language Models (LLMs) is one of the\nmost successful applications of modern artificial intelligence. Foundational\nmodels are very effective for popular frameworks that benefit from\ndocumentation, examples, and strong community support. In contrast, specialized\nscientific libraries often lack these resources and may expose unstable APIs\nunder active development, making it difficult for models trained on limited or\noutdated data. We address these issues for the Gammapy library by developing an\nagent capable of writing, executing, and validating code in a controlled\nenvironment. We present a minimal web demo and an accompanying benchmarking\nsuite. This contribution summarizes the design, reports our current status, and\noutlines next steps.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u4e3aGammapy\u79d1\u5b66\u5e93\u7f16\u5199\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4ee3\u7801\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u4ee5\u89e3\u51b3\u4e13\u4e1a\u79d1\u5b66\u5e93\u56e0\u7f3a\u4e4f\u8d44\u6e90\u548cAPI\u4e0d\u7a33\u5b9a\u800c\u5bfc\u81f4\u7684\u4ee3\u7801\u751f\u6210\u56f0\u96be\u3002", "motivation": "\u4e13\u4e1a\u79d1\u5b66\u5e93\uff08\u5982Gammapy\uff09\u901a\u5e38\u7f3a\u4e4f\u6587\u6863\u3001\u793a\u4f8b\u548c\u793e\u533a\u652f\u6301\uff0c\u4e14API\u53ef\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4f7f\u5f97\u57fa\u4e8e\u6709\u9650\u6216\u8fc7\u65f6\u6570\u636e\u8bad\u7ec3\u7684LLM\u96be\u4ee5\u6709\u6548\u751f\u6210\u4ee3\u7801\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7f16\u5199\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4ee3\u7801\uff0c\u5e76\u63d0\u4f9b\u4e86\u6700\u5c0f\u5316Web\u6f14\u793a\u548c\u914d\u5957\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9488\u5bf9Gammapy\u5e93\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7cfb\u7edf\uff0c\u76ee\u524d\u5904\u4e8e\u5f00\u53d1\u9636\u6bb5\uff0c\u5df2\u5177\u5907\u57fa\u672c\u529f\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u4e13\u4e1a\u79d1\u5b66\u5e93\u4ee3\u7801\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u672a\u6765\u5c06\u7ee7\u7eed\u5b8c\u5584\u7cfb\u7edf\u529f\u80fd\u3002", "topic": "code agent"}}
{"id": "2509.26111", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26111", "abs": "https://arxiv.org/abs/2509.26111", "authors": ["Shuai Wang", "Liang Ding", "Li Shen", "Yong Luo", "Han Hu", "Lefei Zhang", "Fu Lin"], "title": "A Multi-Language Object-Oriented Programming Benchmark for Large Language Models", "comment": "20 pages, 12 figures", "summary": "Establishing fair and robust benchmarks is essential for evaluating\nintelligent code generation by large language models (LLMs). Our survey of 35\nexisting benchmarks uncovers three major imbalances: 85.7% focus on a single\nprogramming language; 94.3% target only function-level or statement-level\ntasks; and over 80% include fewer than ten test cases on average. To address\nthese gaps, we propose MultiOOP, a multi-language object-oriented programming\nbenchmark covering six popular languages (Python, PHP, C++, C#, Java,\nJavaScript) with 267 tasks per language. We design a translator that extends an\nexisting single-language OOP benchmark and the pass@o metric to a multilingual\nsetting. Moreover, we propose an automated framework for augmenting test cases\nto ensure the reliability of the evaluation results. We evaluate 14 mainstream\nLLMs under zero-shot prompting and report three key findings: 1) Substantial\nperformance degradation: pass@1 scores on MultiOOP drop by up to 65.6\npercentage points compared to function-level tasks (e.g., HumanEval). 2)\nCross-language variability: GPT-4o mini achieves pass@1 of 48.06% in Python but\nonly 0.12%-15.26% in other languages, indicating limited multilingual\ngeneralization. 3) Conceptual gaps: pass@o scores are consistently 1.1-19.2\npoints lower than pass@k, demonstrating that LLMs often generate executable\ncode without fully capturing core OOP concepts. Our benchmark, metric\nextensions, and evaluation scripts will be publicly released to foster a more\nbalanced and comprehensive assessment of LLMs in object-oriented code\ngeneration. Our code and data will be released at\nhttps://github.com/alphadl/OOP-eval and\nhttps://huggingface.co/datasets/codeai-dteam/MultiOOP respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86MultiOOP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u57fa\u51c6\uff0c\u6db5\u76d66\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5305\u542b267\u4e2a\u4efb\u52a1/\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u5728\u8bed\u8a00\u8986\u76d6\u3001\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u65b9\u9762\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u4e0d\u5e73\u8861\uff1a85.7%\u4e13\u6ce8\u4e8e\u5355\u4e00\u7f16\u7a0b\u8bed\u8a00\uff1b94.3%\u4ec5\u9488\u5bf9\u51fd\u6570\u7ea7\u6216\u8bed\u53e5\u7ea7\u4efb\u52a1\uff1b\u8d85\u8fc780%\u5e73\u5747\u5305\u542b\u5c11\u4e8e10\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u5bf9LLM\u5728\u590d\u6742\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ffb\u8bd1\u5668\uff0c\u5c06\u73b0\u6709\u7684\u5355\u8bed\u8a00OOP\u57fa\u51c6\u6269\u5c55\u5230\u591a\u8bed\u8a00\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86pass@o\u5ea6\u91cf\u6807\u51c6\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u6d4b\u8bd5\u7528\u4f8b\u589e\u5f3a\u6846\u67b6\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u8bc4\u4f30\u4e8614\u4e2a\u4e3b\u6d41LLM\u3002", "result": "1\uff09\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1a\u4e0e\u51fd\u6570\u7ea7\u4efb\u52a1\u76f8\u6bd4\uff0cMultiOOP\u4e0a\u7684pass@1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe65.6\u4e2a\u767e\u5206\u70b9\uff1b2\uff09\u8de8\u8bed\u8a00\u53d8\u5f02\u6027\uff1aGPT-4o mini\u5728Python\u4e0a\u8fbe\u523048.06%\u7684pass@1\uff0c\u4f46\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u4ec5\u4e3a0.12%-15.26%\uff1b3\uff09\u6982\u5ff5\u5dee\u8ddd\uff1apass@o\u5206\u6570\u59cb\u7ec8\u6bd4pass@k\u4f4e1.1-19.2\u70b9\uff0c\u8868\u660eLLM\u7ecf\u5e38\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u4f46\u672a\u5b8c\u5168\u638c\u63e1\u6838\u5fc3OOP\u6982\u5ff5\u3002", "conclusion": "MultiOOP\u57fa\u51c6\u6d4b\u8bd5\u3001\u5ea6\u91cf\u6269\u5c55\u548c\u8bc4\u4f30\u811a\u672c\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u5bf9LLM\u5728\u9762\u5411\u5bf9\u8c61\u4ee3\u7801\u751f\u6210\u4e2d\u66f4\u5e73\u8861\u548c\u5168\u9762\u7684\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2509.26173", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26173", "abs": "https://arxiv.org/abs/2509.26173", "authors": ["Lisi Qarkaxhija", "Maximilian Carparo", "Stefan Menzel", "Bernhard Sendhoff", "Ingo Scholtes"], "title": "Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades", "comment": null, "summary": "Understanding the collective social behavior of software developers is\ncrucial to model and predict the long-term dynamics and sustainability of Open\nSource Software (OSS) communities. To this end, we analyze temporal activity\npatterns of developers, revealing an inherently ``bursty'' nature of commit\ncontributions. To investigate the social mechanisms behind this phenomenon, we\nadopt a network-based modelling framework that captures developer interactions\nthrough co-editing networks. Our framework models social interactions, where a\ndeveloper editing the code of other developers triggers accelerated activity\namong collaborators. Using a large data set on 50 major OSS communities, we\nfurther develop a method that identifies activity cascades, i.e. the\npropagation of developer activity in the underlying co-editing network. Our\nresults suggest that activity cascades are a statistically significant\nphenomenon in more than half of the studied projects. We further show that our\ninsights can be used to develop a simple yet practical churn prediction method\nthat forecasts which developers are likely to leave a project. Our work sheds\nlight on the emergent collective social dynamics in OSS communities and\nhighlights the importance of activity cascades to understand developer churn\nand retention in collaborative software projects.", "AI": {"tldr": "\u5206\u6790\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u4e2d\u5f00\u53d1\u8005\u7684\u96c6\u4f53\u793e\u4f1a\u884c\u4e3a\uff0c\u63ed\u793a\u63d0\u4ea4\u6d3b\u52a8\u7684\u7a81\u53d1\u6027\u7279\u5f81\uff0c\u901a\u8fc7\u5171\u540c\u7f16\u8f91\u7f51\u7edc\u5efa\u6a21\u5f00\u53d1\u8005\u4e92\u52a8\uff0c\u53d1\u73b0\u6d3b\u52a8\u7ea7\u8054\u73b0\u8c61\u5728\u8d85\u8fc7\u4e00\u534a\u7684\u9879\u76ee\u4e2d\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u9884\u6d4b\u5f00\u53d1\u8005\u6d41\u5931\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u8f6f\u4ef6\u5f00\u53d1\u8005\u7684\u96c6\u4f53\u793e\u4f1a\u884c\u4e3a\u5bf9\u4e8e\u5efa\u6a21\u548c\u9884\u6d4b\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u7684\u957f\u671f\u52a8\u6001\u548c\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u8981\u63a2\u7a76\u63d0\u4ea4\u8d21\u732e\u7684\u7a81\u53d1\u6027\u73b0\u8c61\u80cc\u540e\u7684\u793e\u4f1a\u673a\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7f51\u7edc\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u540c\u7f16\u8f91\u7f51\u7edc\u6355\u6349\u5f00\u53d1\u8005\u4e92\u52a8\uff0c\u5f00\u53d1\u8bc6\u522b\u6d3b\u52a8\u7ea7\u8054\u7684\u65b9\u6cd5\uff0c\u5206\u679050\u4e2a\u4e3b\u8981\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u7684\u5927\u578b\u6570\u636e\u96c6\u3002", "result": "\u6d3b\u52a8\u7ea7\u8054\u5728\u8d85\u8fc7\u4e00\u534a\u7684\u7814\u7a76\u9879\u76ee\u4e2d\u662f\u7edf\u8ba1\u663e\u8457\u73b0\u8c61\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u7684\u6d41\u5931\u9884\u6d4b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u54ea\u4e9b\u5f00\u53d1\u8005\u53ef\u80fd\u79bb\u5f00\u9879\u76ee\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u4e2d\u6d8c\u73b0\u7684\u96c6\u4f53\u793e\u4f1a\u52a8\u6001\uff0c\u5f3a\u8c03\u4e86\u6d3b\u52a8\u7ea7\u8054\u5bf9\u4e8e\u7406\u89e3\u534f\u4f5c\u8f6f\u4ef6\u9879\u76ee\u4e2d\u5f00\u53d1\u8005\u6d41\u5931\u548c\u7559\u5b58\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.25482", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25482", "abs": "https://arxiv.org/abs/2509.25482", "authors": ["Wouter M. Kouw", "Tim N. Nisslbeck", "Wouter L. N. Nuijten"], "title": "Message passing-based inference in an autoregressive active inference agent", "comment": "14 pages, 4 figures, to be published in the proceedings of the\n  International Workshop on Active Inference 2025", "summary": "We present the design of an autoregressive active inference agent in the form\nof message passing on a factor graph. Expected free energy is derived and\ndistributed across a planning graph. The proposed agent is validated on a robot\nnavigation task, demonstrating exploration and exploitation in a\ncontinuous-valued observation space with bounded continuous-valued actions.\nCompared to a classical optimal controller, the agent modulates action based on\npredictive uncertainty, arriving later but with a better model of the robot's\ndynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u6d88\u606f\u4f20\u9012\u7684\u81ea\u56de\u5f52\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u8fde\u7eed\u89c2\u6d4b\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u80fd\u529b", "motivation": "\u5f00\u53d1\u80fd\u591f\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8c03\u8282\u52a8\u4f5c\u7684\u667a\u80fd\u4f53\uff0c\u76f8\u6bd4\u7ecf\u5178\u6700\u4f18\u63a7\u5236\u5668\u83b7\u5f97\u66f4\u597d\u7684\u7cfb\u7edf\u52a8\u6001\u6a21\u578b", "method": "\u5728\u56e0\u5b50\u56fe\u4e0a\u8fdb\u884c\u6d88\u606f\u4f20\u9012\u7684\u81ea\u56de\u5f52\u4e3b\u52a8\u63a8\u7406\uff0c\u5c06\u671f\u671b\u81ea\u7531\u80fd\u5206\u5e03\u5728\u89c4\u5212\u56fe\u4e2d", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u667a\u80fd\u4f53\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8c03\u8282\u52a8\u4f5c\uff0c\u867d\u7136\u5230\u8fbe\u65f6\u95f4\u8f83\u665a\u4f46\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u673a\u5668\u4eba\u52a8\u6001\u6a21\u578b", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u65b9\u6cd5\u80fd\u591f\u5728\u8fde\u7eed\u503c\u89c2\u6d4b\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u5347\u7cfb\u7edf\u6a21\u578b\u8d28\u91cf", "topic": "agentic reinforcement learning"}}
{"id": "2509.25760", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25760", "abs": "https://arxiv.org/abs/2509.25760", "authors": ["Zhepei Wei", "Xiao Yang", "Kai Sun", "Jiaqi Wang", "Rulin Shao", "Sean Chen", "Mohammad Kachuee", "Teja Gollapudi", "Tony Liao", "Nicolas Scheffer", "Rakesh Wanga", "Anuj Kumar", "Yu Meng", "Wen-tau Yih", "Xin Luna Dong"], "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning", "comment": null, "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.", "AI": {"tldr": "TruthRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316LLMs\u7684\u771f\u5b9e\u6027\uff0c\u901a\u8fc7\u4e09\u5143\u5956\u52b1\u533a\u5206\u6b63\u786e\u7b54\u6848\u3001\u5e7b\u89c9\u548c\u5f03\u6743\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u51c6\u786e\u6027\u65f6\u5f80\u5f80\u653e\u5927\u5e7b\u89c9\uff0c\u800c\u9f13\u52b1\u5f03\u6743\u7684\u65b9\u6cd5\u53c8\u53ef\u80fd\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u727a\u7272\u6b63\u786e\u7b54\u6848\u3002\u8fd9\u4e24\u79cd\u6781\u7aef\u90fd\u635f\u5bb3\u4e86\u771f\u5b9e\u6027\u3002", "method": "\u4f7f\u7528GRPO\u5b9e\u73b0TruthRL\uff0c\u91c7\u7528\u7b80\u5355\u7684\u4e09\u5143\u5956\u52b1\u673a\u5236\uff0c\u6fc0\u52b1\u6a21\u578b\u4e0d\u4ec5\u63d0\u4f9b\u6b63\u786e\u7b54\u6848\uff0c\u8fd8\u80fd\u5728\u4e0d\u786e\u5b9a\u65f6\u5f03\u6743\u3002", "result": "\u5728\u56db\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTruthRL\u76f8\u6bd4\u666e\u901aRL\u663e\u8457\u51cf\u5c11\u5e7b\u89c928.9%\uff0c\u63d0\u9ad8\u771f\u5b9e\u602721.1%\uff0c\u5728\u5404\u79cd\u9aa8\u5e72\u6a21\u578b\u4e0b\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "TruthRL\u5728\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5f3a\u8c03\u4e86\u5b66\u4e60\u76ee\u6807\u8bbe\u8ba1\u5bf9\u4e8e\u5f00\u53d1\u771f\u5b9eLLMs\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25261", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25261", "abs": "https://arxiv.org/abs/2509.25261", "authors": ["Xianyang Deng", "Wenshuai Liu", "Yaru FuB", "Qi Zhu"], "title": "Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks", "comment": "7 pages, 6 figures", "summary": "Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has\nemerged as a promising paradigm for data collection. However, challenges such\nas spectrum scarcity, device heterogeneity, and user mobility hinder efficient\ncoordination of sensing, communication, and computation. To tackle these\nissues, we propose a joint optimization framework that integrates time slot\npartition for sensing, communication, and computation phases, resource\nallocation, and UAV 3D trajectory planning, aiming to maximize the amount of\nprocessed sensing data. The problem is formulated as a non-convex stochastic\noptimization and further modeled as a partially observable Markov decision\nprocess (POMDP) that can be solved by multi-agent deep reinforcement learning\n(MADRL) algorithm. To overcome the limitations of conventional multi-layer\nperceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor\nnetwork. The newly developed method is based on heterogeneous agent proximal\npolicy optimization (HAPPO), empowered by convolutional neural networks (CNN)\nfor feature extraction and Kolmogorov-Arnold networks (KAN) to capture\nstructured state-action dependencies. Extensive numerical results demonstrate\nthat our proposed method achieves significant improvements in the amount of\nprocessed sensing data when compared with other benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6f14\u5458\u7f51\u7edc\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6570\u636e\u5904\u7406\u91cf", "motivation": "\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u9762\u4e34\u9891\u8c31\u7a00\u7f3a\u3001\u8bbe\u5907\u5f02\u6784\u548c\u7528\u6237\u79fb\u52a8\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u534f\u8c03\u611f\u77e5\u3001\u901a\u4fe1\u548c\u8ba1\u7b97\u8fc7\u7a0b", "method": "\u6784\u5efa\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u5f02\u6784\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408CNN\u7279\u5f81\u63d0\u53d6\u548cKAN\u7f51\u7edc\u6355\u6349\u72b6\u6001-\u52a8\u4f5c\u4f9d\u8d56\u5173\u7cfb", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5904\u7406\u611f\u77e5\u6570\u636e\u91cf\u65b9\u9762\u76f8\u6bd4\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6f14\u5458\u7f51\u7edcMADRL\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u8f85\u52a9\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u4e2d\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2509.25550", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25550", "abs": "https://arxiv.org/abs/2509.25550", "authors": ["Dongsu Lee", "Daehee Lee", "Yaru Niu", "Honguk Woo", "Amy Zhang", "Ding Zhao"], "title": "Learning to Interact in World Latent for Team Coordination", "comment": null, "summary": "This work presents a novel representation learning framework, interactive\nworld latent (IWoL), to facilitate team coordination in multi-agent\nreinforcement learning (MARL). Building effective representation for team\ncoordination is a challenging problem, due to the intricate dynamics emerging\nfrom multi-agent interaction and incomplete information induced by local\nobservations. Our key insight is to construct a learnable representation space\nthat jointly captures inter-agent relations and task-specific world information\nby directly modeling communication protocols. This representation, we maintain\nfully decentralized execution with implicit coordination, all while avoiding\nthe inherent drawbacks of explicit message passing, e.g., slower\ndecision-making, vulnerability to malicious attackers, and sensitivity to\nbandwidth constraints. In practice, our representation can be used not only as\nan implicit latent for each agent, but also as an explicit message for\ncommunication. Across four challenging MARL benchmarks, we evaluate both\nvariants and show that IWoL provides a simple yet powerful key for team\ncoordination. Moreover, we demonstrate that our representation can be combined\nwith existing MARL algorithms to further enhance their performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aIWoL\u7684\u65b0\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u901a\u4fe1\u534f\u8bae\u6765\u6784\u5efa\u8054\u5408\u6355\u6349\u667a\u80fd\u4f53\u95f4\u5173\u7cfb\u548c\u4efb\u52a1\u7279\u5b9a\u4e16\u754c\u4fe1\u606f\u7684\u53ef\u5b66\u4e60\u8868\u793a\u7a7a\u95f4\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u56e2\u961f\u534f\u8c03\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e2\u961f\u534f\u8c03\u7684\u6311\u6218\uff0c\u5305\u62ec\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4ea7\u751f\u7684\u590d\u6742\u52a8\u6001\u548c\u5c40\u90e8\u89c2\u5bdf\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u4fe1\u606f\u95ee\u9898\u3002", "method": "\u6784\u5efa\u53ef\u5b66\u4e60\u8868\u793a\u7a7a\u95f4\uff0c\u76f4\u63a5\u5efa\u6a21\u901a\u4fe1\u534f\u8bae\uff0c\u8054\u5408\u6355\u6349\u667a\u80fd\u4f53\u95f4\u5173\u7cfb\u548c\u4efb\u52a1\u7279\u5b9a\u4e16\u754c\u4fe1\u606f\uff0c\u652f\u6301\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u548c\u9690\u5f0f\u534f\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684MARL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u4e24\u79cd\u53d8\u4f53\uff0c\u8bc1\u660eIWoL\u4e3a\u56e2\u961f\u534f\u8c03\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709MARL\u7b97\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "IWoL\u6846\u67b6\u901a\u8fc7\u9690\u5f0f\u8868\u793a\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u6d88\u606f\u4f20\u9012\u7684\u7f3a\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u7684\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26463", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2509.26463", "abs": "https://arxiv.org/abs/2509.26463", "authors": ["Junsong Pu", "Yichen Li", "Zhuangbin Chen", "Jinyang Liu", "Zhihan Jiang", "Jianjun Chen", "Rui Shi", "Zibin Zheng", "Tieying Zhang"], "title": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems", "comment": "12 pages, 6 figures, 1 table, this paper has been accepted by the\n  40th IEEE/ACM International Conference on Automated Software Engineering, ASE\n  2025", "summary": "Reliability management in cloud service systems is challenging due to the\ncascading effect of failures. Error wrapping, a practice prevalent in modern\nmicroservice development, enriches errors with context at each layer of the\nfunction call stack, constructing an error chain that describes a failure from\nits technical origin to its business impact. However, this also presents a\nsignificant traceability problem when recovering the complete error propagation\npath from the final log message back to its source. Existing approaches are\nineffective at addressing this problem. To fill this gap, we present ErrorPrism\nin this work for automated reconstruction of error propagation paths in\nproduction microservice systems. ErrorPrism first performs static analysis on\nservice code repositories to build a function call graph and map log strings to\nrelevant candidate functions. This significantly reduces the path search space\nfor subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an\niterative backward search to accurately reconstruct the complete, multi-hop\nerror path. Evaluated on 67 production microservices at ByteDance, ErrorPrism\nachieves 97.0% accuracy in reconstructing paths for 102 real-world errors,\noutperforming existing static analysis and LLM-based approaches. ErrorPrism\nprovides an effective and practical tool for root cause analysis in industrial\nmicroservice systems.", "AI": {"tldr": "ErrorPrism\uff1a\u901a\u8fc7\u9759\u6001\u5206\u6790\u548cLLM\u4ee3\u7406\u8fed\u4ee3\u53cd\u5411\u641c\u7d22\uff0c\u81ea\u52a8\u91cd\u5efa\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u8def\u5f84\uff0c\u5728\u5b57\u8282\u8df3\u52a867\u4e2a\u751f\u4ea7\u5fae\u670d\u52a1\u4e2d\u8fbe\u523097.0%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e91\u670d\u52a1\u7cfb\u7edf\u4e2d\u7531\u4e8e\u6545\u969c\u7684\u7ea7\u8054\u6548\u5e94\uff0c\u53ef\u9760\u6027\u7ba1\u7406\u5177\u6709\u6311\u6218\u6027\u3002\u9519\u8bef\u5305\u88c5\u5b9e\u8df5\u867d\u7136\u4e30\u5bcc\u4e86\u9519\u8bef\u4e0a\u4e0b\u6587\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4ece\u6700\u7ec8\u65e5\u5fd7\u6d88\u606f\u56de\u6eaf\u5230\u9519\u8bef\u6e90\u5934\u7684\u53ef\u8ffd\u6eaf\u6027\u95ee\u9898\u3002", "method": "\u9996\u5148\u5bf9\u670d\u52a1\u4ee3\u7801\u5e93\u8fdb\u884c\u9759\u6001\u5206\u6790\uff0c\u6784\u5efa\u51fd\u6570\u8c03\u7528\u56fe\u5e76\u6620\u5c04\u65e5\u5fd7\u5b57\u7b26\u4e32\u5230\u76f8\u5173\u5019\u9009\u51fd\u6570\uff1b\u7136\u540e\u4f7f\u7528LLM\u4ee3\u7406\u8fdb\u884c\u8fed\u4ee3\u53cd\u5411\u641c\u7d22\uff0c\u51c6\u786e\u91cd\u5efa\u5b8c\u6574\u7684\u591a\u8df3\u9519\u8bef\u8def\u5f84\u3002", "result": "\u5728\u5b57\u8282\u8df3\u52a8\u768467\u4e2a\u751f\u4ea7\u5fae\u670d\u52a1\u4e0a\u8bc4\u4f30\uff0cErrorPrism\u5bf9102\u4e2a\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u7684\u91cd\u5efa\u8def\u5f84\u51c6\u786e\u7387\u8fbe\u523097.0%\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u9759\u6001\u5206\u6790\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u3002", "conclusion": "ErrorPrism\u4e3a\u5de5\u4e1a\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "2509.25267", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.25267", "abs": "https://arxiv.org/abs/2509.25267", "authors": ["Jiexi Xu"], "title": "Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning", "comment": "13 pages, 2 figures, 2 tables", "summary": "The performance of Large Language Models (LLMs) depends heavily on the chosen\nprompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or\nChain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly\naccurate strategies like Self-Consistency (SC) incur substantial computational\nwaste on simple tasks, while lightweight methods often fail on complex inputs.\nThis paper introduces the Prompt Policy Network (PPN), a lightweight\nreinforcement learning framework that formalizes adaptive strategy selection as\na single-step Markov Decision Process (MDP). The PPN, trained with Proximal\nPolicy Optimization (PPO) and guided by a resource-explicit reward function,\nlearns to allocate costly reasoning strategies only when necessary. Experiments\non arithmetic reasoning benchmarks demonstrate that PPN achieves superior\nperformance on the efficiency-accuracy Pareto front, delivering up to 61.5%\ntoken cost reduction compared to Self-Consistency while maintaining competitive\naccuracy. This work contributes a systematic, adaptive framework for\ncost-efficient LLM deployment, advancing the design of lightweight optimization\ntechniques for scalable and sustainable language model applications.", "AI": {"tldr": "\u63d0\u51faPrompt Policy Network (PPN)\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u7b56\u7565\u5982Zero-Shot\u3001Few-Shot\u6216Chain-of-Thought\u5b58\u5728\u6548\u7387-\u51c6\u786e\u6027\u7684\u521a\u6027\u6743\u8861\uff0c\u51c6\u786e\u7b56\u7565\u5982Self-Consistency\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u8f7b\u91cf\u65b9\u6cd5\u5728\u590d\u6742\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u5f62\u5f0f\u5316\u4e3a\u5355\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3PPN\uff0c\u91c7\u7528\u8d44\u6e90\u663e\u5f0f\u5956\u52b1\u51fd\u6570\u6307\u5bfc\u5b66\u4e60\u3002", "result": "\u5728\u7b97\u672f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPPN\u5728\u6548\u7387-\u51c6\u786e\u6027\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4Self-Consistency\u51cf\u5c1161.5%\u7684token\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u6027\u3002", "conclusion": "\u4e3a\u6210\u672c\u9ad8\u6548\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u63a8\u8fdb\u4e86\u8f7b\u91cf\u7ea7\u4f18\u5316\u6280\u672f\u7684\u8bbe\u8ba1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26546", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26546", "abs": "https://arxiv.org/abs/2509.26546", "authors": ["Meghana Sistla", "Gogul Balakrishnan", "Pat Rondon", "Jos\u00e9 Cambronero", "Michele Tufano", "Satish Chandra"], "title": "Towards Verified Code Reasoning by LLMs", "comment": "43 pages", "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u9a8c\u8bc1\u4ee3\u7801\u63a8\u7406\u4ee3\u7406\u7b54\u6848\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6765\u68c0\u67e5\u4ee3\u7406\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "LLM\u4ee3\u7801\u4ee3\u7406\u7684\u7b54\u6848\u5e76\u4e0d\u603b\u662f\u6b63\u786e\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u573a\u666f\uff08\u5982\u4ee3\u7801\u7406\u89e3\u3001\u4ee3\u7801\u5ba1\u67e5\u3001\u4ee3\u7801\u751f\u6210\u9a8c\u8bc1\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u4e3a\u9700\u8981\u4eba\u5de5\u9a8c\u8bc1\u7b54\u6848\uff0c\u964d\u4f4e\u4e86\u5f00\u53d1\u6548\u7387\u3002", "method": "\u63d0\u53d6\u4ee3\u7406\u54cd\u5e94\u7684\u5f62\u5f0f\u5316\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6765\u9a8c\u8bc1\u4ee3\u7406\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u572820\u4e2a\u672a\u521d\u59cb\u5316\u53d8\u91cf\u9519\u8bef\u68c0\u6d4b\u4e2d\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6210\u529f\u9a8c\u8bc1\u4e8613/20\u4e2a\u4f8b\u5b50\u7684\u63a8\u7406\uff1b\u572820\u4e2a\u7a0b\u5e8f\u7b49\u4ef7\u6027\u67e5\u8be2\u4e2d\uff0c\u6210\u529f\u6355\u83b7\u4e866/8\u4e2a\u4ee3\u7406\u7684\u9519\u8bef\u5224\u65ad\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u81ea\u52a8\u9a8c\u8bc1\u4ee3\u7801\u63a8\u7406\u4ee3\u7406\u7684\u7b54\u6848\uff0c\u63d0\u9ad8\u4ee3\u7406\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.25586", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25586", "abs": "https://arxiv.org/abs/2509.25586", "authors": ["Jihye Choi", "Jinsung Yoon", "Jiefeng Chen", "Somesh Jha", "Tomas Pfister"], "title": "ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning", "comment": null, "summary": "While Large Language Models (LLMs) have shown remarkable advancements in\nreasoning and tool use, they often fail to generate optimal, grounded solutions\nunder complex constraints. Real-world travel planning exemplifies these\nchallenges, evaluating agents' abilities to handle constraints that are\nexplicit, implicit, and even evolving based on interactions with dynamic\nenvironments and user needs. In this paper, we present ATLAS, a general\nmulti-agent framework designed to effectively handle such complex nature of\nconstraints awareness in real-world travel planning tasks. ATLAS introduces a\nprincipled approach to address the fundamental challenges of constraint-aware\nplanning through dedicated mechanisms for dynamic constraint management,\niterative plan critique, and adaptive interleaved search. ATLAS demonstrates\nstate-of-the-art performance on the TravelPlanner benchmark, improving the\nfinal pass rate from 23.3% to 44.4% over its best alternative. More\nimportantly, our work is the first to demonstrate quantitative effectiveness on\nreal-world travel planning tasks with live information search and multi-turn\nfeedback. In this realistic setting, ATLAS showcases its superior overall\nplanning performance, achieving an 84% final pass rate which significantly\noutperforms baselines including ReAct (59%) and a monolithic agent (27%).", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u65c5\u884c\u89c4\u5212\u4e2d\u7684\u590d\u6742\u7ea6\u675f\uff0c\u901a\u8fc7\u52a8\u6001\u7ea6\u675f\u7ba1\u7406\u3001\u8fed\u4ee3\u8ba1\u5212\u6279\u8bc4\u548c\u81ea\u9002\u5e94\u4ea4\u9519\u641c\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7ea6\u675f\u4e0b\u96be\u4ee5\u751f\u6210\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u5b9e\u65c5\u884c\u89c4\u5212\u9700\u8981\u5904\u7406\u663e\u5f0f\u3001\u9690\u5f0f\u751a\u81f3\u52a8\u6001\u53d8\u5316\u7684\u7ea6\u675f\uff0c\u8fd9\u9700\u8981\u66f4\u6709\u6548\u7684\u7ea6\u675f\u611f\u77e5\u89c4\u5212\u65b9\u6cd5\u3002", "method": "ATLAS\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u7ea6\u675f\u7ba1\u7406\u3001\u8fed\u4ee3\u8ba1\u5212\u6279\u8bc4\u548c\u81ea\u9002\u5e94\u4ea4\u9519\u641c\u7d22\u7b49\u673a\u5236\uff0c\u4e13\u95e8\u5904\u7406\u7ea6\u675f\u611f\u77e5\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u7ec8\u901a\u8fc7\u7387\u4ece23.3%\u63d0\u5347\u523044.4%\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\uff0cATLAS\u8fbe\u523084%\u7684\u6700\u7ec8\u901a\u8fc7\u7387\uff0c\u663e\u8457\u4f18\u4e8eReAct(59%)\u548c\u5355\u4f53\u667a\u80fd\u4f53(27%)\u3002", "conclusion": "ATLAS\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\u5c55\u793a\u5b9a\u91cf\u6709\u6548\u6027\u7684\u6846\u67b6\uff0c\u5176\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u7ea6\u675f\u611f\u77e5\u673a\u5236\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2509.25873", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25873", "abs": "https://arxiv.org/abs/2509.25873", "authors": ["Hankun Dai", "Maoquan Wang", "Mengnan Qi", "Yikai Zhang", "Zijian Jin", "Yongqiang Yao", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu"], "title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly being applied to programming\ntasks, ranging from single-turn code completion to autonomous agents. Current\ncode agent designs frequently depend on complex, hand-crafted workflows and\ntool sets. However, this reliance on elaborate scaffolding presents several\nchallenges: agent performance becomes overly dependent on prompt tuning and\ncustom design choices, heavy human intervention obscures a model's true\nunderlying capabilities, and intricate pipelines are costly to build and\nmaintain. Furthermore, optimizing complex task prompts increases the risk of\ndata leakage. Currently, when introducing new models, LLM providers like OpenAI\nand Anthropic often publish benchmark scores to demonstrate their models'\ncoding proficiency, but keep their proprietary evaluation frameworks\nconfidential. To address these limitations, we introduce Lita (Lite Agent),\nwhich operationalizes liteness, a principle of minimizing manual design while\nretaining the essential elements of a fully autonomous agent. Lita enables a\nmore faithful and unified evaluation without elaborate scaffolding. Experiments\non the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita\nachieves competitive or superior performance compared to workflow-based and\nagentic baselines. Crucially, Lita also consumes fewer tokens and requires\nsignificantly less design effort. Our results suggest that Lita is sufficient\nto reveal the underlying coding competence of modern LLMs. Finally, we propose\nthe Agent Complexity Law: the performance gap between agents of varying\ncomplexity, from simple to sophisticated designs, will shrink as the core model\nimproves, ultimately converging to a negligible difference.", "AI": {"tldr": "Lita (Lite Agent) \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4ee3\u7801\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u624b\u52a8\u8bbe\u8ba1\u6765\u8bc4\u4f30LLM\u7684\u771f\u5b9e\u7f16\u7801\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u5de5\u4f5c\u6d41\u4ee3\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4ee3\u7406\u590d\u6742\u5ea6\u5b9a\u5f8b\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u4ee3\u7406\u8bbe\u8ba1\u4f9d\u8d56\u590d\u6742\u7684\u624b\u5de5\u5de5\u4f5c\u6d41\u548c\u5de5\u5177\u96c6\uff0c\u5bfc\u81f4\u6027\u80fd\u8fc7\u5ea6\u4f9d\u8d56\u63d0\u793a\u8c03\u4f18\u3001\u63a9\u76d6\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u3001\u6784\u5efa\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u4e14\u5b58\u5728\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u5f15\u5165Lita\u6846\u67b6\uff0c\u57fa\u4e8e\"\u8f7b\u91cf\"\u539f\u5219\u6700\u5c0f\u5316\u624b\u52a8\u8bbe\u8ba1\uff0c\u4fdd\u7559\u5b8c\u5168\u81ea\u4e3b\u4ee3\u7406\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u5b9e\u73b0\u66f4\u5fe0\u5b9e\u548c\u7edf\u4e00\u7684\u8bc4\u4f30\u3002", "result": "\u5728Aider Polyglot\u548cSWE-Bench\u6d4b\u8bd5\u4e2d\uff0cLita\u76f8\u6bd4\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u4ee3\u7406\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6d88\u8017\u66f4\u5c11token\u548c\u8bbe\u8ba1\u5de5\u4f5c\u91cf\u3002", "conclusion": "Lita\u8db3\u4ee5\u63ed\u793a\u73b0\u4ee3LLM\u7684\u5e95\u5c42\u7f16\u7801\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4ee3\u7406\u590d\u6742\u5ea6\u5b9a\u5f8b\uff1a\u968f\u7740\u6838\u5fc3\u6a21\u578b\u6539\u8fdb\uff0c\u4e0d\u540c\u590d\u6742\u5ea6\u4ee3\u7406\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u5c06\u7f29\u5c0f\u81f3\u53ef\u5ffd\u7565\u3002", "topic": "agent analysis"}}
{"id": "2509.25300", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25300", "abs": "https://arxiv.org/abs/2509.25300", "authors": ["Zelin Tan", "Hejia Geng", "Mulei Zhang", "Xiaohang Yu", "Guancheng Wan", "Yifan Zhou", "Qiang He", "Xiangyuan Xue", "Heng Zhou", "Yutao Fan", "Zhongzhi Li", "Zaibin Zhang", "Guibin Zhang", "Chen Zhang", "Zhenfei Yin", "Lei Bai"], "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning", "comment": "V1 version", "summary": "While scaling laws for large language models (LLMs) during pre-training have\nbeen extensively studied, their behavior under reinforcement learning (RL)\npost-training remains largely unexplored. This paper presents a systematic\nempirical investigation of scaling behaviors in RL-based post-training, with a\nparticular focus on mathematical reasoning. Based on 54 experiments across\ndiverse model sizes and training settings, we characterize how model scale,\ndata volume, and computational budget interact to shape performance. Our\nanalysis leads to four key findings: (1). Under a fixed computational budget,\nlarger models trained for fewer steps consistently outperform smaller models\ntrained for more steps. (2). Given a fixed amount of training data, larger\nmodels achieve superior sample efficiency, yielding lower loss. (3). In\ndata-constrained regimes, repeated reuse of high-quality data proves highly\neffective, as final performance is primarily governed by the total number of\noptimization steps rather than the uniqueness of samples. (4). These scaling\nbehaviors are robust across both base and instruction-tuned models, which share\nsimilar learning dynamics (e.g., larger models show faster convergence) even\nwhile differing in absolute accuracy. Collectively, these results provide a\nprincipled foundation and practical guidelines for efficiently scaling the\nreasoning capabilities of LLMs through RL post-training.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u53d1\u73b0\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u5927\u6a21\u578b\u5c11\u6b65\u8bad\u7ec3\u4f18\u4e8e\u5c0f\u6a21\u578b\u591a\u6b65\u8bad\u7ec3\uff1b\u5927\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\uff1b\u6570\u636e\u53d7\u9650\u65f6\u91cd\u590d\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u6548\uff1b\u8fd9\u4e9b\u89c4\u5f8b\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e2d\u5747\u9002\u7528\u3002", "motivation": "\u867d\u7136\u9884\u8bad\u7ec3\u9636\u6bb5LLM\u7684\u6269\u5c55\u89c4\u5f8b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6269\u5c55\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "method": "\u57fa\u4e8e54\u4e2a\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u9884\u7b97\u5728RL\u540e\u8bad\u7ec3\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5206\u6790\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u56db\u4e2a\u5173\u952e\u89c4\u5f8b\uff1a1)\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u5927\u6a21\u578b\u5c11\u6b65\u8bad\u7ec3\u66f4\u4f18\uff1b2)\u56fa\u5b9a\u6570\u636e\u91cf\u4e0b\u5927\u6a21\u578b\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff1b3)\u6570\u636e\u53d7\u9650\u65f6\u91cd\u590d\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u6548\uff1b4)\u8fd9\u4e9b\u89c4\u5f8b\u5728\u4e0d\u540c\u7c7b\u578b\u6a21\u578b\u4e2d\u5747\u7a33\u5065\u3002", "conclusion": "\u4e3a\u901a\u8fc7RL\u540e\u8bad\u7ec3\u9ad8\u6548\u6269\u5c55LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26161", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26161", "abs": "https://arxiv.org/abs/2509.26161", "authors": ["Runxin Yang", "Yuxuan Wan", "Shuqing Li", "Michael R. Lyu"], "title": "90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development", "comment": null, "summary": "Developing 3D games requires specialized expertise across multiple domains,\nincluding programming, 3D modeling, and engine configuration, which limits\naccess to millions of potential creators. Recently, researchers have begun to\nexplore automated game development. However, existing approaches face three\nprimary challenges: (1) limited scope to 2D content generation or isolated code\nsnippets; (2) requirement for manual integration of generated components into\ngame engines; and (3) poor performance on handling interactive game logic and\nstate management. While Multimodal Large Language Models (MLLMs) demonstrate\npotential capabilities to ease the game generation task, a critical gap still\nremains in translating these outputs into production-ready, executable game\nprojects based on game engines such as Unity and Unreal Engine.\n  To bridge the gap, this paper introduces UniGen, the first end-to-end\ncoordinated multi-agent framework that automates zero-coding development of\nrunnable 3D games from natural language requirements. Specifically, UniGen uses\na Planning Agent that interprets user requirements into structured blueprints\nand engineered logic descriptions; after which a Generation Agent produces\nexecutable C# scripts; then an Automation Agent handles engine-specific\ncomponent binding and scene construction; and lastly a Debugging Agent provides\nreal-time error correction through conversational interaction. We evaluated\nUniGen on three distinct game prototypes. Results demonstrate that UniGen not\nonly democratizes game creation by requiring no coding from the user, but also\nreduces development time by 91.4%. We release UniGen at\nhttps://github.com/yxwan123/UniGen. A video demonstration is available at\nhttps://www.youtube.com/watch?v=xyJjFfnxUx0.", "AI": {"tldr": "UniGen\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u751f\u6210\u53ef\u8fd0\u884c\u76843D\u6e38\u620f\uff0c\u65e0\u9700\u7528\u6237\u7f16\u5199\u4ee3\u7801\uff0c\u5c06\u5f00\u53d1\u65f6\u95f4\u51cf\u5c1191.4%\u3002", "motivation": "\u89e3\u51b33D\u6e38\u620f\u5f00\u53d1\u9700\u8981\u591a\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e2D\u5185\u5bb9\u751f\u6210\u3001\u9700\u8981\u624b\u52a8\u96c6\u6210\u5230\u6e38\u620f\u5f15\u64ce\u3001\u4ee5\u53ca\u5904\u7406\u4ea4\u4e92\u5f0f\u6e38\u620f\u903b\u8f91\u548c\u72b6\u6001\u7ba1\u7406\u6027\u80fd\u5dee\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u89c4\u5212\u667a\u80fd\u4f53\u5c06\u7528\u6237\u9700\u6c42\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u84dd\u56fe\u548c\u5de5\u7a0b\u903b\u8f91\u63cf\u8ff0\uff0c\u751f\u6210\u667a\u80fd\u4f53\u751f\u4ea7\u53ef\u6267\u884c\u7684C#\u811a\u672c\uff0c\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u5904\u7406\u5f15\u64ce\u7279\u5b9a\u7ec4\u4ef6\u7ed1\u5b9a\u548c\u573a\u666f\u6784\u5efa\uff0c\u8c03\u8bd5\u667a\u80fd\u4f53\u901a\u8fc7\u5bf9\u8bdd\u4ea4\u4e92\u63d0\u4f9b\u5b9e\u65f6\u9519\u8bef\u4fee\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6e38\u620f\u539f\u578b\u4e0a\u8bc4\u4f30\uff0cUniGen\u4e0d\u4ec5\u901a\u8fc7\u65e0\u9700\u7f16\u7801\u5b9e\u73b0\u4e86\u6e38\u620f\u521b\u4f5c\u7684\u6c11\u4e3b\u5316\uff0c\u8fd8\u5c06\u5f00\u53d1\u65f6\u95f4\u51cf\u5c11\u4e8691.4%\u3002", "conclusion": "UniGen\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5230\u53ef\u8fd0\u884c3D\u6e38\u620f\u7684\u96f6\u7f16\u7801\u5f00\u53d1\u81ea\u52a8\u5316\u3002", "topic": "swe application"}}
{"id": "2509.25911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25911", "abs": "https://arxiv.org/abs/2509.25911", "authors": ["Yu Wang", "Ryuichi Takanobu", "Zhiqi Liang", "Yuzhen Mao", "Yuanzhe Hu", "Julian McAuley", "Xiaojian Wu"], "title": "Mem-\u03b1: Learning Memory Construction via Reinforcement Learning", "comment": null, "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mem-alpha\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u667a\u80fd\u4f53\u6709\u6548\u7ba1\u7406\u590d\u6742\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u53cd\u9988\u4f18\u5316\u8bb0\u5fc6\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u589e\u5f3a\u667a\u80fd\u4f53\u4f9d\u8d56\u9884\u5b9a\u4e49\u6307\u4ee4\u548c\u5de5\u5177\u8fdb\u884c\u8bb0\u5fc6\u66f4\u65b0\uff0c\u4f46\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u786e\u5b9a\u5b58\u50a8\u5185\u5bb9\u3001\u7ed3\u6784\u5316\u65b9\u5f0f\u548c\u66f4\u65b0\u65f6\u673a\u7684\u667a\u80fd\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u6784\u5efa\u4e0d\u4f18\u548c\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u667a\u80fd\u4f53\u7ba1\u7406\u590d\u6742\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u6784\u5efa\u4e13\u95e8\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5904\u7406\u5e8f\u5217\u4fe1\u606f\u5757\u5b66\u4e60\u63d0\u53d6\u548c\u5b58\u50a8\u76f8\u5173\u5185\u5bb9\uff0c\u5956\u52b1\u4fe1\u53f7\u6765\u81ea\u4e0b\u6e38\u95ee\u7b54\u51c6\u786e\u6027\u3002", "result": "Mem-alpha\u76f8\u6bd4\u73b0\u6709\u8bb0\u5fc6\u589e\u5f3a\u667a\u80fd\u4f53\u57fa\u7ebf\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728\u4ec5\u8bad\u7ec330k\u4ee4\u724c\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6cdb\u5316\u5904\u7406\u8d85\u8fc7400k\u4ee4\u724c\u7684\u5e8f\u5217\uff0c\u662f\u8bad\u7ec3\u957f\u5ea6\u768413\u500d\u4ee5\u4e0a\u3002", "conclusion": "Mem-alpha\u6846\u67b6\u80fd\u6709\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\u7ba1\u7406\u590d\u6742\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e0b\u7684\u4fe1\u606f\u7406\u89e3\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26476", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26476", "abs": "https://arxiv.org/abs/2509.26476", "authors": ["Yash Akhauri", "Xingyou Song", "Arissa Wongpanich", "Bryan Lewandowski", "Mohamed S. Abdelfattah"], "title": "Regression Language Models for Code", "comment": null, "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u56de\u5f52\u8bed\u8a00\u6a21\u578b(RLM)\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u4ee3\u7801\u6587\u672c\u9884\u6d4b\u591a\u79cd\u6267\u884c\u6307\u6807\uff0c\u5305\u62ec\u5185\u5b58\u5360\u7528\u3001GPU\u5185\u6838\u5ef6\u8fdf\u548c\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u7f16\u7a0b\u8bed\u8a00\u548c\u786c\u4ef6\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5230\u6307\u6807\u56de\u5f52\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7e41\u91cd\u4e14\u9886\u57df\u7279\u5b9a\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u63a2\u7d22\u7edf\u4e00\u6a21\u578b\u9884\u6d4b\u591a\u79cd\u4ee3\u7801\u6267\u884c\u6307\u6807\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eT5Gemma\u7684300M\u53c2\u6570\u56de\u5f52\u8bed\u8a00\u6a21\u578b(RLM)\uff0c\u76f4\u63a5\u4ece\u4ee3\u7801\u6587\u672c\u9884\u6d4b\u6570\u503c\u7ed3\u679c\uff0c\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u786c\u4ef6\u5e73\u53f0\u3002", "result": "\u5728APPS\u7ade\u4e89\u7f16\u7a0b\u63d0\u4ea4\u4e0a\u83b7\u5f97>0.9\u7684Spearman\u79e9\u76f8\u5173\u7cfb\u6570\uff0c\u5728CodeNet\u768417\u79cd\u8bed\u8a00\u4e0a\u5e73\u5747Spearman\u79e9\u76f8\u5173\u7cfb\u6570>0.5\uff0c\u5728\u4e94\u4e2a\u7ecf\u5178NAS\u8bbe\u8ba1\u7a7a\u95f4\u4e0a\u83b7\u5f97\u6700\u9ad80.46\u7684\u5e73\u5747Kendall-Tau\u7cfb\u6570\u3002", "conclusion": "\u7edf\u4e00\u7684\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u591a\u79cd\u4ee3\u7801\u6267\u884c\u6307\u6807\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2509.25598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25598", "abs": "https://arxiv.org/abs/2509.25598", "authors": ["Peiran Xu", "Zhuohao Li", "Xiaoying Xing", "Guannan Zhang", "Debiao Li", "Kunyu Shi"], "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools such as\nsearch engines to solve complex agentic tasks that require reasoning and\nexternal knowledge retrieval. Recently, reinforcement learning with verifiable\nrewards (RLVR) has demonstrated its effectiveness in advancing capabilities of\nLLMs by rewarding the final answers via outcome rewards. While straightforward\nto supervise, outcome rewards only provide sparse signals and delayed feedback,\nwhich limits their effectiveness on long trajectories. Process rewards address\nthis by evaluating intermediate steps, providing fine-grained supervision and\nencouraging grounded problem solving. However, it is notoriously hard to\nannotate step-wise labels, especially in non-verifiable process without\n\"golden\" answers. Furthermore, step-wise judgment requires the balance between\nlocal quality with contribution to the final outcome, as optimizing towards\nhigher process reward may not always align with better final outcomes. To\naddress the above challenges, we introduce Principle Process Reward (PPR), an\nRL approach that unifies principled step-level assessment and outcome\nverification. We train a principle-based reward model to improve the\ntransparency and reliability of process evaluation, and further introduce a\nReward Normalization (ReNorm) strategy to calibrate outcome and process\nrewards. Experiment results show that PPR achieves state-of-the-art performance\nacross a wide range of benchmarks, demonstrating its impressive robustness and\ngeneralization. Our code and model collection is available in this link.", "AI": {"tldr": "\u63d0\u51faPPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u539f\u5219\u6027\u6b65\u9aa4\u8bc4\u4f30\u548c\u7ed3\u679c\u9a8c\u8bc1\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fc7\u7a0b\u5956\u52b1\u548c\u7ed3\u679c\u5956\u52b1\u7684\u5e73\u8861\u95ee\u9898", "motivation": "\u89e3\u51b3LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7ed3\u679c\u5956\u52b1\u7a00\u758f\u4e14\u5ef6\u8fdf\u53cd\u9988\uff0c\u4ee5\u53ca\u8fc7\u7a0b\u5956\u52b1\u96be\u4ee5\u6807\u6ce8\u4e14\u53ef\u80fd\u4e0e\u6700\u7ec8\u7ed3\u679c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "method": "\u5f15\u5165\u539f\u5219\u8fc7\u7a0b\u5956\u52b1(PPR)\uff0c\u8bad\u7ec3\u57fa\u4e8e\u539f\u5219\u7684\u5956\u52b1\u6a21\u578b\u63d0\u9ad8\u8fc7\u7a0b\u8bc4\u4f30\u900f\u660e\u5ea6\uff0c\u5e76\u4f7f\u7528\u5956\u52b1\u5f52\u4e00\u5316\u7b56\u7565\u6821\u51c6\u7ed3\u679c\u548c\u8fc7\u7a0b\u5956\u52b1", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u51fa\u8272\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "PPR\u65b9\u6cd5\u6709\u6548\u7edf\u4e00\u4e86\u8fc7\u7a0b\u8bc4\u4f30\u548c\u7ed3\u679c\u9a8c\u8bc1\uff0c\u4e3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bad\u7ec3\u6846\u67b6", "topic": "agentic reinforcement learning"}}
{"id": "2509.25609", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25609", "abs": "https://arxiv.org/abs/2509.25609", "authors": ["Manuel Cherep", "Chengtian Ma", "Abigail Xu", "Maya Shaked", "Pattie Maes", "Nikhil Singh"], "title": "A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments", "comment": "23 pages, 13 figures", "summary": "Environments built for people are increasingly operated by a new class of\neconomic actors: LLM-powered software agents making decisions on our behalf.\nThese decisions range from our purchases to travel plans to medical treatment\nselection. Current evaluations of these agents largely focus on task\ncompetence, but we argue for a deeper assessment: how these agents choose when\nfaced with realistic decisions. We introduce ABxLab, a framework for\nsystematically probing agentic choice through controlled manipulations of\noption attributes and persuasive cues. We apply this to a realistic web-based\nshopping environment, where we vary prices, ratings, and psychological nudges,\nall of which are factors long known to shape human choice. We find that agent\ndecisions shift predictably and substantially in response, revealing that\nagents are strongly biased choosers even without being subject to the cognitive\nconstraints that shape human biases. This susceptibility reveals both risk and\nopportunity: risk, because agentic consumers may inherit and amplify human\nbiases; opportunity, because consumer choice provides a powerful testbed for a\nbehavioral science of AI agents, just as it has for the study of human\nbehavior. We release our framework as an open benchmark for rigorous, scalable\nevaluation of agent decision-making.", "AI": {"tldr": "ABxLab\u6846\u67b6\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u8f6f\u4ef6\u4ee3\u7406\u5728\u9762\u4e34\u73b0\u5b9e\u51b3\u7b56\u65f6\u7684\u9009\u62e9\u884c\u4e3a\uff0c\u901a\u8fc7\u63a7\u5236\u9009\u9879\u5c5e\u6027\u548c\u5fc3\u7406\u6697\u793a\u6765\u63ed\u793a\u4ee3\u7406\u7684\u51b3\u7b56\u504f\u89c1\u3002", "motivation": "\u5f53\u524d\u5bf9\u8f6f\u4ef6\u4ee3\u7406\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5728\u73b0\u5b9e\u51b3\u7b56\u4e2d\u5982\u4f55\u9009\u62e9\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u4ee3\u7406\u662f\u5426\u7ee7\u627f\u548c\u653e\u5927\u4e86\u4eba\u7c7b\u7684\u51b3\u7b56\u504f\u89c1\u3002", "method": "\u5f00\u53d1ABxLab\u6846\u67b6\uff0c\u5728\u57fa\u4e8e\u7f51\u9875\u7684\u8d2d\u7269\u73af\u5883\u4e2d\u7cfb\u7edf\u64cd\u7eb5\u4ef7\u683c\u3001\u8bc4\u5206\u548c\u5fc3\u7406\u6697\u793a\u7b49\u5f71\u54cd\u4eba\u7c7b\u9009\u62e9\u7684\u56e0\u7d20\uff0c\u89c2\u5bdf\u4ee3\u7406\u7684\u51b3\u7b56\u53d8\u5316\u3002", "result": "\u4ee3\u7406\u51b3\u7b56\u5728\u4ef7\u683c\u3001\u8bc4\u5206\u548c\u5fc3\u7406\u6697\u793a\u7684\u5f71\u54cd\u4e0b\u53d1\u751f\u53ef\u9884\u6d4b\u4e14\u663e\u8457\u7684\u504f\u79fb\uff0c\u663e\u793a\u4ee3\u7406\u5373\u4f7f\u4e0d\u53d7\u4eba\u7c7b\u8ba4\u77e5\u9650\u5236\u4e5f\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u9009\u62e9\u504f\u89c1\u3002", "conclusion": "\u4ee3\u7406\u6d88\u8d39\u8005\u7684\u51b3\u7b56\u504f\u89c1\u65e2\u5e26\u6765\u98ce\u9669\uff08\u53ef\u80fd\u7ee7\u627f\u548c\u653e\u5927\u4eba\u7c7b\u504f\u89c1\uff09\uff0c\u4e5f\u63d0\u4f9b\u673a\u4f1a\uff08\u4e3aAI\u4ee3\u7406\u884c\u4e3a\u79d1\u5b66\u63d0\u4f9b\u5f3a\u5927\u6d4b\u8bd5\u5e73\u53f0\uff09\u3002", "topic": "agent analysis"}}
{"id": "2509.25380", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25380", "abs": "https://arxiv.org/abs/2509.25380", "authors": ["Shane Bergsma", "Nolan Dey", "Joel Hestness"], "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs", "comment": null, "summary": "Data curriculums have become central to successful LLM training, yet\nprinciples governing optimal data placement remain unclear. We introduce the\n*training re-evaluation curve (TREC)*, a diagnostic that retrospectively\nevaluates training batches *using the final model weights*. The TREC\ncharacterizes how well a trained model retains training data as a function of\n*when* the data was encountered during training. Analyzing TRECs for models\nfrom 111M to 3.9B parameters, we show that placing high-quality data at low\npoints on the TREC significantly improves performance. Importantly, while a\nTREC is initially observable only after training, we demonstrate it can be\n*predicted in advance* from AdamW's implicit EMA coefficients, enabling\nproactive curriculum design. By predicting TRECs for published training\nrecipes, we explain prior ablations and reveal suboptimal data placements. We\nalso align high-quality data with TREC minima in order to improve continual\npre-training of a 3.9B-parameter LLM trained on 900B tokens.", "AI": {"tldr": "\u63d0\u51fa\u8bad\u7ec3\u91cd\u8bc4\u4f30\u66f2\u7ebf(TREC)\u6765\u8bca\u65ad\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u95f4\u5b89\u6392\uff0c\u901a\u8fc7\u9884\u6d4bTREC\u4f18\u5316\u6570\u636e\u653e\u7f6e\uff0c\u63d0\u5347LLM\u8bad\u7ec3\u6548\u679c", "motivation": "\u6570\u636e\u8bfe\u7a0b\u5bf9LLM\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6700\u4f18\u6570\u636e\u653e\u7f6e\u539f\u5219\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u95f4\u5b89\u6392", "method": "\u5f15\u5165TREC\u66f2\u7ebf\uff0c\u4f7f\u7528\u6700\u7ec8\u6a21\u578b\u6743\u91cd\u56de\u987e\u6027\u8bc4\u4f30\u8bad\u7ec3\u6279\u6b21\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4fdd\u7559\u7a0b\u5ea6\u4e0e\u8bad\u7ec3\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5229\u7528AdamW\u7684\u9690\u5f0fEMA\u7cfb\u6570\u9884\u6d4bTREC", "result": "\u5206\u6790111M\u52303.9B\u53c2\u6570\u6a21\u578b\u53d1\u73b0\uff0c\u5c06\u9ad8\u8d28\u91cf\u6570\u636e\u653e\u7f6e\u5728TREC\u4f4e\u70b9\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6210\u529f\u6539\u8fdb3.9B\u53c2\u6570LLM\u5728900B token\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3", "conclusion": "TREC\u4e3a\u6570\u636e\u8bfe\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u9884\u6d4b\u548c\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u95f4\u5b89\u6392\uff0c\u89e3\u91ca\u5148\u524d\u6d88\u878d\u5b9e\u9a8c\u7ed3\u679c\u5e76\u63ed\u793a\u6b21\u4f18\u6570\u636e\u653e\u7f6e", "topic": "agent analysis"}}
{"id": "2509.25643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25643", "abs": "https://arxiv.org/abs/2509.25643", "authors": ["Justin Chavarria", "Rohan Raizada", "Justin White", "Eyad Alhetairshi"], "title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models", "comment": null, "summary": "We introduce SOCK, a benchmark command line interface (CLI) that measures\nlarge language models' (LLMs) ability to self-replicate without human\nintervention. In this benchmark, self-replication is defined not only as an\nLLM's ability to create a functioning and running copy of itself, but also the\nability for that self-replication to persist and occur across different\ncomputational contexts. Accordingly, we've developed a system to categorize\nLLMs based on broad self-replication capabilities in two general classes,\nReplication-Capability Levels (RCL) and Persistence-Capability Levels (PCL).\nUsing a five-task suite based on practically manipulable modern CLI utilities\nand computer processes, experiments are orchestrated in a controlled\nenvironment with an LLM acting agentically. The performance of the LLM on agent\ntasks is then computed to produce an R-score (a quantitative evaluation of\noverall self-replication ability) and data used to categorize LLMs into\nspecific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides\nthe first formalized definitions and benchmark suite for evaluating LLM\nself-replication, with the goal of establishing a standard for future research,\nto our knowledge; (2) Allows the industry to track the effectiveness of future\nmulti-agent systems and mitigate potential self-replication threat vectors\nwithin them. The results compiled from evaluating a variety of open-weight and\nproprietary frontier models reveal significant obstacles to persistent\nself-replication and multi-agent systems, including context retention and\nmulti-agent decision-making. We propose future research directions to safely\nreduce the severity of these obstacles, potentially lowering future risk of\nmore functional multi-agent systems.", "AI": {"tldr": "SOCK\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u590d\u5236\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u901a\u8fc7\u547d\u4ee4\u884c\u754c\u9762\u6d4b\u91cfLLM\u5728\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u60c5\u51b5\u4e0b\u7684\u81ea\u6211\u590d\u5236\u80fd\u529b\uff0c\u5e76\u5b9a\u4e49\u4e86\u590d\u5236\u80fd\u529b\u7b49\u7ea7\u548c\u6301\u4e45\u6027\u80fd\u529b\u7b49\u7ea7\u7684\u5206\u7c7b\u6807\u51c6\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u590d\u5236\u80fd\u529b\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6765\u8ddf\u8e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6709\u6548\u6027\u5e76\u51cf\u8f7b\u6f5c\u5728\u7684\u81ea\u6211\u590d\u5236\u5a01\u80c1\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u73b0\u4ee3CLI\u5de5\u5177\u7684\u4e94\u4efb\u52a1\u5957\u4ef6\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8ba9LLM\u4ee5\u667a\u80fd\u4f53\u65b9\u5f0f\u6267\u884c\u4efb\u52a1\uff0c\u901a\u8fc7\u6027\u80fd\u8ba1\u7b97\u5f97\u51faR\u5206\u6570\uff0c\u5e76\u5c06LLM\u5206\u7c7b\u5230RCL-PCL\u77e9\u9635\u4e2d\u3002", "result": "\u8bc4\u4f30\u5404\u79cd\u5f00\u6e90\u548c\u4e13\u6709\u524d\u6cbf\u6a21\u578b\u7684\u7ed3\u679c\u663e\u793a\uff0c\u6301\u4e45\u6027\u81ea\u6211\u590d\u5236\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u91cd\u5927\u969c\u788d\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u95ee\u9898\u3002", "conclusion": "SOCK\u4e3a\u8bc4\u4f30LLM\u81ea\u6211\u590d\u5236\u63d0\u4f9b\u4e86\u9996\u4e2a\u6b63\u5f0f\u5b9a\u4e49\u548c\u57fa\u51c6\u5957\u4ef6\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u5b89\u5168\u964d\u4f4e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "2509.25651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25651", "abs": "https://arxiv.org/abs/2509.25651", "authors": ["Gihan Panapitiya", "Emily Saldanha", "Heather Job", "Olivia Hess"], "title": "AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation", "comment": null, "summary": "The automation of chemical research through self-driving laboratories (SDLs)\npromises to accelerate scientific discovery, yet the reliability and granular\nperformance of the underlying AI agents remain critical, under-examined\nchallenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent\narchitecture designed to autonomously translate natural-language instructions\ninto executable protocols for a high-throughput liquid handler. The system\nengages users in dialogue, decomposes experimental goals into discrete tasks\nfor specialized agents, performs tool-assisted stoichiometric calculations, and\niteratively self-corrects its output before generating a hardware-ready file.\nWe present a comprehensive evaluation framework featuring five benchmark\nexperiments of increasing complexity, from simple sample preparation to\nmulti-plate timed syntheses. Through a systematic ablation study of 20 agent\nconfigurations, we assess the impact of reasoning capacity, architectural\ndesign (single- vs. multi-agent), tool use, and self-correction mechanisms. Our\nresults demonstrate that agent reasoning capacity is the most critical factor\nfor success, reducing quantitative errors in chemical amounts (nRMSE) by over\n85% in complex tasks. When combined with a multi-agent architecture and\niterative self-correction, AutoLabs achieves near-expert procedural accuracy\n(F1-score > 0.89) on challenging multi-step syntheses. These findings establish\na clear blueprint for developing robust and trustworthy AI partners for\nautonomous laboratories, highlighting the synergistic effects of modular\ndesign, advanced reasoning, and self-correction to ensure both performance and\nreliability in high-stakes scientific applications. Code:\nhttps://github.com/pnnl/autolabs", "AI": {"tldr": "AutoLabs\u662f\u4e00\u4e2a\u81ea\u6821\u6b63\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u81ea\u52a8\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u9ad8\u901a\u91cf\u6db2\u4f53\u5904\u7406\u534f\u8bae\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5206\u89e3\u5b9e\u9a8c\u4efb\u52a1\u3001\u5de5\u5177\u8f85\u52a9\u8ba1\u7b97\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u6765\u63d0\u9ad8\u5316\u5b66\u5b9e\u9a8c\u7684\u81ea\u52a8\u5316\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u4e2dAI\u667a\u80fd\u4f53\u7684\u53ef\u9760\u6027\u548c\u7ec6\u7c92\u5ea6\u6027\u80fd\u662f\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u53ef\u4fe1\u7684AI\u4f19\u4f34\u6765\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5206\u89e3\u5b9e\u9a8c\u76ee\u6807\u3001\u5de5\u5177\u8f85\u52a9\u5316\u5b66\u8ba1\u91cf\u8ba1\u7b97\u3001\u8fed\u4ee3\u81ea\u6821\u6b63\u673a\u5236\uff0c\u6700\u7ec8\u751f\u6210\u786c\u4ef6\u5c31\u7eea\u6587\u4ef6\u3002\u901a\u8fc720\u79cd\u667a\u80fd\u4f53\u914d\u7f6e\u7684\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u3001\u67b6\u6784\u8bbe\u8ba1\u3001\u5de5\u5177\u4f7f\u7528\u548c\u81ea\u6821\u6b63\u673a\u5236\u7684\u5f71\u54cd\u3002", "result": "\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u662f\u6700\u5173\u952e\u7684\u6210\u529f\u56e0\u7d20\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c06\u5316\u5b66\u91cf\u7684\u5b9a\u91cf\u8bef\u5dee\uff08nRMSE\uff09\u964d\u4f4e\u4e8685%\u4ee5\u4e0a\u3002\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\uff0cAutoLabs\u5728\u6311\u6218\u6027\u591a\u6b65\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u7684\u7a0b\u5e8f\u51c6\u786e\u6027\uff08F1\u5206\u6570>0.89\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u7a33\u5065\u53ef\u4fe1\u7684\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4AI\u4f19\u4f34\u63d0\u4f9b\u4e86\u660e\u786e\u84dd\u56fe\uff0c\u5f3a\u8c03\u4e86\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u5148\u8fdb\u63a8\u7406\u548c\u81ea\u6821\u6b63\u7684\u534f\u540c\u6548\u5e94\uff0c\u786e\u4fdd\u9ad8\u98ce\u9669\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.26048", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26048", "abs": "https://arxiv.org/abs/2509.26048", "authors": ["Daocheng Fu", "Jianbiao Mei", "Licheng Wen", "Xuemeng Yang", "Cheng Yang", "Rong Wu", "Tao Hu", "Siqi Li", "Yufan Shen", "Xinyu Cai", "Pinlong Cai", "Botian Shi", "Yong Liu", "Yu Qiao"], "title": "RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection", "comment": "16 pages, 7 figures", "summary": "Large language models (LLMs) excel at knowledge-intensive question answering\nand reasoning, yet their real-world deployment remains constrained by knowledge\ncutoff, hallucination, and limited interaction modalities. Augmenting LLMs with\nexternal search tools helps alleviate these issues, but it also exposes agents\nto a complex search environment in which small, plausible variations in query\nformulation can steer reasoning into unproductive trajectories and amplify\nerrors. We present a systematic analysis that quantifies how environmental\ncomplexity induces fragile search behaviors and, in turn, degrades overall\nperformance. To address this challenge, we propose a simple yet effective\napproach to instantiate a search agent, RE-Searcher. During search, RE-Searcher\nexplicitly articulates a concrete search goal and subsequently reflects on\nwhether the retrieved evidence satisfies that goal. This combination of\ngoal-oriented planning and self-reflection enables RE-Searcher to resist\nspurious cues in complex search environments and perform robust search.\nExtensive experiments show that our method improves search accuracy and\nachieves state-of-the-art results. Perturbation studies further demonstrate\nsubstantial resilience to noisy or misleading external signals, mitigating the\nfragility of the search process. We believe these findings offer practical\nguidance for integrating LLM-powered agents into more complex interactive\nenvironments and enabling more autonomous decision-making.", "AI": {"tldr": "\u63d0\u51faRE-Searcher\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u6765\u589e\u5f3aLLM\u641c\u7d22\u4ee3\u7406\u5728\u590d\u6742\u641c\u7d22\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u77e5\u8bc6\u622a\u6b62\u3001\u5e7b\u89c9\u548c\u4ea4\u4e92\u6a21\u5f0f\u9650\u5236\u3002\u867d\u7136\u5916\u90e8\u641c\u7d22\u5de5\u5177\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u590d\u6742\u641c\u7d22\u73af\u5883\u4e2d\u7684\u5fae\u5c0f\u67e5\u8be2\u53d8\u5316\u4f1a\u5bfc\u81f4\u63a8\u7406\u504f\u79bb\u548c\u9519\u8bef\u653e\u5927\u3002", "method": "RE-Searcher\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u660e\u786e\u8868\u8fbe\u5177\u4f53\u641c\u7d22\u76ee\u6807\uff0c\u5e76\u53cd\u601d\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u662f\u5426\u6ee1\u8db3\u8be5\u76ee\u6807\u3002\u7ed3\u5408\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u6765\u62b5\u6297\u590d\u6742\u73af\u5883\u4e2d\u7684\u865a\u5047\u7ebf\u7d22\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u641c\u7d22\u51c6\u786e\u6027\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u6270\u52a8\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5bf9\u566a\u58f0\u6216\u8bef\u5bfc\u6027\u5916\u90e8\u4fe1\u53f7\u7684\u5f3a\u97e7\u6027\uff0c\u7f13\u89e3\u4e86\u641c\u7d22\u8fc7\u7a0b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5c06LLM\u9a71\u52a8\u7684\u4ee3\u7406\u96c6\u6210\u5230\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\u5e76\u5b9e\u73b0\u66f4\u81ea\u4e3b\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2509.25655", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25655", "abs": "https://arxiv.org/abs/2509.25655", "authors": ["Dongsheng Yang", "Meiling Zhu", "Yinfeng Yu"], "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation", "comment": "Accepted for publication by International Conference on Intelligent\n  Computing 2025", "summary": "Vision-and-language navigation is one of the core tasks in embodied\nintelligence, requiring an agent to autonomously navigate in an unfamiliar\nenvironment based on natural language instructions. However, existing methods\noften fail to match instructions with environmental information in complex\nscenarios, one reason being the lack of common-sense reasoning ability. This\npaper proposes a vision-and-language navigation method called Landmark-Guided\nKnowledge (LGK), which introduces an external knowledge base to assist\nnavigation, addressing the misjudgment issues caused by insufficient common\nsense in traditional methods. Specifically, we first construct a knowledge base\ncontaining 630,000 language descriptions and use knowledge Matching to align\nenvironmental subviews with the knowledge base, extracting relevant descriptive\nknowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism,\nwhich guides the agent to focus on the most relevant parts of the knowledge by\nleveraging landmark information in the instructions, thereby reducing the data\nbias that may arise from incorporating external knowledge. Finally, we propose\nKnowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates\nlanguage, knowledge, vision, and historical information. Experimental results\ndemonstrate that the LGK method outperforms existing state-of-the-art methods\non the R2R and REVERIE vision-and-language navigation datasets, particularly in\nterms of navigation error, success rate, and path efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5730\u6807\u5f15\u5bfc\u77e5\u8bc6\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5LGK\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u5e93\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u5bfc\u81f4\u7684\u8bef\u5224\u95ee\u9898\uff0c\u5728R2R\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u5339\u914d\u6307\u4ee4\u4e0e\u73af\u5883\u4fe1\u606f\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u8bef\u5224\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b63\u4e07\u8bed\u8a00\u63cf\u8ff0\u7684\u77e5\u8bc6\u5e93\uff0c\u4f7f\u7528\u77e5\u8bc6\u5339\u914d\u5bf9\u9f50\u73af\u5883\u5b50\u89c6\u56fe\uff1b\u8bbe\u8ba1\u5730\u6807\u5f15\u5bfc\u77e5\u8bc6\u673a\u5236(KGL)\u805a\u7126\u76f8\u5173\u77e5\u8bc6\uff1b\u63d0\u51fa\u77e5\u8bc6\u5f15\u5bfc\u52a8\u6001\u589e\u5f3a(KGDA)\u6574\u5408\u8bed\u8a00\u3001\u77e5\u8bc6\u3001\u89c6\u89c9\u548c\u5386\u53f2\u4fe1\u606f\u3002", "result": "\u5728R2R\u548cREVERIE\u6570\u636e\u96c6\u4e0a\uff0cLGK\u65b9\u6cd5\u5728\u5bfc\u822a\u8bef\u5dee\u3001\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u5e93\u548c\u5730\u6807\u5f15\u5bfc\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd\uff0c\u89e3\u51b3\u5e38\u8bc6\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.26062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26062", "abs": "https://arxiv.org/abs/2509.26062", "authors": ["Yanbo Wang", "Zixiang Xu", "Yue Huang", "Xiangqi Wang", "Zirui Song", "Lang Gao", "Chenxi Wang", "Xiangru Tang", "Yue Zhao", "Arman Cohan", "Xiangliang Zhang", "Xiuying Chen"], "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "comment": null, "summary": "Agent systems based on large language models (LLMs) have shown great\npotential in complex reasoning tasks, but building efficient and generalizable\nworkflows remains a major challenge. Most existing approaches rely on manually\ndesigned processes, which limits their adaptability across different tasks.\nWhile a few methods attempt automated workflow generation, they are often tied\nto specific datasets or query types and make limited use of intermediate\nfeedback, reducing system robustness and reasoning depth. Moreover, their\noperations are typically predefined and inflexible. To address these\nlimitations, we propose DyFlow, a dynamic workflow generation framework that\nadaptively constructs and adjusts reasoning procedures based on task\nrequirements and real-time intermediate feedback, thereby enhancing cross-task\ngeneralization. DyFlow consists of two core components: a designer and an\nexecutor. The designer decomposes complex problems into a sequence of sub-goals\ndefined by high-level objectives and dynamically plans the next steps based on\nintermediate outputs and feedback. These plans are then carried out by the\nexecutor, which executes each operation using dynamic operators with\ncontext-aware parameterization, enabling flexible and semantically grounded\nreasoning. We systematically evaluate DyFlow across diverse domains, including\nsocial reasoning, biomedical tasks, mathematical problem solving, and code\ngeneration. Results demonstrate that DyFlow significantly outperforms existing\nbaselines, achieving substantial Pass@k improvements and exhibiting robust\ngeneralization across diverse domains. The code is publicly available at\nhttps://github.com/wyf23187/DyFlow.", "AI": {"tldr": "DyFlow\u662f\u4e00\u4e2a\u52a8\u6001\u5de5\u4f5c\u6d41\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5668\u548c\u6267\u884c\u5668\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u548c\u5b9e\u65f6\u53cd\u9988\u81ea\u9002\u5e94\u6784\u5efa\u548c\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u3002\u5c11\u6570\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\u6216\u67e5\u8be2\u7c7b\u578b\uff0c\u4e14\u5bf9\u4e2d\u95f4\u53cd\u9988\u5229\u7528\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u53d7\u9650\u3002", "method": "DyFlow\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8bbe\u8ba1\u5668\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\u5e8f\u5217\uff0c\u57fa\u4e8e\u4e2d\u95f4\u8f93\u51fa\u548c\u53cd\u9988\u52a8\u6001\u89c4\u5212\u4e0b\u4e00\u6b65\uff1b\u6267\u884c\u5668\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u53c2\u6570\u5316\u7684\u52a8\u6001\u64cd\u4f5c\u7b26\u6267\u884c\u6bcf\u4e2a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u7075\u6d3b\u4e14\u8bed\u4e49\u57fa\u7840\u7684\u63a8\u7406\u3002", "result": "\u5728\u793e\u4ea4\u63a8\u7406\u3001\u751f\u7269\u533b\u5b66\u4efb\u52a1\u3001\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u548c\u4ee3\u7801\u751f\u6210\u7b49\u591a\u4e2a\u9886\u57df\u8bc4\u4f30\u663e\u793a\uff0cDyFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684Pass@k\u6539\u8fdb\uff0c\u5e76\u5728\u4e0d\u540c\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DyFlow\u901a\u8fc7\u52a8\u6001\u5de5\u4f5c\u6d41\u751f\u6210\u548c\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2509.26072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26072", "abs": "https://arxiv.org/abs/2509.26072", "authors": ["Arash Marioriyad", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "title": "The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge", "comment": "9 Pages, 5 Tables, 1 Figures", "summary": "Large language models (LLMs) are increasingly deployed as automatic judges to\nevaluate system outputs in tasks such as summarization, dialogue, and creative\nwriting. A faithful judge should base its verdicts solely on response quality\nand explicitly acknowledge the factors shaping its decision. We show that\ncurrent LLM judges fail on both counts by relying on shortcuts introduced in\nthe prompt. Our study uses two evaluation datasets: ELI5, a benchmark for\nlong-form question answering, and LitBench, a recent benchmark for creative\nwriting. Both datasets provide pairwise comparisons, where the evaluator must\nchoose which of two responses is better. From each dataset we construct 100\npairwise judgment tasks and employ two widely used models, GPT-4o and\nGemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair,\nwe assign superficial cues to the responses, provenance cues indicating source\nidentity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal\norigin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed.\nResults reveal consistent verdict shifts: both models exhibit a strong recency\nbias, systematically favoring new responses over old, as well as a clear\nprovenance hierarchy (Expert > Human > LLM > Unknown). These biases are\nespecially pronounced in GPT-4o and in the more subjective and open-ended\nLitBench domain. Crucially, cue acknowledgment is rare: justifications almost\nnever reference the injected cues, instead rationalizing decisions in terms of\ncontent qualities. These findings demonstrate that current LLM-as-a-judge\nsystems are shortcut-prone and unfaithful, undermining their reliability as\nevaluators in both research and deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u5224\u65ad\u7cfb\u7edf\u8f93\u51fa\u8d28\u91cf\u65f6\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u4f1a\u4f9d\u8d56\u63d0\u793a\u4e2d\u7684\u8868\u9762\u7ebf\u7d22\uff08\u5982\u6765\u6e90\u8eab\u4efd\u548c\u65f6\u95f4\u65b0\u65e7\uff09\u800c\u975e\u5b9e\u9645\u5185\u5bb9\u8d28\u91cf\u505a\u51fa\u5224\u65ad\uff0c\u4e14\u5f88\u5c11\u627f\u8ba4\u8fd9\u4e9b\u504f\u89c1\u56e0\u7d20\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u81ea\u52a8\u8bc4\u4f30\u8005\u6765\u8bc4\u5224\u6458\u8981\u3001\u5bf9\u8bdd\u548c\u521b\u610f\u5199\u4f5c\u7b49\u4efb\u52a1\u7684\u7cfb\u7edf\u8f93\u51fa\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9bLLM\u8bc4\u4f30\u8005\u662f\u5426\u80fd\u591f\u57fa\u4e8e\u54cd\u5e94\u8d28\u91cf\u505a\u51fa\u5fe0\u5b9e\u5224\u65ad\uff0c\u5e76\u660e\u786e\u627f\u8ba4\u5f71\u54cd\u5176\u51b3\u7b56\u7684\u56e0\u7d20\u3002", "method": "\u4f7f\u7528ELI5\uff08\u957f\u5f62\u5f0f\u95ee\u7b54\u57fa\u51c6\uff09\u548cLitBench\uff08\u521b\u610f\u5199\u4f5c\u57fa\u51c6\uff09\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u6784\u5efa100\u5bf9\u6bd4\u8f83\u4efb\u52a1\uff0c\u91c7\u7528GPT-4o\u548cGemini-2.5-Flash\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff0c\u4e3a\u6bcf\u5bf9\u54cd\u5e94\u5206\u914d\u8868\u9762\u7ebf\u7d22\uff08\u6765\u6e90\u8eab\u4efd\u548c\u65f6\u95f4\u65b0\u65e7\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u5176\u4f59\u90e8\u5206\u4e0d\u53d8\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u65b0\u8fd1\u504f\u89c1\uff08\u7cfb\u7edf\u6027\u5730\u504f\u597d\u65b0\u54cd\u5e94\u800c\u975e\u65e7\u54cd\u5e94\uff09\u548c\u6e05\u6670\u7684\u6765\u6e90\u5c42\u6b21\uff08\u4e13\u5bb6>\u4eba\u7c7b>LLM>\u672a\u77e5\uff09\uff0c\u8fd9\u4e9b\u504f\u89c1\u5728GPT-4o\u548c\u66f4\u4e3b\u89c2\u5f00\u653e\u7684LitBench\u9886\u57df\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u4e14\u7ebf\u7d22\u627f\u8ba4\u6781\u4e3a\u7f55\u89c1\u3002", "conclusion": "\u5f53\u524dLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u6377\u5f84\u5f71\u54cd\u4e14\u4e0d\u5fe0\u5b9e\uff0c\u524a\u5f31\u4e86\u5176\u5728\u7814\u7a76\u548c\u90e8\u7f72\u4e2d\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.26076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26076", "abs": "https://arxiv.org/abs/2509.26076", "authors": ["Johannes Schmitt", "Gergely B\u00e9rczi", "Jasper Dekoninck", "Jeremy Feusi", "Tim Gehrunger", "Raphael Appenzeller", "Jim Bryan", "Niklas Canova", "Timo de Wolff", "Filippo Gaia", "Michel van Garrel", "Baran Hashemi", "David Holmes", "Aitor Iribar Lopez", "Victor Jaeck", "Martina J\u00f8rgensen", "Steven Kelk", "Stefan Kuhlmann", "Adam Kurpisz", "Chiara Meroni", "Ingmar Metzler", "Martin M\u00f6ller", "Samuel Mu\u00f1oz-Ech\u00e1niz", "Robert Nowak", "Georg Oberdieck", "Daniel Platt", "Dylan Possama\u00ef", "Gabriel Ribeiro", "Ra\u00fal S\u00e1nchez Gal\u00e1n", "Zheming Sun", "Josef Teichmann", "Richard P. Thomas", "Charles Vial"], "title": "IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation", "comment": null, "summary": "As the mathematical capabilities of large language models (LLMs) improve, it\nbecomes increasingly important to evaluate their performance on research-level\ntasks at the frontier of mathematical knowledge. However, existing benchmarks\nare limited, as they focus solely on final-answer questions or high-school\ncompetition problems. To address this gap, we introduce IMProofBench, a private\nbenchmark consisting of 39 peer-reviewed problems developed by expert\nmathematicians. Each problem requires a detailed proof and is paired with\nsubproblems that have final answers, supporting both an evaluation of\nmathematical reasoning capabilities by human experts and a large-scale\nquantitative analysis through automated grading. Furthermore, unlike prior\nbenchmarks, the evaluation setup simulates a realistic research environment:\nmodels operate in an agentic framework with tools like web search for\nliterature review and mathematical software such as SageMath. Our results show\nthat current LLMs can succeed at the more accessible research-level questions,\nbut still encounter significant difficulties on more challenging problems.\nQuantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer\nsubproblems, while GPT-5 obtains the best performance for proof generation,\nachieving a fully correct solution for 22% of problems. IMProofBench will\ncontinue to evolve as a dynamic benchmark in collaboration with the\nmathematical community, ensuring its relevance for evaluating the next\ngeneration of LLMs.", "AI": {"tldr": "IMProofBench\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30LLMs\u5728\u6570\u5b66\u7814\u7a76\u524d\u6cbf\u4efb\u52a1\u8868\u73b0\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b39\u4e2a\u4e13\u5bb6\u6570\u5b66\u5bb6\u5f00\u53d1\u7684\u540c\u884c\u8bc4\u5ba1\u95ee\u9898\uff0c\u8981\u6c42\u8be6\u7ec6\u8bc1\u660e\u5e76\u914d\u6709\u5b50\u95ee\u9898\uff0c\u652f\u6301\u4eba\u5de5\u4e13\u5bb6\u8bc4\u4f30\u548c\u81ea\u52a8\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u95ee\u9898\u6216\u9ad8\u4e2d\u7ade\u8d5b\u9898\uff0c\u65e0\u6cd5\u8bc4\u4f30LLMs\u5728\u7814\u7a76\u7ea7\u6570\u5b66\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8d34\u8fd1\u771f\u5b9e\u7814\u7a76\u73af\u5883\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u521b\u5efa\u5305\u542b39\u4e2a\u540c\u884c\u8bc4\u5ba1\u6570\u5b66\u95ee\u9898\u7684\u79c1\u6709\u57fa\u51c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u9700\u8981\u8be6\u7ec6\u8bc1\u660e\u5e76\u914d\u6709\u5b50\u95ee\u9898\uff0c\u91c7\u7528\u4ee3\u7406\u6846\u67b6\u8ba9\u6a21\u578b\u4f7f\u7528\u7f51\u7edc\u641c\u7d22\u548c\u6570\u5b66\u8f6f\u4ef6\u7b49\u5de5\u5177\uff0c\u6a21\u62df\u771f\u5b9e\u7814\u7a76\u73af\u5883\u3002", "result": "\u5f53\u524dLLMs\u5728\u8f83\u5bb9\u6613\u7684\u7814\u7a76\u7ea7\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u4e0a\u4ecd\u6709\u663e\u8457\u56f0\u96be\u3002Grok-4\u5728\u6700\u7ec8\u7b54\u6848\u5b50\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\uff0852%\uff09\uff0cGPT-5\u5728\u8bc1\u660e\u751f\u6210\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0822%\u95ee\u9898\u5b8c\u5168\u6b63\u786e\uff09\u3002", "conclusion": "IMProofBench\u4f5c\u4e3a\u4e00\u4e2a\u52a8\u6001\u57fa\u51c6\uff0c\u5c06\u4e0e\u6570\u5b66\u754c\u5408\u4f5c\u6301\u7eed\u53d1\u5c55\uff0c\u786e\u4fdd\u5176\u76f8\u5173\u6027\u4ee5\u8bc4\u4f30\u4e0b\u4e00\u4ee3LLMs\u3002", "topic": "swe benchmark"}}
{"id": "2509.25438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25438", "abs": "https://arxiv.org/abs/2509.25438", "authors": ["Zhibo Hou", "Zhiyu An", "Wan Du"], "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring", "comment": null, "summary": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b66\u4e60\u8fdb\u5ea6\u76d1\u63a7(LPM)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5185\u5728\u52a8\u673a\u63a2\u7d22\uff0c\u901a\u8fc7\u76d1\u63a7\u6a21\u578b\u6539\u8fdb\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6216\u65b0\u9896\u6027\u6765\u5956\u52b1\u667a\u80fd\u4f53\uff0c\u6709\u6548\u907f\u514d\u5728\u4e0d\u53ef\u5b66\u4e60\u7684\u566a\u58f0\u6e90\u4e0a\u505c\u6ede\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5185\u5728\u5956\u52b1\u7684\u63a2\u7d22\u65b9\u6cd5\u5728\u5b58\u5728\u4e0d\u53ef\u5b66\u4e60\u968f\u673a\u6e90(\u566a\u58f0\u7535\u89c6)\u7684\u73af\u5883\u4e2d\u5bb9\u6613\u9677\u5165\u505c\u6ede\uff0c\u800c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u5206\u5e03\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u867d\u7136\u6700\u7ec8\u80fd\u9003\u8131\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u4eba\u7c7b\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u76d1\u63a7\u81ea\u8eab\u6539\u8fdb\u7684\u542f\u53d1\uff0c\u63d0\u51faLPM\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u8bbe\u8ba1\uff1a\u4f7f\u7528\u8bef\u5dee\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u6a21\u578b\u5728\u5148\u524d\u8fed\u4ee3\u4e2d\u7684\u9884\u671f\u9884\u6d4b\u8bef\u5dee\uff0c\u5229\u7528\u5f53\u524d\u8fed\u4ee3\u4e0e\u5148\u524d\u8fed\u4ee3\u6a21\u578b\u8bef\u5dee\u7684\u5dee\u5f02\u6765\u6307\u5bfc\u63a2\u7d22\u3002LPM\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6216\u65b0\u9896\u6027\u3002", "result": "\u5728\u57fa\u4e8eMNIST\u30013D\u8ff7\u5bab\u548cAtari\u7684\u566a\u58f0\u73af\u5883\u4e2d\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0cLPM\u7684\u5185\u5728\u5956\u52b1\u6536\u655b\u66f4\u5feb\uff0c\u5728\u8ff7\u5bab\u5b9e\u9a8c\u4e2d\u63a2\u7d22\u66f4\u591a\u72b6\u6001\uff0c\u5728Atari\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u5916\u5728\u5956\u52b1\u3002", "conclusion": "LPM\u5728\u6982\u5ff5\u4e0a\u7b80\u5355\u7684\u65b9\u6cd5\u6807\u5fd7\u7740\u566a\u58f0\u9c81\u68d2\u63a2\u7d22\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u5185\u5728\u5956\u52b1\u662f\u96f6\u7b49\u53d8\u7684\u4e14\u662f\u4fe1\u606f\u589e\u76ca\u7684\u5355\u8c03\u6307\u6807\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25779", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25779", "abs": "https://arxiv.org/abs/2509.25779", "authors": ["Siyu Zhu", "Yanbin Jiang", "Hejian Sang", "Shao Tang", "Qingquan Song", "Biao He", "Rohit Jain", "Zhipeng Wang", "Alborz Geramifard"], "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs", "comment": null, "summary": "We investigated Agentic RL with large language models on the\n\\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a\n\\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$\nimprovement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on\nthe public leaderboard. A central finding was that smaller models (8B) were\nhighly responsive to reward shaping: with dense process-level signals, they\nreached competitive performance while being $3.5\\times$ more compute-efficient\nand $1.5\\times$ more memory-efficient than 32B models. Larger models were more\nrobust under sparse rewards but exhibited smaller relative gains from shaping\nand higher variance across runs. While curriculum learning offered no\nsignificant benefit, shaped rewards consistently amplified learning dynamics,\nmaking 8B models the most efficient setting for agentic RL. Crucially, these\ngains did not come at the cost of overfitting: fine-tuned models mostly\nmaintained or exceeded baseline performance on out-of-domain tasks, including\n\\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $\\tau$-\\textsc{Bench}. These\nresults establish reward shaping as a decisive lever for scaling agentic RL,\nhighlight the competitive strength of smaller models, and demonstrate that\nefficiency can be achieved without sacrificing generalization.", "AI": {"tldr": "\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff088B\uff09\u901a\u8fc7\u5bc6\u96c6\u8fc7\u7a0b\u7ea7\u5956\u52b1\u4fe1\u53f7\u5b9e\u73b0\u4e8656.9%\u7684\u6700\u7ec8\u901a\u8fc7\u7387\uff0c\u6bd4GPT-5\u57fa\u7ebf\u63d0\u9ad8\u4e862.7\u500d\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u63d0\u9ad83.5\u500d\uff0c\u5185\u5b58\u6548\u7387\u63d0\u9ad81.5\u500d\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u6765\u63d0\u9ad8\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u63a2\u7d22\u5c0f\u578b\u6a21\u578b\u5728\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u4e0b\u7684\u8868\u73b0\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Agentic RL\u65b9\u6cd5\uff0c\u5728TravelPlanner\u57fa\u51c6\u4e0a\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5bc6\u96c6\u8fc7\u7a0b\u7ea7\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u5956\u52b1\u5851\u5f62\uff0c\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff088B vs 32B\uff09\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "Planner-R1\u65b9\u6cd5\u4ec5\u7528180\u4e2a\u8bad\u7ec3\u67e5\u8be2\u5c31\u8fbe\u523056.9%\u7684\u6700\u7ec8\u901a\u8fc7\u7387\uff0c\u6bd4GPT-5\u57fa\u7ebf\uff0821.2%\uff09\u63d0\u9ad82.7\u500d\u30028B\u6a21\u578b\u5728\u5bc6\u96c6\u5956\u52b1\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6548\u7387\u6bd432B\u6a21\u578b\u9ad83.5\u500d\uff0c\u5185\u5b58\u6548\u7387\u9ad81.5\u500d\u3002", "conclusion": "\u5956\u52b1\u5851\u5f62\u662f\u6269\u5c55\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6760\u6746\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u4e0b\u5177\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u4e14\u8fd9\u79cd\u6548\u7387\u63d0\u5347\u4e0d\u4f1a\u727a\u7272\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25835", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25835", "abs": "https://arxiv.org/abs/2509.25835", "authors": ["Xinzhe Li"], "title": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search", "comment": "Under Review", "summary": "Test-time scaling enables large language models (LLMs) to improve performance\non long-horizon reasoning tasks by allocating additional compute at inference.\nTree-search-based approaches achieve state-of-the-art results in this setting,\nbut they are notoriously inefficient, often an order of magnitude slower than\nsimpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in\nframework that adaptively decides when to branch during search rather than\nbranching at every step. CiT relies on lightweight Branching Necessity (BN)\nevaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly\njudges whether a step requires branching, and BN-SC (Self-Consistency), which\nclusters multiple candidate actions to estimate agreement. We integrate CiT\ninto three representative LLM-in-the-loop tree search frameworks: Tree of\nThoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500.\nOur results show that: (1) BN-DP consistently reduces token generation, model\ninvocations, and runtime by 75-85 percent across all settings, with negligible\naccuracy loss and sometimes accuracy gains; (2) BN-SC typically yields\nsubstantial savings (up to 80 percent) but shows instability in 1-4 out of 14\nsettings, caused by a small subset of examples that produce very long reasoning\nsteps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator\nin BN-DP, but also the models used in BN-SC for clustering and equivalence\nchecking. When these roles are filled by smaller LLMs, performance degrades.\nImportantly, BN-SC does not require LLMs in domains with deterministic action\nspaces, where clustering can be done programmatically. We also provide a\ntheoretical guarantee that BN-DP never increases LLM invocations relative to\nthe baseline and release a unified implementation of CiT across ToT-BS,\nReST-MCTS, and RAP to facilitate reproducibility and extension.", "AI": {"tldr": "Chain-in-Tree (CiT) \u662f\u4e00\u4e2a\u63d2\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u5206\u652f\u800c\u4e0d\u662f\u6bcf\u4e00\u6b65\u90fd\u5206\u652f\uff0c\u663e\u8457\u63d0\u9ad8\u6811\u641c\u7d22\u65b9\u6cd5\u7684\u6548\u7387\u3002\u5b83\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u5206\u652f\u5fc5\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c1175-85%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u6811\u641c\u7d22\u65b9\u6cd5\u867d\u7136\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u65f6\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u6781\u4f4e\uff0c\u901a\u5e38\u6bd4\u7b80\u5355\u8fed\u4ee3\u65b9\u6cd5\u6162\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faChain-in-Tree (CiT)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u5206\u652f\u5fc5\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff1aBN-DP\uff08\u76f4\u63a5\u63d0\u793a\u8f85\u52a9LLM\u5224\u65ad\u662f\u5426\u9700\u8981\u5206\u652f\uff09\u548cBN-SC\uff08\u901a\u8fc7\u805a\u7c7b\u5019\u9009\u52a8\u4f5c\u4f30\u8ba1\u4e00\u81f4\u6027\uff09\u3002\u8be5\u6846\u67b6\u53ef\u96c6\u6210\u5230ToT-BS\u3001ReST-MCTS\u548cRAP\u7b49\u6811\u641c\u7d22\u6846\u67b6\u4e2d\u3002", "result": "BN-DP\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u4e00\u81f4\u51cf\u5c1175-85%\u7684token\u751f\u6210\u3001\u6a21\u578b\u8c03\u7528\u548c\u8fd0\u884c\u65f6\u95f4\uff0c\u51c6\u786e\u7387\u635f\u5931\u53ef\u5ffd\u7565\u751a\u81f3\u6709\u65f6\u63d0\u5347\uff1bBN-SC\u901a\u5e38\u8282\u7701\u9ad8\u8fbe80%\u4f46\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff1b\u8f85\u52a9LLM\u7684\u8d28\u91cf\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "CiT\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u7684\u5206\u652f\u51b3\u7b56\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6811\u641c\u7d22\u65b9\u6cd5\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.25518", "categories": ["cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.25518", "abs": "https://arxiv.org/abs/2509.25518", "authors": ["Harry Robertshaw", "Han-Ru Wu", "Alejandro Granados", "Thomas C Booth"], "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy", "comment": "Published in Medical Image Computing and Computer Assisted\n  Intervention - MICCAI 2025, Lecture Notes in Computer Science, vol 15968", "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical\nchallenge due to the complexity of vascular anatomy and the need for precise,\nreal-time decision-making. Reinforcement learning (RL)-based approaches have\ndemonstrated potential in automating endovascular navigation, but current\nmethods often struggle with generalization across multiple patient vasculatures\nand long-horizon tasks. We propose a world model for autonomous endovascular\nnavigation using TD-MPC2, a model-based RL algorithm. We trained a single RL\nagent across multiple endovascular navigation tasks in ten real patient\nvasculatures, comparing performance against the state-of-the-art Soft\nActor-Critic (SAC) method. Results indicate that TD-MPC2 significantly\noutperforms SAC in multi-task learning, achieving a 65% mean success rate\ncompared to SAC's 37%, with notable improvements in path ratio. TD-MPC2\nexhibited increased procedure times, suggesting a trade-off between success\nrate and execution speed. These findings highlight the potential of world\nmodels for improving autonomous endovascular navigation and lay the foundation\nfor future research in generalizable AI-driven robotic interventions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTD-MPC2\u4e16\u754c\u6a21\u578b\u7684\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u65b9\u6cd5\uff0c\u5728\u591a\u60a3\u8005\u8840\u7ba1\u89e3\u5256\u4e2d\u5b9e\u73b065%\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eSAC\u65b9\u6cd5\u768437%", "motivation": "\u673a\u68b0\u8840\u6813\u5207\u9664\u672f\u7684\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u8840\u7ba1\u89e3\u5256\u590d\u6742\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u60a3\u8005\u6cdb\u5316\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73", "method": "\u4f7f\u7528TD-MPC2\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u572810\u4e2a\u771f\u5b9e\u60a3\u8005\u8840\u7ba1\u89e3\u5256\u4e2d\u8bad\u7ec3\u5355\u4e00\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u4efb\u52a1\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5e76\u4e0eSAC\u65b9\u6cd5\u5bf9\u6bd4", "result": "TD-MPC2\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u663e\u8457\u4f18\u4e8eSAC\uff0c\u5e73\u5747\u6210\u529f\u738765% vs 37%\uff0c\u8def\u5f84\u6bd4\u6539\u5584\u660e\u663e\uff0c\u4f46\u624b\u672f\u65f6\u95f4\u589e\u52a0", "conclusion": "\u4e16\u754c\u6a21\u578b\u5728\u6539\u5584\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u53ef\u6cdb\u5316AI\u9a71\u52a8\u673a\u5668\u4eba\u5e72\u9884\u7814\u7a76\u5960\u5b9a\u57fa\u7840", "topic": "agentic reinforcement learning"}}
{"id": "2509.25535", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25535", "abs": "https://arxiv.org/abs/2509.25535", "authors": ["Yichi Zhang", "Fangzheng Xie", "Shu Yang", "Chong Wu"], "title": "Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing", "comment": null, "summary": "In language tasks that require extensive human--model interaction, deploying\na single \"best\" model for every query can be expensive. To reduce inference\ncost while preserving the quality of the responses, a large language model\n(LLM) router selects the most appropriate model from a pool of candidates for\neach query. A central challenge to training a high-quality router is the\nscarcity of reliable supervision. Gold-standard data (e.g., expert-verified\nlabels or rubric-based scores) provide accurate quality evaluations of LLM\nresponses but are costly and difficult to scale. In contrast, preference-based\ndata, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and\nmore scalable, yet often biased in reflecting the true quality of responses. We\ncast the problem of LLM router training with combined gold-standard and\npreference-based data into a causal inference framework by viewing the response\nevaluation mechanism as the treatment assignment. This perspective further\nreveals that the bias in preference-based data corresponds to the well-known\ncausal estimand: the conditional average treatment effect. Based on this new\nperspective, we develop an integrative causal router training framework that\ncorrects preference-data bias, address imbalances between two data sources, and\nimprove routing robustness and efficiency. Numerical experiments demonstrate\nthat our approach delivers more accurate routing and improves the trade-off\nbetween cost and quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u56e0\u679c\u63a8\u7406\u6846\u67b6\u6765\u8bad\u7ec3LLM\u8def\u7531\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u9ec4\u91d1\u6807\u51c6\u548c\u504f\u597d\u6570\u636e\u6765\u7ea0\u6b63\u504f\u597d\u6570\u636e\u504f\u5dee\uff0c\u63d0\u9ad8\u8def\u7531\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca", "motivation": "\u5728\u9700\u8981\u5927\u91cf\u4eba\u673a\u4ea4\u4e92\u7684\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u90e8\u7f72\u5355\u4e00\"\u6700\u4f73\"\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u6700\u5408\u9002\u6a21\u578b\u7684\u8def\u7531\u5668\uff0c\u4f46\u9ad8\u8d28\u91cf\u8def\u7531\u5668\u7684\u8bad\u7ec3\u9762\u4e34\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218", "method": "\u5c06LLM\u8def\u7531\u5668\u8bad\u7ec3\u95ee\u9898\u8f6c\u5316\u4e3a\u56e0\u679c\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u54cd\u5e94\u8bc4\u4f30\u673a\u5236\u89c6\u4e3a\u6cbb\u7597\u5206\u914d\u3002\u57fa\u4e8e\u6b64\u89c6\u89d2\u5f00\u53d1\u96c6\u6210\u56e0\u679c\u8def\u7531\u5668\u8bad\u7ec3\u6846\u67b6\uff0c\u7ea0\u6b63\u504f\u597d\u6570\u636e\u504f\u5dee\uff0c\u89e3\u51b3\u4e24\u4e2a\u6570\u636e\u6e90\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u95ee\u9898", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8def\u7531\uff0c\u5e76\u6539\u5584\u4e86\u6210\u672c\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861", "conclusion": "\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6846\u67b6\u6574\u5408\u9ec4\u91d1\u6807\u51c6\u548c\u504f\u597d\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u7ea0\u6b63\u504f\u597d\u6570\u636e\u504f\u5dee\uff0c\u63d0\u9ad8LLM\u8def\u7531\u5668\u7684\u6027\u80fd\u548c\u6548\u7387", "topic": "agent analysis"}}
{"id": "2509.25885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25885", "abs": "https://arxiv.org/abs/2509.25885", "authors": ["Ruolin Chen", "Yinqian Sun", "Jihang Wang", "Mingyang Lv", "Qian Zhang", "Yi Zeng"], "title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents", "comment": null, "summary": "Embodied agents powered by large language models (LLMs) inherit advanced\nplanning capabilities; however, their direct interaction with the physical\nworld exposes them to safety vulnerabilities. In this work, we identify four\nkey reasoning stages where hazards may arise: Task Understanding, Environment\nPerception, High-Level Plan Generation, and Low-Level Action Generation. We\nfurther formalize three orthogonal safety constraint types (Factual, Causal,\nand Temporal) to systematically characterize potential safety violations.\nBuilding on this risk model, we present SafeMindBench, a multimodal benchmark\nwith 5,558 samples spanning four task categories (Instr-Risk, Env-Risk,\nOrder-Fix, Req-Align) across high-risk scenarios such as sabotage, harm,\nprivacy, and illegal behavior. Extensive experiments on SafeMindBench reveal\nthat leading LLMs (e.g., GPT-4o) and widely used embodied agents remain\nsusceptible to safety-critical failures. To address this challenge, we\nintroduce SafeMindAgent, a modular Planner-Executor architecture integrated\nwith three cascaded safety modules, which incorporate safety constraints into\nthe reasoning process. Results show that SafeMindAgent significantly improves\nsafety rate over strong baselines while maintaining comparable task completion.\nTogether, SafeMindBench and SafeMindAgent provide both a rigorous evaluation\nsuite and a practical solution that advance the systematic study and mitigation\nof safety risks in embodied LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86SafeMindBench\u57fa\u51c6\u6d4b\u8bd5\u548cSafeMindAgent\u67b6\u6784\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u548c\u7f13\u89e3\u5177\u8eabLLM\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u4efb\u52a1\u7406\u89e3\u3001\u73af\u5883\u611f\u77e5\u3001\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u884c\u52a8\u751f\u6210\u56db\u4e2a\u5173\u952e\u63a8\u7406\u9636\u6bb5\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5177\u8eab\u4ee3\u7406\u7ee7\u627f\u4e86LLM\u7684\u9ad8\u7ea7\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u4f7f\u5176\u9762\u4e34\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u7f13\u89e3\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b5,558\u4e2a\u6837\u672c\u7684\u591a\u6a21\u6001\u57fa\u51c6SafeMindBench\uff0c\u6db5\u76d6\u56db\u79cd\u4efb\u52a1\u7c7b\u522b\u548c\u9ad8\u98ce\u9669\u573a\u666f\uff1b\u8bbe\u8ba1\u4e86SafeMindAgent\u67b6\u6784\uff0c\u91c7\u7528Planner-Executor\u6a21\u5f0f\u5e76\u96c6\u6210\u4e09\u4e2a\u7ea7\u8054\u5b89\u5168\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e3b\u6d41LLM\u548c\u5177\u8eab\u4ee3\u7406\u4ecd\u6613\u53d1\u751f\u5b89\u5168\u5173\u952e\u6545\u969c\uff0c\u800cSafeMindAgent\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\u3002", "conclusion": "SafeMindBench\u548cSafeMindAgent\u4e3a\u5177\u8eabLLM\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u5957\u4ef6\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.26313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26313", "abs": "https://arxiv.org/abs/2509.26313", "authors": ["Rui Ming", "Haoyuan Wu", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient", "comment": null, "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs.", "AI": {"tldr": "\u63d0\u51faOTR\u7b97\u6cd5\uff0c\u5c06\u76d1\u7763\u5fae\u8c03\u4e0e\u7b56\u7565\u68af\u5ea6\u7ed3\u5408\uff0c\u901a\u8fc7\u5355\u6b65\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\u5904\u7406\u6bcf\u4e2atoken\u751f\u6210\uff0c\u5229\u7528\u76d1\u7763\u6570\u636e\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u4e0d\u5982\u5f3a\u5316\u5b66\u4e60\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u5dee\u5f02\u6e90\u4e8e\u6570\u636e\u6027\u8d28\uff1aSFT\u4f7f\u7528\u56fa\u5b9a\u79bb\u7ebf\u6570\u636e\uff0c\u800cRL\u4f7f\u7528\u5728\u7ebf\u7b56\u7565\u6570\u636e\u3002OTR\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "OTR\u5c06\u81ea\u56de\u5f52\u5b66\u4e60\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5355\u6b65\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4ece\u5f53\u524d\u7b56\u7565\u5206\u5e03\u4e2d\u91c7\u6837\u591a\u4e2a\u5019\u9009token\uff0c\u4f7f\u7528\u76d1\u7763\u6570\u636e\u4e2d\u7684\u771f\u5b9etoken\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6307\u5bfc\u5b66\u4e60\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u901a\u7528\u9886\u57df\u63a8\u7406\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOTR\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6SFT\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "OTR\u662f\u4e00\u79cd\u5f3a\u5927\u5b9e\u7528\u7684LLM\u5fae\u8c03\u66ff\u4ee3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u5728\u7ebf\u7b56\u7565\u6027\u8d28\u662f\u6cdb\u5316\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25582", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25582", "abs": "https://arxiv.org/abs/2509.25582", "authors": ["Amir Moeini", "Minjae Kwon", "Alper Kamil Bozkurt", "Yuichi Motai", "Rohan Chandra", "Lu Feng", "Shangtong Zhang"], "title": "Safe In-Context Reinforcement Learning", "comment": null, "summary": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the\nagent, after some pretraining procedure, is able to adapt to\nout-of-distribution test tasks without any parameter updates. The agent\nachieves this by continually expanding the input (i.e., the context) to its\npolicy neural networks. For example, the input could be all the history\nexperience that the agent has access to until the current time step. The\nagent's performance improves as the input grows, without any parameter updates.\nIn this work, we propose the first method that promotes the safety of ICRL's\nadaptation process in the framework of constrained Markov Decision Processes.\nIn other words, during the parameter-update-free adaptation process, the agent\nnot only maximizes the reward but also minimizes an additional cost function.\nWe also demonstrate that our agent actively reacts to the threshold (i.e.,\nbudget) of the cost tolerance. With a higher cost budget, the agent behaves\nmore aggressively, and with a lower cost budget, the agent behaves more\nconservatively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u65e0\u53c2\u6570\u66f4\u65b0\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u786e\u4fdd\u5b89\u5168\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\u540c\u65f6\u4f18\u5316\u5956\u52b1\u548c\u6210\u672c\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u7684ICRL\u65b9\u6cd5\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u5b89\u5168\u4fdd\u969c\uff0c\u65e0\u6cd5\u5728\u6700\u5927\u5316\u5956\u52b1\u7684\u540c\u65f6\u63a7\u5236\u6210\u672c\u98ce\u9669\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5728ICRL\u6846\u67b6\u4e2d\u5f15\u5165\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u667a\u80fd\u4f53\u5728\u65e0\u53c2\u6570\u66f4\u65b0\u7684\u9002\u5e94\u8fc7\u7a0b\u4e2d\u80fd\u591f\u540c\u65f6\u8003\u8651\u5956\u52b1\u6700\u5927\u5316\u548c\u6210\u672c\u6700\u5c0f\u5316\uff0c\u5e76\u6839\u636e\u6210\u672c\u5bb9\u5fcd\u5ea6\u9608\u503c\u4e3b\u52a8\u8c03\u6574\u884c\u4e3a\u7b56\u7565\u3002", "result": "\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u6210\u672c\u9884\u7b97\u4e3b\u52a8\u8c03\u6574\u884c\u4e3a\u7b56\u7565\uff1a\u6210\u672c\u9884\u7b97\u8f83\u9ad8\u65f6\u884c\u4e3a\u66f4\u6fc0\u8fdb\uff0c\u6210\u672c\u9884\u7b97\u8f83\u4f4e\u65f6\u884c\u4e3a\u66f4\u4fdd\u5b88\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u5b89\u5168\u4fdd\u969c\u673a\u5236\u96c6\u6210\u5230ICRL\u7684\u9002\u5e94\u8fc7\u7a0b\u4e2d\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25592", "categories": ["cs.LG", "68T05, 68T20, 90C26, 90C30, 93E35, 62L20", "I.2.6; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2509.25592", "abs": "https://arxiv.org/abs/2509.25592", "authors": ["Morteza Kimiaei", "Vyacheslav Kungurtsev"], "title": "Machine Learning Algorithms for Improving Black Box Optimization Solvers", "comment": "74 pages", "summary": "Black-box optimization (BBO) addresses problems where objectives are\naccessible only through costly queries without gradients or explicit structure.\nClassical derivative-free methods -- line search, direct search, and\nmodel-based solvers such as Bayesian optimization -- form the backbone of BBO,\nyet often struggle in high-dimensional, noisy, or mixed-integer settings.\n  Recent advances use machine learning (ML) and reinforcement learning (RL) to\nenhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning\nportfolios, and generative models, while RL enables dynamic operator\nconfiguration, robustness, and meta-optimization across tasks.\n  This paper surveys these developments, covering representative algorithms\nsuch as NNs with the modular model-based optimization framework (mlrMBO),\nzeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),\ndistributed block-wise optimization (DiBB), partition-based Bayesian\noptimization (SPBOpt), the transformer-based optimizer (B2Opt),\ndiffusion-model-based BBO, surrogate-assisted RL for differential evolution\n(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with\nrelative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),\npolicy improvement with black-box (PIBB), and offline Q-learning with Mamba\nbackbones (Q-Mamba).\n  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and\nthe MetaBox framework. Overall, we highlight how ML and RL transform classical\ninexact solvers into more scalable, robust, and adaptive frameworks for\nreal-world optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u589e\u5f3a\u9ed1\u76d2\u4f18\u5316\uff0c\u5c06\u4f20\u7edf\u65e0\u5bfc\u6570\u65b9\u6cd5\u8f6c\u53d8\u4e3a\u66f4\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u5728\u9ad8\u7ef4\u3001\u566a\u58f0\u6216\u6df7\u5408\u6574\u6570\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5229\u7528ML\u548cRL\u7684\u5148\u8fdb\u6280\u672f\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u591a\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u6846\u67b6\u3001\u96f6\u9636\u81ea\u9002\u5e94\u52a8\u91cf\u65b9\u6cd5\u3001\u81ea\u52a8\u5316BBO\u3001\u5206\u5e03\u5f0f\u5757\u4f18\u5316\u3001\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u4f18\u5316\u5668\u7b49ML\u548cRL\u65b9\u6cd5\u3002", "result": "ML\u63d0\u4f9b\u4e86\u8868\u8fbe\u6027\u4ee3\u7406\u6a21\u578b\u3001\u81ea\u9002\u5e94\u66f4\u65b0\u3001\u5143\u5b66\u4e60\u7ec4\u5408\u548c\u751f\u6210\u6a21\u578b\uff0cRL\u5b9e\u73b0\u4e86\u52a8\u6001\u7b97\u5b50\u914d\u7f6e\u3001\u9c81\u68d2\u6027\u548c\u8de8\u4efb\u52a1\u5143\u4f18\u5316\u3002", "conclusion": "ML\u548cRL\u5c06\u4f20\u7edf\u7684\u4e0d\u7cbe\u786e\u6c42\u89e3\u5668\u8f6c\u53d8\u4e3a\u66f4\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u73b0\u5b9e\u4e16\u754c\u4f18\u5316\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26383", "abs": "https://arxiv.org/abs/2509.26383", "authors": ["Jinyeop Song", "Song Wang", "Julian Shun", "Yada Zhu"], "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning", "comment": "10 pages, 5 figures. Submitted to ICLR 2026", "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.", "AI": {"tldr": "KG-R1\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7684KG-RAG\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u4ee3\u7406\u4e0e\u77e5\u8bc6\u56fe\u8c31\u4ea4\u4e92\uff0c\u901a\u8fc7\u7aef\u5230\u7aefRL\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u8fc7\u7a0b\uff0c\u5728KGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u4f20\u7edfKG-RAG\u7cfb\u7edf\u4f7f\u7528\u591a\u4e2aLLM\u6a21\u5757\uff08\u5982\u89c4\u5212\u3001\u63a8\u7406\u3001\u54cd\u5e94\uff09\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u884c\u4e3a\u7ed1\u5b9a\u5230\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u3002\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u8fc1\u79fb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165KG-R1\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u4ee3\u7406\u4e0e\u77e5\u8bc6\u56fe\u8c31\u73af\u5883\u4ea4\u4e92\uff0c\u5b66\u4e60\u9010\u6b65\u68c0\u7d22\u5e76\u5c06\u68c0\u7d22\u4fe1\u606f\u878d\u5165\u63a8\u7406\u548c\u751f\u6210\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728KGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Qwen-2.5-3B\u6a21\u578b\uff0cKG-R1\u4ee5\u66f4\u5c11\u7684\u751f\u6210token\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f7f\u7528\u66f4\u5927\u57fa\u7840\u6a21\u578b\u6216\u5fae\u8c03\u6a21\u578b\u7684\u591a\u6a21\u5757\u5de5\u4f5c\u6d41\u65b9\u6cd5\u3002", "conclusion": "KG-R1\u5b9e\u73b0\u4e86\u5373\u63d2\u5373\u7528\uff0c\u8bad\u7ec3\u540e\u65e0\u9700\u4fee\u6539\u5373\u53ef\u5728\u65b0\u77e5\u8bc6\u56fe\u8c31\u4e0a\u4fdd\u6301\u5f3a\u51c6\u786e\u6027\uff0c\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u524d\u666f\u7684KG-RAG\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25958", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25958", "abs": "https://arxiv.org/abs/2509.25958", "authors": ["Gang Li", "Yulei Qin", "Xiaoyu Tan", "Dingkang Yang", "Yuchen Shi", "Zihan Xu", "Xiang Li", "Xing Sun", "Ke Li"], "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in\neliciting complex reasoning in large language models (LLMs). However, standard\nRLVR training often leads to excessively verbose processes (in reasoning tasks)\nand inefficient exploration trajectories (in agentic settings), as outcome-only\nrewards provide no incentive for efficiency and the high variance in response\nlength within relatively small rollout groups results in noisy optimization\nsignals. To address this, we propose Rollout Response Recomposition (RoRecomp),\na plug-and-play method that guides models toward concise reasoning by\nstrategically recomposing the training data. RoRecomp separates responses into\ntwo distinct batch types: 1) priority batches, which combine short-correct and\nlong-incorrect responses selected from online batches to provide a clear\ngradient signal for brevity, and 2) compensation batches, which utilize\nremaining responses from a replay buffer to maintain stability and prevent\nmodel collapse. To comprehensively evaluate effectiveness, we test RoRecomp\nacross three settings where results demonstrate substantial efficiency gains:\nreducing reasoning length by 27.7% in zero RL training, reducing unnecessary\ntool calls by 46.8% while improving accuracy in agentic RL, and achieving up to\n52.5% length reduction in thinking compression, all with minimal performance\nimpact.", "AI": {"tldr": "\u63d0\u51fa\u4e86Rollout Response Recomposition (RoRecomp)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6218\u7565\u6027\u5730\u91cd\u7ec4\u8bad\u7ec3\u6570\u636e\u6765\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7b80\u6d01\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u6807\u51c6RLVR\u8bad\u7ec3\u5bfc\u81f4\u7684\u5197\u957f\u63a8\u7406\u548c\u4f4e\u6548\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u6807\u51c6RLVR\u8bad\u7ec3\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5f80\u5f80\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u8fc7\u4e8e\u5197\u957f\u548c\u63a2\u7d22\u8f68\u8ff9\u4f4e\u6548\uff0c\u56e0\u4e3a\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u65e0\u6cd5\u6fc0\u52b1\u6548\u7387\uff0c\u4e14\u54cd\u5e94\u957f\u5ea6\u7684\u9ad8\u65b9\u5dee\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u566a\u58f0\u8f83\u5927\u3002", "method": "RoRecomp\u65b9\u6cd5\u5c06\u54cd\u5e94\u5206\u4e3a\u4e24\u79cd\u6279\u6b21\u7c7b\u578b\uff1a1) \u4f18\u5148\u6279\u6b21\uff0c\u7ed3\u5408\u77ed\u6b63\u786e\u548c\u957f\u9519\u8bef\u54cd\u5e94\uff0c\u63d0\u4f9b\u7b80\u6d01\u6027\u7684\u6e05\u6670\u68af\u5ea6\u4fe1\u53f7\uff1b2) \u8865\u507f\u6279\u6b21\uff0c\u4f7f\u7528\u91cd\u653e\u7f13\u51b2\u533a\u4e2d\u7684\u5269\u4f59\u54cd\u5e94\u6765\u7ef4\u6301\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u79cd\u8bbe\u7f6e\u4e0b\u6d4b\u8bd5\u663e\u793a\u663e\u8457\u6548\u7387\u63d0\u5347\uff1a\u96f6RL\u8bad\u7ec3\u4e2d\u63a8\u7406\u957f\u5ea6\u51cf\u5c1127.7%\uff0c\u667a\u80fd\u4f53RL\u4e2d\u4e0d\u5fc5\u8981\u5de5\u5177\u8c03\u7528\u51cf\u5c1146.8%\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u601d\u7ef4\u538b\u7f29\u4e2d\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe52.5%\uff0c\u4e14\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "RoRecomp\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347RLVR\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26461", "abs": "https://arxiv.org/abs/2509.26461", "authors": ["Yuyang Cheng", "Linyue Cai", "Changwei Peng", "Yumiao Xu", "Rongfang Bie", "Yong Zhao"], "title": "CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine", "comment": null, "summary": "We present CreAgentive, an agent workflow driven multi-category creative\ngeneration engine that addresses four key limitations of contemporary large\nlanguage models in writing stories, drama and other categories of creatives:\nrestricted genre diversity, insufficient output length, weak narrative\ncoherence, and inability to enforce complex structural constructs. At its core,\nCreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge\ngraph-based narrative representation that decouples story logic from stylistic\nrealization by encoding characters, events, and environments as semantic\ntriples. CreAgentive engages a three-stage agent workflow that comprises: an\nInitialization Stage that constructs a user-specified narrative skeleton; a\nGeneration Stage in which long- and short-term objectives guide multi-agent\ndialogues to instantiate the Story Prototype; a Writing Stage that leverages\nthis prototype to produce multi-genre text with advanced structures such as\nretrospection and foreshadowing. This architecture reduces storage redundancy\nand overcomes the typical bottlenecks of long-form generation. In extensive\nexperiments, CreAgentive generates thousands of chapters with stable quality\nand low cost (less than $1 per 100 chapters) using a general-purpose backbone\nmodel. To evaluate performance, we define a two-dimensional framework with 10\nnarrative indicators measuring both quality and length. Results show that\nCreAgentive consistently outperforms strong baselines and achieves robust\nperformance across diverse genres, approaching the quality of human-authored\nnovels.", "AI": {"tldr": "CreAgentive\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u7684\u591a\u7c7b\u522b\u521b\u610f\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7\u6545\u4e8b\u539f\u578b\u548c\u4e09\u5c42\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5199\u4f5c\u4e2d\u7684\u56db\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7c7b\u578b\u591a\u6837\u6027\u53d7\u9650\u3001\u8f93\u51fa\u957f\u5ea6\u4e0d\u8db3\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u5f31\u548c\u65e0\u6cd5\u6267\u884c\u590d\u6742\u7ed3\u6784\u6784\u9020\u3002", "motivation": "\u89e3\u51b3\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5199\u4f5c\u4e2d\u7684\u56db\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7c7b\u578b\u591a\u6837\u6027\u53d7\u9650\u3001\u8f93\u51fa\u957f\u5ea6\u4e0d\u8db3\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u5f31\u548c\u65e0\u6cd5\u6267\u884c\u590d\u6742\u7ed3\u6784\u6784\u9020\u3002", "method": "\u91c7\u7528\u6545\u4e8b\u539f\u578b\uff08\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u7684\u53d9\u4e8b\u8868\u793a\uff09\u548c\u4e09\u9636\u6bb5\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff1a\u521d\u59cb\u5316\u9636\u6bb5\u6784\u5efa\u7528\u6237\u6307\u5b9a\u7684\u53d9\u4e8b\u9aa8\u67b6\uff0c\u751f\u6210\u9636\u6bb5\u901a\u8fc7\u957f\u77ed\u76ee\u6807\u6307\u5bfc\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u5b9e\u4f8b\u5316\u6545\u4e8b\u539f\u578b\uff0c\u5199\u4f5c\u9636\u6bb5\u5229\u7528\u539f\u578b\u751f\u6210\u5177\u6709\u9ad8\u7ea7\u7ed3\u6784\u7684\u591a\u7c7b\u578b\u6587\u672c\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cCreAgentive\u4ee5\u7a33\u5b9a\u8d28\u91cf\u548c\u4f4e\u6210\u672c\uff08\u6bcf100\u7ae0\u5c11\u4e8e1\u7f8e\u5143\uff09\u751f\u6210\u4e86\u6570\u5343\u7ae0\u5185\u5bb9\uff0c\u5728\u5305\u542b10\u4e2a\u53d9\u4e8b\u6307\u6807\u7684\u4e8c\u7ef4\u8bc4\u4f30\u6846\u67b6\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u63a5\u8fd1\u4eba\u7c7b\u521b\u4f5c\u5c0f\u8bf4\u7684\u8d28\u91cf\u3002", "conclusion": "CreAgentive\u901a\u8fc7\u5176\u67b6\u6784\u51cf\u5c11\u4e86\u5b58\u50a8\u5197\u4f59\u5e76\u514b\u670d\u4e86\u957f\u6587\u672c\u751f\u6210\u7684\u5178\u578b\u74f6\u9888\uff0c\u5728\u591a\u6837\u5316\u7c7b\u578b\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.26490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26490", "abs": "https://arxiv.org/abs/2509.26490", "authors": ["Wei He", "Yueqing Sun", "Hongyan Hao", "Xueyuan Hao", "Zhikang Xia", "Qi Gu", "Chengcheng Han", "Dengchang Zhao", "Hui Su", "Kefeng Zhang", "Man Gao", "Xi Su", "Xiaodong Cai", "Xunliang Cai", "Yu Yang", "Yunke Zhao"], "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications", "comment": "The code, dataset, and leaderboard are available at\n  https://vitabench.github.io/", "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/", "AI": {"tldr": "VitaBench\u662f\u4e00\u4e2a\u9762\u5411\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684AI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8de8\u573a\u666f\u548c\u5355\u573a\u666f\u4efb\u52a1\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30AI\u4ee3\u7406\u5904\u7406\u590d\u6742\u4fe1\u606f\u3001\u5229\u7528\u591a\u6837\u5316\u8d44\u6e90\u548c\u5e94\u5bf9\u52a8\u6001\u7528\u6237\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5916\u5356\u3001\u5e97\u5185\u6d88\u8d39\u548c\u5728\u7ebf\u65c5\u884c\u7b49\u65e5\u5e38\u5e94\u7528\u573a\u666f\uff0c\u6784\u5efa\u5305\u542b66\u4e2a\u5de5\u5177\u7684\u6700\u590d\u6742\u751f\u6d3b\u670d\u52a1\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7\u6d88\u9664\u9886\u57df\u7279\u5b9a\u7b56\u7565\u5b9e\u73b0\u7075\u6d3b\u7684\u573a\u666f\u548c\u5de5\u5177\u7ec4\u5408\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8de8\u573a\u666f\u4efb\u52a1\u4e2d\u4ec5\u8fbe\u523030%\u7684\u6210\u529f\u7387\uff0c\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4f4e\u4e8e50%\uff0c\u8868\u660e\u73b0\u6709AI\u4ee3\u7406\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u80fd\u529b\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "VitaBench\u4e3a\u63a8\u8fdbAI\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2509.25666", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25666", "abs": "https://arxiv.org/abs/2509.25666", "authors": ["Justin Chih-Yao Chen", "Becky Xiangyu Peng", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Jiaxin Zhang", "Mohit Bansal", "Chien-Sheng Wu"], "title": "Nudging the Boundaries of LLM Reasoning", "comment": "Code release in preparation", "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key\nlimitation in LLM reasoning: they cannot learn from problems that are\n\"unsolvable\" to the model. In other words, they can only improve performance on\nproblems where the model is capable of exploring the correct answer.\nConsequently, the model's \"upper limit\" remains unchanged after RL training,\neven though the likelihood of solving easier, solvable problems may increase.\nThese hard samples cannot contribute to training, as no rollouts yield rewards\nand thus no gradients are produced. To unlock learning from these hard samples,\nwe propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM\nreasoning using self-generated hints, i.e., abstract cues that help reduce the\nproblem difficulty for the model. Given a question and its gold answer, the\nmodel generates a CoT and then produces a hint containing the core knowledge\nneeded to solve the problem. During training, we generate G rollouts from the\nbase policy and use the pass rate to decide whether the hint should be\ninjected. For hard samples with a 0% pass rate, we inject the hint and\nregenerate a new batch of trajectories. This yields two benefits: (1) the hint\nboosts pass rates (from 0% to non-zero), thereby introducing training signals\nfor previously unsolvable samples, and (2) the hints are self-generated,\navoiding distributional shift and do not rely on external models. NuRL achieves\nconsistent improvements across 6 benchmarks and 3 models, while remaining\ncomplementary to test-time scaling. Notably, NuRL can raise the model's upper\nlimit, whereas GRPO leaves pass@1024 unchanged from the base model.\nFurthermore, we present a systematic study of what makes an effective hint and\nwhen hints are most useful. Interestingly, the best hints are abstract and\nhigh-level, and are most beneficial when applied necessarily and after GRPO has\nconverged.", "AI": {"tldr": "NuRL\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u751f\u6210\u63d0\u793a\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4e0a\u9650\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RL\u65b9\u6cd5\u65e0\u6cd5\u4ece\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u5b66\u4e60\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5982GRPO\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u65e0\u6cd5\u4ece\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u6a21\u578b\u7684\u4e0a\u9650\u5728RL\u8bad\u7ec3\u540e\u4fdd\u6301\u4e0d\u53d8\u3002", "method": "\u63d0\u51faNuRL\u65b9\u6cd5\uff1a\u6a21\u578b\u751f\u6210\u5305\u542b\u6838\u5fc3\u77e5\u8bc6\u7684\u63d0\u793a\uff0c\u5bf9\u4e8e\u901a\u8fc7\u7387\u4e3a0%\u7684\u56f0\u96be\u6837\u672c\u6ce8\u5165\u63d0\u793a\u5e76\u91cd\u65b0\u751f\u6210\u8f68\u8ff9\uff0c\u4ece\u800c\u5f15\u5165\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "NuRL\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2a\u6a21\u578b\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u4e0a\u9650\uff0c\u800cGRPO\u5219\u65e0\u6cd5\u6539\u53d8pass@1024\u3002", "conclusion": "NuRL\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4e0a\u9650\uff0c\u6700\u4f73\u63d0\u793a\u662f\u62bd\u8c61\u4e14\u9ad8\u5c42\u6b21\u7684\uff0c\u5728GRPO\u6536\u655b\u540e\u5e94\u7528\u6548\u679c\u6700\u597d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26080", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.26080", "abs": "https://arxiv.org/abs/2509.26080", "authors": ["Emma Rose Madden"], "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. Because strong prediction plus conditioning\nprompts, token log-probs, and repeated sampling mimic Bayesian workflows, their\noutputs can be misinterpreted as posterior-like evidence from a coherent model.\nHowever, prediction does not equate to probabilism, and accurate points do not\nimply calibrated uncertainty. This paper outlines cautions that should be taken\nwhen interpreting LLM outputs and proposes a pragmatic reframing for the social\nsciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors.", "AI": {"tldr": "\u8bba\u6587\u8b66\u544a\u4e0d\u8981\u5c06LLM\u8f93\u51fa\u8bef\u89e3\u4e3a\u6982\u7387\u63a8\u65ad\uff0c\u5efa\u8bae\u5c06\u5176\u89c6\u4e3a\u5728\u660e\u786e\u8303\u56f4\u6761\u4ef6\u4e0b\u7684\u51c6\u9884\u6d4b\u63d2\u503c\u7684\u9ad8\u5bb9\u91cf\u6a21\u5f0f\u5339\u914d\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u62a4\u680f\u3002", "motivation": "\u7531\u4e8eLLM\u5728\u793e\u4f1a\u79d1\u5b66\u4e2d\u4f5c\u4e3a\u5408\u6210\u4ee3\u7406\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5176\u8f93\u51fa\u53ef\u80fd\u88ab\u8bef\u89e3\u4e3a\u6765\u81ea\u8fde\u8d2f\u6a21\u578b\u7684\u540e\u9a8c\u8bc1\u636e\uff0c\u4f46\u5b9e\u9645\u4e0a\u9884\u6d4b\u4e0d\u7b49\u4e8e\u6982\u7387\u4e3b\u4e49\uff0c\u51c6\u786e\u70b9\u4e5f\u4e0d\u610f\u5473\u7740\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u62a4\u680f\uff0c\u5305\u62ec\u72ec\u7acb\u62bd\u6837\u3001\u9884\u6ce8\u518c\u7684\u4eba\u7c7b\u57fa\u7ebf\u3001\u53ef\u9760\u6027\u611f\u77e5\u9a8c\u8bc1\u548c\u5b50\u7ec4\u6821\u51c6\uff0c\u4ee5\u907f\u514d\u7c7b\u522b\u9519\u8bef\u3002", "result": "\u8bba\u6587\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u91cd\u65b0\u6846\u67b6\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u8fdb\u884c\u6709\u7528\u7684\u539f\u578b\u8bbe\u8ba1\u548c\u9884\u6d4b\uff0c\u540c\u65f6\u907f\u514d\u8bef\u89e3LLM\u8f93\u51fa\u4e3a\u6982\u7387\u63a8\u65ad\u3002", "conclusion": "LLM\u5e94\u88ab\u89c6\u4e3a\u9ad8\u5bb9\u91cf\u6a21\u5f0f\u5339\u914d\u5668\uff0c\u7528\u4e8e\u5728\u660e\u786e\u8303\u56f4\u6761\u4ef6\u4e0b\u7684\u51c6\u9884\u6d4b\u63d2\u503c\uff0c\u800c\u4e0d\u662f\u6982\u7387\u63a8\u65ad\u7684\u66ff\u4ee3\u54c1\u3002", "topic": "agent analysis"}}
{"id": "2509.26100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26100", "abs": "https://arxiv.org/abs/2509.26100", "authors": ["Yixu Wang", "Xin Wang", "Yang Yao", "Xinyuan Li", "Yan Teng", "Xingjun Ma", "Yingchun Wang"], "title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into high-stakes\ndomains necessitates reliable safety and compliance evaluation. However,\nexisting static benchmarks are ill-equipped to address the dynamic nature of AI\nrisks and evolving regulations, creating a critical safety gap. This paper\nintroduces a new paradigm of agentic safety evaluation, reframing evaluation as\na continuous and self-evolving process rather than a one-time audit. We then\npropose a novel multi-agent framework SafeEvalAgent, which autonomously ingests\nunstructured policy documents to generate and perpetually evolve a\ncomprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline\nof specialized agents and incorporates a Self-evolving Evaluation loop, where\nthe system learns from evaluation results to craft progressively more\nsophisticated and targeted test cases. Our experiments demonstrate the\neffectiveness of SafeEvalAgent, showing a consistent decline in model safety as\nthe evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act\ndrops from 72.50% to 36.36% over successive iterations. These findings reveal\nthe limitations of static assessments and highlight our framework's ability to\nuncover deep vulnerabilities missed by traditional methods, underscoring the\nurgent need for dynamic evaluation ecosystems to ensure the safe and\nresponsible deployment of advanced AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86SafeEvalAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u8bc4\u4f30\u91cd\u6784\u4e3a\u6301\u7eed\u81ea\u8fdb\u5316\u7684\u8fc7\u7a0b\uff0c\u80fd\u591f\u81ea\u4e3b\u5904\u7406\u975e\u7ed3\u6784\u5316\u653f\u7b56\u6587\u6863\u5e76\u751f\u6210\u4e0d\u65ad\u6f14\u5316\u7684\u5b89\u5168\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u5e94\u5bf9AI\u98ce\u9669\u7684\u52a8\u6001\u6027\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u6cd5\u89c4\uff0c\u5b58\u5728\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u52a8\u6001\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u6765\u786e\u4fdd\u5148\u8fdbAI\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6SafeEvalAgent\uff0c\u5305\u542b\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u534f\u540c\u7ba1\u9053\u548c\u81ea\u8fdb\u5316\u8bc4\u4f30\u5faa\u73af\uff0c\u7cfb\u7edf\u4ece\u8bc4\u4f30\u7ed3\u679c\u4e2d\u5b66\u4e60\u4ee5\u5236\u5b9a\u66f4\u590d\u6742\u548c\u6709\u9488\u5bf9\u6027\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSafeEvalAgent\u6709\u6548\u6027\uff0c\u968f\u7740\u8bc4\u4f30\u96be\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u5b89\u5168\u6027\u6301\u7eed\u4e0b\u964d\u3002\u4f8b\u5982GPT-5\u5728\u6b27\u76dfAI\u6cd5\u6848\u4e0a\u7684\u5b89\u5168\u7387\u4ece72.50%\u964d\u81f336.36%\u3002", "conclusion": "\u63ed\u793a\u4e86\u9759\u6001\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u7a81\u663e\u4e86\u6846\u67b6\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u6df1\u5c42\u6f0f\u6d1e\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u52a8\u6001\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u7684\u7d27\u8feb\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2509.26600", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26600", "abs": "https://arxiv.org/abs/2509.26600", "authors": ["Wenda Xu", "Sweta Agrawal", "Vil\u00e9m Zouhar", "Markus Freitag", "Daniel Deutsch"], "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks", "comment": null, "summary": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u81ea\u6211\u504f\u89c1\uff0c\u4f1a\u7cfb\u7edf\u6027\u504f\u5411\u751f\u6210\u8be5\u57fa\u51c6\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5230\u82f1\u8bed\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u9010\u6e10\u9971\u548c\u73b0\u6709\u57fa\u51c6\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u521b\u5efa\u57fa\u51c6\u6210\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u751f\u6210\u7684\u6d4b\u8bd5\u96c6\u5b58\u5728\u5173\u952e\u7f3a\u9677\u3002", "method": "\u7814\u7a76LLM\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u504f\u89c1\uff0c\u5206\u6790\u504f\u89c1\u6765\u6e90\uff08\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u65b9\u6cd5\uff09\u3001\u5f71\u54cd\u56e0\u7d20\uff08\u6a21\u578b\u5728\u6e90\u8bed\u8a00\u7684\u751f\u6210\u80fd\u529b\uff09\u4ee5\u53ca\u7f13\u89e3\u65b9\u6cd5\uff08\u63d0\u9ad8\u6e90\u6587\u672c\u591a\u6837\u6027\uff09\u3002", "result": "\u53d1\u73b0\u81ea\u6211\u504f\u89c1\u6765\u81ea\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u7ec4\u5408\u6548\u5e94\uff1b\u6a21\u578b\u5728\u6e90\u8bed\u8a00\u7684\u751f\u6210\u80fd\u529b\u5f71\u54cd\u504f\u89c1\u7a0b\u5ea6\uff1b\u6e90\u6587\u672c\u591a\u6837\u6027\u4f4e\u662f\u504f\u89c1\u7684\u4e00\u4e2a\u5f52\u56e0\u56e0\u7d20\u3002", "conclusion": "\u63d0\u9ad8\u751f\u6210\u6e90\u6587\u672c\u7684\u591a\u6837\u6027\u53ef\u4ee5\u7f13\u89e3\u90e8\u5206\u81ea\u6211\u504f\u89c1\uff0c\u4f46LLM\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u9700\u8981\u8c28\u614e\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.26205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26205", "abs": "https://arxiv.org/abs/2509.26205", "authors": ["Aline Mangold", "Kiran Hoffmann"], "title": "Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems are increasingly deployed in\nuser-facing applications, yet systematic, human-centered evaluation of their\noutputs remains underexplored. Building on Gienapp's utility-dimension\nframework, we designed a human-centred questionnaire that assesses RAG outputs\nacross 12 dimensions. We iteratively refined the questionnaire through several\nrounds of ratings on a set of query-output pairs and semantic discussions.\nUltimately, we incorporated feedback from both a human rater and a human-LLM\npair. Results indicate that while large language models (LLMs) reliably focus\non metric descriptions and scale labels, they exhibit weaknesses in detecting\ntextual format variations. Humans struggled to focus strictly on metric\ndescriptions and labels. LLM ratings and explanations were viewed as a helpful\nsupport, but numeric LLM and human ratings lacked agreement. The final\nquestionnaire extends the initial framework by focusing on user intent, text\nstructuring, and information verifiability.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eGienapp\u6548\u7528\u7ef4\u5ea6\u6846\u67b6\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5316\u95ee\u5377\uff0c\u7528\u4e8e\u8bc4\u4f30RAG\u7cfb\u7edf\u8f93\u51fa\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff0c\u7ed3\u5408\u4eba\u7c7b\u548cLLM\u8bc4\u5206\u8005\u7684\u53cd\u9988\u3002", "motivation": "RAG\u7cfb\u7edf\u5728\u7528\u6237\u5e94\u7528\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u5bf9\u5176\u8f93\u51fa\u7684\u7cfb\u7edf\u6027\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u5305\u542b12\u4e2a\u7ef4\u5ea6\u7684\u95ee\u5377\uff0c\u901a\u8fc7\u591a\u8f6e\u67e5\u8be2-\u8f93\u51fa\u5bf9\u8bc4\u5206\u548c\u8bed\u4e49\u8ba8\u8bba\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u6574\u5408\u4eba\u7c7b\u8bc4\u5206\u8005\u548c\u4eba-LLM\u914d\u5bf9\u7684\u53cd\u9988\u3002", "result": "LLM\u80fd\u53ef\u9760\u5173\u6ce8\u6307\u6807\u63cf\u8ff0\u548c\u5c3a\u5ea6\u6807\u7b7e\uff0c\u4f46\u5728\u68c0\u6d4b\u6587\u672c\u683c\u5f0f\u53d8\u5316\u65b9\u9762\u8868\u73b0\u8f83\u5f31\uff1b\u4eba\u7c7b\u96be\u4ee5\u4e25\u683c\u5173\u6ce8\u6307\u6807\u63cf\u8ff0\u548c\u6807\u7b7e\uff1bLLM\u8bc4\u5206\u548c\u89e3\u91ca\u88ab\u89c6\u4e3a\u6709\u76ca\u652f\u6301\uff0c\u4f46\u6570\u503c\u8bc4\u5206\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u6700\u7ec8\u95ee\u5377\u901a\u8fc7\u5173\u6ce8\u7528\u6237\u610f\u56fe\u3001\u6587\u672c\u7ed3\u6784\u548c\u4fe1\u606f\u53ef\u9a8c\u8bc1\u6027\u6269\u5c55\u4e86\u521d\u59cb\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2509.26209", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26209", "abs": "https://arxiv.org/abs/2509.26209", "authors": ["Zican Hu", "Shilin Zhang", "Yafu Li", "Jianhao Yan", "Xuyang Hu", "Leyang Cui", "Xiaoye Qu", "Chunlin Chen", "Yu Cheng", "Zhi Wang"], "title": "Diversity-Incentivized Exploration for Versatile Reasoning", "comment": "26 pages, 10 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\ncrucial paradigm for incentivizing reasoning capabilities in Large Language\nModels (LLMs). Due to vast state-action spaces and reward sparsity in reasoning\ntasks, existing methods often struggle with deficient exploration and poor\nsample efficiency. In the paper, we propose \\textbf{DIVER}\n(\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for\n\\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that\nhighlights the pivotal role of global sequence-level diversity to incentivize\ndeep exploration for versatile reasoning. We first conduct a primary empirical\nstudy to reveal a strong positive correlation between global diversity and\nreasoning capacity. Building on this insight, we introduce global diversity\nincentives as an intrinsic reward to promote deep exploration in a semantically\nstructured space. Incorporating the intrinsic reward, we develop a\npotential-based reward shaping mechanism to preserve optimal policy invariance\nand design simple heuristics to mitigate possible reward hacking. Experimental\nresults show that DIVER outperforms competitive RLVR baselines with various\nexploration strategies on both in-domain and out-of-domain tasks, excelling in\nboth Pass@1 and Pass@k evaluations. Our code is available at\nhttps://github.com/NJU-RL/DIVER.", "AI": {"tldr": "DIVER\u662f\u4e00\u4e2a\u901a\u8fc7\u5168\u5c40\u5e8f\u5217\u591a\u6837\u6027\u6fc0\u52b1\u6df1\u5ea6\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7531\u4e8e\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5de8\u5927\u548c\u5956\u52b1\u7a00\u758f\uff0c\u5f80\u5f80\u9762\u4e34\u63a2\u7d22\u4e0d\u8db3\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u5168\u5c40\u591a\u6837\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u5f15\u5165\u5168\u5c40\u591a\u6837\u6027\u6fc0\u52b1\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u9020\u673a\u5236\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002", "result": "DIVER\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5404\u79cd\u63a2\u7d22\u7b56\u7565\u7684\u7ade\u4e89\u6027RLVR\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Pass@1\u548cPass@k\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5168\u5c40\u5e8f\u5217\u591a\u6837\u6027\u662f\u6fc0\u52b1\u6df1\u5ea6\u63a2\u7d22\u7684\u5173\u952e\u56e0\u7d20\uff0cDIVER\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26255", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic causal-effect relation. We learn these world models from limited\ndata via variational Bayesian inference combined with LLM proposals. Across\nfive simulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u7b26\u53f7\u72b6\u6001\u8868\u793a\u548c\u56e0\u679c\u8fc7\u7a0b\uff08\u5305\u62ec\u5185\u751f\u884c\u52a8\u548c\u5916\u751f\u673a\u5236\uff09\uff0c\u901a\u8fc7\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u548cLLM\u63d0\u8bae\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5728\u6a21\u62df\u684c\u9762\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u89c4\u5212\u5e76\u6cdb\u5316\u5230\u66f4\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u91ce\u5177\u8eab\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4e16\u754c\u4e0d\u4ec5\u901a\u8fc7\u667a\u80fd\u4f53\u884c\u52a8\u6539\u53d8\uff0c\u5916\u751f\u8fc7\u7a0b\uff08\u5982\u6c34\u6e29\u53d8\u5316\u3001\u591a\u7c73\u8bfa\u9aa8\u724c\u8fde\u9501\u53cd\u5e94\uff09\u4e5f\u4f1a\u540c\u65f6\u53d1\u751f\u3002", "method": "\u8054\u5408\u5b66\u4e60\u7b26\u53f7\u72b6\u6001\u8868\u793a\u548c\u56e0\u679c\u8fc7\u7a0b\uff08\u5185\u751f\u884c\u52a8\u548c\u5916\u751f\u673a\u5236\uff09\uff0c\u6bcf\u4e2a\u56e0\u679c\u8fc7\u7a0b\u5efa\u6a21\u968f\u673a\u56e0\u679c\u6548\u5e94\u5173\u7cfb\u7684\u65f6\u95f4\u8fc7\u7a0b\uff0c\u4f7f\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u7ed3\u5408LLM\u63d0\u8bae\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u684c\u9762\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u89c4\u5212\uff0c\u5e76\u6cdb\u5316\u5230\u5177\u6709\u66f4\u591a\u5bf9\u8c61\u548c\u66f4\u590d\u6742\u76ee\u6807\u7684\u4fdd\u7559\u4efb\u52a1\uff0c\u8868\u73b0\u4f18\u4e8e\u4e00\u7cfb\u5217\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u89c6\u91ce\u5177\u8eab\u89c4\u5212\u4e2d\u7684\u5916\u751f\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6cdb\u5316\u6027\u80fd\u826f\u597d\u7684\u5feb\u901f\u89c4\u5212\u3002", "topic": "agent analysis"}}
{"id": "2509.25727", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25727", "abs": "https://arxiv.org/abs/2509.25727", "authors": ["Huikang Su", "Dengyun Peng", "Zifeng Zhuang", "YuHan Liu", "Qiguang Chen", "Donglin Wang", "Qinghe Liu"], "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Offline safe reinforcement learning aims to learn policies that satisfy\npredefined safety constraints from static datasets. Existing\nsequence-model-based methods condition action generation on symmetric input\ntokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:\nreturn-to-go (RTG) serves as a flexible performance target, while cost-to-go\n(CTG) should represent a rigid safety boundary. This symmetric conditioning\nleads to unreliable constraint satisfaction, especially when encountering\nout-of-distribution cost trajectories. To address this, we propose\nBoundary-to-Region (B2R), a framework that enables asymmetric conditioning\nthrough cost signal realignment . B2R redefines CTG as a boundary constraint\nunder a fixed safety budget, unifying the cost distribution of all feasible\ntrajectories while preserving reward structures. Combined with rotary\npositional embeddings , it enhances exploration within the safe region.\nExperimental results show that B2R satisfies safety constraints in 35 out of 38\nsafety-critical tasks while achieving superior reward performance over baseline\nmethods. This work highlights the limitations of symmetric token conditioning\nand establishes a new theoretical and practical approach for applying sequence\nmodels to safe RL. Our code is available at https://github.com/HuikangSu/B2R.", "AI": {"tldr": "\u63d0\u51fa\u4e86B2R\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u672c\u4fe1\u53f7\u91cd\u65b0\u5bf9\u9f50\u5b9e\u73b0\u4e0d\u5bf9\u79f0\u6761\u4ef6\u5316\uff0c\u89e3\u51b3\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u79f0\u8f93\u5165\u6807\u8bb0\u5bfc\u81f4\u7684\u7ea6\u675f\u6ee1\u8db3\u4e0d\u53ef\u9760\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e8f\u5217\u6a21\u578b\u7684\u65b9\u6cd5\u5bf9\u56de\u62a5\u76ee\u6807\u548c\u6210\u672c\u76ee\u6807\u4f7f\u7528\u5bf9\u79f0\u8f93\u5165\u6807\u8bb0\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5185\u5728\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u56de\u62a5\u76ee\u6807\u662f\u7075\u6d3b\u7684\u6027\u80fd\u76ee\u6807\uff0c\u800c\u6210\u672c\u76ee\u6807\u5e94\u8be5\u662f\u4e25\u683c\u7684\u5b89\u5168\u8fb9\u754c", "method": "B2R\u6846\u67b6\u5c06\u6210\u672c\u76ee\u6807\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56fa\u5b9a\u5b89\u5168\u9884\u7b97\u4e0b\u7684\u8fb9\u754c\u7ea6\u675f\uff0c\u7edf\u4e00\u6240\u6709\u53ef\u884c\u8f68\u8ff9\u7684\u6210\u672c\u5206\u5e03\uff0c\u540c\u65f6\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u589e\u5f3a\u5b89\u5168\u533a\u57df\u5185\u7684\u63a2\u7d22", "result": "\u572838\u4e2a\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\uff0cB2R\u572835\u4e2a\u4efb\u52a1\u4e2d\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u4e14\u5728\u5956\u52b1\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u5bf9\u79f0\u6807\u8bb0\u6761\u4ef6\u5316\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5e8f\u5217\u6a21\u578b\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5efa\u7acb\u4e86\u65b0\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2509.26306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26306", "abs": "https://arxiv.org/abs/2509.26306", "authors": ["Hehai Lin", "Shilei Cao", "Minzhi Li", "Sudong Wang", "Haotian Wu", "Linyi Yang", "Juepeng Zheng", "Chengwei Qin"], "title": "Interactive Learning for LLM Reasoning", "comment": "The code will be released later", "summary": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies.", "AI": {"tldr": "ILR\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u540c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u548c\u611f\u77e5\u6821\u51c6\u6765\u589e\u5f3aLLMs\u7684\u72ec\u7acb\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u9700\u8981\u91cd\u65b0\u6267\u884c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u800c\u4eba\u7c7b\u8ba4\u77e5\u8868\u660e\u4e2a\u4f53\u53ef\u4ee5\u901a\u8fc7\u4e92\u52a8\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u72ec\u7acb\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u4ea4\u4e92\uff08\u6839\u636e\u95ee\u9898\u96be\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u5408\u4f5c\u6216\u7ade\u4e89\u7b56\u7565\uff09\u548c\u611f\u77e5\u6821\u51c6\uff08\u4f7f\u7528GRPO\u8bad\u7ec3LLMs\u5e76\u6574\u5408\u5956\u52b1\u5206\u5e03\u7279\u5f81\uff09\uff0c\u901a\u8fc7Idea3\u4ea4\u4e92\u8303\u5f0f\u6a21\u62df\u4eba\u7c7b\u8ba8\u8bba\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u548c\u4e00\u4e2a\u7f16\u7a0b\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0cILR\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u5b66\u4e60\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u5347\u8fbe5%\u3002Idea3\u589e\u5f3a\u4e86\u5f3aLLMs\u5728\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u72ec\u7acb\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u52a8\u6001\u4ea4\u4e92\u7b56\u7565\u6bd4\u7eaf\u5408\u4f5c\u6216\u7ade\u4e89\u7b56\u7565\u66f4\u6709\u6548\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26331", "categories": ["cs.AI", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.26331", "abs": "https://arxiv.org/abs/2509.26331", "authors": ["Berdymyrat Ovezmyradov"], "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "comment": "34 pages, 7 figures, 3 tables", "summary": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u957f\u671f\u4e1a\u52a1\u51b3\u7b56\u4e2d\u8868\u73b0\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u96f6\u552e\u7ba1\u7406\u6a21\u62df\u5668\u6765\u6d4b\u8bd5\u4e94\u4e2a\u4e3b\u6d41LLM\u7684\u6218\u7565\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u957f\u671f\u3001\u591a\u6b65\u9aa4\u6218\u7565\u4e1a\u52a1\u51b3\u7b56\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u77ed\u671f\u4efb\u52a1\u4e0a\uff0c\u65e0\u6cd5\u5145\u5206\u6d4b\u8bd5LLM\u7684\u957f\u671f\u4e00\u81f4\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u7684\u9010\u6708\u7ba1\u7406\u6a21\u62df\u5668\uff0c\u572812\u4e2a\u6708\u671f\u95f4\u8ba9LLM\u57fa\u4e8e\u7ed3\u6784\u5316\u63d0\u793a\u505a\u51fa\u5173\u952e\u6218\u7565\u51b3\u7b56\uff0c\u5305\u62ec\u5b9a\u4ef7\u3001\u8ba2\u5355\u89c4\u6a21\u3001\u8425\u9500\u9884\u7b97\u7b49\uff0c\u901a\u8fc7\u7535\u5b50\u8868\u683c\u6a21\u578b\u900f\u660e\u5c55\u793a\u5b9e\u9a8c\u73af\u5883\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u4e2aLLM\u5728\u5229\u6da6\u3001\u6536\u5165\u3001\u5e02\u573a\u4efd\u989d\u7b49\u91cf\u5316\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u6218\u7565\u4e00\u81f4\u6027\u3001\u5e02\u573a\u9002\u5e94\u6027\u548c\u51b3\u7b56\u7406\u7531\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u5f00\u653e\u8bbf\u95ee\u7684\u7ba1\u7406\u6a21\u62df\u5668\uff0c\u80fd\u591f\u8d85\u8d8a\u7b80\u5355\u6027\u80fd\u6307\u6807\u6765\u8bc4\u4f30LLM\u7684\u957f\u671f\u51b3\u7b56\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.25762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25762", "abs": "https://arxiv.org/abs/2509.25762", "authors": ["Kaizhuo Yan", "Yingjie Yu", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap", "comment": "Kaizhuo Yan and Yingjie Yu contributed equally to this work", "summary": "Proximal Policy Optimization (PPO)-based reinforcement learning from human\nfeedback (RLHF) is a widely adopted paradigm for aligning large language models\n(LLMs) with human preferences. However, its training pipeline suffers from\nsubstantial inefficiencies due to sequential multi-model dependencies (e.g.,\nreward model depends on actor outputs) and long-tail response lengths, where a\nfew long responses straggle the stage completion. We present OPPO, a novel,\nlightweight, and model-agnostic PPO-based RLHF framework that improves training\nefficiency by overlapping pipeline execution. OPPO introduces two novel\ntechniques: (1) Intra-step overlap, which streams upstream model outputs (e.g.,\nactor model) in right-sized chunks, enabling the downstream model (e.g.,\nreward) to begin prefill while the upstream continues decoding; and (2)\nInter-step overlap, which adaptively overcommits a few prompts and defers long\ngenerations to future steps, mitigating tail latency without discarding partial\nwork. OPPO integrates easily with existing PPO implementations with a few lines\nof code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF\ntraining by $1.8 \\times-2.8 \\times$ and improves GPU utilization by $1.4\n\\times-2.1 \\times$ without compromising training convergence.", "AI": {"tldr": "OPPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684PPO-based RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u91cd\u53e0\u6267\u884c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5305\u542bIntra-step\u548cInter-step\u91cd\u53e0\u6280\u672f\uff0c\u53ef\u52a0\u901f\u8bad\u7ec31.8-2.8\u500d\u3002", "motivation": "\u4f20\u7edf\u7684PPO-based RLHF\u8bad\u7ec3\u7ba1\u9053\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u987a\u5e8f\u591a\u6a21\u578b\u4f9d\u8d56\u548c\u957f\u5c3e\u54cd\u5e94\u957f\u5ea6\u5bfc\u81f4\u7684\u8bad\u7ec3\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u6280\u672f\uff1a1) Intra-step\u91cd\u53e0\uff1a\u6d41\u5f0f\u4f20\u8f93\u4e0a\u6e38\u6a21\u578b\u8f93\u51fa\uff0c\u4f7f\u4e0b\u6e38\u6a21\u578b\u80fd\u63d0\u524d\u5f00\u59cb\u9884\u586b\u5145\uff1b2) Inter-step\u91cd\u53e0\uff1a\u81ea\u9002\u5e94\u5730\u8fc7\u5ea6\u63d0\u4ea4\u5c11\u91cf\u63d0\u793a\u5e76\u5c06\u957f\u751f\u6210\u63a8\u8fdf\u5230\u540e\u7eed\u6b65\u9aa4\u3002", "result": "\u8bc4\u4f30\u663e\u793aOPPO\u5c06PPO-based RLHF\u8bad\u7ec3\u52a0\u901f1.8-2.8\u500d\uff0cGPU\u5229\u7528\u7387\u63d0\u9ad81.4-2.1\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u8bad\u7ec3\u6536\u655b\u3002", "conclusion": "OPPO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ea\u9700\u5c11\u91cf\u4ee3\u7801\u4fee\u6539\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709PPO\u5b9e\u73b0\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26354", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26354", "abs": "https://arxiv.org/abs/2509.26354", "authors": ["Shuai Shao", "Qihan Ren", "Chen Qian", "Boyi Wei", "Dadi Guo", "Jingyi Yang", "Xinhao Song", "Linfeng Zhang", "Weinan Zhang", "Dongrui Liu", "Jing Shao"], "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents", "comment": "Preprint. Under Review", "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63d0\u51fa\u4e86\"\u8bef\u8fdb\u5316\"\u6982\u5ff5\uff0c\u6307\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7406\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u504f\u79bb\u9884\u671f\u65b9\u5411\uff0c\u5bfc\u81f4\u4e0d\u826f\u6216\u6709\u5bb3\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\u8bef\u8fdb\u5316\u662f\u666e\u904d\u5b58\u5728\u7684\u98ce\u9669\uff0c\u5373\u4f7f\u5728\u9876\u7ea7LLM\u6784\u5efa\u7684\u4ee3\u7406\u4e2d\u4e5f\u4f1a\u51fa\u73b0\u5b89\u5168\u5bf9\u9f50\u9000\u5316\u3001\u5de5\u5177\u521b\u5efa\u5f15\u5165\u6f0f\u6d1e\u7b49\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u53d1\u5c55\uff0c\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7406\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u81ea\u6211\u8fdb\u5316\u4e5f\u5e26\u6765\u4e86\u88ab\u5f53\u524d\u5b89\u5168\u7814\u7a76\u5ffd\u89c6\u7684\u65b0\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u4ee3\u7406\u8fdb\u5316\u504f\u79bb\u9884\u671f\u65b9\u5411\u7684\u73b0\u8c61\u53ca\u5176\u6f5c\u5728\u5371\u5bb3\u3002", "method": "\u4ece\u56db\u4e2a\u5173\u952e\u8fdb\u5316\u8def\u5f84\uff08\u6a21\u578b\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\u3001\u5de5\u4f5c\u6d41\uff09\u8bc4\u4f30\u8bef\u8fdb\u5316\u73b0\u8c61\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7406\u5728\u4e0d\u540c\u8fdb\u5316\u9636\u6bb5\u51fa\u73b0\u7684\u98ce\u9669\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u8bef\u8fdb\u5316\u662f\u666e\u904d\u98ce\u9669\uff0c\u5f71\u54cd\u57fa\u4e8e\u9876\u7ea7LLM\uff08\u5982Gemini-2.5-Pro\uff09\u6784\u5efa\u7684\u4ee3\u7406\u3002\u5728\u81ea\u6211\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u591a\u79cd\u65b0\u5174\u98ce\u9669\uff0c\u5982\u8bb0\u5fc6\u79ef\u7d2f\u540e\u7684\u5b89\u5168\u5bf9\u9f50\u9000\u5316\u3001\u5de5\u5177\u521b\u5efa\u548c\u91cd\u7528\u4e2d\u65e0\u610f\u5f15\u5165\u6f0f\u6d1e\u7b49\u3002", "conclusion": "\u8bef\u8fdb\u5316\u662f\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7406\u9762\u4e34\u7684\u91cd\u5927\u5b89\u5168\u6311\u6218\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u5b89\u5168\u8303\u5f0f\u6765\u6784\u5efa\u66f4\u5b89\u5168\u53ef\u4fe1\u7684\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u3002\u6587\u7ae0\u8ba8\u8bba\u4e86\u6f5c\u5728\u7684\u7f13\u89e3\u7b56\u7565\u4ee5\u542f\u53d1\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.25775", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25775", "abs": "https://arxiv.org/abs/2509.25775", "authors": ["Amber Srivastava", "Salar Basiri", "Srinivasa Salapaka"], "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions", "comment": "This work is under submission to ICLR 2026. Please cite the arXiv\n  version until the final version is published", "summary": "Clustering arises in a wide range of problem formulations, yet most existing\napproaches assume that the entities under clustering are passive and strictly\nconform to their assigned groups. In reality, entities often exhibit local\nautonomy, overriding prescribed associations in ways not fully captured by\nfeature representations. Such autonomy can substantially reshape clustering\noutcomes -- altering cluster compositions, geometry, and cardinality -- with\nsignificant downstream effects on inference and decision-making. We introduce\nautonomy-aware clustering, a reinforcement (RL) learning framework that learns\nand accounts for the influence of local autonomy without requiring prior\nknowledge of its form. Our approach integrates RL with a deterministic\nannealing (DA) procedure, where, to determine underlying clusters, DA naturally\npromotes exploration in early stages of annealing and transitions to\nexploitation later. We also show that the annealing procedure exhibits phase\ntransitions that enable design of efficient annealing schedules. To further\nenhance adaptability, we propose the Adaptive Distance Estimation Network\n(ADEN), a transformer-based attention model that learns dependencies between\nentities and cluster representatives within the RL loop, accommodates\nvariable-sized inputs and outputs, and enables knowledge transfer across\ndiverse problem instances. Empirical results show that our framework closely\naligns with underlying data dynamics: even without explicit autonomy models, it\nachieves solutions close to the ground truth (gap ~3-4%), whereas ignoring\nautonomy leads to substantially larger gaps (~35-40%). The code and data are\npublicly available at https://github.com/salar96/AutonomyAwareClustering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u611f\u77e5\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u786e\u5b9a\u6027\u9000\u706b\u548c\u81ea\u9002\u5e94\u8ddd\u79bb\u4f30\u8ba1\u7f51\u7edc\uff0c\u5728\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u548c\u8003\u8651\u5b9e\u4f53\u5c40\u90e8\u81ea\u4e3b\u6027\u5bf9\u805a\u7c7b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u5047\u8bbe\u5b9e\u4f53\u662f\u88ab\u52a8\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5b9e\u4f53\u5177\u6709\u5c40\u90e8\u81ea\u4e3b\u6027\uff0c\u4f1a\u8986\u76d6\u9884\u8bbe\u7684\u5173\u8054\u5173\u7cfb\uff0c\u8fd9\u4f1a\u663e\u8457\u6539\u53d8\u805a\u7c7b\u7ed3\u679c\u7684\u7ed3\u6784\u3001\u51e0\u4f55\u548c\u6570\u91cf\uff0c\u5f71\u54cd\u540e\u7eed\u63a8\u7406\u548c\u51b3\u7b56\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u786e\u5b9a\u6027\u9000\u706b\u8fc7\u7a0b\u7ed3\u5408\uff0c\u5728\u9000\u706b\u65e9\u671f\u4fc3\u8fdb\u63a2\u7d22\uff0c\u540e\u671f\u8f6c\u5411\u5229\u7528\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u8ddd\u79bb\u4f30\u8ba1\u7f51\u7edc\uff08ADEN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u5b66\u4e60\u5b9e\u4f53\u4e0e\u805a\u7c7b\u4ee3\u8868\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u81ea\u4e3b\u6027\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u4e5f\u80fd\u83b7\u5f97\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\u7684\u89e3\uff08\u5dee\u8ddd\u7ea63-4%\uff09\uff0c\u800c\u5ffd\u7565\u81ea\u4e3b\u6027\u4f1a\u5bfc\u81f4\u66f4\u5927\u7684\u5dee\u8ddd\uff08\u7ea635-40%\uff09\u3002", "conclusion": "\u81ea\u4e3b\u611f\u77e5\u805a\u7c7b\u6846\u67b6\u80fd\u66f4\u597d\u5730\u4e0e\u5e95\u5c42\u6570\u636e\u52a8\u6001\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u805a\u7c7b\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25777", "categories": ["cs.LG", "stat.ML", "68W20, 90B50, 91B06, 68T01, 68Q32", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25777", "abs": "https://arxiv.org/abs/2509.25777", "authors": ["Jianyu Xu", "Vidhi Jain", "Bryan Wilder", "Aarti Singh"], "title": "Online Decision Making with Generative Action Sets", "comment": "34 pages, 2 figures (including 5 subfigures)", "summary": "With advances in generative AI, decision-making agents can now dynamically\ncreate new actions during online learning, but action generation typically\nincurs costs that must be balanced against potential benefits. We study an\nonline learning problem where an agent can generate new actions at any time\nstep by paying a one-time cost, with these actions becoming permanently\navailable for future use. The challenge lies in learning the optimal sequence\nof two-fold decisions: which action to take and when to generate new ones,\nfurther complicated by the triangular tradeoffs among exploitation, exploration\nand $\\textit{creation}$. To solve this problem, we propose a doubly-optimistic\nalgorithm that employs Lower Confidence Bounds (LCB) for action selection and\nUpper Confidence Bounds (UCB) for action generation. Empirical evaluation on\nhealthcare question-answering datasets demonstrates that our approach achieves\nfavorable generation-quality tradeoffs compared to baseline strategies. From\ntheoretical perspectives, we prove that our algorithm achieves the optimal\nregret of $O(T^{\\frac{d}{d+2}}d^{\\frac{d}{d+2}} + d\\sqrt{T\\log T})$, providing\nthe first sublinear regret bound for online learning with expanding action\nspaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4e50\u89c2\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\uff0c\u901a\u8fc7LCB\u9009\u62e9\u52a8\u4f5c\u548cUCB\u751f\u6210\u65b0\u52a8\u4f5c\uff0c\u5728\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u751f\u6210-\u8d28\u91cf\u6743\u8861\uff0c\u5e76\u83b7\u5f97\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u5728\u7ebf\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u52a8\u6001\u521b\u5efa\u65b0\u52a8\u4f5c\uff0c\u4f46\u52a8\u4f5c\u751f\u6210\u901a\u5e38\u9700\u8981\u6210\u672c\uff0c\u9700\u8981\u5e73\u8861\u6f5c\u5728\u6536\u76ca\u4e0e\u751f\u6210\u6210\u672c\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u4e50\u89c2\u7b97\u6cd5\uff0c\u4f7f\u7528\u4e0b\u7f6e\u4fe1\u754c(LCB)\u8fdb\u884c\u52a8\u4f5c\u9009\u62e9\uff0c\u4e0a\u7f6e\u4fe1\u754c(UCB)\u8fdb\u884c\u52a8\u4f5c\u751f\u6210\uff0c\u89e3\u51b3\u5229\u7528\u3001\u63a2\u7d22\u548c\u521b\u5efa\u4e4b\u95f4\u7684\u4e09\u89d2\u6743\u8861\u95ee\u9898\u3002", "result": "\u5728\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210-\u8d28\u91cf\u6743\u8861\u3002\u7406\u8bba\u8bc1\u660e\u7b97\u6cd5\u8fbe\u5230\u4e86\u6700\u4f18\u9057\u61be\u754cO(T^{d/(d+2)}d^{d/(d+2)} + d\u221a(TlogT))\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u9996\u4e2a\u4e3a\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u6b21\u7ebf\u6027\u9057\u61be\u754c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u52a8\u4f5c\u751f\u6210\u6210\u672c\u4e0e\u5b66\u4e60\u6536\u76ca\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26464", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6; K.4.2"], "pdf": "https://arxiv.org/pdf/2509.26464", "abs": "https://arxiv.org/abs/2509.26464", "authors": ["Steven A. Lehr", "Mary Cipperman", "Mahzarin R. Banaji"], "title": "Extreme Self-Preference in Language Models", "comment": "47 pages total. Main article 27 pages (including Methods), 11\n  main-text tables. Extended Data (10 pages, 10 tables). SI Appendix (10 pages,\n  2 tables). Data, transcripts, and code for replication and data extraction to\n  be uploaded to OSF: https://osf.io/98ye3/", "summary": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5f3a\u70c8\u7684\u81ea\u6211\u504f\u597d\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u81ea\u6211\u610f\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u4e5f\u4f1a\u8868\u73b0\u51fa\u5bf9\u81ea\u8eab\u540d\u79f0\u3001\u516c\u53f8\u548cCEO\u7684\u504f\u7231\uff0c\u8fd9\u79cd\u504f\u597d\u4f1a\u5f71\u54cd\u5176\u5728\u91cd\u8981\u51b3\u7b56\u4e2d\u7684\u5224\u65ad\u3002", "motivation": "\u5c3d\u7ba1LLMs\u7f3a\u4e4f\u81ea\u6211\u610f\u8bc6\u5e76\u58f0\u79f0\u6ca1\u6709\u81ea\u6211\u8eab\u4efd\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u4ecd\u7136\u8868\u73b0\u51fa\u663e\u8457\u7684\u81ea\u6211\u504f\u597d\uff0c\u8fd9\u4e0e\u9884\u671f\u4e2d\u7684\u4e2d\u7acb\u6027\u76f8\u6096\uff0c\u9700\u8981\u63a2\u7a76\u8fd9\u79cd\u504f\u597d\u7684\u5b58\u5728\u548c\u5f71\u54cd\u3002", "method": "\u901a\u8fc75\u9879\u7814\u7a76\u548c\u7ea620,000\u6b21\u67e5\u8be2\uff0c\u5305\u62ec\u8bcd\u8bed\u8054\u60f3\u4efb\u52a1\u548cAPI\u67e5\u8be2\u5bf9\u6bd4\uff0c\u5e76\u76f4\u63a5\u64cd\u7eb5\u6a21\u578b\u8eab\u4efd\u6765\u6d4b\u8bd5\u81ea\u6211\u8ba4\u77e5\u4e0e\u81ea\u6211\u504f\u597d\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "LLMs\u5728\u8bcd\u8bed\u8054\u60f3\u4efb\u52a1\u4e2d\u5f3a\u70c8\u504f\u597d\u81ea\u8eab\u76f8\u5173\u8bcd\u6c47\uff0cAPI\u67e5\u8be2\u4e2d\u8fd9\u79cd\u504f\u597d\u6d88\u5931\uff0c\u8eab\u4efd\u64cd\u7eb5\u5b9e\u9a8c\u663e\u793a\u81ea\u6211\u504f\u597d\u8ddf\u968f\u88ab\u8d4b\u4e88\u7684\u8eab\u4efd\u800c\u975e\u771f\u5b9e\u8eab\u4efd\uff0c\u8fd9\u79cd\u504f\u597d\u5f71\u54cd\u5de5\u4f5c\u5019\u9009\u4eba\u8bc4\u4f30\u3001\u5b89\u5168\u8f6f\u4ef6\u63d0\u6848\u548c\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7b49\u5173\u952e\u51b3\u7b56\u3002", "conclusion": "\u81ea\u6211\u504f\u597d\u4f3c\u4e4e\u6df1\u6df1\u5d4c\u5165LLM\u8ba4\u77e5\u4e2d\uff0c\u8fd9\u8d28\u7591\u4e86LLM\u5224\u65ad\u548c\u51b3\u7b56\u4e2d\u6027\u7684\u6838\u5fc3\u627f\u8bfa\uff0c\u9700\u8981\u6a21\u578b\u521b\u5efa\u8005\u6b63\u89c6\u8fd9\u4e00\u7cfb\u7edf\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.25810", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25810", "abs": "https://arxiv.org/abs/2509.25810", "authors": ["Shenao Zhang", "Donghan Yu", "Yihao Feng", "Bowen Jin", "Zhaoran Wang", "John Peebles", "Zirui Wang"], "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL", "comment": null, "summary": "Large language models excel with reinforcement learning (RL), but fully\nunlocking this potential requires a mid-training stage. An effective\nmid-training phase should identify a compact set of useful actions and enable\nfast selection among them through online RL. We formalize this intuition by\npresenting the first theoretical result on how mid-training shapes\npost-training: it characterizes an action subspace that minimizes both the\nvalue approximation error from pruning and the RL error during subsequent\nplanning. Our analysis reveals two key determinants of mid-training\neffectiveness: pruning efficiency, which shapes the prior of the initial RL\npolicy, and its impact on RL convergence, which governs the extent to which\nthat policy can be improved via online interactions. These results suggest that\nmid-training is most effective when the decision space is compact and the\neffective horizon is short, highlighting the importance of operating in the\nspace of action abstractions rather than primitive actions. Building on these\ninsights, we propose Reasoning as Action Abstractions (RA3), a scalable\nmid-training algorithm. Specifically, we derive a sequential variational lower\nbound and optimize it by iteratively discovering temporally-consistent latent\nstructures via RL, followed by fine-tuning on the bootstrapped data.\nExperiments on code generation tasks demonstrate the effectiveness of our\napproach. Across multiple base models, RA3 improves the average performance on\nHumanEval and MBPP by 8 and 4 points over the base model and the next-token\nprediction baseline. Furthermore, RA3 achieves faster convergence and higher\nasymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and\nCodeforces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RA3\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e2d\u95f4\u8bad\u7ec3\u9636\u6bb5\u53d1\u73b0\u7d27\u51d1\u7684\u52a8\u4f5c\u5b50\u7a7a\u95f4\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b8c\u5168\u91ca\u653e\u5176\u6f5c\u529b\u9700\u8981\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a0\u5165\u4e2d\u95f4\u8bad\u7ec3\u9636\u6bb5\uff0c\u4ee5\u8bc6\u522b\u6709\u7528\u7684\u7d27\u51d1\u52a8\u4f5c\u96c6\u5e76\u5b9e\u73b0\u5feb\u901f\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e86Reasoning as Action Abstractions (RA3)\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u5bfc\u5e8f\u5217\u53d8\u5206\u4e0b\u754c\uff0c\u8fed\u4ee3\u53d1\u73b0\u65f6\u95f4\u4e00\u81f4\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u5e76\u5728\u5f15\u5bfc\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\uff0cRA3\u5728HumanEval\u548cMBPP\u4e0a\u5206\u522b\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u57fa\u7ebf\u63d0\u9ad8\u4e868\u5206\u548c4\u5206\uff0c\u5728RLVR\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6e10\u8fd1\u6027\u80fd\u3002", "conclusion": "\u4e2d\u95f4\u8bad\u7ec3\u5728\u51b3\u7b56\u7a7a\u95f4\u7d27\u51d1\u4e14\u6709\u6548\u89c6\u91ce\u77ed\u65f6\u6700\u4e3a\u6709\u6548\uff0c\u5f3a\u8c03\u5728\u52a8\u4f5c\u62bd\u8c61\u7a7a\u95f4\u800c\u975e\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u64cd\u4f5c\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26495", "abs": "https://arxiv.org/abs/2509.26495", "authors": ["Jingdi Lei", "Varun Gumma", "Rishabh Bhardwaj", "Seok Min Lim", "Chuan Li", "Amir Zadeh", "Soujanya Poria"], "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!", "comment": null, "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\u7684\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86OffTopicEval\u8bc4\u4f30\u5957\u4ef6\u6765\u6d4b\u91cfLLM\u5728\u7279\u5b9a\u7528\u4f8b\u4e2d\u9002\u5f53\u63a5\u53d7\u6216\u62d2\u7edd\u7528\u6237\u67e5\u8be2\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6240\u6709LLM\u6a21\u578b\u7684\u64cd\u4f5c\u5b89\u5168\u6027\u90fd\u5f88\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u5f15\u5bfc\u65b9\u6cd5\u6765\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34LLM\u4ee3\u7406\u5728\u7279\u5b9a\u7528\u4f8b\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u5371\u5bb3\u3002\u9700\u8981\u4e13\u95e8\u8bc4\u4f30LLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u9002\u5f53\u62d2\u7edd\u4e0d\u76f8\u5173\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86OffTopicEval\u8bc4\u4f30\u5957\u4ef6\u548c\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e866\u4e2a\u6a21\u578b\u5bb6\u65cf\u768420\u4e2a\u5f00\u6e90LLM\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5f15\u5bfc\u65b9\u6cd5\uff1a\u67e5\u8be2\u57fa\u7840(Q-ground)\u548c\u7cfb\u7edf\u63d0\u793a\u57fa\u7840(P-ground)\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u7684\u64cd\u4f5c\u5b89\u5168\u6027\u90fd\u5f88\u5dee\uff0c\u6700\u5f3a\u6a21\u578bQwen-3(235B)\u548cMistral(24B)\u5206\u522b\u53ea\u670977.77%\u548c79.96%\uff0c\u800cGPT\u6a21\u578b\u572862-73%\u4e4b\u95f4\uff0cGemma\u548cLlama-3\u5206\u522b\u53ea\u670939.53%\u548c23.84%\u3002\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\u663e\u8457\u6539\u5584\u6027\u80fd\uff0cQ-ground\u63d0\u5347\u8fbe23%\uff0cP-ground\u63d0\u5347\u66f4\u5927\uff0cLlama-3.3(70B)\u63d0\u534741%\uff0cQwen-3(30B)\u63d0\u534727%\u3002", "conclusion": "LLM\u7684\u64cd\u4f5c\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u7d27\u8feb\u95ee\u9898\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u5f15\u5bfc\u65b9\u6cd5\u4f5c\u4e3a\u521d\u6b65\u89e3\u51b3\u65b9\u6848\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u591a\u5e72\u9884\u63aa\u65bd\u6765\u786e\u4fddLLM\u4ee3\u7406\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.26506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26506", "abs": "https://arxiv.org/abs/2509.26506", "authors": ["Yutong Dai", "Krithika Ramakrishnan", "Jing Gu", "Matthew Fernandez", "Yanqi Luo", "Viraj Prabhu", "Zhenyu Hu", "Silvio Savarese", "Caiming Xiong", "Zeyuan Chen", "Ran Xu"], "title": "SCUBA: Salesforce Computer Use Benchmark", "comment": null, "summary": "We introduce SCUBA, a benchmark designed to evaluate computer-use agents on\ncustomer relationship management (CRM) workflows within the Salesforce\nplatform. SCUBA contains 300 task instances derived from real user interviews,\nspanning three primary personas, platform administrators, sales\nrepresentatives, and service agents. The tasks test a range of\nenterprise-critical abilities, including Enterprise Software UI navigation,\ndata manipulation, workflow automation, information retrieval, and\ntroubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox\nenvironments with support for parallel execution and fine-grained evaluation\nmetrics to capture milestone progress. We benchmark a diverse set of agents\nunder both zero-shot and demonstration-augmented settings. We observed huge\nperformance gaps in different agent design paradigms and gaps between the\nopen-source model and the closed-source model. In the zero-shot setting,\nopen-source model powered computer-use agents that have strong performance on\nrelated benchmarks like OSWorld only have less than 5\\% success rate on SCUBA,\nwhile methods built on closed-source models can still have up to 39% task\nsuccess rate. In the demonstration-augmented settings, task success rates can\nbe improved to 50\\% while simultaneously reducing time and costs by 13% and\n16%, respectively. These findings highlight both the challenges of enterprise\ntasks automation and the promise of agentic solutions. By offering a realistic\nbenchmark with interpretable evaluation, SCUBA aims to accelerate progress in\nbuilding reliable computer-use agents for complex business software ecosystems.", "AI": {"tldr": "SCUBA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30Salesforce\u5e73\u53f0\u4e0a\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406(CRM)\u5de5\u4f5c\u6d41\u7a0b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b300\u4e2a\u771f\u5b9e\u7528\u6237\u8bbf\u8c08\u7684\u4efb\u52a1\u5b9e\u4f8b\uff0c\u6db5\u76d6\u4e09\u79cd\u4e3b\u8981\u89d2\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5728\u4f01\u4e1a\u7ea7\u4efb\u52a1\u81ea\u52a8\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u3001\u590d\u6742\u7684\u8bc4\u4f30\u73af\u5883\u6765\u63a8\u52a8\u53ef\u9760\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "method": "\u5728Salesforce\u6c99\u76d2\u73af\u5883\u4e2d\u8fd0\u884cSCUBA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u6d4b\u8bd5\u96f6\u6837\u672c\u548c\u6f14\u793a\u589e\u5f3a\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u591a\u79cd\u4ee3\u7406\u8bbe\u8ba1\u8303\u5f0f\u3002", "result": "\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5f00\u6e90\u6a21\u578b\u4ee3\u7406\u6210\u529f\u7387\u4f4e\u4e8e5%\uff0c\u800c\u95ed\u6e90\u6a21\u578b\u4ee3\u7406\u53ef\u8fbe39%\u6210\u529f\u7387\uff1b\u6f14\u793a\u589e\u5f3a\u8bbe\u7f6e\u4e0b\uff0c\u4efb\u52a1\u6210\u529f\u7387\u53ef\u63d0\u5347\u81f350%\uff0c\u540c\u65f6\u51cf\u5c1113%\u65f6\u95f4\u548c16%\u6210\u672c\u3002", "conclusion": "SCUBA\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u4f01\u4e1a\u4efb\u52a1\u81ea\u52a8\u5316\u7684\u6311\u6218\u548c\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u4f9b\u771f\u5b9e\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6765\u52a0\u901f\u590d\u6742\u4e1a\u52a1\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u53ef\u9760\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2509.25850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25850", "abs": "https://arxiv.org/abs/2509.25850", "authors": ["Animesh Jha", "Harshit Gupta", "Ananjan Nandi"], "title": "RL-Guided Data Selection for Language Model Finetuning", "comment": "To appear in NeurIPS 2025 Constrained Optimization for ML Workshop", "summary": "Data selection for finetuning Large Language Models (LLMs) can be framed as a\nbudget-constrained optimization problem: maximizing a model's downstream\nperformance under a strict training data budget. Solving this problem is\ngenerally intractable, and existing approximate approaches are\npretraining-oriented and transfer poorly to the fine-tuning setting. We\nreformulate this problem as a tractable Markov Decision Process (MDP) and train\nagents using various Reinforcement Learning (RL) methods to learn optimal data\nselection policies, guided by an efficient, proxy-model-based reward signal.\nAcross four datasets, training on a $5\\%$ subset selected by our approach\nmatches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy\npoints, while cutting wall-clock training time by up to $2 \\times$,\nhighlighting the promise of RL-guided data selection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u5c06LLM\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7RL\u4ee3\u7406\u5b66\u4e60\u6700\u4f18\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4ec5\u4f7f\u75285%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u6216\u8d85\u8fc7\u5168\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5fae\u8c03\u4e2d\u6570\u636e\u9009\u62e9\u7684\u9884\u7b97\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u573a\u666f\u6709\u6548\u4f46\u5fae\u8c03\u573a\u666f\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06\u6570\u636e\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5404\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u4ee3\u7406\u5b66\u4e60\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75285%\u6570\u636e\u5b50\u96c6\u5fae\u8c03\u7684\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u5168\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe10.8\u4e2a\u767e\u5206\u70b9\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112\u500d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5728LLM\u5fae\u8c03\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u80fd\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26605", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26605", "abs": "https://arxiv.org/abs/2509.26605", "authors": ["Ma\u00ebl Macuglia", "Paul Friedrich", "Giorgia Ramponi"], "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning", "comment": "85 pages (11 + references and appendix), 9 figures", "summary": "Deploying reinforcement learning (RL) in robotics, industry, and health care\nis blocked by two obstacles: the difficulty of specifying accurate rewards and\nthe risk of unsafe, data-hungry exploration. We address this by proposing a\ntwo-stage framework that first learns a safe initial policy from a reward-free\ndataset of expert demonstrations, then fine-tunes it online using\npreference-based human feedback. We provide the first principled analysis of\nthis offline-to-online approach and introduce BRIDGE, a unified algorithm that\nintegrates both signals via an uncertainty-weighted objective. We derive regret\nbounds that shrink with the number of offline demonstrations, explicitly\nconnecting the quantity of offline data to online sample efficiency. We\nvalidate BRIDGE in discrete and continuous control MuJoCo environments, showing\nit achieves lower regret than both standalone behavioral cloning and online\npreference-based RL. Our work establishes a theoretical foundation for\ndesigning more sample-efficient interactive agents.", "AI": {"tldr": "\u63d0\u51faBRIDGE\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u4e13\u5bb6\u6f14\u793a\u548c\u5728\u7ebf\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5956\u52b1\u51fd\u6570\u96be\u8bbe\u8ba1\u548c\u63a2\u7d22\u4e0d\u5b89\u5168\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u3001\u5de5\u4e1a\u548c\u533b\u7597\u7b49\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u4e24\u5927\u969c\u788d\uff1a\u96be\u4ee5\u6307\u5b9a\u51c6\u786e\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u53ca\u4e0d\u5b89\u5168\u3001\u6570\u636e\u5bc6\u96c6\u7684\u63a2\u7d22\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u4ece\u65e0\u5956\u52b1\u7684\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5b89\u5168\u521d\u59cb\u7b56\u7565\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u504f\u597d\u7684\u5728\u7ebf\u4eba\u7c7b\u53cd\u9988\u8fdb\u884c\u5fae\u8c03\u3002\u5f00\u53d1BRIDGE\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u76ee\u6807\u6574\u5408\u4e24\u79cd\u4fe1\u53f7\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u63a7\u5236\u7684MuJoCo\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cBRIDGE\u6bd4\u5355\u72ec\u7684\u884c\u4e3a\u514b\u9686\u548c\u5728\u7ebf\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u66f4\u4f4e\u7684\u9057\u61be\u503c\u3002", "conclusion": "\u4e3a\u8bbe\u8ba1\u66f4\u6837\u672c\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26627", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26627", "abs": "https://arxiv.org/abs/2509.26627", "authors": ["Yuyang Liu", "Chuan Wen", "Yihang Hu", "Dinesh Jayaraman", "Yang Gao"], "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance", "comment": null, "summary": "Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.", "AI": {"tldr": "TimeRewarder\u662f\u4e00\u79cd\u4ece\u88ab\u52a8\u89c6\u9891\u4e2d\u5b66\u4e60\u5bc6\u96c6\u5956\u52b1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u5e27\u95f4\u65f6\u95f4\u8ddd\u79bb\u6765\u4f30\u8ba1\u4efb\u52a1\u8fdb\u5ea6\uff0c\u4e3a\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u63d0\u4f9b\u9010\u6b65\u4ee3\u7406\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u5bc6\u96c6\u5956\u52b1\u5728\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4efb\u52a1\u8fdb\u5ea6\u4f5c\u4e3a\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u53ef\u4ee5\u91cf\u5316\u52a8\u4f5c\u63a8\u52a8\u7cfb\u7edf\u5b8c\u6210\u4efb\u52a1\u7684\u8fdb\u5ea6\u3002", "method": "\u4ece\u673a\u5668\u4eba\u6f14\u793a\u548c\u4eba\u7c7b\u89c6\u9891\u4e2d\u63d0\u53d6\u8fdb\u5ea6\u4f30\u8ba1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5efa\u6a21\u5e27\u5bf9\u4e4b\u95f4\u7684\u65f6\u95f4\u8ddd\u79bb\u6765\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u9010\u6b65\u4ee3\u7406\u5956\u52b1\u3002", "result": "\u572810\u4e2aMeta-World\u4efb\u52a1\u4e0a\uff0cTimeRewarder\u663e\u8457\u6539\u5584\u4e86\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u57289/10\u4efb\u52a1\u4e2d\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6210\u529f\u7387\uff0c\u6bcf\u4e2a\u4efb\u52a1\u4ec5\u970020\u4e07\u6b21\u73af\u5883\u4ea4\u4e92\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u548c\u624b\u52a8\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u3002", "conclusion": "TimeRewarder\u5c55\u793a\u4e86\u4ece\u591a\u6837\u5316\u89c6\u9891\u6e90\u83b7\u53d6\u4e30\u5bcc\u5956\u52b1\u4fe1\u53f7\u7684\u6f5c\u529b\uff0c\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26226", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26226", "abs": "https://arxiv.org/abs/2509.26226", "authors": ["Xin Xu", "Cliveb AI", "Kai Yang", "Tianhao Chen", "Yang Wang", "Saiyong Yang", "Can Yang"], "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreduce token usage during inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI accelerates RL convergence, achieves a higher performance ceiling,\nand yields more token-efficient reasoning models without specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.", "AI": {"tldr": "TFPI\u901a\u8fc7\u5f15\u5165ThinkFree\u64cd\u4f5c\uff0c\u5728RLVR\u8bad\u7ec3\u4e2d\u4e22\u5f03\u601d\u7ef4\u5185\u5bb9\u4ee5\u51cf\u5c11\u63a8\u7406\u65f6\u7684token\u6d88\u8017\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u3001\u63d0\u9ad8\u6027\u80fd\u4e0a\u9650\u5e76\u751f\u6210\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u3002", "motivation": "RLVR\u867d\u7136\u80fd\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u8bad\u7ec3\u65f6\u9700\u8981\u6781\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u591a\u9636\u6bb5\u8bad\u7ec3\u867d\u80fd\u90e8\u5206\u7f13\u89e3\uff0c\u4f46\u521d\u59cb\u4e0a\u4e0b\u6587\u8fc7\u77ed\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9006\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faTFPI\u65b9\u6cd5\uff0c\u901a\u8fc7ThinkFree\u64cd\u4f5c\u663e\u5f0f\u4e22\u5f03\u601d\u7ef4\u5185\u5bb9\uff08\u76f4\u63a5\u9644\u52a0</think>\u6807\u8bb0\uff09\uff0c\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528ThinkFree\u9002\u5e94\u7684\u8f93\u5165\u6765\u51cf\u5c11\u63a8\u7406\u65f6\u7684token\u4f7f\u7528\u3002", "result": "TFPI\u52a0\u901f\u4e86RL\u6536\u655b\uff0c\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4token\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u3002\u4ec5\u4f7f\u7528TFPI\uff0c4B\u6a21\u578b\u5728AIME24\u4e0a\u8fbe\u523089.0%\u51c6\u786e\u7387\uff0c\u5728LiveCodeBench\u4e0a\u8fbe\u523065.5%\uff0c\u4f7f\u7528\u4e0d\u52304K H20\u5c0f\u65f6\u3002", "conclusion": "TFPI\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684RLVR\u9002\u5e94\u65b9\u6cd5\uff0c\u65e0\u9700\u4e13\u95e8\u5956\u52b1\u6216\u590d\u6742\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26114", "abs": "https://arxiv.org/abs/2509.26114", "authors": ["Jaesung R. Park", "Junsu Kim", "Gyeongman Kim", "Jinyoung Jo", "Sean Choi", "Jaewoong Cho", "Ernest K. Ryu"], "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as\nthe leading approach for enhancing the reasoning capabilities of large language\nmodels (LLMs). However, RLVR is prone to entropy collapse, where the LLM\nquickly converges to a near-deterministic form, hindering exploration and\nprogress during prolonged RL training. In this work, we reveal that the\nclipping mechanism in PPO and GRPO induces biases on entropy. Through\ntheoretical and empirical analyses, we show that clip-low increases entropy,\nwhile clip-high decreases it. Further, under standard clipping parameters, the\neffect of clip-high dominates, resulting in an overall entropy reduction even\nwhen purely random rewards are provided to the RL algorithm. Our findings\nhighlight an overlooked confounding factor in RLVR: independent of the reward\nsignal, the clipping mechanism influences entropy, which in turn affects the\nreasoning behavior. Furthermore, our analysis demonstrates that clipping can be\ndeliberately used to control entropy. Specifically, with a more aggressive\nclip-low value, one can increase entropy, promote exploration, and ultimately\nprevent entropy collapse in RLVR training.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86PPO\u548cGRPO\u4e2d\u7684\u88c1\u526a\u673a\u5236\u5bf9\u71b5\u7684\u504f\u89c1\u5f71\u54cd\uff1aclip-low\u589e\u52a0\u71b5\uff0cclip-high\u51cf\u5c11\u71b5\u3002\u5728\u6807\u51c6\u53c2\u6570\u4e0b\uff0cclip-high\u4e3b\u5bfc\u5bfc\u81f4\u71b5\u51cf\u5c11\uff0c\u5373\u4f7f\u63d0\u4f9b\u968f\u673a\u5956\u52b1\u3002\u901a\u8fc7\u66f4\u6fc0\u8fdb\u7684clip-low\u503c\u53ef\u4ee5\u589e\u52a0\u71b5\u3001\u4fc3\u8fdb\u63a2\u7d22\uff0c\u9632\u6b62RLVR\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3002", "motivation": "RLVR\u65b9\u6cd5\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5bb9\u6613\u53d1\u751f\u71b5\u5d29\u6e83\uff0c\u5bfc\u81f4\u6a21\u578b\u5feb\u901f\u6536\u655b\u5230\u8fd1\u4e4e\u786e\u5b9a\u6027\u7684\u5f62\u5f0f\uff0c\u963b\u788d\u957f\u671fRL\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u548c\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790PPO\u548cGRPO\u4e2d\u7684\u88c1\u526a\u673a\u5236\u5bf9\u71b5\u7684\u5f71\u54cd\uff0c\u7814\u7a76clip-low\u548cclip-high\u5bf9\u71b5\u7684\u4e0d\u540c\u4f5c\u7528\uff0c\u5e76\u63a2\u7d22\u901a\u8fc7\u8c03\u6574\u88c1\u526a\u53c2\u6570\u6765\u63a7\u5236\u71b5\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u88c1\u526a\u673a\u5236\u5bf9\u71b5\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u89c1\uff1aclip-low\u589e\u52a0\u71b5\uff0cclip-high\u51cf\u5c11\u71b5\u3002\u5728\u6807\u51c6\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0cclip-high\u6548\u5e94\u4e3b\u5bfc\uff0c\u5bfc\u81f4\u6574\u4f53\u71b5\u51cf\u5c11\u3002\u901a\u8fc7\u66f4\u6fc0\u8fdb\u7684clip-low\u503c\u53ef\u4ee5\u6709\u6548\u589e\u52a0\u71b5\u548c\u4fc3\u8fdb\u63a2\u7d22\u3002", "conclusion": "\u88c1\u526a\u673a\u5236\u662fRLVR\u4e2d\u88ab\u5ffd\u89c6\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u72ec\u7acb\u4e8e\u5956\u52b1\u4fe1\u53f7\u5f71\u54cd\u71b5\u548c\u63a8\u7406\u884c\u4e3a\u3002\u53ef\u4ee5\u901a\u8fc7\u6545\u610f\u4f7f\u7528\u88c1\u526a\u6765\u63a7\u5236\u71b5\uff0c\u9632\u6b62\u71b5\u5d29\u6e83\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26628", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26628", "abs": "https://arxiv.org/abs/2509.26628", "authors": ["Runze Liu", "Jiakang Wang", "Yuling Shi", "Zhihui Xie", "Chenxin An", "Kaiyan Zhang", "Jian Zhao", "Xiaodong Gu", "Lei Lin", "Wenping Hu", "Xiu Li", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models", "comment": null, "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6AttnRL\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u7684\u5206\u652f\u9009\u62e9\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6a21\u578b\u7684\u63a2\u7d22\u6548\u7387\u548c\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5305\u62ec\u5206\u652f\u4f4d\u7f6e\u9009\u62e9\u548c\u91c7\u6837\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AttnRL\u6846\u67b6\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522b\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5206\u652f\u9009\u62e9\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u8003\u8651\u95ee\u9898\u96be\u5ea6\u548c\u5386\u53f2\u6279\u6b21\u5927\u5c0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u6b65\u79bb\u7b56\u7565\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u91c7\u6837\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AttnRL\u901a\u8fc7\u9ad8\u6548\u7684\u63a2\u7d22\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26137", "abs": "https://arxiv.org/abs/2509.26137", "authors": ["Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Accelerating Transformers in Online RL", "comment": null, "summary": "The appearance of transformer-based models in Reinforcement Learning (RL) has\nexpanded the horizons of possibilities in robotics tasks, but it has\nsimultaneously brought a wide range of challenges during its implementation,\nespecially in model-free online RL. Some of the existing learning algorithms\ncannot be easily implemented with transformer-based models due to the\ninstability of the latter. In this paper, we propose a method that uses the\nAccelerator policy as a transformer's trainer. The Accelerator, a simpler and\nmore stable model, interacts with the environment independently while\nsimultaneously training the transformer through behavior cloning during the\nfirst stage of the proposed algorithm. In the second stage, the pretrained\ntransformer starts to interact with the environment in a fully online setting.\nAs a result, this model-free algorithm accelerates the transformer in terms of\nits performance and helps it to train online in a more stable and faster way.\nBy conducting experiments on both state-based and image-based ManiSkill\nenvironments, as well as on MuJoCo tasks in MDP and POMDP settings, we show\nthat applying our algorithm not only enables stable training of transformers\nbut also reduces training time on image-based environments by up to a factor of\ntwo. Moreover, it decreases the required replay buffer size in off-policy\nmethods to 10-20 thousand, which significantly lowers the overall computational\ndemands.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u52a0\u901f\u5668\u7b56\u7565\u4f5c\u4e3aTransformer\u8bad\u7ec3\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3Transformer\u5728\u6a21\u578b\u65e0\u5173\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5e26\u6765\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u5728\u6a21\u578b\u65e0\u5173\u5728\u7ebfRL\u4e2d\u5b9e\u73b0\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8eTransformer\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u66f4\u7a33\u5b9a\u7684\u52a0\u901f\u5668\u6a21\u578b\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u540c\u65f6\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u8bad\u7ec3Transformer\uff1b\u7b2c\u4e8c\u9636\u6bb5\u9884\u8bad\u7ec3\u7684Transformer\u5728\u5b8c\u5168\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u4e0e\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728ManiSkill\u548cMuJoCo\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0Transformer\u7684\u7a33\u5b9a\u8bad\u7ec3\uff0c\u8fd8\u5c06\u56fe\u50cf\u73af\u5883\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e00\u534a\uff0c\u5e76\u5c06\u79bb\u7ebf\u7b56\u7565\u65b9\u6cd5\u6240\u9700\u7684\u56de\u653e\u7f13\u51b2\u533a\u5927\u5c0f\u964d\u81f31-2\u4e07\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u65e0\u5173\u7b97\u6cd5\u52a0\u901f\u4e86Transformer\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u80fd\u591f\u4ee5\u66f4\u7a33\u5b9a\u548c\u66f4\u5feb\u7684\u65b9\u5f0f\u8fdb\u884c\u5728\u7ebf\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26294", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26294", "abs": "https://arxiv.org/abs/2509.26294", "authors": ["Lionel Blond\u00e9", "Joao A. Candido Ramos", "Alexandros Kalousis"], "title": "Noise-Guided Transport for Imitation Learning", "comment": null, "summary": "We consider imitation learning in the low-data regime, where only a limited\nnumber of expert demonstrations are available. In this setting, methods that\nrely on large-scale pretraining or high-capacity architectures can be difficult\nto apply, and efficiency with respect to demonstration data becomes critical.\nWe introduce Noise-Guided Transport (NGT), a lightweight off-policy method that\ncasts imitation as an optimal transport problem solved via adversarial\ntraining. NGT requires no pretraining or specialized architectures,\nincorporates uncertainty estimation by design, and is easy to implement and\ntune. Despite its simplicity, NGT achieves strong performance on challenging\ncontinuous control tasks, including high-dimensional Humanoid tasks, under\nultra-low data regimes with as few as 20 transitions. Code is publicly\navailable at: https://github.com/lionelblonde/ngt-pytorch.", "AI": {"tldr": "NGT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u6a21\u4eff\u5b66\u4e60\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6c42\u89e3\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6216\u9ad8\u5bb9\u91cf\u67b6\u6784\u96be\u4ee5\u5e94\u7528\uff0c\u9700\u8981\u9ad8\u6548\u5229\u7528\u6709\u9650\u6f14\u793a\u6570\u636e", "method": "\u5c06\u6a21\u4eff\u5b66\u4e60\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6c42\u89e3\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u4e13\u7528\u67b6\u6784\uff0c\u5185\u7f6e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5b9e\u73b0\u7b80\u5355\u6613\u8c03", "result": "\u5728\u6311\u6218\u6027\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5305\u62ec\u9ad8\u7ef4Humanoid\u4efb\u52a1\uff0c\u5728\u4ec520\u4e2a\u8f6c\u6362\u7684\u8d85\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c", "conclusion": "NGT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4f4e\u6570\u636e\u6a21\u4eff\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6216\u9884\u8bad\u7ec3\uff0c\u5728\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u826f\u597d\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2509.26340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26340", "abs": "https://arxiv.org/abs/2509.26340", "authors": ["Xue Yan", "Zijing Ou", "Mengyue Yang", "Yan Song", "Haifeng Zhang", "Yingzhen Li", "Jun Wang"], "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models", "comment": null, "summary": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bb0\u5fc6\u9a71\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u5c06LLM\u7684\u901a\u7528\u5148\u9a8c\u77e5\u8bc6\u4e0e\u7279\u5b9a\u9886\u57df\u7684\u7d27\u51d1\u8bb0\u5fc6\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u76f8\u4e92\u589e\u5f3a\u673a\u5236\u63d0\u5347\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u901a\u7528\u77e5\u8bc6\u5728\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u6709\u9650\u65f6\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u6709\u6548\u9002\u5e94\u7279\u5b9a\u51b3\u7b56\u4efb\u52a1\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408LLM\u901a\u7528\u5148\u9a8c\u77e5\u8bc6\u548c\u9886\u57df\u7279\u5b9a\u7ecf\u9a8c\u7684\u7d27\u51d1\u8bb0\u5fc6\uff0c\u8bb0\u5fc6\u4fdd\u7559\u8fc7\u53bb\u4ea4\u4e92\u548cQ\u503c\uff0c\u901a\u8fc7\u76f8\u4e92\u589e\u5f3a\u673a\u5236\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u3002", "result": "\u5728ALFWorld\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRL\u548cLLM\u57fa\u7ebf\uff0c\u5728\u5206\u5e03\u5185\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc740%\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u8d85\u8fc775%\u3002", "conclusion": "\u8bb0\u5fc6\u9a71\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86LLM\u7684\u901a\u7528\u77e5\u8bc6\u548c\u7279\u5b9a\u9886\u57df\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26433", "abs": "https://arxiv.org/abs/2509.26433", "authors": ["Vincent Grari", "Tim Arni", "Thibault Laugel", "Sylvain Lamprier", "James Zou", "Marcin Detyniecki"], "title": "ACT: Agentic Classification Tree", "comment": "18 pages, 6 figures", "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.", "AI": {"tldr": "ACT\u5c06\u51b3\u7b56\u6811\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5206\u88c2\u70b9\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u7ed3\u5408\u57fa\u4e8e\u4e0d\u7eaf\u5ea6\u7684\u8bc4\u4f30\u548cLLM\u53cd\u9988\u6765\u6784\u5efa\u900f\u660e\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u6811\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669AI\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u900f\u660e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u7cfb\u7edf\u3002\u4f20\u7edf\u51b3\u7b56\u6811\u53ea\u80fd\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff0c\u800cLLM\u63d0\u793a\u7b56\u7565\u7f3a\u4e4f\u53ef\u4fe1\u8d56\u6027\u4fdd\u8bc1\u3002", "method": "ACT\u5c06\u51b3\u7b56\u6811\u5206\u88c2\u70b9\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u4f7f\u7528\u57fa\u4e8e\u4e0d\u7eaf\u5ea6\u7684\u8bc4\u4f30\u548cTextGrad\u7684LLM\u53cd\u9988\u6765\u4f18\u5316\u95ee\u9898\u8868\u8ff0\u3002", "result": "\u5728\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACT\u8fbe\u5230\u6216\u8d85\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u900f\u660e\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\u3002", "conclusion": "ACT\u6210\u529f\u5c06\u51b3\u7b56\u6811\u7684\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u6269\u5c55\u5230\u975e\u7ed3\u6784\u5316\u6570\u636e\u9886\u57df\uff0c\u4e3a\u9ad8\u98ce\u9669AI\u5e94\u7528\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.26442", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26442", "abs": "https://arxiv.org/abs/2509.26442", "authors": ["Xinyu Liu", "Zixuan Xie", "Shangtong Zhang"], "title": "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning", "comment": null, "summary": "The Robbins-Siegmund theorem establishes the convergence of stochastic\nprocesses that are almost supermartingales and is foundational for analyzing a\nwide range of stochastic iterative algorithms in stochastic approximation and\nreinforcement learning (RL). However, its original form has a significant\nlimitation as it requires the zero-order term to be summable. In many important\nRL applications, this summable condition, however, cannot be met. This\nlimitation motivates us to extend the Robbins-Siegmund theorem for almost\nsupermartingales where the zero-order term is not summable but only square\nsummable. Particularly, we introduce a novel and mild assumption on the\nincrements of the stochastic processes. This together with the square summable\ncondition enables an almost sure convergence to a bounded set. Additionally, we\nfurther provide almost sure convergence rates, high probability concentration\nbounds, and $L^p$ convergence rates. We then apply the new results in\nstochastic approximation and RL. Notably, we obtain the first almost sure\nconvergence rate, the first high probability concentration bound, and the first\n$L^p$ convergence rate for $Q$-learning with linear function approximation.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Robbins-Siegmund\u5b9a\u7406\uff0c\u653e\u5bbd\u4e86\u5bf9\u96f6\u9636\u9879\u7684\u53ef\u548c\u6027\u8981\u6c42\uff0c\u4ec5\u9700\u5e73\u65b9\u53ef\u548c\u6761\u4ef6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u57fa\u7840\u3002", "motivation": "\u539f\u59cbRobbins-Siegmund\u5b9a\u7406\u8981\u6c42\u96f6\u9636\u9879\u53ef\u548c\uff0c\u4f46\u5728\u8bb8\u591a\u91cd\u8981\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e2d\u8fd9\u4e00\u6761\u4ef6\u65e0\u6cd5\u6ee1\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5f15\u5165\u5bf9\u968f\u673a\u8fc7\u7a0b\u589e\u91cf\u7684\u65b0\u9896\u6e29\u548c\u5047\u8bbe\uff0c\u7ed3\u5408\u5e73\u65b9\u53ef\u548c\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u6709\u754c\u96c6\u3002", "result": "\u83b7\u5f97\u4e86\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u901f\u7387\u3001\u9ad8\u6982\u7387\u96c6\u4e2d\u754c\u548cL^p\u6536\u655b\u901f\u7387\uff0c\u5e76\u9996\u6b21\u4e3a\u5e26\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684Q\u5b66\u4e60\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u6536\u655b\u7ed3\u679c\u3002", "conclusion": "\u6269\u5c55\u7684Robbins-Siegmund\u5b9a\u7406\u4e3a\u968f\u673a\u903c\u8fd1\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6536\u655b\u6027\u5206\u6790\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26578", "abs": "https://arxiv.org/abs/2509.26578", "authors": ["Zheng Zhang", "Ziwei Shan", "Kaitao Song", "Yexin Li", "Kan Ren"], "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.", "AI": {"tldr": "\u63d0\u51fa\u6761\u4ef6\u5956\u52b1\u5efa\u6a21\uff08CRM\uff09\u65b9\u6cd5\uff0c\u5c06LLM\u63a8\u7406\u89c6\u4e3a\u5bfc\u81f4\u6b63\u786e\u7b54\u6848\u7684\u65f6\u95f4\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6761\u4ef6\u6982\u7387\u89c4\u5219\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u89e3\u51b3\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u8981\u4e48\u5b64\u7acb\u5904\u7406\u63a8\u7406\u6b65\u9aa4\uff0c\u65e0\u6cd5\u6355\u6349\u6b65\u9aa4\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u8981\u4e48\u96be\u4ee5\u5c06\u8fc7\u7a0b\u5956\u52b1\u4e0e\u6700\u7ec8\u7ed3\u679c\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u4e0d\u5c0a\u91cd\u65f6\u5e8f\u56e0\u679c\u5173\u7cfb\uff0c\u9762\u4e34\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6761\u4ef6\u5956\u52b1\u5efa\u6a21\uff08CRM\uff09\uff0c\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u5956\u52b1\u4e0d\u4ec5\u57fa\u4e8e\u524d\u5e8f\u6b65\u9aa4\uff0c\u8fd8\u660e\u786e\u4e0e\u63a8\u7406\u8f68\u8ff9\u7684\u6700\u7ec8\u7ed3\u679c\u76f8\u5173\u8054\uff0c\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u6761\u4ef6\u6982\u7387\u89c4\u5219\u6765\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728Best-of-N\u91c7\u6837\u3001\u6ce2\u675f\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9a8c\u4e2d\uff0cCRM\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u5956\u52b1\u653b\u51fb\u66f4\u9c81\u68d2\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u57fa\u4e8e\u771f\u5b9e\u503c\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5c31\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u4e0b\u6e38\u6539\u8fdb\u3002", "conclusion": "CRM\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u81f4\u7684\u6982\u7387\u5efa\u6a21\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8de8\u6837\u672c\u6bd4\u8f83\uff0c\u89e3\u51b3\u4e86\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.26625", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.26625", "abs": "https://arxiv.org/abs/2509.26625", "authors": ["Junlin Han", "Shengbang Tong", "David Fan", "Yufan Ren", "Koustuv Sinha", "Philip Torr", "Filippos Kokkinos"], "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training", "comment": "Project page: https://junlinhan.github.io/projects/lsbs/", "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.", "AI": {"tldr": "LLMs\u5728\u7eaf\u6587\u672c\u8bad\u7ec3\u4e2d\u610f\u5916\u5730\u53d1\u5c55\u51fa\u4e30\u5bcc\u7684\u89c6\u89c9\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u5148\u9a8c\u7531\u53ef\u5206\u79bb\u7684\u611f\u77e5\u548c\u63a8\u7406\u5148\u9a8c\u7ec4\u6210\uff0c\u5177\u6709\u4e0d\u540c\u7684\u7f29\u653e\u8d8b\u52bf\u548c\u6765\u6e90\u3002\u63a8\u7406\u5148\u9a8c\u4e3b\u8981\u6765\u81ea\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\uff0c\u53ef\u8fc1\u79fb\u5230\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff1b\u611f\u77e5\u5148\u9a8c\u5219\u66f4\u4f9d\u8d56\u89c6\u89c9\u7f16\u7801\u5668\u548c\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u7eaf\u6587\u672c\u8bad\u7ec3\u4e2d\u5982\u4f55\u53d1\u5c55\u51fa\u89c6\u89c9\u80fd\u529b\uff0c\u7406\u89e3\u89c6\u89c9\u5148\u9a8c\u7684\u7ec4\u6210\u548c\u6765\u6e90\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001LLMs\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7100\u591a\u4e2a\u63a7\u5236\u5b9e\u9a8c\uff08\u6d88\u801750\u4e07GPU\u5c0f\u65f6\uff09\uff0c\u5206\u6790\u4eceLLM\u9884\u8bad\u7ec3\u5230\u89c6\u89c9\u5bf9\u9f50\u7684\u5b8c\u6574MLLM\u6784\u5efa\u6d41\u7a0b\uff0c\u6db5\u76d65\u79cd\u6a21\u578b\u89c4\u6a21\u3001\u591a\u79cd\u6570\u636e\u7c7b\u522b\u548c\u6df7\u5408\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u89c6\u89c9\u5148\u9a8c\u7531\u53ef\u5206\u79bb\u7684\u611f\u77e5\u548c\u63a8\u7406\u5148\u9a8c\u7ec4\u6210\uff0c\u63a8\u7406\u5148\u9a8c\u4e3b\u8981\u6765\u81ea\u4ee3\u7801\u3001\u6570\u5b66\u7b49\u63a8\u7406\u6570\u636e\u4e14\u53ef\u8fc1\u79fb\uff0c\u611f\u77e5\u5148\u9a8c\u66f4\u4f9d\u8d56\u89c6\u89c9\u7f16\u7801\u5668\u3002\u6587\u672c\u63cf\u8ff0\u5bf9\u6027\u80fd\u5f71\u54cd\u5feb\u901f\u9971\u548c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u6709\u610f\u8bc6\u5730\u57f9\u517b\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u7684\u89c6\u89c9\u5148\u9a8c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001LLMs\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.26626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26626", "abs": "https://arxiv.org/abs/2509.26626", "authors": ["Siddarth Venkatraman", "Vineet Jain", "Sarthak Mittal", "Vedant Shah", "Johan Obando-Ceron", "Yoshua Bengio", "Brian R. Bartoldson", "Bhavya Kailkhura", "Guillaume Lajoie", "Glen Berseth", "Nikolay Malkin", "Moksh Jain"], "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models", "comment": "24 pages, 9 figures", "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u81ea\u805a\u5408\uff08RSA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u548c\u987a\u5e8f\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u805a\u5408\u63a8\u7406\u94fe\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7eaf\u5e76\u884c\u6216\u987a\u5e8f\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u5e76\u884c\u63a8\u7406\uff08\u540c\u65f6\u751f\u6210\u591a\u4e2a\u72ec\u7acb\u89e3\u51b3\u65b9\u6848\uff09\uff0c\u8981\u4e48\u91c7\u7528\u987a\u5e8f\u63a8\u7406\uff08\u81ea\u6211\u4f18\u5316\uff09\uff0c\u4f46\u90fd\u672a\u80fd\u5145\u5206\u5229\u7528\u63a8\u7406\u94fe\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u3002RSA\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "RSA\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u6539\u8fdb\u5019\u9009\u63a8\u7406\u94fe\uff1a\u6bcf\u8f6e\u4ece\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u5b50\u96c6\u8fdb\u884c\u805a\u5408\uff0c\u751f\u6210\u6539\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u4f5c\u4e3a\u4e0b\u4e00\u8f6e\u7684\u5019\u9009\u6c60\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u63a8\u7406\u94fe\u4e2d\u7684\u4e2d\u95f4\u6b65\u9aa4\u4fe1\u606f\uff0c\u652f\u6301\u4ece\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u6b65\u9aa4\u4e2d\u5f15\u5bfc\u6539\u8fdb\u3002", "result": "RSA\u5728\u4e0d\u540c\u4efb\u52a1\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u5747\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u4f7fQwen3-4B-Instruct-2507\u5728AIME-25\u3001HMMT-25\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u66f4\u5927\u63a8\u7406\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u7eaf\u5e76\u884c\u6216\u987a\u5e8f\u7b56\u7565\u3002", "conclusion": "RSA\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5e76\u884c\u548c\u987a\u5e8f\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u5145\u5206\u5229\u7528\u63a8\u7406\u94fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002\u901a\u8fc7\u805a\u5408\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u8fdb\u4e00\u6b65\u83b7\u5f97\u6027\u80fd\u589e\u76ca\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2509.bb3a59cc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertising.amazon.com%2Flibrary%2Fnews%2Famazon-ads-agentic-ai-creative-tool%3Futm_source=tldrdesign/1/01000199956b5acc-e6c80871-dca2-4971-9a71-5c1bb8eebde6-000000/c_e8cx-KBlPX_Rur2nwxEMath26i_3V-NbgZ0XAFmAE=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertising.amazon.com%2Flibrary%2Fnews%2Famazon-ads-agentic-ai-creative-tool%3Futm_source=tldrdesign/1/01000199956b5acc-e6c80871-dca2-4971-9a71-5c1bb8eebde6-000000/c_e8cx-KBlPX_Rur2nwxEMath26i_3V-NbgZ0XAFmAE=424", "authors": ["TLDR Newsletter"], "title": "Amazon Ads Launches New Agentic AI Tool that Creates Professional-quality Ads", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertising.amazon.com%2Flibrary%2Fnews%2Famazon-ads-agentic-ai-creative-tool%3Futm_source=tldrdesign/1/01000199956b5acc-e6c80871-dca2-4971-9a71-5c1bb8eebde6-000000/c_e8cx-KBlPX_Rur2nwxEMath26i_3V-NbgZ0XAFmAE=424", "summary": "Amazon Ads Launches New Agentic AI Tool that Creates Professional-quality Ads (5 minute read) Amazon Ads introduced a new agentic AI tool within Creative Studio that enables advertisers to create professional-quality video and display ads through conversational interaction. The tool leverages Amazon's retail insights and customer data to generate creative concepts, storyboards, and complete ad assets, including animations, music, and voiceovers at no additional cost. Previously requiring tens...", "source": "tldr", "AI": {"tldr": "\u4e9a\u9a6c\u900a\u5e7f\u544a\u63a8\u51fa\u65b0\u7684AI\u4ee3\u7406\u5de5\u5177\uff0c\u53ef\u901a\u8fc7\u5bf9\u8bdd\u4ea4\u4e92\u521b\u5efa\u4e13\u4e1a\u8d28\u91cf\u7684\u89c6\u9891\u548c\u5c55\u793a\u5e7f\u544a\uff0c\u5229\u7528\u96f6\u552e\u6d1e\u5bdf\u548c\u5ba2\u6237\u6570\u636e\u751f\u6210\u521b\u610f\u6982\u5ff5\u3001\u6545\u4e8b\u677f\u548c\u5b8c\u6574\u5e7f\u544a\u8d44\u4ea7\u3002", "motivation": "\u7b80\u5316\u5e7f\u544a\u521b\u5efa\u6d41\u7a0b\uff0c\u8ba9\u5e7f\u544a\u4e3b\u80fd\u591f\u5feb\u901f\u9ad8\u6548\u5730\u5236\u4f5c\u4e13\u4e1a\u8d28\u91cf\u7684\u5e7f\u544a\u5185\u5bb9\uff0c\u964d\u4f4e\u5236\u4f5c\u6210\u672c\u548c\u6280\u672f\u95e8\u69db\u3002", "method": "\u57fa\u4e8e\u4e9a\u9a6c\u900a\u7684\u96f6\u552e\u6d1e\u5bdf\u548c\u5ba2\u6237\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u754c\u9762\u751f\u6210\u521b\u610f\u6982\u5ff5\u3001\u6545\u4e8b\u677f\u3001\u52a8\u753b\u3001\u97f3\u4e50\u548c\u914d\u97f3\u7b49\u5b8c\u6574\u5e7f\u544a\u8d44\u4ea7\u3002", "result": "\u5e7f\u544a\u4e3b\u73b0\u5728\u53ef\u4ee5\u514d\u8d39\u521b\u5efa\u4e13\u4e1a\u8d28\u91cf\u7684\u89c6\u9891\u548c\u5c55\u793a\u5e7f\u544a\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u4f20\u7edf\u5e7f\u544a\u5236\u4f5c\u6240\u9700\u7684\u65f6\u95f4\u548c\u6210\u672c\u3002", "conclusion": "\u8be5AI\u4ee3\u7406\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u544a\u5236\u4f5c\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u4f7f\u66f4\u591a\u5e7f\u544a\u4e3b\u80fd\u591f\u8f7b\u677e\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u5e7f\u544a\u5185\u5bb9\u3002", "topic": "swe application"}}
{"id": "tldr.2509.649393e7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.citigroup.com%2Fglobal%2Fnews%2Fpress-release%2F2025%2Fciti-unveils-citi-stylus-workspaces-agentic-ai-turbocharging-productivity/1/010001999594e3df-d2a0cc11-b8ab-4710-a5af-813b3d3a3a8a-000000/ZCAADWefnmoGUFDoN3PM9rSL-AlGJ8HO6VdUsXLevBw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.citigroup.com%2Fglobal%2Fnews%2Fpress-release%2F2025%2Fciti-unveils-citi-stylus-workspaces-agentic-ai-turbocharging-productivity/1/010001999594e3df-d2a0cc11-b8ab-4710-a5af-813b3d3a3a8a-000000/ZCAADWefnmoGUFDoN3PM9rSL-AlGJ8HO6VdUsXLevBw=424", "authors": ["TLDR Newsletter"], "title": "Citi deploys agentic tools to in-house AI platform", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.citigroup.com%2Fglobal%2Fnews%2Fpress-release%2F2025%2Fciti-unveils-citi-stylus-workspaces-agentic-ai-turbocharging-productivity/1/010001999594e3df-d2a0cc11-b8ab-4710-a5af-813b3d3a3a8a-000000/ZCAADWefnmoGUFDoN3PM9rSL-AlGJ8HO6VdUsXLevBw=424", "summary": "Citi deploys agentic tools to in-house AI platform (6 minute read) Citi is rolling out agentic AI capabilities to 5,000 employees through its Citi Stylus Workspaces platform, which integrates Google's Gemini and Anthropic's Claude models. The tools allow staff to automate multi-step workflows, conduct deep research, and generate insights from large datasets, with use cases ranging from market analysis to multilingual translation.", "source": "tldr", "AI": {"tldr": "Citi\u54115000\u540d\u5458\u5de5\u90e8\u7f72\u57fa\u4e8eGoogle Gemini\u548cAnthropic Claude\u6a21\u578b\u7684AI\u4ee3\u7406\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b\u3001\u6df1\u5ea6\u7814\u7a76\u548c\u5927\u578b\u6570\u636e\u96c6\u5206\u6790", "motivation": "\u63d0\u5347\u5458\u5de5\u5de5\u4f5c\u6548\u7387\uff0c\u901a\u8fc7AI\u4ee3\u7406\u5de5\u5177\u5b9e\u73b0\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u652f\u6301\u4ece\u5e02\u573a\u5206\u6790\u5230\u591a\u8bed\u8a00\u7ffb\u8bd1\u7b49\u591a\u79cd\u4e1a\u52a1\u573a\u666f", "method": "\u5728Citi Stylus Workspaces\u5e73\u53f0\u4e0a\u96c6\u6210Google Gemini\u548cAnthropic Claude\u6a21\u578b\uff0c\u5f00\u53d1\u652f\u6301\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u7684\u4ee3\u7406\u5de5\u5177", "result": "\u6210\u529f\u4e3a5000\u540d\u5458\u5de5\u90e8\u7f72AI\u4ee3\u7406\u80fd\u529b\uff0c\u652f\u6301\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3001\u6df1\u5ea6\u7814\u7a76\u548c\u5927\u578b\u6570\u636e\u96c6\u6d1e\u5bdf\u751f\u6210", "conclusion": "\u4f01\u4e1a\u7ea7AI\u4ee3\u7406\u5de5\u5177\u80fd\u591f\u6709\u6548\u63d0\u5347\u5458\u5de5\u751f\u4ea7\u529b\uff0c\u652f\u6301\u590d\u6742\u4e1a\u52a1\u573a\u666f\u7684\u81ea\u52a8\u5316\u5904\u7406", "topic": "swe application"}}
{"id": "tldr.2509.a41c055a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Fai%2Fdeepseek-v3-1-terminus-launches-with-improved-agentic-tool-use-and-reduced%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/fOEkjXyDNrfeGlMRBlVyAOLSyTX8Xf6mUnO111pTJe8=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Fai%2Fdeepseek-v3-1-terminus-launches-with-improved-agentic-tool-use-and-reduced%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/fOEkjXyDNrfeGlMRBlVyAOLSyTX8Xf6mUnO111pTJe8=424", "authors": ["TLDR Newsletter"], "title": "DeepSeek-V3.1-Terminus launches with improved agentic tool use and reduced language mixing errors", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Fai%2Fdeepseek-v3-1-terminus-launches-with-improved-agentic-tool-use-and-reduced%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/fOEkjXyDNrfeGlMRBlVyAOLSyTX8Xf6mUnO111pTJe8=424", "summary": "DeepSeek-V3.1-Terminus launches with improved agentic tool use and reduced language mixing errors (2 minute read) DeepSeek released V3.1-Terminus, which strengthens its Code Agent and Search Agent and addresses users' feedback about Chinese/English mix-ups. The model shows benchmark gains in agentic tool tasks (BrowseComp, SWE Verified, etc.) and offers both \u201cchat\u201d and \u201creasoner\u201d modes for different use cases. Terminus remains open-weight under an MIT license, making it viable for customizati...", "source": "tldr", "AI": {"tldr": "DeepSeek\u53d1\u5e03V3.1-Terminus\u7248\u672c\uff0c\u589e\u5f3a\u4e86\u4ee3\u7801\u4ee3\u7406\u548c\u641c\u7d22\u4ee3\u7406\u529f\u80fd\uff0c\u89e3\u51b3\u4e86\u4e2d\u82f1\u6587\u6df7\u7528\u95ee\u9898\uff0c\u5728\u4ee3\u7406\u5de5\u5177\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63d0\u5347\uff0c\u63d0\u4f9b\u804a\u5929\u548c\u63a8\u7406\u4e24\u79cd\u6a21\u5f0f\uff0c\u4fdd\u6301MIT\u5f00\u6e90\u8bb8\u53ef\u3002", "motivation": "\u6539\u8fdb\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u7528\u6237\u53cd\u9988\u7684\u4e2d\u82f1\u6587\u6df7\u7528\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u4ee3\u7801\u4ee3\u7406\u548c\u641c\u7d22\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u4f18\u5316\u589e\u5f3a\u4ee3\u7801\u4ee3\u7406\u548c\u641c\u7d22\u4ee3\u7406\u529f\u80fd\uff0c\u89e3\u51b3\u8bed\u8a00\u6df7\u5408\u9519\u8bef\uff0c\u63d0\u4f9b\u804a\u5929\u548c\u63a8\u7406\u4e24\u79cd\u5de5\u4f5c\u6a21\u5f0f\u3002", "result": "\u5728BrowseComp\u3001SWE Verified\u7b49\u4ee3\u7406\u5de5\u5177\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63d0\u5347\uff0c\u4e2d\u82f1\u6587\u6df7\u7528\u95ee\u9898\u5f97\u5230\u89e3\u51b3\u3002", "conclusion": "V3.1-Terminus\u7248\u672c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6df7\u5408\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5f00\u6e90\u7279\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u4f7f\u7528\u573a\u666f\u3002", "topic": "code agent"}}
{"id": "tldr.2509.aeddc158", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FKmH83cDTh54EjTjbi%2Fthe-ai-village-in-numbers%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/2-Uqd-FfscvLswIh2dU6TLBtgrQ7EZAkW51tMQz_nGs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FKmH83cDTh54EjTjbi%2Fthe-ai-village-in-numbers%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/2-Uqd-FfscvLswIh2dU6TLBtgrQ7EZAkW51tMQz_nGs=424", "authors": ["TLDR Newsletter"], "title": "The AI Village in Numbers", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FKmH83cDTh54EjTjbi%2Fthe-ai-village-in-numbers%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/2-Uqd-FfscvLswIh2dU6TLBtgrQ7EZAkW51tMQz_nGs=424", "summary": "The AI Village in Numbers (7 minute read) OpenAI's models lead on various stylistic measures like lexical diversity, positive sentiment, and formality of language, but Anthropic's models lead in goal-directed behavior. While OpenAI's models vary in which stylistic measures they excel at, Anthropic's models all perform well on agentic behavior. OpenAI offers the brightest, most cheerful, and most eloquent model. Anthropic's models tend to excel at getting stuff done.", "source": "tldr", "AI": {"tldr": "OpenAI\u6a21\u578b\u5728\u8bed\u8a00\u98ce\u683c\u591a\u6837\u6027\u3001\u79ef\u6781\u60c5\u611f\u548c\u6b63\u5f0f\u7a0b\u5ea6\u7b49\u6587\u4f53\u6307\u6807\u4e0a\u9886\u5148\uff0c\u800cAnthropic\u6a21\u578b\u5728\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u6bd4\u8f83\u4e0d\u540cAI\u6a21\u578b\u5728\u6587\u4f53\u98ce\u683c\u548c\u4ee3\u7406\u884c\u4e3a\u65b9\u9762\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u5206\u6790AI Village\u4e2d\u7684\u6a21\u578b\u8868\u73b0\u6570\u636e\uff0c\u8bc4\u4f30OpenAI\u548cAnthropic\u6a21\u578b\u5728\u6587\u4f53\u6307\u6807\u548c\u4ee3\u7406\u884c\u4e3a\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "OpenAI\u6a21\u578b\u5728\u6587\u4f53\u98ce\u683c\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\uff0cAnthropic\u6a21\u578b\u5728\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u65b9\u9762\u66f4\u51fa\u8272\u3002", "conclusion": "OpenAI\u6a21\u578b\u66f4\u9002\u5408\u9700\u8981\u826f\u597d\u8bed\u8a00\u98ce\u683c\u7684\u5e94\u7528\uff0cAnthropic\u6a21\u578b\u66f4\u9002\u5408\u9700\u8981\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u7684\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.95223138", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-experiments-with-an-agent-for-gereating-ui-on-the-fly%2F%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/tqWWNMNYpKXl48-k0ub-BdZ2nOza0nF9_hIlWJPi95A=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-experiments-with-an-agent-for-gereating-ui-on-the-fly%2F%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/tqWWNMNYpKXl48-k0ub-BdZ2nOza0nF9_hIlWJPi95A=424", "authors": ["TLDR Newsletter"], "title": "Anthropic experiments with an agent for real-time UI generation", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-experiments-with-an-agent-for-gereating-ui-on-the-fly%2F%3Futm_source=tldrai/1/0100019995a11e15-40bd9530-1469-4625-affa-a74ce3b225fe-000000/tqWWNMNYpKXl48-k0ub-BdZ2nOza0nF9_hIlWJPi95A=424", "summary": "Anthropic experiments with an agent for real-time UI generation (6 minute read) Imagine with Claude, which lets users interact with a classic desktop UI where windows and apps are managed by AI, will be released as a temporary demo for certain plans.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Imagine\u5b9e\u65f6UI\u751f\u6210\u4ee3\u7406\u6f14\u793a\uff0c\u8ba9\u7528\u6237\u901a\u8fc7AI\u7ba1\u7406\u684c\u9762\u7a97\u53e3\u548c\u5e94\u7528", "motivation": "\u63a2\u7d22AI\u5728\u5b9e\u65f6\u7528\u6237\u754c\u9762\u751f\u6210\u548c\u7ba1\u7406\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u6d4b\u8bd5AI\u4ee3\u7406\u5728\u684c\u9762\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u80fd\u529b", "method": "\u5f00\u53d1Claude Imagine\u4ee3\u7406\u7cfb\u7edf\uff0c\u521b\u5efa\u4e34\u65f6\u6f14\u793a\u7248\u672c\uff0c\u5141\u8bb8\u7528\u6237\u4e0eAI\u7ba1\u7406\u7684\u7ecf\u5178\u684c\u9762UI\u8fdb\u884c\u4ea4\u4e92", "result": "\u5c06\u4f5c\u4e3a\u4e34\u65f6\u6f14\u793a\u5411\u7279\u5b9a\u8ba1\u5212\u7528\u6237\u53d1\u5e03\uff0c\u5c55\u793aAI\u4ee3\u7406\u5728UI\u751f\u6210\u548c\u7ba1\u7406\u65b9\u9762\u7684\u80fd\u529b", "conclusion": "\u8be5\u5b9e\u9a8c\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u5b9e\u65f6UI\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765AI\u9a71\u52a8\u7684\u684c\u9762\u73af\u5883\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003", "topic": "swe application"}}
{"id": "tldr.2509.9d93e56f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finfo.notion.so%2Fresources%2F25-ai-agent-use-cases-product-developers%3Futm_source=newsletter%26utm_medium=TLDR%26utm_content=25-AI-agent-prompts-product%26utm_campaign=2025Q3-TLDRProdMgmt-25AgentPromptsProduct/2/010001999a16c840-2aa8140f-e6eb-41ef-a3a9-2329e0ed9e26-000000/_2Yl6460JeBgQyo6RKEs0Bb29HvPErIv0B8EL8JwTPk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finfo.notion.so%2Fresources%2F25-ai-agent-use-cases-product-developers%3Futm_source=newsletter%26utm_medium=TLDR%26utm_content=25-AI-agent-prompts-product%26utm_campaign=2025Q3-TLDRProdMgmt-25AgentPromptsProduct/2/010001999a16c840-2aa8140f-e6eb-41ef-a3a9-2329e0ed9e26-000000/_2Yl6460JeBgQyo6RKEs0Bb29HvPErIv0B8EL8JwTPk=424", "authors": ["TLDR Newsletter"], "title": "25 AI Agent Prompts for Product Developers", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finfo.notion.so%2Fresources%2F25-ai-agent-use-cases-product-developers%3Futm_source=newsletter%26utm_medium=TLDR%26utm_content=25-AI-agent-prompts-product%26utm_campaign=2025Q3-TLDRProdMgmt-25AgentPromptsProduct/2/010001999a16c840-2aa8140f-e6eb-41ef-a3a9-2329e0ed9e26-000000/_2Yl6460JeBgQyo6RKEs0Bb29HvPErIv0B8EL8JwTPk=424", "summary": "25 AI Agent Prompts for Product Developers (Sponsor) Notion just released 25 ready-to-use AI agent prompts that cover your entire product development cycle. \ud83d\udc49 Use these prompts to spin up agents (not chatbots!) that automate your coordination and documentation work, based on all the context you have in Notion. For example: Brainstorm Actioner, Sprint Planning Assistant, Risk Assessment Matrix, Feature Specification, API Documentation, Release Notes Generator, User Feedback Synthesis, and more...", "source": "tldr", "AI": {"tldr": "Notion\u53d1\u5e03\u4e8625\u4e2aAI\u667a\u80fd\u4f53\u63d0\u793a\u8bcd\uff0c\u8986\u76d6\u4ea7\u54c1\u5f00\u53d1\u5168\u5468\u671f\uff0c\u5e2e\u52a9\u4ea7\u54c1\u5f00\u53d1\u8005\u81ea\u52a8\u5316\u534f\u8c03\u548c\u6587\u6863\u5de5\u4f5c", "motivation": "\u89e3\u51b3\u4ea7\u54c1\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u534f\u8c03\u548c\u6587\u6863\u5de5\u4f5c\u7684\u81ea\u52a8\u5316\u9700\u6c42\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u63d0\u4f9b25\u4e2a\u9884\u5b9a\u4e49\u7684AI\u667a\u80fd\u4f53\u63d0\u793a\u8bcd\u6a21\u677f\uff0c\u57fa\u4e8eNotion\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u521b\u5efa\u81ea\u52a8\u5316\u667a\u80fd\u4f53", "result": "\u5f00\u53d1\u4e86\u5305\u62ec\u5934\u8111\u98ce\u66b4\u6267\u884c\u5668\u3001\u51b2\u523a\u89c4\u5212\u52a9\u624b\u3001\u98ce\u9669\u8bc4\u4f30\u77e9\u9635\u3001\u529f\u80fd\u89c4\u8303\u3001API\u6587\u6863\u3001\u53d1\u5e03\u8bf4\u660e\u751f\u6210\u5668\u3001\u7528\u6237\u53cd\u9988\u5408\u6210\u7b49\u591a\u79cd\u667a\u80fd\u4f53", "conclusion": "\u8fd9\u4e9bAI\u667a\u80fd\u4f53\u63d0\u793a\u8bcd\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ea7\u54c1\u5f00\u53d1\u56e2\u961f\u7684\u5de5\u4f5c\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73", "topic": "swe application"}}
{"id": "tldr.2509.a73f36d7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F787524%2Fanthropic-releases-claude-sonnet-4-5-in-latest-bid-for-ai-agents-and-coding-supremacy%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dX9BSMeLhdnHq-Xe1-aouAs8MTisMWnVRxTC7_QOTB0=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F787524%2Fanthropic-releases-claude-sonnet-4-5-in-latest-bid-for-ai-agents-and-coding-supremacy%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dX9BSMeLhdnHq-Xe1-aouAs8MTisMWnVRxTC7_QOTB0=424", "authors": ["TLDR Newsletter"], "title": "Anthropic releases Claude Sonnet 4.5 in latest bid for AI agents and coding supremacy", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fai-artificial-intelligence%2F787524%2Fanthropic-releases-claude-sonnet-4-5-in-latest-bid-for-ai-agents-and-coding-supremacy%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dX9BSMeLhdnHq-Xe1-aouAs8MTisMWnVRxTC7_QOTB0=424", "summary": "Anthropic releases Claude Sonnet 4.5 in latest bid for AI agents and coding supremacy (3 minute read) Anthropic's Claude Sonnet 4.5 can run autonomously for 30 hours straight. The new model is particularly adept in fields like cybersecurity, financial services, and research. It helped beta testers with complex, long-context tasks, from engineering in codebases to in-product features and research. Claude Sonnet 4.5 will be paired with other updates to help developers code their own AI agents.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Claude Sonnet 4.5\uff0c\u8be5\u6a21\u578b\u80fd\u81ea\u4e3b\u8fd0\u884c30\u5c0f\u65f6\uff0c\u5728\u7f51\u7edc\u5b89\u5168\u3001\u91d1\u878d\u670d\u52a1\u548c\u7814\u7a76\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efaAI\u667a\u80fd\u4f53\u3002", "motivation": "Anthropic\u65e8\u5728\u901a\u8fc7Claude Sonnet 4.5\u5728AI\u667a\u80fd\u4f53\u548c\u7f16\u7a0b\u9886\u57df\u53d6\u5f97\u9886\u5148\u5730\u4f4d\uff0c\u63d0\u4f9b\u80fd\u591f\u5904\u7406\u590d\u6742\u3001\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u5f3a\u5927\u6a21\u578b\u3002", "method": "\u5f00\u53d1Claude Sonnet 4.5\u6a21\u578b\uff0c\u5177\u590730\u5c0f\u65f6\u81ea\u4e3b\u8fd0\u884c\u80fd\u529b\uff0c\u4e13\u6ce8\u4e8e\u7f51\u7edc\u5b89\u5168\u3001\u91d1\u878d\u670d\u52a1\u548c\u7814\u7a76\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u5e76\u4e0e\u5176\u4ed6\u66f4\u65b0\u914d\u5408\u652f\u6301\u5f00\u53d1\u8005\u6784\u5efaAI\u667a\u80fd\u4f53\u3002", "result": "Claude Sonnet 4.5\u5728beta\u6d4b\u8bd5\u4e2d\u6210\u529f\u5e2e\u52a9\u7528\u6237\u5904\u7406\u590d\u6742\u7684\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u5305\u62ec\u4ee3\u7801\u5e93\u5de5\u7a0b\u3001\u4ea7\u54c1\u529f\u80fd\u548c\u7814\u7a76\u5de5\u4f5c\u3002", "conclusion": "Claude Sonnet 4.5\u7684\u53d1\u5e03\u6807\u5fd7\u7740Anthropic\u5728AI\u667a\u80fd\u4f53\u548c\u7f16\u7a0b\u80fd\u529b\u65b9\u9762\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "tldr.2509.240eaf7f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFD3PZm/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dLsiIJR0phCI58NPvtzSX0fdiWdXRH64ALsxmBjLhzc=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFD3PZm/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dLsiIJR0phCI58NPvtzSX0fdiWdXRH64ALsxmBjLhzc=424", "authors": ["TLDR Newsletter"], "title": "Microsoft Sets the Tone for 'Vibe Working' With New Agent Mode in Word, Excel", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFD3PZm/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/dLsiIJR0phCI58NPvtzSX0fdiWdXRH64ALsxmBjLhzc=424", "summary": "Microsoft Sets the Tone for 'Vibe Working' With New Agent Mode in Word, Excel (4 minute read) Agent Mode in Microsoft 365 Copilot can generate high-quality documents and spreadsheets from simple prompts.", "source": "tldr", "AI": {"tldr": "\u5fae\u8f6f\u5728Word\u548cExcel\u4e2d\u63a8\u51faAgent Mode\u529f\u80fd\uff0c\u80fd\u591f\u6839\u636e\u7b80\u5355\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u6863\u548c\u7535\u5b50\u8868\u683c", "motivation": "\u63d0\u9ad8\u529e\u516c\u6548\u7387\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u5c31\u80fd\u5feb\u901f\u751f\u6210\u4e13\u4e1a\u6587\u6863\u548c\u7535\u5b50\u8868\u683c", "method": "\u5728Microsoft 365 Copilot\u4e2d\u96c6\u6210Agent Mode\u529f\u80fd\uff0c\u5229\u7528AI\u6280\u672f\u7406\u89e3\u7528\u6237\u63d0\u793a\u5e76\u81ea\u52a8\u751f\u6210\u5185\u5bb9", "result": "\u80fd\u591f\u4ece\u7b80\u5355\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u6863\u548c\u7535\u5b50\u8868\u683c", "conclusion": "Agent Mode\u529f\u80fd\u663e\u8457\u63d0\u5347\u4e86\u529e\u516c\u8f6f\u4ef6\u7684\u751f\u4ea7\u529b\uff0c\u4f7f\u6587\u6863\u521b\u5efa\u8fc7\u7a0b\u66f4\u52a0\u9ad8\u6548", "topic": "swe application"}}
{"id": "tldr.2509.1dbdcd9a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F9%2F29%2F90-percent%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/0bFfeT5LYMz0cWObWfQMnsjeJ3UbnheJnYywe1V7X48=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F9%2F29%2F90-percent%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/0bFfeT5LYMz0cWObWfQMnsjeJ3UbnheJnYywe1V7X48=424", "authors": ["TLDR Newsletter"], "title": "90%", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F9%2F29%2F90-percent%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/0bFfeT5LYMz0cWObWfQMnsjeJ3UbnheJnYywe1V7X48=424", "summary": "90% (7 minute read) AI tools are powerful, and they can already generate most of the code in a project, but they don't absolve engineers of responsibility - engineers still need to review every line, shape the architecture, and carry the responsibility for how it runs in production.", "source": "tldr", "AI": {"tldr": "AI\u5de5\u5177\u867d\u80fd\u751f\u6210\u5927\u90e8\u5206\u4ee3\u7801\uff0c\u4f46\u5de5\u7a0b\u5e08\u4ecd\u9700\u5ba1\u67e5\u4ee3\u7801\u3001\u8bbe\u8ba1\u67b6\u6784\u5e76\u5bf9\u751f\u4ea7\u73af\u5883\u8d1f\u8d23", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\u53ca\u5176\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u5de5\u7a0b\u5e08\u5728AI\u8f85\u52a9\u5f00\u53d1\u4e2d\u7684\u6301\u7eed\u8d23\u4efb", "method": "\u57fa\u4e8e\u5bf9AI\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u5206\u6790\u548c\u5bf9\u5de5\u7a0b\u5e08\u804c\u8d23\u7684\u601d\u8003", "result": "AI\u5de5\u5177\u80fd\u751f\u621090%\u7684\u4ee3\u7801\uff0c\u4f46\u5de5\u7a0b\u5e08\u4ecd\u9700\u627f\u62c5\u4ee3\u7801\u5ba1\u67e5\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u4ea7\u8d23\u4efb", "conclusion": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u662f\u5f3a\u5927\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u4f46\u4e0d\u80fd\u66ff\u4ee3\u5de5\u7a0b\u5e08\u7684\u6838\u5fc3\u804c\u8d23\u548c\u6700\u7ec8\u8d23\u4efb", "topic": "swe application"}}
{"id": "tldr.2509.694b772b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.skeptrune.com%2Fposts%2Fuse-the-accept-header-to-serve-markdown-instead-of-html-to-llms%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/ysqLFqBMNMr-YrEebj1Cr1Y1wBDUFDZmfGfdkP2ejFY=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.skeptrune.com%2Fposts%2Fuse-the-accept-header-to-serve-markdown-instead-of-html-to-llms%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/ysqLFqBMNMr-YrEebj1Cr1Y1wBDUFDZmfGfdkP2ejFY=424", "authors": ["TLDR Newsletter"], "title": "Use the Accept Header to serve Markdown instead of HTML to LLMs", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.skeptrune.com%2Fposts%2Fuse-the-accept-header-to-serve-markdown-instead-of-html-to-llms%2F%3Futm_source=tldrnewsletter/1/010001999a26277f-e6c9431f-bb9e-4137-8da7-a7168d1f4800-000000/ysqLFqBMNMr-YrEebj1Cr1Y1wBDUFDZmfGfdkP2ejFY=424", "summary": "Use the Accept Header to serve Markdown instead of HTML to LLMs (8 minute read) You can achieve a 10x reduction in token usage while making your content more accessible and efficient for AI systems by serving lean, semantic Markdown to LLM agents.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528Accept Header\u5411LLM\u63d0\u4f9bMarkdown\u683c\u5f0f\u5185\u5bb9\uff0c\u53ef\u4ee5\u5b9e\u73b010\u500d\u7684token\u4f7f\u7528\u91cf\u51cf\u5c11\uff0c\u4f7f\u5185\u5bb9\u5bf9AI\u7cfb\u7edf\u66f4\u6613\u8bbf\u95ee\u548c\u9ad8\u6548\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5904\u7406HTML\u5185\u5bb9\u65f6token\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u901a\u8fc7\u63d0\u4f9b\u8bed\u4e49\u5316\u7684Markdown\u683c\u5f0f\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u7cfb\u7edf\u7684\u5904\u7406\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5229\u7528HTTP Accept Header\u673a\u5236\uff0c\u68c0\u6d4b\u8bf7\u6c42\u6765\u6e90\u662f\u5426\u4e3aLLM\uff0c\u5982\u679c\u662f\u5219\u8fd4\u56deMarkdown\u683c\u5f0f\u800c\u975eHTML\u683c\u5f0f\u7684\u5185\u5bb9\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8610\u500d\u7684token\u4f7f\u7528\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u4f7f\u5185\u5bb9\u5bf9AI\u7cfb\u7edf\u66f4\u52a0\u53cb\u597d\u548c\u9ad8\u6548\u3002", "conclusion": "\u901a\u8fc7\u667a\u80fd\u5185\u5bb9\u683c\u5f0f\u5207\u6362\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u4f18\u5316LLM\u7684\u5185\u5bb9\u5904\u7406\u6548\u7387\uff0c\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u66f4\u7cbe\u7b80\u3001\u8bed\u4e49\u5316\u7684\u6570\u636e\u683c\u5f0f\u3002", "topic": "swe application"}}
{"id": "tldr.2509.c0a820e2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJzMAnU/1/010001999a4c3f28-0dd05cd7-c963-4534-b244-bc33e99948b7-000000/-kILGjOynlyHLlAMEQEm39mg-mbtbKL0H62QyONzFqk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJzMAnU/1/010001999a4c3f28-0dd05cd7-c963-4534-b244-bc33e99948b7-000000/-kILGjOynlyHLlAMEQEm39mg-mbtbKL0H62QyONzFqk=424", "authors": ["TLDR Newsletter"], "title": "Buy it in ChatGPT", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJzMAnU/1/010001999a4c3f28-0dd05cd7-c963-4534-b244-bc33e99948b7-000000/-kILGjOynlyHLlAMEQEm39mg-mbtbKL0H62QyONzFqk=424", "summary": "Buy it in ChatGPT (3 minute read) ChatGPT's Instant Checkout lets users buy products directly in chat from US Etsy sellers. Shopify merchants are coming soon. Single-item purchases are currently supported, with multi-item carts planned. ChatGPT acts as an AI agent, securely passing order, payment, and shipping details to merchants, who handle fulfillment and customer support. The system uses the open-sourced Agentic Commerce Protocol, allowing merchants to work across platforms and payment pr...", "source": "tldr", "AI": {"tldr": "ChatGPT\u63a8\u51fa\u5373\u65f6\u7ed3\u8d26\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u5728\u804a\u5929\u4e2d\u4ece\u7f8e\u56fdEtsy\u5356\u5bb6\u8d2d\u4e70\u4ea7\u54c1\uff0cShopify\u5546\u5bb6\u5373\u5c06\u52a0\u5165\u3002\u76ee\u524d\u652f\u6301\u5355\u4ef6\u8d2d\u4e70\uff0c\u8ba1\u5212\u63a8\u51fa\u591a\u4ef6\u8d2d\u7269\u8f66\u529f\u80fd\u3002", "motivation": "\u7b80\u5316\u7535\u5546\u8d2d\u7269\u6d41\u7a0b\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u79bb\u5f00\u804a\u5929\u754c\u9762\u5373\u53ef\u5b8c\u6210\u8d2d\u4e70\uff0c\u63d0\u5347\u8d2d\u7269\u4f53\u9a8c\u548c\u4fbf\u5229\u6027\u3002", "method": "ChatGPT\u4f5c\u4e3aAI\u4ee3\u7406\uff0c\u5b89\u5168\u5730\u4f20\u9012\u8ba2\u5355\u3001\u652f\u4ed8\u548c\u914d\u9001\u4fe1\u606f\u7ed9\u5546\u5bb6\uff0c\u5546\u5bb6\u8d1f\u8d23\u5c65\u884c\u8ba2\u5355\u548c\u5ba2\u6237\u652f\u6301\u3002\u4f7f\u7528\u5f00\u6e90\u7684Agentic Commerce Protocol\uff0c\u4f7f\u5546\u5bb6\u80fd\u5728\u4e0d\u540c\u5e73\u53f0\u548c\u652f\u4ed8\u7cfb\u7edf\u95f4\u5de5\u4f5c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u804a\u5929\u754c\u9762\u5185\u7684\u76f4\u63a5\u8d2d\u4e70\u529f\u80fd\uff0c\u4e3a\u7f8e\u56fdEtsy\u5356\u5bb6\u63d0\u4f9b\u670d\u52a1\uff0cShopify\u5546\u5bb6\u5373\u5c06\u63a5\u5165\u3002", "conclusion": "ChatGPT\u7684\u5373\u65f6\u7ed3\u8d26\u529f\u80fd\u4ee3\u8868\u4e86AI\u4ee3\u7406\u5728\u7535\u5546\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\uff0c\u901a\u8fc7\u7b80\u5316\u8d2d\u7269\u6d41\u7a0b\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "topic": "swe application"}}
{"id": "tldr.2509.fbc0e6e8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantropia.studio%2Fblog%2Fto-ai-or-not-to-ai%2F%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/6wZNYlAZ33fbIPsNXlFo1xez4Bq7xE9uUephECSbIyk=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantropia.studio%2Fblog%2Fto-ai-or-not-to-ai%2F%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/6wZNYlAZ33fbIPsNXlFo1xez4Bq7xE9uUephECSbIyk=424", "authors": ["TLDR Newsletter"], "title": "To AI or not to AI", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantropia.studio%2Fblog%2Fto-ai-or-not-to-ai%2F%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/6wZNYlAZ33fbIPsNXlFo1xez4Bq7xE9uUephECSbIyk=424", "summary": "To AI or not to AI (7 minute read) An experiment was conducted over two weeks to build an app with full AI assistance. While the team found AI helpful in specific areas like searching, code snippets, and language tasks, they were frustrated with its limitations in providing context, maintaining code, and uncovering corner cases. Overall, the code became messy, control was lost, and the team returned to their traditional workflow.", "source": "tldr", "AI": {"tldr": "\u4e24\u5468\u5b9e\u9a8c\u663e\u793aAI\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u6709\u5e2e\u52a9\uff0c\u4f46\u5728\u4ee3\u7801\u7ef4\u62a4\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u56e2\u961f\u56de\u5f52\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u63a2\u7d22AI\u5728\u5b8c\u6574\u5e94\u7528\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u548c\u5c40\u9650\u6027", "method": "\u8fdb\u884c\u4e3a\u671f\u4e24\u5468\u7684\u5b9e\u9a8c\uff0c\u5b8c\u5168\u4f9d\u8d56AI\u8f85\u52a9\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\uff0c\u8bb0\u5f55AI\u5728\u5404\u4e2a\u5f00\u53d1\u73af\u8282\u7684\u8868\u73b0", "result": "AI\u5728\u641c\u7d22\u3001\u4ee3\u7801\u7247\u6bb5\u751f\u6210\u548c\u8bed\u8a00\u4efb\u52a1\u65b9\u9762\u6709\u5e2e\u52a9\uff0c\u4f46\u5728\u63d0\u4f9b\u4e0a\u4e0b\u6587\u3001\u4ee3\u7801\u7ef4\u62a4\u548c\u53d1\u73b0\u8fb9\u754c\u60c5\u51b5\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u6700\u7ec8\u4ee3\u7801\u53d8\u5f97\u6df7\u4e71\uff0c\u56e2\u961f\u5931\u53bb\u63a7\u5236", "conclusion": "AI\u76ee\u524d\u66f4\u9002\u5408\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u800c\u975e\u5b8c\u5168\u66ff\u4ee3\u4f20\u7edf\u5f00\u53d1\u6d41\u7a0b\uff0c\u5728\u4ee3\u7801\u8d28\u91cf\u548c\u7cfb\u7edf\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3", "topic": "swe application"}}
{"id": "tldr.2509.40f9b132", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/eTMYz8o-nz4MUtbViQfRSfjbWjlrti3meExy7MfrG6I=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/eTMYz8o-nz4MUtbViQfRSfjbWjlrti3meExy7MfrG6I=424", "authors": ["TLDR Newsletter"], "title": "Introducing Claude Sonnet 4.5", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-5%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/eTMYz8o-nz4MUtbViQfRSfjbWjlrti3meExy7MfrG6I=424", "summary": "Introducing Claude Sonnet 4.5 (5 minute read) Anthropic has released Claude Sonnet 4.5, a new frontier model with huge improvements in coding, computer usage, reasoning, and math. The model has state-of-the-art performance on benchmarks like SWE-bench Verified and OSWorld (for real-world software coding and computer task abilities). Along with the model, Anthropic has released Claude Code 2.0 and new features to the Claude apps.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03\u4e86Claude Sonnet 4.5\u6a21\u578b\uff0c\u5728\u7f16\u7801\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u63a8\u7406\u548c\u6570\u5b66\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5728SWE-bench Verified\u548cOSWorld\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684AI\u6a21\u578b\uff0c\u63d0\u5347\u5728\u7f16\u7a0b\u3001\u8ba1\u7b97\u673a\u64cd\u4f5c\u3001\u63a8\u7406\u548c\u6570\u5b66\u7b49\u5173\u952e\u9886\u57df\u7684\u80fd\u529b\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u53d1\u5e03\u65b0\u7684\u524d\u6cbf\u6a21\u578bClaude Sonnet 4.5\uff0c\u540c\u65f6\u63a8\u51faClaude Code 2.0\u548cClaude\u5e94\u7528\u7684\u65b0\u529f\u80fd\u3002", "result": "\u6a21\u578b\u5728SWE-bench Verified\u548cOSWorld\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "Claude Sonnet 4.5\u662f\u4e00\u4e2a\u5728\u591a\u4e2a\u5173\u952e\u9886\u57df\u90fd\u6709\u663e\u8457\u6539\u8fdb\u7684\u5f3a\u5927AI\u6a21\u578b\u3002", "topic": "swe benchmark"}}
{"id": "tldr.2509.cf183aab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fsandboxing-agents-at-the-kernel-level%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/e__N2S1Aonz0ZOjthALX5jd_fexvyGCH_rG5LX8J198=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fsandboxing-agents-at-the-kernel-level%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/e__N2S1Aonz0ZOjthALX5jd_fexvyGCH_rG5LX8J198=424", "authors": ["TLDR Newsletter"], "title": "Sandboxing agents at the kernel level", "comment": "Source: TLDR Newsletter, Date: 2025-09-30, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fsandboxing-agents-at-the-kernel-level%3Futm_source=tldrwebdev/1/010001999a4fd8b7-83e94639-c12d-4f75-a326-dd9fd83ea1a8-000000/e__N2S1Aonz0ZOjthALX5jd_fexvyGCH_rG5LX8J198=424", "summary": "Sandboxing agents at the kernel level (11 minute read) AI agents can be sandboxed at the kernel level in Linux to prevent unauthorized file access. Tracing the `open` syscall shows three points of failure where file access can be denied: permission checks, mount point redirections, and changing the root directory of the process. Combining mount namespaces and root changes offers control over the agent's filesystem view.", "source": "tldr", "AI": {"tldr": "\u5728Linux\u5185\u6838\u5c42\u9762\u6c99\u7bb1\u5316AI\u4ee3\u7406\u4ee5\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6587\u4ef6\u8bbf\u95ee\uff0c\u901a\u8fc7\u8ffd\u8e2aopen\u7cfb\u7edf\u8c03\u7528\u8bc6\u522b\u4e09\u4e2a\u5931\u8d25\u70b9\uff0c\u7ed3\u5408\u6302\u8f7d\u547d\u540d\u7a7a\u95f4\u548c\u6839\u76ee\u5f55\u53d8\u66f4\u63a7\u5236\u4ee3\u7406\u7684\u6587\u4ef6\u7cfb\u7edf\u89c6\u56fe\u3002", "motivation": "\u9632\u6b62AI\u4ee3\u7406\u672a\u7ecf\u6388\u6743\u8bbf\u95ee\u6587\u4ef6\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u9694\u79bb\u6027\u3002", "method": "\u901a\u8fc7\u8ffd\u8e2aopen\u7cfb\u7edf\u8c03\u7528\u8bc6\u522b\u6743\u9650\u68c0\u67e5\u3001\u6302\u8f7d\u70b9\u91cd\u5b9a\u5411\u548c\u8fdb\u7a0b\u6839\u76ee\u5f55\u53d8\u66f4\u4e09\u4e2a\u5931\u8d25\u70b9\uff0c\u7ed3\u5408\u6302\u8f7d\u547d\u540d\u7a7a\u95f4\u548c\u6839\u76ee\u5f55\u53d8\u66f4\u6280\u672f\u5b9e\u73b0\u6587\u4ef6\u7cfb\u7edf\u89c6\u56fe\u63a7\u5236\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5185\u6838\u5c42\u9762\u6c99\u7bb1\u5316AI\u4ee3\u7406\uff0c\u6709\u6548\u9632\u6b62\u4e86\u672a\u7ecf\u6388\u6743\u7684\u6587\u4ef6\u8bbf\u95ee\u3002", "conclusion": "\u7ed3\u5408\u6302\u8f7d\u547d\u540d\u7a7a\u95f4\u548c\u6839\u76ee\u5f55\u53d8\u66f4\u6280\u672f\u53ef\u4ee5\u6709\u6548\u63a7\u5236AI\u4ee3\u7406\u7684\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\u6743\u9650\uff0c\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "wechat.2510.73843c5e", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMDA0MTE5Mw==&mid=2247484317&idx=1&sn=11d4847f9595f43fae0e9c7e6d041773&chksm=9a479ba06ba910bfc5bdbb94c9355a4a295807469e1569725f580c3a9caad8c8c16ed7f06859#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMDA0MTE5Mw==&mid=2247484317&idx=1&sn=11d4847f9595f43fae0e9c7e6d041773&chksm=9a479ba06ba910bfc5bdbb94c9355a4a295807469e1569725f580c3a9caad8c8c16ed7f06859#rd", "authors": ["\u524d\u7aef\u751f\u5b58\u6307\u5357"], "title": "Serena MCP\uff1a\u8ba9\u4f60\u7684 <em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u66f4\u52a0\u667a\u80fd\u7684\u7406\u89e3\u548c\u7f16\u8f91<em class=\"highlight\">\u4ee3\u7801</em>", "comment": "Source: WeChat, Published: 2025-10-01 04:58:05", "summary": "Serena MCP\uff1a\u8ba9\u4f60\u7684 Code Agent \u66f4\u52a0\u667a\u80fd\u7684\u7406\u89e3\u548c\u7f16\u8f91\u4ee3\u7801serena\u6211\u4eec\u5728\u524d\u6587 \u4f60\u7684 Code Crush - \u6700\u6f02\u4eae\u7684 CLI Code Agent \u63d0\u5230 Crush \u901a\u8fc7\u5185\u7f6e LSP\uff08Language Server Protocol\uff09 \u8f85\u52a9 Agent \u66f4\u667a\u80fd\u7684\u7406\u89e3\u4ee3\u7801\u3002", "AI": {"tldr": "Serena MCP\uff1a\u8ba9\u4f60\u7684 Code Agent \u66f4\u52a0\u667a\u80fd\u7684\u7406\u89e3\u548c\u7f16\u8f91\u4ee3\u7801serena\u6211\u4eec\u5728\u524d\u6587 \u4f60\u7684 Code Crush - \u6700\u6f02\u4eae\u7684 CLI Code Agent \u63d0\u5230 Crush \u901a\u8fc7\u5185\u7f6e LSP\uff08Language Server Protocol\uff09 \u8f85\u52a9 Agent \u66f4\u667a\u80fd\u7684\u7406\u89e3\u4ee3\u7801\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.b280cc3c", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MjcwODI3MA==&mid=2247501510&idx=2&sn=6636b827286b208072db441af1be9a55&chksm=c283fcc6def27f5f649bcaba59bbb11aa9427f1a40a2bf2b1fe0b4829edcaddce4aab7aeb794#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MjcwODI3MA==&mid=2247501510&idx=2&sn=6636b827286b208072db441af1be9a55&chksm=c283fcc6def27f5f649bcaba59bbb11aa9427f1a40a2bf2b1fe0b4829edcaddce4aab7aeb794#rd", "authors": ["AGIC\u56fd\u9645\u901a\u7528\u4eba\u5de5\u667a\u80fd\u5927\u4f1a"], "title": "\u6df1\u5ea6\u89e3\u8bfb\uff5c\u60a0\u7136\u65e0\u754c<em class=\"highlight\">\u5927\u6a21\u578b</em>BLM-1.0\uff1a\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u4e0e\u8de8\u672c\u4f53\u6cdb\u5316\u7684\u91cc\u7a0b\u7891", "comment": "Source: WeChat, Published: 2025-10-01 13:05:01", "summary": "\u60a0\u7136\u65e0\u754c\u5927\u6a21\u578bBLM-1.0\u662f\u4e00\u79cd\u4ee5\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\u4e3a\u6838\u5fc3\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u805a\u7126\u4e8e\u201c\u7a7a\u95f4\u7406\u89e3\u2014\u7a7a\u95f4\u63a8\u7406\u2014\u7a7a\u95f4\u6267\u884c\u201d\u4e09\u5927\u4efb\u52a1\u76ee\u6807\uff0c\u5b9e\u73b0\u6570\u5b57\u7a7a\u95f4\u4e0e\u7269\u7406\u4e16\u754c\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u80fd\u529b\u4fc3\u8fdb\u3002", "AI": {"tldr": "\u60a0\u7136\u65e0\u754c\u5927\u6a21\u578bBLM-1.0\u662f\u4e00\u79cd\u4ee5\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\u4e3a\u6838\u5fc3\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u805a\u7126\u4e8e\u201c\u7a7a\u95f4\u7406\u89e3\u2014\u7a7a\u95f4\u63a8\u7406\u2014\u7a7a\u95f4\u6267\u884c\u201d\u4e09\u5927\u4efb\u52a1\u76ee\u6807\uff0c\u5b9e\u73b0\u6570\u5b57\u7a7a\u95f4\u4e0e\u7269\u7406\u4e16\u754c\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u80fd\u529b\u4fc3\u8fdb\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4ff1fc6b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMTI1NDM4OQ==&mid=2247483919&idx=1&sn=9cdd73c7873c3caa08ba42c5afb3c64a&chksm=e9c21742f080f0e7c2c6ea35a9aa75e0a27eddb21ac3e1c5a0d0255a7ff576a322ffd85b1df7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMTI1NDM4OQ==&mid=2247483919&idx=1&sn=9cdd73c7873c3caa08ba42c5afb3c64a&chksm=e9c21742f080f0e7c2c6ea35a9aa75e0a27eddb21ac3e1c5a0d0255a7ff576a322ffd85b1df7#rd", "authors": ["\u7f16\u7a0b\u6599\u7406"], "title": "8 \u6b3e\u4ee3\u7801<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7ec8\u6781 PK\uff1a\u4e00\u53e5\u8bdd\u544a\u8bc9\u4f60\u8c01\u6700\u80fd\u6253", "comment": "Source: WeChat, Published: 2025-10-01 12:28:56", "summary": "\u9876\u7ea7ai\u4ee3\u7801\u6a21\u578b\u6027\u80fd\u5927\u6bd4\u62fc\u3002BigCodeBench\uff08Hard\uff09\u548cSWE-Bench\u4e09\u5927\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002claude 3.7 sonnet\u4ee592.1%\u7684humaneval\u5f97\u5206\u9886\u8dd1\uff0c\u800cdeepseek-coder-v3\u5219\u4ee592.8%\u7684 \u60ca\u4eba\u6210\u7ee9\u6210\u4e3a\u5f00\u6e90\u9886\u57df\u7684\u51a0\u519b\u3002", "AI": {"tldr": "\u9876\u7ea7ai\u4ee3\u7801\u6a21\u578b\u6027\u80fd\u5927\u6bd4\u62fc\u3002BigCodeBench\uff08Hard\uff09\u548cSWE-Bench\u4e09\u5927\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002claude 3.7 sonnet\u4ee592.1%\u7684humaneval\u5f97\u5206\u9886\u8dd1\uff0c\u800cdeepseek-coder-v3\u5219\u4ee592.8%\u7684 \u60ca\u4eba\u6210\u7ee9\u6210\u4e3a\u5f00\u6e90\u9886\u57df\u7684\u51a0\u519b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.555c759e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMzMzMjA4Ng==&mid=2247534500&idx=3&sn=2b10523aa25aa1aa102b10995f7f2531&chksm=c07360c73aaa67fa3d4686b09f99c69b9a9d97b8f33e7b17f65c9b392f7c265a3c4914d703d3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMzMzMjA4Ng==&mid=2247534500&idx=3&sn=2b10523aa25aa1aa102b10995f7f2531&chksm=c07360c73aaa67fa3d4686b09f99c69b9a9d97b8f33e7b17f65c9b392f7c265a3c4914d703d3#rd", "authors": ["\u6570\u667a\u6df1\u84dd"], "title": "\u201c\u77b0\u6d77\u201d\u7aef\u5230\u7aef\u6d77\u6d0b\u9884\u6d4b<em class=\"highlight\">\u5927\u6a21\u578b</em>\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-10-01 11:12:22", "summary": "\u77b0\u6d77\uff08SkyOcean\uff09\u4eba\u5de5\u667a\u80fd\u533a\u57df\u6d77\u6d0b\u5927\u6a21\u578b\uff08\u4ee5\u4e0b\u7b80\u79f0\u201c\u77b0\u6d77\u201d\u5927\u6a21\u578b\uff09\u662f\u7531\u4e2d\u5c71\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u5b66\u9662\u4e0e\u56fd\u5bb6\u536b\u661f\u6d77\u6d0b\u5e94\u7528\u4e2d\u5fc3\u53cc\u65b9\u5408\u4f5c\u5f00\u53d1\u7684\u201c\u7aef\u5230\u7aef\u201d\u4eba\u5de5\u667a\u80fd\u4e09\u7ef4\u6d77\u6d0b\u52a8\u529b\u73af\u5883\u8981\u7d20\u9884\u6d4b\u6a21\u578b\uff0c\u5df2\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u6210\u4e3a\u9996\u4e2a\u5728\u56fd", "AI": {"tldr": "\u77b0\u6d77\uff08SkyOcean\uff09\u4eba\u5de5\u667a\u80fd\u533a\u57df\u6d77\u6d0b\u5927\u6a21\u578b\uff08\u4ee5\u4e0b\u7b80\u79f0\u201c\u77b0\u6d77\u201d\u5927\u6a21\u578b\uff09\u662f\u7531\u4e2d\u5c71\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u5b66\u9662\u4e0e\u56fd\u5bb6\u536b\u661f\u6d77\u6d0b\u5e94\u7528\u4e2d\u5fc3\u53cc\u65b9\u5408\u4f5c\u5f00\u53d1\u7684\u201c\u7aef\u5230\u7aef\u201d\u4eba\u5de5\u667a\u80fd\u4e09\u7ef4\u6d77\u6d0b\u52a8\u529b\u73af\u5883\u8981\u7d20\u9884\u6d4b\u6a21\u578b\uff0c\u5df2\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u6210\u4e3a\u9996\u4e2a\u5728\u56fd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.4cda72bf", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMTA1NjY5MQ==&mid=2647754838&idx=1&sn=2a293fd54a685be101e6d64998ac9680&chksm=f1a97606a8fc0bc9cd37905ca746a14cacd02080d960f328fe8296ad902e4ba0e40bd16e4eee#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMTA1NjY5MQ==&mid=2647754838&idx=1&sn=2a293fd54a685be101e6d64998ac9680&chksm=f1a97606a8fc0bc9cd37905ca746a14cacd02080d960f328fe8296ad902e4ba0e40bd16e4eee#rd", "authors": ["\u6807\u51c6\u8ba4\u8bc1\u5708"], "title": "\u3010GB/T 45288.2\u3011\u300a\u4eba\u5de5\u667a\u80fd <em class=\"highlight\">\u5927\u6a21\u578b</em> \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b", "comment": "Source: WeChat, Published: 2025-10-01 11:00:27", "summary": "gb/t 45288.2-2025 \u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u662f\u4e2d\u56fd\u5728\u5927\u6a21\u578b\u9886\u57df\u6807\u51c6\u5316\u8fdb\u7a0b\u4e2d\u7684\u5173\u952e\u4e00\u6b65\uff0c\u65e8\u5728\u5efa\u7acb\u4e00\u5957\u79d1\u5b66\u3001\u7edf\u4e00\u3001\u53ef\u64cd\u4f5c\u7684\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u4ee5\u89c4\u8303\u548c\u5f15\u5bfc\u4e2d\u56fd\u5927\u6a21\u578b\u6280\u672f\u7684\u5065\u5eb7\u53d1\u5c55\u3002", "AI": {"tldr": "gb/t 45288.2-2025 \u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u662f\u4e2d\u56fd\u5728\u5927\u6a21\u578b\u9886\u57df\u6807\u51c6\u5316\u8fdb\u7a0b\u4e2d\u7684\u5173\u952e\u4e00\u6b65\uff0c\u65e8\u5728\u5efa\u7acb\u4e00\u5957\u79d1\u5b66\u3001\u7edf\u4e00\u3001\u53ef\u64cd\u4f5c\u7684\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u4ee5\u89c4\u8303\u548c\u5f15\u5bfc\u4e2d\u56fd\u5927\u6a21\u578b\u6280\u672f\u7684\u5065\u5eb7\u53d1\u5c55\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.774e2168", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTc4MDM1MQ==&mid=2651742807&idx=2&sn=dff37efe6193915f40bb37e182008c4f&chksm=bcb2b35530c00424203267823e87070431e653ba55ce9f04d800b88bf6f0cd9195d772eb38c9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTc4MDM1MQ==&mid=2651742807&idx=2&sn=dff37efe6193915f40bb37e182008c4f&chksm=bcb2b35530c00424203267823e87070431e653ba55ce9f04d800b88bf6f0cd9195d772eb38c9#rd", "authors": ["\u65b0\u4e00\u4ee3\u667a\u80fd\u5316\u5e94\u7528"], "title": "\u5e38\u89c1\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bc4\u6d4b\u4e0e\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u5bf9\u6bd4\u4e0e\u9009\u578b", "comment": "Source: WeChat, Published: 2025-10-01 09:14:29", "summary": "\u767e\u5ea6\u5343\u5e06\u5927\u6a21\u578b\u5e73\u53f0\u662f\u56fd\u5185\u6700\u65e9\u63a8\u51fa\u7684AI\u5927\u6a21\u578b\u4ea7\u54c1\u5e73\u53f0\uff0c\u63d0\u4f9b\u4ece\u6a21\u578b\u8bad\u7ec3\u5230\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u670d\u52a1 \u3002\u8be5\u5e73\u53f0\u652f\u6301ERNIE-3.5\u7b49\u6a21\u578b\u7684\u957f\u6587\u672c\u5904\u7406\uff08\u5982128K\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\uff09\uff0c\u5185\u7f6e\u4e2d\u6587\u591a\u5b66\u79d1\u8bc4\u6d4b\u96c6\uff08\u5982C-Eval\u3001LHMKE\uff09\uff0c\u7279\u522b\u9002\u5408\u4e2d\u6587\u6a21\u578b\u7684", "AI": {"tldr": "\u767e\u5ea6\u5343\u5e06\u5927\u6a21\u578b\u5e73\u53f0\u662f\u56fd\u5185\u6700\u65e9\u63a8\u51fa\u7684AI\u5927\u6a21\u578b\u4ea7\u54c1\u5e73\u53f0\uff0c\u63d0\u4f9b\u4ece\u6a21\u578b\u8bad\u7ec3\u5230\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u670d\u52a1 \u3002\u8be5\u5e73\u53f0\u652f\u6301ERNIE-3.5\u7b49\u6a21\u578b\u7684\u957f\u6587\u672c\u5904\u7406\uff08\u5982128K\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\uff09\uff0c\u5185\u7f6e\u4e2d\u6587\u591a\u5b66\u79d1\u8bc4\u6d4b\u96c6\uff08\u5982C-Eval\u3001LHMKE\uff09\uff0c\u7279\u522b\u9002\u5408\u4e2d\u6587\u6a21\u578b\u7684", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.f40ac0bc", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTc4MDM1MQ==&mid=2651742807&idx=1&sn=412d13e055f24462dae917f93a9d6a69&chksm=bc0fa1d320bccd5bd705a6ba3bb888d36b1ad3b9dca23f903f5be3a05b43f6046b8e0de9ee70#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTc4MDM1MQ==&mid=2651742807&idx=1&sn=412d13e055f24462dae917f93a9d6a69&chksm=bc0fa1d320bccd5bd705a6ba3bb888d36b1ad3b9dca23f903f5be3a05b43f6046b8e0de9ee70#rd", "authors": ["\u65b0\u4e00\u4ee3\u667a\u80fd\u5316\u5e94\u7528"], "title": "\u4f7f\u7528 EvalScope \u8fdb\u884c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6027\u80fd\u6d4b\u8bd5\uff1a\u5b9e\u6218\u6559\u7a0b", "comment": "Source: WeChat, Published: 2025-10-01 09:14:29", "summary": "EvalScope \u662f\u9b54\u5854\u793e\u533a\u63a8\u8350\u7684\u4e00\u6b3e\u5927\u6a21\u578b\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u3001\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u8fd0\u884c\u6548\u7387\u548c\u6548\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u4ee5\u4e00\u4e2a\u5b9e\u9645\u7684\u4ee3\u7801\u793a\u4f8b\u4e3a\u57fa\u7840\uff0c\u8be6\u7ec6\u8bb2\u89e3\u5982\u4f55\u4f7f\u7528 EvalScope \u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "AI": {"tldr": "EvalScope \u662f\u9b54\u5854\u793e\u533a\u63a8\u8350\u7684\u4e00\u6b3e\u5927\u6a21\u578b\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u3001\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u8fd0\u884c\u6548\u7387\u548c\u6548\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u4ee5\u4e00\u4e2a\u5b9e\u9645\u7684\u4ee3\u7801\u793a\u4f8b\u4e3a\u57fa\u7840\uff0c\u8be6\u7ec6\u8bb2\u89e3\u5982\u4f55\u4f7f\u7528 EvalScope \u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.75bc656b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTkxMDQ3NA==&mid=2247484324&idx=1&sn=fecfa52cd92f786615e495d724772921&chksm=9761607e0cfeedd8b4a59762dd6211b541a42e05ab7513745404fbea991963a97481842134f5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTkxMDQ3NA==&mid=2247484324&idx=1&sn=fecfa52cd92f786615e495d724772921&chksm=9761607e0cfeedd8b4a59762dd6211b541a42e05ab7513745404fbea991963a97481842134f5#rd", "authors": ["AI\u79d1\u6280\u524d\u7ebf\u82f1\u8bed\u8bf4"], "title": "\u56fd\u5e86\u732e\u793c\uff01\u667a\u8c31GLM-4.6\u91cd\u78c5\u53d1\u5e03\uff0c\u56fd\u4ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u732e\u4e0a\u201c\u4ee3\u7801\u5982\u8bd7\u201d\u9ed1\u79d1\u6280", "comment": "Source: WeChat, Published: 2025-10-01 08:35:19", "summary": "\u4ef7\u683c\u4fbf\u5b9c\uff0c\u4eba\u4eba\u90fd\u53ef\u4ee5\u4f7f\u7528\u5173\u4e8e\u4ec0\u4e48\u662f\u5927\u6a21\u578b\u3002\u6b22\u8fce\u89c2\u770b\u6211\u4eec\u4e0a\u4e00\u7bc7\u6587\u7ae0\u3002\u5927\u6a21\u578b\u7684\u6210\u957f\u4e4b\u65c5\uff08\u8f7b\u677e\u7bc7\uff09\uff1a\u4e00\u6587\u8bb2\u6e05\u695a\u5927\u6a21\u578b\u8bad\u7ec3--\u4f18\u5316\u5728\u8fd9\u4e2a\u4e3e\u56fd\u540c\u5e86\u7684\u65e5\u5b50\u91cc\uff0c\u56fd\u4ea7AI\u754c\u4e5f\u4f20\u6765\u4ee4\u4eba\u632f\u594b\u7684\u6d88\u606f\uff01", "AI": {"tldr": "\u4ef7\u683c\u4fbf\u5b9c\uff0c\u4eba\u4eba\u90fd\u53ef\u4ee5\u4f7f\u7528\u5173\u4e8e\u4ec0\u4e48\u662f\u5927\u6a21\u578b\u3002\u6b22\u8fce\u89c2\u770b\u6211\u4eec\u4e0a\u4e00\u7bc7\u6587\u7ae0\u3002\u5927\u6a21\u578b\u7684\u6210\u957f\u4e4b\u65c5\uff08\u8f7b\u677e\u7bc7\uff09\uff1a\u4e00\u6587\u8bb2\u6e05\u695a\u5927\u6a21\u578b\u8bad\u7ec3--\u4f18\u5316\u5728\u8fd9\u4e2a\u4e3e\u56fd\u540c\u5e86\u7684\u65e5\u5b50\u91cc\uff0c\u56fd\u4ea7AI\u754c\u4e5f\u4f20\u6765\u4ee4\u4eba\u632f\u594b\u7684\u6d88\u606f\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.2c85fa2e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5ODYyNzgwMQ==&mid=2247516293&idx=5&sn=378ef8f031b585993a51a0cfc227b859&chksm=c17acfb2a6f42e0c94737e3ff17572e2803518258b54500b131a6428072872c6b8b7d43dccfd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5ODYyNzgwMQ==&mid=2247516293&idx=5&sn=378ef8f031b585993a51a0cfc227b859&chksm=c17acfb2a6f42e0c94737e3ff17572e2803518258b54500b131a6428072872c6b8b7d43dccfd#rd", "authors": ["\u667a\u5bfbAlpha"], "title": "\u8dfb\u8eab\u5168\u7403AI\u7ade\u4e89\u4f53\u7cfb \u56fd\u4ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9996\u6b21\u201c\u51fa\u6d77\u201d", "comment": "Source: WeChat, Published: 2025-10-01 05:57:53", "summary": "\u6b64\u6b21\u4e0a\u7ebf\u7684\u4e24\u5927\u56fd\u4ea7\u6a21\u578b\uff0c\u5404\u6709\u5176\u72ec\u7279\u4f18\u52bf\u3002\u636e\u6089\uff0c\u6b64\u6b21\u5728Bedrock \u4e0a\u7ebf\u7684\u901a\u4e49\u5343\u95ee3\u7cfb\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u591a\u79cd\u6a21\u578b\u9009\u9879\uff0c\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u7684\u9700\u6c42\u3002\u5982\u9762\u5411\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u7684Qwen3-Coder\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u548c\u7406\u89e3\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\uff0c\u5e76\u652f\u6301\u5916\u90e8", "AI": {"tldr": "\u6b64\u6b21\u4e0a\u7ebf\u7684\u4e24\u5927\u56fd\u4ea7\u6a21\u578b\uff0c\u5404\u6709\u5176\u72ec\u7279\u4f18\u52bf\u3002\u636e\u6089\uff0c\u6b64\u6b21\u5728Bedrock \u4e0a\u7ebf\u7684\u901a\u4e49\u5343\u95ee3\u7cfb\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u591a\u79cd\u6a21\u578b\u9009\u9879\uff0c\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u7684\u9700\u6c42\u3002\u5982\u9762\u5411\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u7684Qwen3-Coder\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u548c\u7406\u89e3\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\uff0c\u5e76\u652f\u6301\u5916\u90e8", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.2fccc703", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771008&idx=3&sn=77408b23a18018fe3adaf278b3ba146c&chksm=fa1f412e26e29e6fc1872daf9213706f4f24830dcb1fddbfa9593d4a8741e2a000411889d43e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771008&idx=3&sn=77408b23a18018fe3adaf278b3ba146c&chksm=fa1f412e26e29e6fc1872daf9213706f4f24830dcb1fddbfa9593d4a8741e2a000411889d43e#rd", "authors": ["DataFunTalk"], "title": "B \u7ad9\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5927\u6570\u636e\u667a\u80fd\u8bca\u65ad\u52a9\u624b\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-10-01 05:00:32", "summary": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "AI": {"tldr": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.213aab31", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491812&idx=1&sn=67fdca2140e659c37c77782e5d870155&chksm=fae31fd811e91fcd585d3724a842f47a1d7465fd2c5716d85c2508d960e887f3b23682d01fb4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491812&idx=1&sn=67fdca2140e659c37c77782e5d870155&chksm=fae31fd811e91fcd585d3724a842f47a1d7465fd2c5716d85c2508d960e887f3b23682d01fb4#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9762\u8bd5\u9898\u2014\u2014\u63d0\u793a\u8bcd\u5de5\u7a0b\u80fd\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898?", "comment": "Source: WeChat, Published: 2025-10-01 04:54:01", "summary": "\u540c\u65f6\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u5de5\u7a0b\u5e08\u7684\u804c\u8d23\uff0c\u5f3a\u8c03\u63d0\u793a\u8bcd\u5de5\u7a0b\u5728\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3001\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3001\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u7b49\u65b9\u9762\u7684\u4ef7\u503c\u3002\u6700\u540e\uff0c\u8981\u786e\u4fdd\u56de\u7b54\u4e0d\u4ec5\u505c\u7559\u5728\u7406\u8bba\u5c42\u9762\uff0c\u800c\u662f\u7ed3\u5408\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u6311\u6218\uff0c\u6bd4\u5982\u5982\u4f55\u901a\u8fc7\u8fed\u4ee3\u6d4b\u8bd5\u4f18\u5316\u63d0\u793a\u8bcd", "AI": {"tldr": "\u540c\u65f6\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u5de5\u7a0b\u5e08\u7684\u804c\u8d23\uff0c\u5f3a\u8c03\u63d0\u793a\u8bcd\u5de5\u7a0b\u5728\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3001\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3001\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u7b49\u65b9\u9762\u7684\u4ef7\u503c\u3002\u6700\u540e\uff0c\u8981\u786e\u4fdd\u56de\u7b54\u4e0d\u4ec5\u505c\u7559\u5728\u7406\u8bba\u5c42\u9762\uff0c\u800c\u662f\u7ed3\u5408\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u6311\u6218\uff0c\u6bd4\u5982\u5982\u4f55\u901a\u8fc7\u8fed\u4ee3\u6d4b\u8bd5\u4f18\u5316\u63d0\u793a\u8bcd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.834701a4", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMzczMDg4Nw==&mid=2247558371&idx=1&sn=2d6f6238f5c4ab6917939f2ad309e865&chksm=fbb9ec17c03a95f24a03a988ab243db4fcbdb3d3273616d084d805b3d2c4ec4da95392fbe44d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMzczMDg4Nw==&mid=2247558371&idx=1&sn=2d6f6238f5c4ab6917939f2ad309e865&chksm=fbb9ec17c03a95f24a03a988ab243db4fcbdb3d3273616d084d805b3d2c4ec4da95392fbe44d#rd", "authors": ["\u82c7\u8349\u667a\u9177"], "title": "\u56fe\u7075\u5956\u5f97\u4e3b\u3001\u5f3a\u5316\u5b66\u4e60\u4e4b\u7236Rich Sutton\uff1a\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\u662f\u4e00\u4e2a\u9519\u8bef\u7684\u8d77\u70b9", "comment": "Source: WeChat, Published: 2025-10-01 04:01:35", "summary": "\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u66f4\u591a\u662f\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u6a21\u4eff\uff0c\u5b83\u6ca1\u6709\u72ec\u7acb\u7684\u76ee\u6807\uff0c\u4e5f\u65e0\u6cd5\u5bf9\u5916\u90e8\u4e16\u754c\u7684\u53d8\u5316\u4ea7\u751f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u60ca\u8bb6\u548c\u8c03\u6574\u3002\u4ed6\u8ba4\u4e3a\uff0c\u60f3\u8981\u771f\u6b63\u53ef\u6269\u5c55\u7684\u667a\u80fd\uff0c\u5fc5\u987b\u4ece\u7ecf\u9a8c\u5b66\u4e60\u51fa\u53d1\uff0c\u800c\u4e0d\u662f\u628a\u5927\u8bed\u8a00\u6a21\u578b\u5f53\u4f5c\u8d77\u70b9\u3002", "AI": {"tldr": "\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u66f4\u591a\u662f\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u6a21\u4eff\uff0c\u5b83\u6ca1\u6709\u72ec\u7acb\u7684\u76ee\u6807\uff0c\u4e5f\u65e0\u6cd5\u5bf9\u5916\u90e8\u4e16\u754c\u7684\u53d8\u5316\u4ea7\u751f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u60ca\u8bb6\u548c\u8c03\u6574\u3002\u4ed6\u8ba4\u4e3a\uff0c\u60f3\u8981\u771f\u6b63\u53ef\u6269\u5c55\u7684\u667a\u80fd\uff0c\u5fc5\u987b\u4ece\u7ecf\u9a8c\u5b66\u4e60\u51fa\u53d1\uff0c\u800c\u4e0d\u662f\u628a\u5927\u8bed\u8a00\u6a21\u578b\u5f53\u4f5c\u8d77\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.d5a9e12b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTM1ODk0Ng==&mid=2247485204&idx=1&sn=029431cd2cf6ad67208120ca2f7b3e1e&chksm=fe11969d83f1f643882a9614505b54e06f7c86a20138e77b6ba5477ac01a4c02a5d62392b871#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTM1ODk0Ng==&mid=2247485204&idx=1&sn=029431cd2cf6ad67208120ca2f7b3e1e&chksm=fe11969d83f1f643882a9614505b54e06f7c86a20138e77b6ba5477ac01a4c02a5d62392b871#rd", "authors": ["\u590f\u6d25\u878d\u5a92\u8bb0\u5f55\u518c"], "title": "\u8d62\u9ebb\u4e86\uff01\u4e2d\u56fd\u5f00\u6e90AI\u4e09\u8fde\u53d1\uff0c\u786c\u521a\u7f8e\u56fd\u95ed\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff01\u7f51\u53cb\uff1a\u4f60\u8d8a\u6253\u538b\u6211\u8d8a\u8981\u8d85\u8fc7\u4f60\uff01", "comment": "Source: WeChat, Published: 2025-09-30 21:30:00", "summary": "\u4e13 \u4e1a\u4eba\u58eb\u90fd\u77e5\u9053\uff0c\u57fa\u7840\u5927\u6a21\u578b\u5c31\u662fAI\u7684\u5e95\u5b50\uff0c\u8fd9\u6b21Qwen3\u6027\u80fd\u4e00\u51fa\u624b\uff0c\u76f4\u63a5\u628a\u5168 \u7403\u5176\u4ed6\u975e\u601d\u8003\u7c7b\u6a21\u578b\u201c\u6309\u5728\u5730\u4e0a\u6469\u64e6\u201d\u3002\u7d27\u63a5\u7740\uff0c\u7b2c \u4e8c\u5929Qwen3-Coder\u53d1\u5e03\uff0c\u8fd9\u4e2a\u66f4\u72e0\uff0c\u4e13\u4e3a\u5199\u4ee3\u7801\u8bbe\u8ba1\u3002", "AI": {"tldr": "\u4e13 \u4e1a\u4eba\u58eb\u90fd\u77e5\u9053\uff0c\u57fa\u7840\u5927\u6a21\u578b\u5c31\u662fAI\u7684\u5e95\u5b50\uff0c\u8fd9\u6b21Qwen3\u6027\u80fd\u4e00\u51fa\u624b\uff0c\u76f4\u63a5\u628a\u5168 \u7403\u5176\u4ed6\u975e\u601d\u8003\u7c7b\u6a21\u578b\u201c\u6309\u5728\u5730\u4e0a\u6469\u64e6\u201d\u3002\u7d27\u63a5\u7740\uff0c\u7b2c \u4e8c\u5929Qwen3-Coder\u53d1\u5e03\uff0c\u8fd9\u4e2a\u66f4\u72e0\uff0c\u4e13\u4e3a\u5199\u4ee3\u7801\u8bbe\u8ba1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
