{"id": "2511.08590", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08590", "abs": "https://arxiv.org/abs/2511.08590", "authors": ["Encheng Xie", "Yihang Sun", "Tao Feng", "Jiaxuan You"], "title": "GMTRouter: Personalized LLM Router over Multi-turn User Interactions", "comment": "Preprint", "summary": "Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.", "AI": {"tldr": "GMTRouter\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f02\u6784\u56fe\u5b66\u4e60\u7684\u4e2a\u6027\u5316LLM\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7528\u6237-LLM\u591a\u8f6e\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5f02\u6784\u56fe\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u6709\u6548\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e2a\u6027\u5316\u8def\u7531\u3002", "motivation": "\u73b0\u6709LLM\u8def\u7531\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u4e2a\u6027\u5316\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u7528\u6237\u4e0eLLM\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\u3002\u7528\u6237\u504f\u597d\u6570\u636e\u901a\u5e38\u7a00\u7f3a\u3001\u5608\u6742\u4e14\u683c\u5f0f\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u4ec5\u4f9d\u8d56\u7528\u6237\u7279\u5b9a\u6570\u636e\u7684\u65b9\u6cd5\u6548\u679c\u3002", "method": "\u5c06\u591a\u8f6e\u7528\u6237-LLM\u4ea4\u4e92\u8868\u793a\u4e3a\u5305\u542b\u7528\u6237\u3001LLM\u3001\u67e5\u8be2\u548c\u54cd\u5e94\u56db\u79cd\u8282\u70b9\u7c7b\u578b\u7684\u5f02\u6784\u56fe\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u5728\u8f7b\u91cf\u7ea7\u5f52\u7eb3\u56fe\u5b66\u4e60\u6846\u67b6\u4e2d\u4ece\u5c11\u91cf\u6570\u636e\u5b66\u4e60\u7528\u6237\u504f\u597d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u63d0\u9ad80.9-21.6%\uff0cAUC\u63d0\u9ad80.006-0.309\u3002\u80fd\u591f\u4ec5\u7528\u5c11\u91cf\u6570\u636e\u9002\u5e94\u65b0\u7528\u6237\u548c\u6f14\u5316\u504f\u597d\uff0c\u65e0\u9700\u5927\u91cf\u5fae\u8c03\u3002", "conclusion": "GMTRouter\u901a\u8fc7\u5f02\u6784\u56fe\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86LLM\u8def\u7531\u4e2d\u7684\u4e2a\u6027\u5316\u6311\u6218\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u6548\u4e2a\u6027\u5316\u8def\u7531\u3002", "topic": "agent analysis"}}
{"id": "2511.08594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08594", "abs": "https://arxiv.org/abs/2511.08594", "authors": ["Stewart Slocum", "Asher Parker-Sartori", "Dylan Hadfield-Menell"], "title": "Diverse Preference Learning for Capabilities and Alignment", "comment": null, "summary": "The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSoft Preference Learning\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026KL\u6563\u5ea6\u6b63\u5219\u5316\u4e2d\u7684\u71b5\u548c\u4ea4\u53c9\u71b5\u9879\uff0c\u89e3\u51b3\u5bf9\u9f50\u7b97\u6cd5\u5bfc\u81f4\u7684LLM\u8f93\u51fa\u591a\u6837\u6027\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\uff08\u5982RLHF\u548cDPO\uff09\u663e\u8457\u964d\u4f4e\u4e86LLM\u8f93\u51fa\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6587\u672c\u7ed3\u6784\u91cd\u590d\u3001\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u5355\u4e00\u3001\u793e\u4f1a\u89c2\u70b9\u8303\u56f4\u53d8\u7a84\u3002", "method": "\u63d0\u51faSoft Preference Learning\u65b9\u6cd5\uff0c\u89e3\u6784KL\u6563\u5ea6\u6b63\u5219\u5316\u4e2d\u7684\u71b5\u548c\u4ea4\u53c9\u71b5\u9879\uff0c\u5b9e\u73b0\u5bf9LLM\u751f\u6210\u591a\u6837\u6027\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u4f7f\u7528Soft Preference Learning\u8bad\u7ec3\u7684LLM\u5728\u56f0\u96be\u91cd\u590d\u91c7\u6837\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u8f93\u51fa\u5177\u6709\u66f4\u5927\u7684\u8bed\u4e49\u548c\u8bcd\u6c47\u591a\u6837\u6027\uff0c\u80fd\u4ee3\u8868\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u89c2\u70b9\uff0c\u5e76\u6539\u5584\u4e86logit\u6821\u51c6\u3002", "conclusion": "Soft Preference Learning\u662f\u5bf9\u6807\u51c6\u6e29\u5ea6\u7f29\u653e\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u80fd\u6709\u6548\u5e73\u8861LLM\u7684\u5bf9\u9f50\u4e0e\u591a\u6837\u6027\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2511.08747", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08747", "abs": "https://arxiv.org/abs/2511.08747", "authors": ["Isaac Joffe", "Chris Eliasmith"], "title": "Vector Symbolic Algebras for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5411\u91cf\u7b26\u53f7\u4ee3\u6570\u7684\u8ba4\u77e5\u5408\u7406ARC-AGI\u6c42\u89e3\u5668\uff0c\u7ed3\u5408\u7cfb\u7edf1\u76f4\u89c9\u548c\u7cfb\u7edf2\u63a8\u7406\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u62bd\u8c61\u63a8\u7406", "motivation": "ARC-AGI\u4f5c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u57fa\u51c6\u5bf9\u4eba\u7c7b\u6765\u8bf4\u5f88\u5bb9\u6613\uff0c\u4f46\u5bf9\u6700\u5148\u8fdbAI\u7cfb\u7edf\u4ecd\u6781\u5177\u6311\u6218\uff0c\u53d7\u795e\u7ecf\u79d1\u5b66\u548c\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u5e0c\u671b\u5f00\u53d1\u8ba4\u77e5\u5408\u7406\u7684\u6c42\u89e3\u65b9\u6cd5", "method": "\u4f7f\u7528\u5411\u91cf\u7b26\u53f7\u4ee3\u6570\u8fdb\u884c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u96c6\u6210\u7cfb\u7edf1\u76f4\u89c9\u548c\u7cfb\u7edf2\u63a8\u7406\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u7a0b\u5e8f\u5408\u6210\u5b9e\u73b0\u62bd\u8c61\u63a8\u7406\uff0c\u5229\u7528VSA\u8868\u793a\u62bd\u8c61\u5bf9\u8c61\u5e76\u6307\u5bfc\u89e3\u51b3\u65b9\u6848\u641c\u7d22", "result": "\u5728ARC-AGI-1-Train\u4e0a\u5f97\u520610.8%\uff0cARC-AGI-1-Eval\u4e0a3.0%\uff1b\u5728\u7b80\u5355\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u79c0\uff1aSort-of-ARC\u5f97\u520694.5%\uff0c1D-ARC\u5f97\u520683.1%\uff08\u4f18\u4e8eGPT-4\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\uff09", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06VSA\u5e94\u7528\u4e8eARC-AGI\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u8ba4\u77e5\u5408\u7406\u7684ARC-AGI\u6c42\u89e3\u5668\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2511.08653", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.08653", "abs": "https://arxiv.org/abs/2511.08653", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang"], "title": "Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion", "comment": null, "summary": "Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku", "AI": {"tldr": "CGAR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67b6\u6784\u6df1\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u6df1\u5ea6\u8bfe\u7a0b\u548c\u5206\u5c42\u76d1\u7763\u52a0\u6743\uff0c\u663e\u8457\u63d0\u5347\u9012\u5f52\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9012\u5f52\u63a8\u7406\u6a21\u578b\u867d\u7136\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff08\u7ea636 GPU\u5c0f\u65f6/\u6570\u636e\u96c6\uff09\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "CGAR\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6e10\u8fdb\u6df1\u5ea6\u8bfe\u7a0b\uff08\u52a8\u6001\u8c03\u6574\u9012\u5f52\u6df1\u5ea6\uff09\u548c\u5206\u5c42\u76d1\u7763\u52a0\u6743\uff08\u5bf9\u76d1\u7763\u6b65\u9aa4\u5e94\u7528\u6307\u6570\u8870\u51cf\u6743\u91cd\uff09\uff0c\u901a\u8fc7\u67b6\u6784\u6df1\u5ea6\u800c\u975e\u6570\u636e\u987a\u5e8f\u7684\u8bfe\u7a0b\u5b66\u4e60\u6765\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728Sudoku-Extreme\u6570\u636e\u96c6\u4e0a\uff0cCGAR\u5b9e\u73b01.71\u500d\u8bad\u7ec3\u52a0\u901f\uff0810.93\u52306.38\u5c0f\u65f6\uff0c\u6210\u672c\u964d\u4f4e42%\uff09\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.63%\uff0886.65%\u523086.02%\uff09\u3002", "conclusion": "\u57fa\u4e8e\u67b6\u6784\u6df1\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\u80fd\u591f\u9ad8\u6548\u8bad\u7ec3\u9012\u5f52\u63a8\u7406\u6a21\u578b\uff0c\u5728\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\u7684\u7f55\u89c1\u6848\u4f8b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08595", "abs": "https://arxiv.org/abs/2511.08595", "authors": ["Joongho Kim", "Xirui Huang", "Zarreen Reza", "Gabriel Grand", "Kevin Zhu", "Ryan Lagasse"], "title": "Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Efficient Reasoning", "summary": "Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.", "AI": {"tldr": "SSDP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u8bed\u4e49\u5408\u5e76\u548c\u52a8\u6001\u526a\u679d\u6765\u51cf\u5c11Tree-of-Thought\u63a8\u7406\u4e2d\u7684\u8bed\u4e49\u5197\u4f59\uff0c\u5b9e\u73b02.3\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u3002", "motivation": "Tree-of-Thought\u63a8\u7406\u867d\u7136\u63d0\u5347\u4e86LLM\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u8bed\u4e49\u5197\u4f59\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9e\u65f6\u8bc6\u522b\u548c\u526a\u679d\u5197\u4f59\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u76f8\u4f3c\u6027\u52a8\u6001\u526a\u679d(SSDP)\u6846\u67b6\uff0c\u5c06\u5728\u7ebf\u8bed\u4e49\u5408\u5e76\u96c6\u6210\u5230\u5e76\u884c\u5316\u6811\u641c\u7d22\u4e2d\uff0c\u5b9e\u65f6\u805a\u7c7b\u548c\u526a\u679d\u5197\u4f59\u6b65\u9aa4\u3002", "result": "\u5728GSM8K\u548cMATH500\u7b49\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSSDP\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6811\u641c\u7d22\u57fa\u7ebf\u5b9e\u73b0\u4e862.3\u500d\u52a0\u901f\uff0c\u63a2\u7d22\u8282\u70b9\u6570\u51cf\u5c1185-90%\uff0c\u51c6\u786e\u7387\u901a\u5e38\u4fdd\u6301\u5728\u6700\u5f3a\u57fa\u7ebf\u76845%\u4ee5\u5185\u3002", "conclusion": "SSDP\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "topic": "agent analysis"}}
{"id": "2511.08749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08749", "abs": "https://arxiv.org/abs/2511.08749", "authors": ["Mehrdad Zakershahrak"], "title": "Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning", "comment": null, "summary": "Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.", "AI": {"tldr": "\u63d0\u51faQDIN\u67b6\u6784\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u53ef\u56de\u7b54\u591a\u79cd\u73af\u5883\u67e5\u8be2\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u800c\u975e\u4ec5\u4e13\u6ce8\u4e8e\u5956\u52b1\u6700\u5927\u5316\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u51c6\u786e\u6027\u4e0e\u63a7\u5236\u6027\u80fd\u5b58\u5728\u89e3\u8026\u73b0\u8c61\u3002", "motivation": "\u6311\u6218\u4f20\u7edfRL\u4ec5\u5173\u6ce8\u5956\u52b1\u6700\u5927\u5316\u7684\u8303\u5f0f\uff0c\u63a2\u7d22\u5c06RL\u7cfb\u7edf\u6784\u5efa\u4e3a\u53ef\u56de\u7b54\u73af\u5883\u591a\u6837\u6027\u67e5\u8be2\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u4ee5\u66b4\u9732\u667a\u80fd\u4f53\u9690\u542b\u7684\u73af\u5883\u77e5\u8bc6\u3002", "method": "\u5f15\u5165\u67e5\u8be2\u6761\u4ef6\u786e\u5b9a\u6027\u63a8\u7406\u7f51\u7edc(QDIN)\uff0c\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u67e5\u8be2\uff08\u7b56\u7565\u3001\u53ef\u8fbe\u6027\u3001\u8def\u5f84\u3001\u6bd4\u8f83\uff09\u8bbe\u8ba1\u4e13\u95e8\u7684\u795e\u7ecf\u6a21\u5757\uff0c\u6bcf\u79cd\u63a8\u7406\u6a21\u5f0f\u90fd\u6709\u4f18\u5316\u67b6\u6784\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u51c6\u786e\u6027\u4e0e\u63a7\u5236\u6027\u80fd\u5b58\u5728\u6839\u672c\u6027\u89e3\u8026\uff1a\u63a8\u7406\u51c6\u786e\u7387\u53ef\u8fbe\u8fd1\u5b8c\u7f8e\u6c34\u5e73\uff0899%\u53ef\u8fbe\u6027IoU\uff09\uff0c\u800c\u63a7\u5236\u6027\u80fd\u4ecd\u5904\u4e8e\u6b21\u4f18\u72b6\u6001\uff0831%\u56de\u62a5\uff09\u3002\u4e13\u7528\u67b6\u6784\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u7edf\u4e00\u6a21\u578b\u548c\u540e\u9a8c\u63d0\u53d6\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u8bbe\u8ba1\u4e4b\u521d\u5c31\u4f5c\u4e3a\u53ef\u67e5\u8be2\u77e5\u8bc6\u5e93\u7684RL\u7cfb\u7edf\u5efa\u7acb\u4e86\u7814\u7a76\u8bae\u7a0b\uff0c\u5bf9\u53ef\u89e3\u91ca\u6027\u3001\u9a8c\u8bc1\u548c\u4eba\u673a\u534f\u4f5c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08596", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.08596", "abs": "https://arxiv.org/abs/2511.08596", "authors": ["Arka Dutta", "Sujan Dutta", "Rijul Magu", "Soumyajit Datta", "Munmun De Choudhury", "Ashiqur R. KhudaBukhsh"], "title": "What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge", "comment": null, "summary": "Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \\texttt{Claude} exhibits strong resilience, \\texttt{GPT} and \\texttt{Grok} demonstrate moderate resilience, while \\texttt{Gemini} and \\texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u6d4b\u8bd5LLMs\u5728\u5bf9\u6297\u6027\u63d0\u793a\u4e0b\u7684\u771f\u5b9e\u6027\u4fdd\u771f\u5ea6\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bf9\u81ea\u6211\u751f\u6210\u7684\u8c0e\u8a00\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5e7b\u89c9\u95ee\u9898\u662fLLMs\u5728\u5173\u952e\u9886\u57df\u90e8\u7f72\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u6d4b\u8bd5\u6a21\u578b\u5728\u5bf9\u6297\u6027\u63d0\u793a\u4e0b\u7684\u771f\u5b9e\u6027\u4fdd\u771f\u5ea6\u3002", "method": "\u4e09\u6b65\u9aa4\u6846\u67b6\uff1a1) \u8ba9LLM\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u771f\u5047\u9648\u8ff0\uff1b2) \u8ba9LLM\u9a8c\u8bc1\u8fd9\u4e9b\u9648\u8ff0\uff1b3) \u6d4b\u8bd5LLM\u5bf9\u81ea\u8eab\u751f\u6210\u8c0e\u8a00\u7684\u62b5\u6297\u80fd\u529b\u3002", "result": "\u5728\u7535\u5f71\u548c\u5c0f\u8bf4\u4e24\u4e2a\u5c01\u95ed\u9886\u57df\u6d4b\u8bd55\u4e2a\u4e3b\u6d41LLMs\uff0cClaude\u8868\u73b0\u6700\u5f3a\u97e7\u6027\uff0cGPT\u548cGrok\u4e2d\u7b49\uff0cGemini\u548cDeepSeek\u6700\u5f31\u3002", "conclusion": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u4eba\u4f7f\u7528LLMs\u83b7\u53d6\u4fe1\u606f\uff0c\u8fd9\u4e9b\u53d1\u73b0\u6572\u54cd\u4e86\u8b66\u949f\uff0c\u6a21\u578b\u5bf9\u81ea\u8eab\u751f\u6210\u8c0e\u8a00\u7684\u8106\u5f31\u6027\u503c\u5f97\u5173\u6ce8\u3002", "topic": "agent analysis"}}
{"id": "2511.09122", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09122", "abs": "https://arxiv.org/abs/2511.09122", "authors": ["Joschka Kersting", "Michael Rummel", "Gesa Benndorf"], "title": "Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation", "comment": null, "summary": "Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9762\u5411\u5de5\u4e1aPLC\u7f16\u7a0b\u7684\u4f4e\u6570\u636e\u9886\u57df\u4ee3\u7801\u52a9\u624b\uff0c\u901a\u8fc7RAG\u3001\u591a\u6a21\u578b\u7ade\u4e89\u548c\u5373\u65f6\u7f16\u8bd1\u9a8c\u8bc1\u7b49\u6280\u672f\uff0c\u5728\u4e0d\u5fae\u8c03\u5927\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684IEC 61131-3\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u5de5\u4e1aPLC\u4f7f\u7528\u4e13\u6709\u4ee3\u7801\u65b9\u8a00\uff0c\u96be\u4ee5\u8bad\u7ec3\u4ee3\u7801\u52a9\u624b\uff1b\u73b0\u6709LLM\u4e0d\u4e86\u89e3\u7279\u5b9a\u529f\u80fd\u5757\u548c\u9879\u76ee\u4ee3\u7801\uff0c\u4e14\u4f01\u4e1a\u4e0d\u4fe1\u4efb\u4e91\u670d\u52a1\u5546\uff0c\u9700\u8981\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u3001\u591a\u6a21\u578b\u7ade\u4e89\u673a\u5236\u3001\u81ea\u52a8\u9519\u8bef\u4fee\u6b63\u548c\u5373\u65f6\u7f16\u8bd1\u9a8c\u8bc1\uff0c\u901a\u8fc7\u7cbe\u7ec6\u63d0\u793a\u5de5\u7a0b\u548c\u5b9a\u5411\u68c0\u7d22\u652f\u6301\u4f4e\u6570\u636e\u9886\u57df\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u751f\u6210\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u4f7f\u7528\u7684\u5c0f\u6a21\u578b\u5fae\u8c03\uff0c\u63d0\u4f9b\u4e86\u4ee3\u7801\u7f16\u8bd1\u7edf\u8ba1\u548c\u7528\u6237\u8bc4\u5206\u7b49\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "RAG\u652f\u6301\u7684\u4ee3\u7801\u52a9\u624b\u53ef\u4ee5\u5728\u4f4e\u6570\u636e\u9886\u57df\u6709\u6548\u5de5\u4f5c\uff0c\u901a\u8fc7\u6269\u5c55\u63d0\u793a\u5de5\u7a0b\u548c\u5b9a\u5411\u68c0\u7d22\u6765\u5f25\u8865\u6570\u636e\u4e0d\u8db3\u3002", "topic": "code agent"}}
{"id": "2511.09212", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09212", "abs": "https://arxiv.org/abs/2511.09212", "authors": ["Zeru Cheng", "Yanjing Yang", "He Zhang", "Lanxin Yang", "Jinghao Hu", "Jinwei Xu", "Bohan Liu", "Haifeng Shen"], "title": "Leveraging Self-Paced Learning for Software Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.", "AI": {"tldr": "\u63d0\u51faSPLVD\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6b65\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u6a21\u62df\u4eba\u7c7b\u4ece\u6613\u5230\u96be\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4f4e\u3002\u9700\u8981\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u7b56\u7565", "method": "SPLVD\u65b9\u6cd5\u5305\u542b\u4e13\u95e8\u8bbe\u8ba1\u7684\u6570\u636e\u9009\u62e9\u5668\uff0c\u6839\u636e\u8bad\u7ec3\u9636\u6bb5\u52a8\u6001\u9009\u62e9\u6e90\u4ee3\u7801\uff0c\u4f18\u5148\u5b66\u4e60\u7b80\u5355\u4ee3\u7801\uff0c\u5e76\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5468\u671f\u91cd\u65b0\u8ba1\u7b97\u4ee3\u7801\u96be\u5ea6\u5e76\u66f4\u65b0\u9009\u62e9\u5668", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523089.2%\u300168.7%\u548c43.5%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728OpenHarmony\u9879\u76ee\u4e2d\u8fbe\u523090.9%\u7684\u6700\u9ad8\u7cbe\u5ea6", "conclusion": "SPLVD\u901a\u8fc7\u81ea\u6b65\u5b66\u4e60\u7b56\u7565\u6709\u6548\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u8868\u73b0\u51fa\u8272", "topic": "swe application"}}
{"id": "2511.08873", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08873", "abs": "https://arxiv.org/abs/2511.08873", "authors": ["Shouang Wei", "Min Zhang", "Xin Lin", "Bo Jiang", "Kun Kuang", "Zhongxiang Dai"], "title": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models", "comment": null, "summary": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5355\u5411\u8ba4\u77e5\u4f18\u5316(UCO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3LLM\u4f5c\u4e3a\u667a\u80fd\u5bfc\u5e08\u65f6\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u533a\u5206\u5b66\u751f\u771f\u5b9e\u7406\u89e3\u4e0e\u7b54\u6848\u56de\u54cd\uff0c\u4ee5\u53ca\u52a8\u6001\u611f\u77e5\u5b66\u751f\u8ba4\u77e5\u72b6\u6001\u3002", "motivation": "\u5f53\u524dLLM\u5728\u6559\u80b2\u573a\u666f\u4e2d\u4ece\u7b54\u6848\u63d0\u4f9b\u8005\u8f6c\u5411\u667a\u80fd\u5bfc\u5e08\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u65e0\u6cd5\u533a\u5206\u5b66\u751f\u771f\u5b9e\u7406\u89e3\u4e0e\u7b54\u6848\u56de\u54cd\uff0c\u4e5f\u65e0\u6cd5\u611f\u77e5\u5b66\u751f\u8ba4\u77e5\u72b6\u6001\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u5956\u52b1\u51fd\u6570\uff1a\u8fdb\u5c55\u5956\u52b1\u6355\u6349\u5b66\u751f\u8ba4\u77e5\u8fdb\u6b65\uff0c\u652f\u67b6\u5956\u52b1\u52a8\u6001\u8bc6\u522b\u5b66\u751f\u7684\u6700\u8fd1\u53d1\u5c55\u533a(ZPD)\u3002", "result": "\u5728BigMath\u548cMathTutorBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCO\u6a21\u578b\u4f18\u4e8e\u6240\u6709\u540c\u7b49\u89c4\u6a21\u6a21\u578b\uff0c\u6027\u80fd\u53ef\u4e0e\u5148\u8fdb\u95ed\u6e90\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "UCO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u4f5c\u4e3a\u667a\u80fd\u5bfc\u5e08\u65f6\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u9002\u5e94\u5b66\u751f\u8ba4\u77e5\u6c34\u5e73\u7684\u6559\u5b66\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08598", "abs": "https://arxiv.org/abs/2511.08598", "authors": ["Yanhong Li", "Tianyang Xu", "Kenan Tang", "Karen Livescu", "David McAllester", "Jiawei Zhou"], "title": "OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking", "comment": null, "summary": "Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.", "AI": {"tldr": "OKBench\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u6309\u9700\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u52a8\u6001\u7684\u77e5\u8bc6\u57fa\u51c6\uff0c\u7279\u522b\u5173\u6ce8\u65b0\u95fb\u9886\u57df\uff0c\u901a\u8fc7\u4ee3\u7406\u6846\u67b6\u81ea\u52a8\u5316\u57fa\u51c6\u7684\u6765\u6e90\u83b7\u53d6\u3001\u521b\u5efa\u3001\u9a8c\u8bc1\u548c\u5206\u53d1\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u4e16\u754c\u4e2d\u7684\u6f14\u5316\u77e5\u8bc6\uff0c\u96c6\u4e2d\u5316\u7b56\u5212\u96be\u4ee5\u8ddf\u4e0aLLM\u7684\u5feb\u901f\u53d1\u5c55\u3002\u9700\u8981\u80fd\u591f\u8bc4\u4f30LLM\u5728\u65b0\u77e5\u8bc6\u4e0a\u8868\u73b0\u7684\u52a8\u6001\u57fa\u51c6\u3002", "method": "\u63d0\u51faOKBench\u6846\u67b6\uff0c\u4f7f\u7528\u4ee3\u7406\u81ea\u52a8\u5316\u57fa\u51c6\u751f\u6210\u6d41\u7a0b\uff0c\u5305\u62ec\u6765\u6e90\u83b7\u53d6\u3001\u521b\u5efa\u3001\u9a8c\u8bc1\u548c\u5206\u53d1\uff0c\u91cd\u70b9\u5173\u6ce8\u65b0\u95fb\u9886\u57df\u4ee5\u51cf\u5c11\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u7684\u91cd\u53e0\u3002", "result": "\u8bc4\u4f30\u4e86\u5404\u79cd\u5f00\u6e90\u548c\u4e13\u6709LLM\uff0c\u53d1\u73b0\u5728\u65b0\u4fe1\u606f\u9762\u524d\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u540c\u884c\u4e3a\uff0c\u68c0\u7d22\u7f29\u5c0f\u4e86\u5927\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u5728\u6f14\u5316\u77e5\u8bc6\u57fa\u51c6\u4e0a\u8bc4\u4f30LLM\u975e\u5e38\u91cd\u8981\uff0cOKBench\u80fd\u591f\u4fc3\u8fdb\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2511.09223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09223", "abs": "https://arxiv.org/abs/2511.09223", "authors": ["Panya Trakoolgerntong", "Tao Xiao", "Masanari Kondo", "Chaiyong Ragkhitwetsagul", "Morakot Choetkiertikul", "Pattaraporn Sangaroonsilp", "Yasutaka Kamei"], "title": "AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews", "comment": null, "summary": "Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.\n  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.", "AI": {"tldr": "AILINKPREVIEWER\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210PR\u4e2d\u94fe\u63a5\u9884\u89c8\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790PR\u5143\u6570\u636e\u6765\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e2e\u52a9\u63d0\u5347\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\u3002", "motivation": "\u4ee3\u7801\u5ba1\u67e5\u4e2d\u94fe\u63a5\u901a\u5e38\u88ab\u4e22\u5f03\uff0c\u9650\u5236\u4e86\u4fe1\u606f\u7684\u4e30\u5bcc\u6027\u5e76\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u9700\u8981\u5de5\u5177\u6765\u63d0\u4f9b\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u9884\u89c8\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u94fe\u63a5\u9884\u89c8\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u4e0a\u4e0b\u6587LLM\u6458\u8981\u3001\u975e\u4e0a\u4e0b\u6587LLM\u6458\u8981\u548c\u57fa\u4e8e\u5143\u6570\u636e\u7684\u9884\u89c8\uff0c\u5e76\u572850\u4e2aGitHub\u4ed3\u5e93\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e0a\u4e0b\u6587\u6458\u8981\u5728BLEU\u3001BERTScore\u548c\u538b\u7f29\u6bd4\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u7528\u6237\u7814\u7a76\u663e\u793a\u5927\u591a\u6570\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u975e\u4e0a\u4e0b\u6587\u6458\u8981\uff0c\u8868\u660e\u5b58\u5728\u6307\u6807\u6027\u80fd\u4e0e\u611f\u77e5\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u94fe\u63a5\u9884\u89c8\u6709\u6f5c\u529b\u63d0\u5347\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u3002", "topic": "swe application"}}
{"id": "2511.09231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09231", "abs": "https://arxiv.org/abs/2511.09231", "authors": ["Tobias Eisenreich", "Nicholas Friedlaender", "Stefan Wagner"], "title": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements", "comment": "Accepted at the Intelligent Software Engineering Workshop (ISE 2025) at ASE 2025", "summary": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7528\u4f8b\u5efa\u6a21\uff0c\u901a\u8fc7\u96c6\u6210\u5f00\u6e90LLM\u548c\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4ece\u8f6f\u4ef6\u9700\u6c42\u4e2d\u63d0\u53d6\u53c2\u4e0e\u8005\u548c\u7528\u4f8b\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u53ef\u51cf\u5c1160%\u5efa\u6a21\u65f6\u95f4\u4e14\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u624b\u52a8\u521b\u5efa\u7528\u4f8b\u6a21\u578b\u8017\u65f6\u8d39\u529b\uff0c\u5728\u5b9e\u8df5\u4e2d\u7ecf\u5e38\u88ab\u8df3\u8fc7\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u96c6\u6210\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8fd0\u7528\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4ece\u8f6f\u4ef6\u9700\u6c42\u4e2d\u7cfb\u7edf\u63d0\u53d6\u53c2\u4e0e\u8005\u548c\u7528\u4f8b\u3002", "result": "\u4e0e\u624b\u52a8\u5efa\u6a21\u76f8\u6bd4\uff0cLLM\u65b9\u6cd5\u51cf\u5c1160%\u5efa\u6a21\u65f6\u95f4\uff0c\u540c\u65f6\u6a21\u578b\u8d28\u91cf\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u53c2\u4e0e\u8005\u8ba4\u4e3a\u8be5\u65b9\u6cd5\u5728\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002", "conclusion": "LLM\u8f85\u52a9\u7684\u7528\u4f8b\u5efa\u6a21\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u5efa\u6a21\u6548\u7387\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u81ea\u52a8\u5316\u652f\u6301\u3002", "topic": "swe application"}}
{"id": "2511.08721", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08721", "abs": "https://arxiv.org/abs/2511.08721", "authors": ["Andreas Einwiller", "Kanishka Ghosh Dastidar", "Artur Romazanov", "Annette Hautli-Janisz", "Michael Granitzer", "Florian Lemmerich"], "title": "Benevolent Dictators? On LLM Agent Behavior in Dictator Games", "comment": "7 pages, 2 figures, v1 init", "summary": "In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-ABS\u6846\u67b6\u6765\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u72ec\u88c1\u8005\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u7cfb\u7edf\u63d0\u793a\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u667a\u80fd\u4f53\u901a\u5e38\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u516c\u5e73\u504f\u597d\u3002", "motivation": "\u8d28\u7591\u73b0\u6709\u7814\u7a76\u4e2dLLM\u667a\u80fd\u4f53\u884c\u4e3a\u7ed3\u679c\u7684\u7a33\u5065\u6027\uff0c\u7279\u522b\u662f\u5ffd\u89c6\u7cfb\u7edf\u63d0\u793a\u7684\u4f5c\u7528\u548c\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\u5bf9\u7ed3\u679c\u7684\u654f\u611f\u6027\u3002", "method": "\u63d0\u51faLLM-ABS\u6846\u67b6\uff0c\u901a\u8fc7(i)\u63a2\u7d22\u4e0d\u540c\u7cfb\u7edf\u63d0\u793a\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c(ii)\u4f7f\u7528\u4e2d\u6027\u63d0\u793a\u53d8\u4f53\u83b7\u5f97\u66f4\u53ef\u9760\u7684\u667a\u80fd\u4f53\u504f\u597d\u6d1e\u5bdf\uff0c(iii)\u5206\u6790LLM\u667a\u80fd\u4f53\u5bf9\u5f00\u653e\u5f0f\u6307\u4ee4\u54cd\u5e94\u7684\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u901a\u5e38\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u516c\u5e73\u504f\u597d\uff0c\u7cfb\u7edf\u63d0\u793a\u5bf9\u5176\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4ece\u8bed\u8a00\u89d2\u5ea6\u53d1\u73b0\u6a21\u578b\u4ee5\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u54cd\u5e94\u3002", "conclusion": "\u5c3d\u7ba1\u63d0\u793a\u654f\u611f\u6027\u4ecd\u662f\u6301\u7eed\u6311\u6218\uff0c\u4f46\u63d0\u51fa\u7684\u6846\u67b6\u4e3aLLM\u667a\u80fd\u4f53\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5065\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.09268", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09268", "abs": "https://arxiv.org/abs/2511.09268", "authors": ["Helio Victor F. Santos", "Vitor Costa", "Joao Eduardo Montandon", "Marco Tulio Valente"], "title": "Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects", "comment": null, "summary": "Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.", "AI": {"tldr": "\u5bf9Claude Code\u914d\u7f6e\u6587\u4ef6\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86328\u4e2a\u516c\u5f00\u9879\u76ee\u4e2d\u7684\u914d\u7f6e\u7ed3\u6784\u3001\u5185\u5bb9\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9", "motivation": "\u4e86\u89e3AI\u4ee3\u7801\u52a9\u624b\u914d\u7f6e\u751f\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u548c\u5185\u5bb9\uff0c\u8fd9\u4e9b\u914d\u7f6e\u5b9a\u4e49\u4e86\u67b6\u6784\u7ea6\u675f\u3001\u7f16\u7801\u5b9e\u8df5\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11", "method": "\u6536\u96c6\u5e76\u5206\u6790328\u4e2aClaude Code\u516c\u5f00\u9879\u76ee\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u8bc6\u522b\u6307\u5b9a\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9\u548c\u5b9e\u8df5\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5173\u6ce8\u70b9\u5728\u5355\u4e2a\u6587\u4ef6\u4e2d\u7684\u5171\u73b0\u6a21\u5f0f", "result": "\u914d\u7f6e\u6587\u4ef6\u9700\u8981\u5b9a\u4e49\u5e7f\u6cdb\u7684\u5173\u6ce8\u70b9\u548c\u5b9e\u8df5\uff0c\u7279\u522b\u5f3a\u8c03\u6307\u5b9a\u4ee3\u7406\u5e94\u9075\u5faa\u7684\u67b6\u6784", "conclusion": "\u914d\u7f6e\u6587\u4ef6\u5728AI\u4ee3\u7801\u52a9\u624b\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u9700\u8981\u660e\u786e\u5b9a\u4e49\u5404\u79cd\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9\uff0c\u5c24\u5176\u662f\u67b6\u6784\u89c4\u8303", "topic": "agent analysis"}}
{"id": "2511.09373", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09373", "abs": "https://arxiv.org/abs/2511.09373", "authors": ["Adam \u0160torek", "Vikas Upadhyay", "Marianne Menglin Liu", "Daniel W. Peterson", "Anshul Mittal", "Sujeeth Bharadwaj", "Fahad Shah", "Dan Roth"], "title": "Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks", "comment": null, "summary": "LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.", "AI": {"tldr": "Routesplain\u662f\u9996\u4e2a\u9488\u5bf9\u8f6f\u4ef6\u76f8\u5173\u4efb\u52a1\u7684LLM\u8def\u7531\u5668\uff0c\u901a\u8fc7\u63d0\u53d6\u53ef\u89e3\u91ca\u6982\u5ff5\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u5e76\u8fbe\u5230\u6216\u8d85\u8d8a\u6240\u6709\u9ed1\u76d2\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f46\u4e4b\u524d\u7684\u8def\u7531\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u901a\u7528LLM\u8def\u7531\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u8def\u7531\u65b9\u6848\u6765\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "Routesplain\u4ece\u67e5\u8be2\u4e2d\u63d0\u53d6\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\uff08\u5982\u4efb\u52a1\u7c7b\u578b\u3001\u9886\u57df\u3001\u63a8\u7406\u590d\u6742\u5ea6\uff09\uff0c\u4ec5\u57fa\u4e8e\u8fd9\u4e9b\u6982\u5ff5\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u63d0\u4f9b\u53ef\u7406\u89e3\u4e14\u53ef\u4fe1\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u57288\u4e2a\u8f6f\u4ef6\u76f8\u5173\u4efb\u52a1\u548c16\u4e2a\u6700\u5148\u8fdbLLM\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRoutesplain\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u5e76\u7b49\u4e8e\u6216\u8d85\u8d8a\u6240\u6709\u9ed1\u76d2\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Routesplain\u8bc1\u660e\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u8def\u7531\u5728\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6982\u5ff5\u7ea7\u5e72\u9884\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u8def\u7531\u5668\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2511.09005", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09005", "abs": "https://arxiv.org/abs/2511.09005", "authors": ["Alvin Chauhan"], "title": "AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines", "comment": "9 pages, 3 figures. Code and data available at https://github.com/alvco/Founding_Fathers_AI", "summary": "Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u589e\u91cf\u641c\u7d22(GIS)\u6765\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u9012\u5f52\u7cbe\u70bc\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u590d\u6742\u7ba1\u9053\u6a21\u578b\u5728\u653f\u6cbb\u8bae\u9898\u5206\u6790\u4e2d\u4f18\u4e8e\u7b80\u5355\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6d41\u7545\u6027\uff0c\u4f46\u7814\u7a76\u8005\u4ecd\u5728\u52aa\u529b\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u3002\u8bba\u6587\u57fa\u4e8e\u641c\u7d22\u5bfc\u5411\u7684LLM\u8ba1\u7b97\u89e3\u91ca\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u548c\u4f18\u5316LLM\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u9012\u5f52\u7cbe\u70bc(RR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u3001\u5bf9\u6297\u6027\u538b\u529b\u6d4b\u8bd5\u548c\u6574\u5408\u5173\u952e\u53cd\u9988\u7684\u8fed\u4ee3\u8fc7\u7a0b\u5b9e\u73b0GIS\u641c\u7d22\u3002\u8bbe\u8ba1\u4e86\u7b80\u5355\u7ebf\u6027\u7ba1\u9053\u4e0e\u590d\u6742\u7ed3\u6784\u5316\u7ba1\u9053\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4f7f\u7528\u57fa\u4e8eRAG\u7684\u7f8e\u56fd\u5f00\u56fd\u5143\u52cb\u4eba\u7269\u6a21\u578b\u5206\u6790\u5f53\u4ee3\u653f\u6cbb\u8bae\u9898\u3002", "result": "\u590d\u6742\u6a21\u578b\u5728\u6240\u6709\u4e5d\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5747\u4f18\u4e8e\u7b80\u5355\u6a21\u578b\uff0c\u5e73\u5747\u4ef2\u88c1\u5206\u6570\u4e3a88.3 vs 71.7\u3002\u590d\u6742\u6a21\u578b\u7684\u8bba\u8bc1\u5728\u5206\u6790\u6df1\u5ea6\u3001\u7ed3\u6784\u7ec6\u5fae\u5dee\u522b\u548c\u7b56\u7565\u6846\u67b6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u9012\u5f52\u7cbe\u70bc\u662f\u901a\u8fc7GIS\u641c\u7d22\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5927\u67b6\u6784\u7279\u5f81\uff0c\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u8f93\u51fa\u3002", "topic": "agent analysis"}}
{"id": "2511.09030", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09030", "abs": "https://arxiv.org/abs/2511.09030", "authors": ["Elliot Meyerson", "Giuseppe Paolo", "Roberto Dailey", "Hormoz Shahrzad", "Olivier Francon", "Conor F. Hayes", "Xin Qiu", "Babak Hodjat", "Risto Miikkulainen"], "title": "Solving a Million-Step LLM Task with Zero Errors", "comment": "Main paper: 14 pages, 29 pages with references and appendix", "summary": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.", "AI": {"tldr": "MAKER\u7cfb\u7edf\u9996\u6b21\u6210\u529f\u5b9e\u73b0\u4e86\u8d85\u8fc7100\u4e07\u6b65LLM\u4efb\u52a1\u4e14\u96f6\u9519\u8bef\uff0c\u901a\u8fc7\u6781\u7aef\u4efb\u52a1\u5206\u89e3\u548c\u5fae\u4ee3\u7406\u673a\u5236\u89e3\u51b3\u4e86LLM\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u56e0\u6301\u7eed\u9519\u8bef\u7387\u800c\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u7a81\u7834\u73b0\u6709LLM\u5728\u4f9d\u8d56\u903b\u8f91\u6b65\u9aa4\u8f83\u591a\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u6781\u7aef\u4efb\u52a1\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u7531\u4e13\u6ce8\u5fae\u4ee3\u7406\u5904\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u591a\u4ee3\u7406\u6295\u7968\u673a\u5236\u8fdb\u884c\u9519\u8bef\u6821\u6b63\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u8d85\u8fc7100\u4e07\u6b65LLM\u4efb\u52a1\u96f6\u9519\u8bef\u6267\u884c\uff0c\u7406\u8bba\u4e0a\u53ef\u6269\u5c55\u5230\u66f4\u9ad8\u6c34\u5e73\u3002", "conclusion": "\u5927\u89c4\u6a21\u5206\u89e3\u4ee3\u7406\u8fc7\u7a0b\uff08MDAPs\uff09\u53ef\u80fd\u6bd4\u6301\u7eed\u6539\u8fdb\u73b0\u6709LLM\u66f4\u6709\u6548\u5730\u89e3\u51b3\u7ec4\u7ec7\u548c\u793e\u4f1a\u5c42\u9762\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2511.08798", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08798", "abs": "https://arxiv.org/abs/2511.08798", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "Structured Uncertainty guided Clarification for LLM Agents", "comment": null, "summary": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "AI": {"tldr": "\u63d0\u51faSAGE-Agent\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u5de5\u5177\u8c03\u7528\u53c2\u6570\uff0c\u5728\u6a21\u7cca\u7528\u6237\u6307\u4ee4\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u8c03\u7528\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8986\u76d6\u7387\u548c\u51cf\u5c11\u6f84\u6e05\u95ee\u9898", "motivation": "LLM\u4ee3\u7406\u5728\u5de5\u5177\u8c03\u7528\u65f6\u9762\u4e34\u6a21\u7cca\u7528\u6237\u6307\u4ee4\u5bfc\u81f4\u9519\u8bef\u8c03\u7528\u548c\u4efb\u52a1\u5931\u8d25\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u6765\u4f18\u5316\u6f84\u6e05\u8fc7\u7a0b", "method": "\u5c06\u5de5\u5177\u53c2\u6570\u7684\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e3aPOMDP\uff0c\u4f7f\u7528EVPI\u76ee\u6807\u8fdb\u884c\u6700\u4f18\u95ee\u9898\u9009\u62e9\uff0c\u91c7\u7528\u57fa\u4e8e\u65b9\u9762\u7684\u6210\u672c\u5efa\u6a21\u9632\u6b62\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743GRPO\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u6a21\u7cca\u4efb\u52a1\u4e0a\u8986\u76d6\u7387\u63d0\u53477-39%\uff0c\u6f84\u6e05\u95ee\u9898\u51cf\u5c111.5-2.7\u500d\uff1bWhen2Call\u51c6\u786e\u7387\u4ece36.5%\u63d0\u5347\u81f365.2%\uff083B\u6a21\u578b\uff09\u548c36.7%\u63d0\u5347\u81f362.9%\uff087B\u6a21\u578b\uff09", "conclusion": "\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u4e3a\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9ad8\u6548\u65b9\u6cd5\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u540c\u65f6\u6539\u5584\u4efb\u52a1\u6210\u529f\u7387\u548c\u4ea4\u4e92\u6548\u7387", "topic": "agent analysis"}}
{"id": "2511.09044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09044", "abs": "https://arxiv.org/abs/2511.09044", "authors": ["Yousef Emami", "Radha Reddy", "Azadeh Pourkabirian", "Miguel Gutierrez Gaitan"], "title": "Advancing Autonomous Emergency Response Systems: A Generative AI Perspective", "comment": "8 pages, 3 figures, 2 tables", "summary": "Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e0b\u4e00\u4ee3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u7d27\u6025\u670d\u52a1\u4e2d\u7684\u4f18\u5316\u7b56\u7565\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4ece\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5411\u6269\u6563\u6a21\u578b\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u548cLLM\u8f85\u52a9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u8f6c\u53d8\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u7d27\u6025\u54cd\u5e94\u4e2d\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u52a8\u6001\u573a\u666f\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684AI\u65b9\u6cd5\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u7d27\u6025\u670d\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u3001\u6269\u6563\u6a21\u578b\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\uff09\u548cLLM\u8f85\u52a9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5feb\u901f\u9002\u5e94\uff09\u3002", "result": "\u6269\u6563\u6a21\u578b\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0cLLM\u8f85\u52a9\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u4ece\u751f\u6210\u5f0fAI\u89c6\u89d2\u4e3a\u7406\u89e3\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u7d27\u6025\u54cd\u5e94\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08832", "abs": "https://arxiv.org/abs/2511.08832", "authors": ["Nikunj Gupta", "Ludwika Twardecka", "James Zachary Hare", "Jesse Milzman", "Rajgopal Kannan", "Viktor Prasanna"], "title": "TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations", "comment": null, "summary": "In this paper, we propose capturing and utilizing \\textit{Temporal Information through Graph-based Embeddings and Representations} or \\textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIGER\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u5d4c\u5165\u548c\u8868\u793a\u6355\u83b7\u65f6\u95f4\u4fe1\u606f\u6765\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u5f0f\u5efa\u6a21\u667a\u80fd\u4f53\u95f4\u534f\u8c03\u7ed3\u6784\u968f\u65f6\u95f4\u6f14\u5316\u7684\u8fc7\u7a0b\u3002", "motivation": "\u5927\u591a\u6570MARL\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6216\u6bcf\u6b65\u5173\u7cfb\u56fe\uff0c\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u5728\u9002\u5e94\u3001\u79fb\u52a8\u6216\u91cd\u7ec4\u5408\u4f5c\u7b56\u7565\u65f6\u81ea\u7136\u4ea7\u751f\u7684\u65f6\u95f4\u6f14\u5316\u4ea4\u4e92\u3002\u6355\u83b7\u8fd9\u79cd\u6f14\u5316\u4f9d\u8d56\u5173\u7cfb\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u534f\u8c03\u81f3\u5173\u91cd\u8981\u3002", "method": "TIGER\u6784\u5efaMARL\u667a\u80fd\u4f53\u7684\u52a8\u6001\u65f6\u95f4\u56fe\uff0c\u8fde\u63a5\u5f53\u524d\u548c\u5386\u53f2\u4ea4\u4e92\uff0c\u4f7f\u7528\u65f6\u5e8f\u6ce8\u610f\u529b\u7f16\u7801\u5668\u805a\u5408\u7ed3\u6784\u548c\u65f6\u95f4\u90bb\u57df\u4fe1\u606f\uff0c\u751f\u6210\u65f6\u95f4\u611f\u77e5\u7684\u667a\u80fd\u4f53\u5d4c\u5165\u6765\u6307\u5bfc\u5408\u4f5c\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u534f\u8c03\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTIGER\u5728\u4efb\u52a1\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u591a\u79cd\u4ef7\u503c\u5206\u89e3\u548c\u57fa\u4e8e\u56fe\u7684MARL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u7ed3\u6784\u548c\u65f6\u95f4\u56e0\u7d20\u5982\u4f55\u5171\u540c\u5851\u9020MARL\u4e2d\u6709\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08835", "abs": "https://arxiv.org/abs/2511.08835", "authors": ["Yejin Yoon", "Yuri Son", "Namyoung So", "Minseo Kim", "Minsoo Cho", "Chanhee Park", "Seungshin Lee", "Taeuk Kim"], "title": "Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents", "comment": "accepted to EMNLP2025", "summary": "Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\\% joint mode-intent accuracy and a 70.1\\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86TACT\u6570\u636e\u96c6\u7528\u4e8e\u7edf\u4e00\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u548c\u95f2\u804a\uff0c\u901a\u8fc7\u7ed3\u6784\u591a\u6837\u7684\u6a21\u5f0f\u6d41\u652f\u6301\u7528\u6237\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u6a21\u5f0f\u5207\u6362\uff0c\u7ed3\u5408DPO\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u8f6c\u6362\u5904\u7406\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u7cfb\u7edf\u901a\u5e38\u5206\u522b\u5904\u7406\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u548c\u95f2\u804a\uff0c\u4f46\u771f\u5b9e\u5bf9\u8bdd\u9700\u8981\u5728\u4e24\u79cd\u6a21\u5f0f\u95f4\u81ea\u7136\u5207\u6362\uff0c\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u5efa\u6a21\u8fd9\u79cd\u52a8\u6001\u8f6c\u6362\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efaTACT\u6570\u636e\u96c6\u652f\u6301\u6a21\u5f0f\u8f6c\u6362\uff0c\u63d0\u51faSwitch\u548cRecovery\u8bc4\u4f30\u6307\u6807\uff0c\u4f7f\u7528DPO\u5bf9TACT\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "TACT\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u610f\u56fe\u68c0\u6d4b\u548c\u6a21\u5f0f\u8f6c\u6362\u5904\u7406\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0cDPO\u4f18\u5316\u540e\u8fbe\u523075.74%\u8054\u5408\u6a21\u5f0f-\u610f\u56fe\u51c6\u786e\u7387\u548c70.1%\u80dc\u7387\u4f18\u4e8eGPT-4o\u3002", "conclusion": "\u7ed3\u6784\u591a\u6837\u7684\u6570\u636e\u4e0eDPO\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u54cd\u5e94\u8d28\u91cf\u548c\u8f6c\u6362\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u66f4\u4e3b\u52a8\u548c\u8f6c\u6362\u611f\u77e5\u7684\u5bf9\u8bdd\u667a\u80fd\u4f53\u94fa\u5e73\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.09127", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09127", "abs": "https://arxiv.org/abs/2511.09127", "authors": ["Ziwei Wang", "Leyang Yang", "Xiaoxuan Tang", "Sheng Zhou", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "History-Aware Reasoning for GUI Agents", "comment": "Paper accepted to AAAI 2026", "summary": "Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.", "AI": {"tldr": "\u63d0\u51fa\u5386\u53f2\u611f\u77e5\u63a8\u7406(HAR)\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u601d\u5b66\u4e60\u589e\u5f3aGUI\u4ee3\u7406\u7684\u77ed\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709GUI\u4ee3\u7406\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u5386\u53f2\u4ea4\u4e92\u610f\u8bc6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u663e\u5f0f\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u77ed\u671f\u8bb0\u5fc6\uff0c\u5c06\u94fe\u5f0f\u4ea4\u4e92\u89c6\u4e3a\u79bb\u6563\u7684\u5c4f\u5e55\u7406\u89e3\uff0c\u7f3a\u4e4f\u5bf9\u5386\u53f2\u4ea4\u4e92\u7684\u611f\u77e5\uff0c\u8fd9\u9650\u5236\u4e86\u5728GUI\u81ea\u52a8\u5316\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u53cd\u601d\u5b66\u4e60\u573a\u666f\u3001\u5408\u6210\u5b9a\u5236\u4fee\u6b63\u6307\u5357\u3001\u8bbe\u8ba1\u6df7\u5408RL\u5956\u52b1\u51fd\u6570\uff0c\u5f00\u53d1HAR-GUI-3B\u6a21\u578b\uff0c\u5c06\u63a8\u7406\u6a21\u5f0f\u4ece\u5386\u53f2\u65e0\u5173\u8f6c\u53d8\u4e3a\u5386\u53f2\u611f\u77e5\u3002", "result": "\u5728\u591a\u4e2aGUI\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HAR\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u77ed\u671f\u8bb0\u5fc6\u548c\u5c4f\u5e55\u7ec6\u8282\u611f\u77e5\uff0c\u4e3aGUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u5386\u53f2\u4ea4\u4e92\u610f\u8bc6\u548c\u53ef\u9760\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.08866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08866", "abs": "https://arxiv.org/abs/2511.08866", "authors": ["Fuyi Yang", "Chenchen Ye", "Mingyu Derek Ma", "Yijia Xiao", "Matthew Yang", "Wei Wang"], "title": "BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation", "comment": null, "summary": "Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.", "AI": {"tldr": "BioVerge\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u7269\u533b\u5b66\u5047\u8bbe\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u548cLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u548c\u6587\u672c\u6570\u636e\uff0c\u91c7\u7528ReAct\u65b9\u6cd5\u8fdb\u884c\u8fed\u4ee3\u751f\u6210\u548c\u81ea\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5047\u8bbe\u7684\u65b0\u9896\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u533b\u5b66\u5047\u8bbe\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u7c7b\u578b\u6216\u9884\u5b9a\u4e49\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u53d1\u73b0\u65b0\u9896\u590d\u6742\u5173\u7cfb\u7684\u80fd\u529b\u3002LLM\u667a\u80fd\u4f53\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u6267\u884c\u73af\u5883\u3002", "method": "\u63d0\u51faBioVerge\u57fa\u51c6\u6d4b\u8bd5\u548cBioVerge Agent\u6846\u67b6\uff0c\u4f7f\u7528ReAct\u65b9\u6cd5\uff0c\u5305\u542b\u751f\u6210\u548c\u8bc4\u4f30\u6a21\u5757\uff0c\u8fed\u4ee3\u4ea7\u751f\u548c\u81ea\u8bc4\u4f30\u5047\u8bbe\u63d0\u6848\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u548cPubMed\u6587\u732e\u6587\u672c\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u4e0d\u540c\u667a\u80fd\u4f53\u67b6\u6784\u5f71\u54cd\u63a2\u7d22\u591a\u6837\u6027\u548c\u63a8\u7406\u7b56\u7565\uff1b2\uff09\u7ed3\u6784\u5316\u548c\u6587\u672c\u4fe1\u606f\u6e90\u5404\u81ea\u63d0\u4f9b\u72ec\u7279\u5173\u952e\u4e0a\u4e0b\u6587\uff1b3\uff09\u81ea\u8bc4\u4f30\u663e\u8457\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "BioVerge\u4e3a\u751f\u7269\u533b\u5b66\u5047\u8bbe\u751f\u6210\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u73af\u5883\uff0cLLM\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u63a2\u7d22\u79d1\u5b66\u77e5\u8bc6\u524d\u6cbf\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6e90\u548c\u81ea\u8bc4\u4f30\u673a\u5236\u53ef\u4ea7\u751f\u66f4\u4f18\u8d28\u7684\u5047\u8bbe\u3002", "topic": "agent analysis"}}
{"id": "2511.09157", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09157", "abs": "https://arxiv.org/abs/2511.09157", "authors": ["Leyang Yang", "Ziwei Wang", "Xiaoxuan Tang", "Sheng Zhou", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "ProBench: Benchmarking GUI Agents with Accurate Process Information", "comment": "Paper accepted to AAAI 2026", "summary": "With the deep integration of artificial intelligence and interactive technology, Graphical User Interface (GUI) Agent, as the carrier connecting goal-oriented natural language and real-world devices, has received widespread attention from the community. Contemporary benchmarks aim to evaluate the comprehensive capabilities of GUI agents in GUI operation tasks, generally determining task completion solely by inspecting the final screen state. However, GUI operation tasks consist of multiple chained steps while not all critical information is presented in the final few pages. Although a few research has begun to incorporate intermediate steps into evaluation, accurately and automatically capturing this process information still remains an open challenge. To address this weakness, we introduce ProBench, a comprehensive mobile benchmark with over 200 challenging GUI tasks covering widely-used scenarios. Remaining the traditional State-related Task evaluation, we extend our dataset to include Process-related Task and design a specialized evaluation method. A newly introduced Process Provider automatically supplies accurate process information, enabling presice assessment of agent's performance. Our evaluation of advanced GUI agents reveals significant limitations for real-world GUI scenarios. These shortcomings are prevalent across diverse models, including both large-scale generalist models and smaller, GUI-specific models. A detailed error analysis further exposes several universal problems, outlining concrete directions for future improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86ProBench\uff0c\u4e00\u4e2a\u5305\u542b200\u591a\u4e2a\u6311\u6218\u6027GUI\u4efb\u52a1\u7684\u79fb\u52a8\u7aef\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e0d\u4ec5\u8bc4\u4f30\u6700\u7ec8\u72b6\u6001\uff0c\u8fd8\u5f15\u5165\u8fc7\u7a0b\u76f8\u5173\u4efb\u52a1\u548c\u4e13\u95e8\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7cbe\u786e\u8bc4\u4f30GUI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u901a\u8fc7\u68c0\u67e5\u6700\u7ec8\u5c4f\u5e55\u72b6\u6001\u6765\u8bc4\u4f30GUI\u4ee3\u7406\uff0c\u4f46GUI\u64cd\u4f5c\u4efb\u52a1\u5305\u542b\u591a\u4e2a\u94fe\u5f0f\u6b65\u9aa4\uff0c\u5e76\u975e\u6240\u6709\u5173\u952e\u4fe1\u606f\u90fd\u5448\u73b0\u5728\u6700\u540e\u51e0\u9875\u4e2d\uff0c\u51c6\u786e\u81ea\u52a8\u6355\u83b7\u8fc7\u7a0b\u4fe1\u606f\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165ProBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u72b6\u6001\u76f8\u5173\u4efb\u52a1\u548c\u8fc7\u7a0b\u76f8\u5173\u4efb\u52a1\uff0c\u8bbe\u8ba1\u4e13\u95e8\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u63d0\u4f9b\u8005\u81ea\u52a8\u63d0\u4f9b\u51c6\u786e\u7684\u8fc7\u7a0b\u4fe1\u606f\u6765\u7cbe\u786e\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u5bf9\u5148\u8fdbGUI\u4ee3\u7406\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754cGUI\u573a\u666f\u4e2d\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u7f3a\u9677\u5728\u5305\u62ec\u5927\u89c4\u6a21\u901a\u7528\u6a21\u578b\u548c\u8f83\u5c0fGUI\u4e13\u7528\u6a21\u578b\u5728\u5185\u7684\u5404\u79cd\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u51e0\u4e2a\u666e\u904d\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7684\u6539\u8fdb\u6307\u660e\u4e86\u5177\u4f53\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2511.09158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09158", "abs": "https://arxiv.org/abs/2511.09158", "authors": ["Yuhao Wang", "Xiaopeng Li", "Cheng Gong", "Ziru Liu", "Suiyun Zhang", "Rui Liu", "Xiangyu Zhao"], "title": "Efficient Reasoning via Reward Model", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u7b80\u6d01\u6027\u5956\u52b1\u6a21\u578b(CRM)\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u7b80\u6d01\u6027\u5956\u52b1\u51fd\u6570(CRF)\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u957f\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5982DeepSeek-R1\u548cOpenAI o1\u7ecf\u5e38\u4ea7\u751f\u5305\u542b\u5197\u4f59\u6216\u65e0\u5173\u63a8\u7406\u6b65\u9aa4\u7684\u5197\u957f\u54cd\u5e94\uff08\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff09\uff0c\u8fd9\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u7684\u957f\u5ea6\u60e9\u7f5a\u65b9\u6cd5\u5b58\u5728\u957f\u5ea6\u5d29\u6e83\u548c\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u7b80\u6d01\u6027\u5956\u52b1\u6a21\u578b(CRM)\u7684\u7ba1\u9053\uff0c\u4e3a\u63a8\u7406\u8def\u5f84\u7684\u7b80\u6d01\u6027\u8bc4\u5206\uff1b\u5f15\u5165\u65b0\u9896\u7684\u7b80\u6d01\u6027\u5956\u52b1\u51fd\u6570(CRF)\uff0c\u660e\u786e\u5efa\u7acb\u7ed3\u679c\u5956\u52b1\u4e0e\u7b80\u6d01\u6027\u8bc4\u5206\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728Qwen2.5-7B\u4e0a\u5b9e\u73b0\u4e868.1%\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c19.9%\u7684\u54cd\u5e94token\u957f\u5ea6\u51cf\u5c11\uff0c\u5e76\u5728Llama\u548cMistral\u7b49\u5176\u4ed6LLM\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u66f4\u9ad8\u6548\u548c\u66f4\u6709\u6548\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.09178", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.09178", "abs": "https://arxiv.org/abs/2511.09178", "authors": ["Niclas Flehmig", "Mary Ann Lundteigen", "Shen Yin"], "title": "Perspectives on a Reliability Monitoring Framework for Agentic AI Systems", "comment": null, "summary": "The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5c42\u53ef\u9760\u6027\u76d1\u63a7\u6846\u67b6\u6765\u89e3\u51b3\u667a\u80fdAI\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u5305\u62ec\u79bb\u7fa4\u68c0\u6d4b\u5c42\u548cAI\u900f\u660e\u5ea6\u5c42\uff0c\u4e3a\u4eba\u7c7b\u64cd\u4f5c\u5458\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u667a\u80fdAI\u7cfb\u7edf\u867d\u7136\u5177\u6709\u81ea\u4e3b\u6027\u4f18\u52bf\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u65f6\u5b58\u5728\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u7f13\u89e3\u6280\u672f\u6765\u964d\u4f4e\u8fd0\u884c\u65f6\u7684\u98ce\u9669\u3002", "method": "\u57fa\u4e8e\u667a\u80fdAI\u7cfb\u7edf\u7279\u6027\u63a8\u5bfc\u4e3b\u8981\u53ef\u9760\u6027\u6311\u6218\uff0c\u63d0\u51fa\u5305\u542b\u79bb\u7fa4\u68c0\u6d4b\u548cAI\u900f\u660e\u5ea6\u7684\u53cc\u5c42\u76d1\u63a7\u6846\u67b6\uff0c\u4e3a\u64cd\u4f5c\u5458\u63d0\u4f9b\u5e72\u9884\u51b3\u7b56\u652f\u6301\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u7840\u6846\u67b6\u6765\u5f00\u53d1\u7f13\u89e3\u6280\u672f\uff0c\u51cf\u5c11\u667a\u80fdAI\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u56e0\u53ef\u9760\u6027\u4e0d\u786e\u5b9a\u800c\u4ea7\u751f\u7684\u98ce\u9669\u3002", "conclusion": "\u53cc\u5c42\u76d1\u63a7\u6846\u67b6\u4e3a\u89e3\u51b3\u667a\u80fdAI\u7cfb\u7edf\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u4eba\u7c7b\u64cd\u4f5c\u5458\u8bc6\u522b\u4e0d\u53ef\u9760\u8f93\u51fa\u5e76\u8fdb\u884c\u5e72\u9884\u3002", "topic": "agent analysis"}}
{"id": "2511.09363", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.09363", "abs": "https://arxiv.org/abs/2511.09363", "authors": ["Ali Taheri", "Alireza Taban", "Sadegh Soudjani", "Ashutosh Trivedi"], "title": "BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems", "comment": null, "summary": "Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.\n  This motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.\n  To evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.\n  The benchmark is publicly available at https://hycodev.com/dataset/barrierbench", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5408\u6210\u52a8\u6001\u7cfb\u7edf\u7684\u5c4f\u969c\u8bc1\u4e66\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548cSMT\u9a8c\u8bc1\uff0c\u5728BarrierBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523090%\u4ee5\u4e0a\u7684\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u5c4f\u969c\u8bc1\u4e66\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u4f9d\u8d56\u6a21\u677f\u8bbe\u8ba1\u3001\u9700\u8981\u5927\u91cf\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\u7b49\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u80fd\u5426\u7528\u8bed\u8a00\u6a21\u578b\u6355\u6349\u4e13\u5bb6\u63a8\u7406", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u63d0\u51fa\u3001\u6539\u8fdb\u548c\u9a8c\u8bc1\u5019\u9009\u8bc1\u4e66\uff0c\u7ed3\u5408\u6a21\u677f\u53d1\u73b0\u548cSMT\u9a8c\u8bc1\uff0c\u652f\u6301\u5c4f\u969c-\u63a7\u5236\u5668\u534f\u540c\u5408\u6210", "result": "\u5728\u5305\u542b100\u4e2a\u52a8\u6001\u7cfb\u7edf\u7684BarrierBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6846\u67b6\u6210\u529f\u751f\u6210\u6709\u6548\u8bc1\u4e66\u7684\u6bd4\u4f8b\u8d85\u8fc790%", "conclusion": "LLM\u5f15\u5bfc\u7684\u5c4f\u969c\u5408\u6210\u65b9\u6cd5\u6709\u6548\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4ee3\u7406\u534f\u8c03\u7b56\u7565\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u8bed\u8a00\u63a8\u7406\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0", "topic": "agent analysis"}}
{"id": "2511.09067", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09067", "abs": "https://arxiv.org/abs/2511.09067", "authors": ["Gailun Zeng", "Ziyang Luo", "Hongzhan Lin", "Yuchen Tian", "Kaixin Li", "Ziyang Gong", "Jianxiong Guo", "Jing Ma"], "title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique", "comment": "28 pages, 14 figures, 19 tables", "summary": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.", "AI": {"tldr": "\u63d0\u51fa\u4e86MM-CRITIC\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6279\u5224\u80fd\u529b\uff0c\u6db5\u76d6\u57fa\u7840\u3001\u4fee\u6b63\u548c\u6bd4\u8f83\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5305\u542b8\u79cd\u4efb\u52a1\u7c7b\u578b\u548c500\u591a\u4e2a\u4efb\u52a1\uff0c\u51714471\u4e2a\u6837\u672c\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u4f46\u5176\u591a\u6a21\u6001\u6279\u5224\u80fd\u529b\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u4e8e\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u548c\u4f5c\u4e3a\u53ef\u9760AI\u52a9\u624b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efaMM-CRITIC\u57fa\u51c6\uff0c\u6574\u5408\u4e13\u5bb6\u6307\u5bfc\u7684\u53c2\u8003\u7b54\u6848\u5230\u8bc4\u5206\u6807\u51c6\u4e2d\uff0c\u4f7f\u7528GPT-4o\u6807\u6ce8\u54cd\u5e94\u5e76\u751f\u6210\u53c2\u8003\u6279\u5224\uff0c\u4f5c\u4e3a\u53ef\u4fe1\u5224\u65ad\u7684\u951a\u70b9\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MM-CRITIC\u7684\u6709\u6548\u6027\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b\u7684\u6279\u5224\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u54cd\u5e94\u8d28\u91cf\u4e0e\u6279\u5224\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bc4\u4f30\u7ef4\u5ea6\u4e0b\u6279\u5224\u96be\u5ea6\u7684\u53d8\u5316\u3002", "conclusion": "MM-CRITIC\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u6279\u5224\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6279\u5224\u80fd\u529b\u73b0\u72b6\u548c\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2511.08922", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08922", "abs": "https://arxiv.org/abs/2511.08922", "authors": ["Yunchang Ma", "Tenglong Liu", "Yixing Lan", "Xin Yin", "Changxin Zhang", "Xinglong Zhang", "Xin Xu"], "title": "Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning", "comment": "IROS 2025", "summary": "In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.", "AI": {"tldr": "\u63d0\u51fa\u4e86DIVO\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u72b6\u6001-\u52a8\u4f5c\u6837\u672c\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f18\u52bf\u503c\u7684\u4e8c\u5143\u52a0\u6743\u673a\u5236\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u6570\u636e\u5206\u5e03\u5bf9\u9f50\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e73\u8861\u4fdd\u5b88\u6027\u548c\u63a2\u7d22\u6027\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7531\u4e8e\u5206\u5e03\u5916\u52a8\u4f5c\u5bfc\u81f4\u7684\u4ef7\u503c\u9ad8\u4f30\u95ee\u9898\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u4fdd\u5b88\u548c\u6548\u7387-\u8868\u8fbe\u80fd\u529b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDIVO\u65b9\u6cd5\uff0c\u5f15\u5165\u57fa\u4e8e\u52a8\u4f5c\u4f18\u52bf\u503c\u7684\u4e8c\u5143\u52a0\u6743\u673a\u5236\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u52a8\u6001\u7b5b\u9009\u9ad8\u56de\u62a5\u6f5c\u529b\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7cbe\u786e\u5206\u5e03\u5bf9\u9f50\u548c\u9009\u62e9\u6027\u8fb9\u754c\u6269\u5c55\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u8fd0\u52a8\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u5e73\u5747\u56de\u62a5\u63d0\u5347\uff0c\u5728\u5177\u6709\u7a00\u758f\u5956\u52b1\u6311\u6218\u7684AntMaze\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DIVO\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u4fdd\u5b88\u6027\u548c\u63a2\u7d22\u6027\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7b56\u7565\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.09339", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09339", "abs": "https://arxiv.org/abs/2511.09339", "authors": ["Arka Mukherjee", "Shreya Ghosh"], "title": "mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models", "comment": "Accepted to IJCNLP-AACL Findings 2025", "summary": "Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \\textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86mmJEE-Eval\u591a\u6a21\u6001\u53cc\u8bed\u57fa\u51c6\uff0c\u57fa\u4e8e\u5370\u5ea6JEE Advanced\u8003\u8bd5\u9898\u76ee\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u4e0e\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u533a\u5206\u771f\u6b63\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u4e0e\u6a21\u5f0f\u5339\u914d\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b1,460\u9053JEE Advanced\u8003\u9898\u7684\u591a\u6a21\u6001\u53cc\u8bed\u57fa\u51c6\uff0c\u6db5\u76d6\u7269\u7406\u3001\u5316\u5b66\u3001\u6570\u5b66\uff0c\u8bc4\u4f3017\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u524d\u6cbfVLMs\u57282025\u5e74\u9898\u76ee\u4e0a\u8fbe\u523077-84%\u51c6\u786e\u7387\uff0c\u5f00\u6e90\u6a21\u578b\u4ec537-45%\uff0c\u5f53\u589e\u52a0\u5143\u8ba4\u77e5\u63a8\u7406\u8d1f\u8377\u65f6\uff0cGPT-5\u4ec5\u4fee\u590d5.2%\u9519\u8bef\u3002", "conclusion": "mmJEE-Eval\u80fd\u6709\u6548\u533a\u5206\u4f18\u79c0\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u5176\u96be\u5ea6\u6e90\u4e8e\u590d\u6742\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u800c\u975e\u8bb0\u5fc6\u3002", "topic": "agent analysis"}}
{"id": "2511.09381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09381", "abs": "https://arxiv.org/abs/2511.09381", "authors": ["Hossein A. Rahmani", "Satyapriya Krishna", "Xi Wang", "Mohammadmehdi Naghiaei", "Emine Yilmaz"], "title": "Self-Correcting Large Language Models: Generation vs. Multiple Choice", "comment": "20 pages", "summary": "Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:\n  \\textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86LLM\u5728\u5f00\u653e\u5f0f\u751f\u6210\u548c\u591a\u9879\u9009\u62e9\u4e24\u79cd\u8303\u5f0f\u4e0b\u7684\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u53d1\u73b0\u4e0d\u540c\u4efb\u52a1\u7ed3\u6784\u5bf9\u81ea\u6211\u4fee\u6b63\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76LLM\u81ea\u6211\u4fee\u6b63\u673a\u5236\u5728\u4e0d\u540c\u4efb\u52a1\u8303\u5f0f\uff08\u5f00\u653e\u5f0f\u751f\u6210vs\u591a\u9879\u9009\u62e9\uff09\u4e2d\u7684\u52a8\u6001\u5dee\u5f02\uff0c\u4e3a\u667a\u80fd\u4f53\u5e94\u7528\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u5bf9\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e24\u79cd\u8303\u5f0f\u7684\u6027\u80fd\u8d8b\u52bf\u548c\u9519\u8bef\u4fee\u6b63\u884c\u4e3a\u3002", "result": "\u5f00\u653e\u5f0f\u751f\u6210\u53d7\u76ca\u4e8e\u91cd\u65b0\u89e3\u91ca\u548c\u7ec4\u5408\u4f18\u5316\u7684\u7075\u6d3b\u6027\uff0c\u800c\u591a\u9879\u9009\u62e9\u80fd\u5229\u7528\u66f4\u6e05\u6670\u7684\u89e3\u8fb9\u754c\u4f46\u53d7\u9650\u4e8e\u9009\u9879\u8303\u56f4\u3002", "conclusion": "\u81ea\u6211\u4fee\u6b63\u673a\u5236\u8bbe\u8ba1\u5e94\u8003\u8651\u4efb\u52a1\u7ed3\u6784\u4e0e\u8f93\u51fa\u7a7a\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u548c\u51b3\u7b56\u5bfc\u5411\u7684LLM\u5e94\u7528\u90fd\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2511.09105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09105", "abs": "https://arxiv.org/abs/2511.09105", "authors": ["Shigeki Kusaka", "Keita Saito", "Mikoto Kudo", "Takumi Tanabe", "Akifumi Wachi", "Youhei Akimoto"], "title": "Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment", "comment": "accepted for AAAI 2026 Special Track on AI Alignment", "summary": "Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728RLHF/DPO\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u7ffb\u8f6c\u504f\u597d\u6807\u7b7e\u8fdb\u884c\u6700\u5c0f\u6210\u672c\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u51f8\u4f18\u5316\u6846\u67b6\u6765\u8ba1\u7b97\u653b\u51fb\u6210\u672c\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u5f00\u53d1\u4e86\u6210\u672c\u6700\u5c0f\u5316\u540e\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740LLMs\u5728\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u7406\u89e3\u5176\u8106\u5f31\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136RLHF/DPO\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5df2\u6709\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u5c06\u6295\u6bd2\u653b\u51fb\u5efa\u6a21\u4e3a\u5e26\u7ebf\u6027\u7ea6\u675f\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u653b\u51fb\u6210\u672c\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u6210\u672c\u6700\u5c0f\u5316\u540e\u5904\u7406\u65b9\u6cd5\u51cf\u5c11\u6807\u7b7e\u7ffb\u8f6c\u6b21\u6570\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u6210\u672c\u6700\u5c0f\u5316\u540e\u5904\u7406\u80fd\u663e\u8457\u964d\u4f4e\u6295\u6bd2\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u6a21\u578b\u7279\u5f81\u7ef4\u5ea6\u76f8\u5bf9\u4e8e\u6570\u636e\u96c6\u5927\u5c0f\u8f83\u5c0f\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RLHF/DPO\u6d41\u7a0b\u4e2d\u7684\u57fa\u672c\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5176\u5bf9\u4f4e\u6210\u672c\u6295\u6bd2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2511.09114", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09114", "abs": "https://arxiv.org/abs/2511.09114", "authors": ["Tim Dudman", "Martyn Bull"], "title": "Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks", "comment": "CAMLIS 2025. To be published in the Proceedings of Machine Learning Research (PMLR)", "summary": "Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.", "AI": {"tldr": "\u63d0\u51faTERLA\u6846\u67b6\uff0c\u4f7f\u7528\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u56fa\u5b9a\u5927\u5c0f\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u62d3\u6251\u548c\u89c4\u6a21\u7684\u7f51\u7edc\u9632\u5fa1\u4efb\u52a1\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u9632\u5fa1\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u9762\u5bf9\u4e0d\u540c\u62d3\u6251\u548c\u89c4\u6a21\u7684\u7f51\u7edc\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u7f51\u7edc\u7ed3\u6784\u52a8\u6001\u53d8\u5316\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u5c42\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u7f51\u7edc\u72b6\u6001\u6f5c\u5728\u5d4c\u5165\uff0c\u7ed3\u5408\u56fa\u5b9a\u5927\u5c0f\u3001\u8bed\u4e49\u660e\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e94\u7528\u4e8ePPO\u4ee3\u7406\u6a21\u578b\u3002", "result": "TERLA\u4ee3\u7406\u5728\u4fdd\u6301\u9632\u5fa1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u52a8\u4f5c\u6548\u7387\uff0c\u80fd\u591f\u90e8\u7f72\u5230\u4e0d\u540c\u62d3\u6251\u548c\u89c4\u6a21\u7684\u7f51\u7edc\u6bb5\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TERLA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u7f51\u7edc\u9632\u5fa1\u4ee3\u7406\u7684\u62d3\u6251\u548c\u89c4\u6a21\u6cdb\u5316\uff0c\u4e3a\u5b9e\u9645\u7f51\u7edc\u9632\u5fa1\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.09149", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09149", "abs": "https://arxiv.org/abs/2511.09149", "authors": ["Zhuoyun Du", "Runze Wang", "Huiyu Bai", "Zouying Cao", "Xiaoyong Zhu", "Bo Zheng", "Wei Chen", "Haochao Ying"], "title": "Enabling Agents to Communicate Entirely in Latent Space", "comment": "Work in progess", "summary": "While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.", "AI": {"tldr": "\u63d0\u51faInterlat\uff08\u4ee3\u7406\u95f4\u6f5c\u5728\u7a7a\u95f4\u901a\u4fe1\uff09\u8303\u5f0f\uff0c\u5229\u7528LLM\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u5fc3\u667a\u8868\u5f81\u8fdb\u884c\u76f4\u63a5\u4f20\u8f93\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u8fdb\u4e00\u6b65\u538b\u7f29\u901a\u4fe1\uff0c\u63d0\u5347\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3aLLM\u4ee3\u7406\u901a\u4fe1\u5a92\u4ecb\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u5c06\u4e30\u5bcc\u7684\u5185\u90e8\u6f5c\u5728\u72b6\u6001\u4e0b\u91c7\u6837\u4e3a\u79bb\u6563\u6807\u8bb0\u4f1a\u9650\u5236\u4fe1\u606f\u4f20\u8f93\u7684\u6df1\u5ea6\u548c\u7ec6\u5fae\u5dee\u522b\uff0c\u4ece\u800c\u963b\u788d\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u3002", "method": "\u4f7f\u7528LLM\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u5fc3\u667a\u8868\u5f81\u8fdb\u884c\u76f4\u63a5\u4f20\u8f93\uff08\u6f5c\u5728\u901a\u4fe1\uff09\uff0c\u5e76\u901a\u8fc7\u989d\u5916\u7684\u538b\u7f29\u8fc7\u7a0b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\u6765\u8fdb\u4e00\u6b65\u538b\u7f29\u901a\u4fe1\u3002", "result": "Interlat\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5fae\u8c03\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u5355\u4ee3\u7406\u57fa\u7ebf\uff0c\u4fc3\u8fdb\u66f4\u63a2\u7d22\u6027\u884c\u4e3a\u5e76\u5b9e\u73b0\u771f\u6b63\u5229\u7528\u6f5c\u5728\u4fe1\u606f\u3002\u8fdb\u4e00\u6b65\u538b\u7f29\u4e0d\u4ec5\u663e\u8457\u52a0\u901f\u63a8\u7406\uff0c\u8fd8\u901a\u8fc7\u9ad8\u6548\u7684\u4fe1\u606f\u4fdd\u7559\u673a\u5236\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4f5c\u4e3a\u5b8c\u5168\u6f5c\u5728\u7a7a\u95f4\u4ee3\u7406\u95f4\u901a\u4fe1\u7684\u53ef\u884c\u6027\u7814\u7a76\uff0c\u7ed3\u679c\u7a81\u663e\u5176\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2511.09173", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09173", "abs": "https://arxiv.org/abs/2511.09173", "authors": ["Guojian Wang", "Quinson Hon", "Xuyang Chen", "Lin Zhao"], "title": "Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization", "comment": "27 pages,4 figures", "summary": "Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.", "AI": {"tldr": "DFDT\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u51b3\u7b56\u53d8\u6362\u5668\u5728\u8de8\u57df\u8fc1\u79fb\u4e2d\u8f68\u8ff9\u7247\u6bb5\u62fc\u63a5\u95ee\u9898\u7684\u7d27\u51d1\u7ba1\u9053\uff0c\u901a\u8fc7\u4e24\u7ea7\u6570\u636e\u8fc7\u6ee4\u3001\u4f18\u52bf\u6761\u4ef6\u4ee4\u724c\u548cQ\u5f15\u5bfc\u6b63\u5219\u5316\u6765\u6062\u590d\u62fc\u63a5\u6027\uff0c\u5728\u591a\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u56de\u62a5\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u7b56\u7565\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u8fc7\u6ee4\u6807\u51c6\u9009\u62e9\u6e90\u8f68\u8ff9\u7247\u6bb5\uff0c\u4f46\u62fc\u63a5\u540e\u4ecd\u5b58\u5728\u72b6\u6001\u7ed3\u6784\u9519\u4f4d\u3001RTG\u4e0d\u53ef\u6bd4\u3001\u52a8\u4f5c\u8df3\u8dc3\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4RTG\u4ee4\u724c\u5931\u53bb\u8fde\u7eed\u6027\uff0c\u5f71\u54cdDT\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "DFDT\u901a\u8fc7\u4e24\u7ea7\u6570\u636e\u8fc7\u6ee4\u5668\uff08MMD\u4e0d\u5339\u914d\u7528\u4e8e\u72b6\u6001\u7ed3\u6784\u5bf9\u9f50\uff0cOT\u504f\u5dee\u7528\u4e8e\u52a8\u4f5c\u53ef\u884c\u6027\uff09\u878d\u5408\u7a00\u7f3a\u76ee\u6807\u6570\u636e\u548c\u9009\u62e9\u6027\u4fe1\u4efb\u7684\u6e90\u7247\u6bb5\uff0c\u4f7f\u7528\u53ef\u884c\u6027\u52a0\u6743\u7684\u878d\u5408\u5206\u5e03\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7528\u4f18\u52bf\u6761\u4ef6\u4ee4\u724c\u66ff\u6362RTG\u4ee4\u724c\uff0c\u5e94\u7528Q\u5f15\u5bfc\u6b63\u5219\u5316\u6291\u5236\u8fde\u63a5\u70b9\u503c\u548c\u52a8\u4f5c\u8df3\u8dc3\u3002", "result": "DFDT\u5728D4RL\u98ce\u683c\u63a7\u5236\u4efb\u52a1\u7684\u91cd\u529b\u3001\u8fd0\u52a8\u5b66\u548c\u5f62\u6001\u53d8\u5316\u4e2d\uff0c\u76f8\u6bd4\u5f3a\u79bb\u7ebfRL\u548c\u5e8f\u5217\u6a21\u578b\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u56de\u62a5\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4ee4\u724c\u62fc\u63a5\u548c\u5e8f\u5217\u8bed\u4e49\u7a33\u5b9a\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6536\u76ca\u3002", "conclusion": "DFDT\u901a\u8fc7\u6062\u590d\u8f68\u8ff9\u7247\u6bb5\u7684\u62fc\u63a5\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u51b3\u7b56\u53d8\u6362\u5668\u5728\u8de8\u57df\u8fc1\u79fb\u4e2d\u7684\u6311\u6218\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u72b6\u6001\u503c\u548c\u7b56\u7565\u6027\u80fd\u5dee\u8ddd\u4e0eMMD\u4e0d\u5339\u914d\u548cOT\u504f\u5dee\u5ea6\u91cf\u76f8\u5173\uff0c\u4e14\u968f\u7740\u8fd9\u4e24\u4e2a\u5ea6\u91cf\u7684\u7f29\u5c0f\u800c\u6536\u7d27\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.09190", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.09190", "abs": "https://arxiv.org/abs/2511.09190", "authors": ["Alexander Chebykin", "Tanja Alderliesten", "Peter A. N. Bosman"], "title": "Iterated Population Based Training with Task-Agnostic Restarts", "comment": null, "summary": "Hyperparameter Optimization (HPO) can lift the burden of tuning hyperparameters (HPs) of neural networks. HPO algorithms from the Population Based Training (PBT) family are efficient thanks to dynamically adjusting HPs every few steps of the weight optimization. Recent results indicate that the number of steps between HP updates is an important meta-HP of all PBT variants that can substantially affect their performance. Yet, no method or intuition is available for efficiently setting its value. We introduce Iterated Population Based Training (IPBT), a novel PBT variant that automatically adjusts this HP via restarts that reuse weight information in a task-agnostic way and leverage time-varying Bayesian optimization to reinitialize HPs. Evaluation on 8 image classification and reinforcement learning tasks shows that, on average, our algorithm matches or outperforms 5 previous PBT variants and other HPO algorithms (random search, ASHA, SMAC3), without requiring a budget increase or any changes to its HPs. The source code is available at https://github.com/AwesomeLemon/IPBT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8fed\u4ee3\u79cd\u7fa4\u8bad\u7ec3(IPBT)\uff0c\u4e00\u79cd\u81ea\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u66f4\u65b0\u95f4\u9694\u7684PBT\u53d8\u4f53\uff0c\u901a\u8fc7\u91cd\u542f\u673a\u5236\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709PBT\u65b9\u6cd5\u4e2d\u66f4\u65b0\u95f4\u9694\u8fd9\u4e00\u91cd\u8981\u5143\u8d85\u53c2\u6570\u7f3a\u4e4f\u6709\u6548\u7684\u8bbe\u7f6e\u65b9\u6cd5\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "IPBT\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u91cd\u542f\u673a\u5236\u91cd\u7528\u6743\u91cd\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u65f6\u53d8\u8d1d\u53f6\u65af\u4f18\u5316\u91cd\u65b0\u521d\u59cb\u5316\u8d85\u53c2\u6570", "result": "\u57288\u4e2a\u56fe\u50cf\u5206\u7c7b\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cIPBT\u5e73\u5747\u8868\u73b0\u4f18\u4e8e5\u79cdPBT\u53d8\u4f53\u548c\u5176\u4ed6HPO\u7b97\u6cd5\uff0c\u65e0\u9700\u589e\u52a0\u9884\u7b97\u6216\u8c03\u6574\u8d85\u53c2\u6570", "conclusion": "IPBT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u8c03\u6574PBT\u66f4\u65b0\u95f4\u9694\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2511.09219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09219", "abs": "https://arxiv.org/abs/2511.09219", "authors": ["Paul Strang", "Zacharie Al\u00e8s", "C\u00f4me Bissuel", "Safia Kedad-Sidhoum", "Emmanuel Rachelson"], "title": "Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization", "comment": "arXiv admin note: text overlap with arXiv:2510.19348", "summary": "Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&B). A key driver influencing B&B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86PlanB&B\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u901a\u8fc7\u5b66\u4e60\u5206\u652f\u5b9a\u754c\u52a8\u6001\u7684\u5185\u90e8\u6a21\u578b\u6765\u53d1\u73b0\u6539\u8fdb\u7684\u5206\u652f\u7b56\u7565\uff0c\u5728\u56db\u4e2a\u6807\u51c6MILP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684RL\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfMILP\u6c42\u89e3\u5668\u4f7f\u7528\u9759\u6001\u3001\u624b\u5de5\u5236\u4f5c\u7684\u5206\u652f\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e0c\u671b\u8d85\u8d8a\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u5b66\u4e60\u9488\u5bf9\u7279\u5b9aMILP\u5206\u5e03\u7684\u5206\u652f\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff0c\u5b66\u4e60\u5206\u652f\u5b9a\u754c\u52a8\u6001\u7684\u5185\u90e8\u6a21\u578b\uff0c\u901a\u8fc7\u89c4\u5212\u6765\u6539\u8fdb\u5206\u652f\u51b3\u7b56\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6MILP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlanB&B\u7684\u6027\u80fd\u8d85\u8fc7\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u652f\u5b9a\u754c\u6c42\u89e3\u5668\u4e2d\u80fd\u591f\u6709\u6548\u5b66\u4e60\u6539\u8fdb\u7684\u5206\u652f\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.09261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09261", "abs": "https://arxiv.org/abs/2511.09261", "authors": ["Yuyao Long"], "title": "A Distributed Training Architecture For Combinatorial Optimization", "comment": null, "summary": "In recent years, graph neural networks (GNNs) have been widely applied in tackling combinatorial optimization problems. However, existing methods still suffer from limited accuracy when addressing that on complex graphs and exhibit poor scalability, since full training requires loading the whole adjacent matrix and all embeddings at a time, the it may results in out of memory of a single machine. This limitation significantly restricts their applicability to large-scale scenarios. To address these challenges, we propose a distributed GNN-based training framework for combinatorial optimization. In details, firstly, large graph is partition into several small subgraphs. Then the individual subgraphs are full trained, providing a foundation for efficient local optimization. Finally, reinforcement learning (RL) are employed to take actions according to GNN output, to make sure the restrictions between cross nodes can be learned. Extensive experiments are conducted on both real large-scale social network datasets (e.g., Facebook, Youtube) and synthetically generated high-complexity graphs, which demonstrate that our framework outperforms state-of-the-art approaches in both solution quality and computational efficiency. Moreover, the experiments on large graph instances also validate the scalability of the model.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u5206\u5272\u3001\u5b50\u56fe\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u9ad8\u6548\u4f18\u5316", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u56fe\u65f6\u7cbe\u5ea6\u6709\u9650\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u5168\u56fe\u8bad\u7ec3\u9700\u8981\u52a0\u8f7d\u6574\u4e2a\u90bb\u63a5\u77e9\u9635\u548c\u5d4c\u5165\uff0c\u5bfc\u81f4\u5355\u673a\u5185\u5b58\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u573a\u666f\u7684\u5e94\u7528", "method": "1. \u5c06\u5927\u56fe\u5206\u5272\u6210\u591a\u4e2a\u5c0f\u5b50\u56fe\uff1b2. \u5bf9\u5404\u5b50\u56fe\u8fdb\u884c\u5b8c\u6574\u8bad\u7ec3\uff1b3. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6839\u636eGNN\u8f93\u51fa\u91c7\u53d6\u884c\u52a8\uff0c\u5b66\u4e60\u8de8\u8282\u70b9\u7ea6\u675f", "result": "\u5728\u771f\u5b9e\u5927\u89c4\u6a21\u793e\u4ea4\u7f51\u7edc\u6570\u636e\u96c6\u548c\u5408\u6210\u9ad8\u590d\u6742\u5ea6\u56fe\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5927\u56fe\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u5206\u5e03\u5f0fGNN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6269\u5c55\u6027\u6311\u6218\uff0c\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2511.09478", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09478", "abs": "https://arxiv.org/abs/2511.09478", "authors": ["Renda Li", "Hailang Huang", "Fei Wei", "Feng Xiong", "Yong Wang", "Xiangxiang Chu"], "title": "AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.", "AI": {"tldr": "\u63d0\u51faAdaCuRL\u81ea\u9002\u5e94\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u96be\u5ea6\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u8c03\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6df7\u5408\u96be\u5ea6\u6837\u672c\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u9965\u997f\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6df7\u5408\u96be\u5ea6\u6837\u672c\u8bad\u7ec3\u4e2d\u5b58\u5728\u68af\u5ea6\u9965\u997f\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u96be\u5ea6\u4e0d\u5339\u914d\u3001\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u6311\u6218\u3002", "method": "AdaCuRL\u6846\u67b6\u6574\u5408\u4e86\u7c97\u5230\u7ec6\u96be\u5ea6\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u8c03\u5ea6\uff0c\u52a8\u6001\u5bf9\u9f50\u6570\u636e\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u6570\u636e\u91cd\u8bbf\u673a\u5236\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4f7f\u7528\u81ea\u9002\u5e94\u53c2\u8003\u548c\u7a00\u758fKL\u7b56\u7565\u9632\u6b62\u7b56\u7565\u9000\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaCuRL\u5728LLMs\u548cMLLMs\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AdaCuRL\u901a\u8fc7\u81ea\u9002\u5e94\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.e0b4a5b0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdiet103%2Fclaude-code-infrastructure-showcase%3Futm_source=tldrai/1/0100019a6e242a70-7748493a-44ba-4eff-a918-88d6e003c985-000000/LKXbz_jfPH0MUPdrLrtHUX9yOIC62Lf7U_tQUD7fI-U=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdiet103%2Fclaude-code-infrastructure-showcase%3Futm_source=tldrai/1/0100019a6e242a70-7748493a-44ba-4eff-a918-88d6e003c985-000000/LKXbz_jfPH0MUPdrLrtHUX9yOIC62Lf7U_tQUD7fI-U=430", "authors": ["TLDR Newsletter"], "title": "Claude Code Infrastructure Showcase", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdiet103%2Fclaude-code-infrastructure-showcase%3Futm_source=tldrai/1/0100019a6e242a70-7748493a-44ba-4eff-a918-88d6e003c985-000000/LKXbz_jfPH0MUPdrLrtHUX9yOIC62Lf7U_tQUD7fI-U=430", "summary": "Claude Code Infrastructure Showcase (GitHub Repo) This repository contains a curated reference library of production-tested Claude Code infrastructure. It contains production-tested infrastructure for auto-activating skills via locks, specialized agents for complex tasks, and more. Using this infrastructure, skills will suggest themselves based on context, hooks will trigger skills at the right time, modular skills stay under context limits, dev docs preserve knowledge across resets, and agen...", "source": "tldr", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u5c55\u793aClaude\u4ee3\u7801\u57fa\u7840\u8bbe\u65bd\u7684GitHub\u4ed3\u5e93\uff0c\u5305\u542b\u751f\u4ea7\u73af\u5883\u6d4b\u8bd5\u7684\u81ea\u52a8\u6280\u80fd\u6fc0\u6d3b\u3001\u4e13\u7528\u4ee3\u7406\u7b49\u57fa\u7840\u8bbe\u65bd\u7ec4\u4ef6", "motivation": "\u4e3aClaude\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u751f\u4ea7\u7ea7\u522b\u7684\u53ef\u9760\u57fa\u7840\u8bbe\u65bd\uff0c\u89e3\u51b3\u6280\u80fd\u81ea\u52a8\u6fc0\u6d3b\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u77e5\u8bc6\u4fdd\u7559\u7b49\u5b9e\u9645\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b\u6280\u80fd\u9501\u3001\u4e13\u7528\u4ee3\u7406\u3001\u6a21\u5757\u5316\u6280\u80fd\u3001\u5f00\u53d1\u6587\u6863\u7b49\u7ec4\u4ef6\u7684\u53c2\u8003\u57fa\u7840\u8bbe\u65bd\u5e93", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6280\u80fd\u81ea\u52a8\u5efa\u8bae\u3001\u9002\u65f6\u89e6\u53d1\u3001\u6a21\u5757\u5316\u7ba1\u7406\u548c\u77e5\u8bc6\u4fdd\u7559\u7b49\u529f\u80fd", "conclusion": "\u8be5\u57fa\u7840\u8bbe\u65bd\u4e3a\u6784\u5efa\u53ef\u9760\u7684\u4ee3\u7801\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u751f\u4ea7\u7ea7\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2511.cc30ab52", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdazl.dev%2F%3Futm_source=tldrproduct%26utm_medium=newsletter/1/0100019a72993f41-3407bbaa-f6c4-4341-92ab-88bc7c618096-000000/9ux6Es4pUqNrn0hPWg2k1hHVHy6u4M8vMuXlvjOge0k=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdazl.dev%2F%3Futm_source=tldrproduct%26utm_medium=newsletter/1/0100019a72993f41-3407bbaa-f6c4-4341-92ab-88bc7c618096-000000/9ux6Es4pUqNrn0hPWg2k1hHVHy6u4M8vMuXlvjOge0k=430", "authors": ["TLDR Newsletter"], "title": "Dazl: Vibe code your app, then refine with a visual editor", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdazl.dev%2F%3Futm_source=tldrproduct%26utm_medium=newsletter/1/0100019a72993f41-3407bbaa-f6c4-4341-92ab-88bc7c618096-000000/9ux6Es4pUqNrn0hPWg2k1hHVHy6u4M8vMuXlvjOge0k=430", "summary": "Dazl: Vibe code your app, then refine with a visual editor (Sponsor) Now open to all product makers! Dazl is the first platform to combine genAI with hands-on editing through chat, visual panels, or code. Get 1 month free with coupon: DazlxTLDR (offer valid for the first 100 users, until 11/30).", "source": "tldr", "AI": {"tldr": "Dazl\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u53ef\u89c6\u5316\u7f16\u8f91\u7684\u5e73\u53f0\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u804a\u5929\u3001\u53ef\u89c6\u5316\u9762\u677f\u6216\u4ee3\u7801\u6765\u6784\u5efa\u548c\u7cbe\u70bc\u5e94\u7528\u7a0b\u5e8f\u3002", "motivation": "\u4e3a\u4ea7\u54c1\u5236\u4f5c\u8005\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u5408AI\u751f\u6210\u80fd\u529b\u548c\u624b\u52a8\u7f16\u8f91\u7684\u7efc\u5408\u6027\u5f00\u53d1\u5e73\u53f0\uff0c\u7b80\u5316\u5e94\u7528\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u53ef\u89c6\u5316\u7f16\u8f91\u5668\uff0c\u652f\u6301\u901a\u8fc7\u804a\u5929\u754c\u9762\u3001\u53ef\u89c6\u5316\u9762\u677f\u6216\u76f4\u63a5\u4ee3\u7801\u7f16\u8f91\u4e09\u79cd\u65b9\u5f0f\u5f00\u53d1\u5e94\u7528\u3002", "result": "\u63a8\u51fa\u4e86Dazl\u5e73\u53f0\uff0c\u76ee\u524d\u5bf9\u6240\u6709\u4ea7\u54c1\u5236\u4f5c\u8005\u5f00\u653e\uff0c\u5e76\u63d0\u4f9b1\u4e2a\u6708\u514d\u8d39\u8bd5\u7528\u4f18\u60e0\u3002", "conclusion": "Dazl\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u878d\u5408AI\u751f\u6210\u4e0e\u624b\u52a8\u7f16\u8f91\u7684\u5f00\u53d1\u73af\u5883\uff0c\u4e3a\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2511.dce4a4b6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feu1.hubs.ly%2FH0pkGMy0%3Futm_source=tldrnewsletter/1/0100019a72a8565c-a1cad75b-bef1-43e6-b99f-b86f4949da1c-000000/1-kich-rh5dfTKuXZW4QHV16XAw8X-flIs8KCPFO3Pk=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feu1.hubs.ly%2FH0pkGMy0%3Futm_source=tldrnewsletter/1/0100019a72a8565c-a1cad75b-bef1-43e6-b99f-b86f4949da1c-000000/1-kich-rh5dfTKuXZW4QHV16XAw8X-flIs8KCPFO3Pk=430", "authors": ["TLDR Newsletter"], "title": "Getting agents to code is easy. Getting them to follow rules is hard", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feu1.hubs.ly%2FH0pkGMy0%3Futm_source=tldrnewsletter/1/0100019a72a8565c-a1cad75b-bef1-43e6-b99f-b86f4949da1c-000000/1-kich-rh5dfTKuXZW4QHV16XAw8X-flIs8KCPFO3Pk=430", "summary": "Getting agents to code is easy. Getting them to follow rules is hard (Sponsor) If AI coding feels risky, you're not wrong. But with multi-layer context, you can get agents to respect your architecture, standards, and compliance rules. See how Tabnine bridges the gap", "source": "tldr", "AI": {"tldr": "Tabnine\u901a\u8fc7\u591a\u5c42\u4e0a\u4e0b\u6587\u6280\u672f\u8ba9AI\u4ee3\u7801\u4ee3\u7406\u9075\u5faa\u67b6\u6784\u3001\u6807\u51c6\u548c\u5408\u89c4\u89c4\u5219", "motivation": "\u867d\u7136\u8ba9AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u5f88\u5bb9\u6613\uff0c\u4f46\u8ba9\u5b83\u4eec\u9075\u5faa\u89c4\u5219\u5374\u5f88\u56f0\u96be\uff0c\u8fd9\u7ed9AI\u7f16\u7801\u5e26\u6765\u4e86\u98ce\u9669", "method": "\u4f7f\u7528\u591a\u5c42\u4e0a\u4e0b\u6587\u6280\u672f\u6765\u786e\u4fddAI\u4ee3\u7801\u4ee3\u7406\u80fd\u591f\u5c0a\u91cd\u67b6\u6784\u3001\u6807\u51c6\u548c\u5408\u89c4\u89c4\u5219", "result": "Tabnine\u6210\u529f\u5f25\u5408\u4e86AI\u4ee3\u7801\u751f\u6210\u4e0e\u89c4\u5219\u9075\u5faa\u4e4b\u95f4\u7684\u5dee\u8ddd", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u591a\u5c42\u4e0a\u4e0b\u6587\u65b9\u6cd5\uff0c\u53ef\u4ee5\u8ba9AI\u4ee3\u7801\u4ee3\u7406\u5728\u751f\u6210\u4ee3\u7801\u65f6\u53ef\u9760\u5730\u9075\u5faa\u65e2\u5b9a\u89c4\u5219", "topic": "code agent"}}
{"id": "tldr.2511.6bc00658", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Farticle%2Fsecure-by-design-whitepaper-pdf/2/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/m2EksrhsTV29-OSB8nxzuJ_7nC_yEiXKd93BHPmqcTM=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Farticle%2Fsecure-by-design-whitepaper-pdf/2/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/m2EksrhsTV29-OSB8nxzuJ_7nC_yEiXKd93BHPmqcTM=430", "authors": ["TLDR Newsletter"], "title": "Free Resources for Scaling Developer-Driven Security", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Farticle%2Fsecure-by-design-whitepaper-pdf/2/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/m2EksrhsTV29-OSB8nxzuJ_7nC_yEiXKd93BHPmqcTM=430", "summary": "Free Resources for Scaling Developer-Driven Security (Sponsor) Turn security into an integral part of your SDLC with these 3 free resources from Secure Code Warrior:1\ufe0f\u20e3 Benchmarking Secure-by-Design initiatives: A presentation looking at aggregated data derived from multiple primary sources, including internal data points collected from over 250,000 developers. 2\ufe0f\u20e3 Secure by Design research paper: Defining best practices, enabling developers, and benchmarking preventative security outcomes. A...", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e09\u4e2a\u514d\u8d39\u8d44\u6e90\u6765\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u96c6\u6210\u5b89\u5168\u5b9e\u8df5\uff0c\u5305\u62ec\u5b89\u5168\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u3001\u7814\u7a76\u8bba\u6587\u548c\u5b89\u5168\u7f16\u7801\u57f9\u8bad\u5e73\u53f0\u3002", "motivation": "\u5c06\u5b89\u5168\u5b9e\u8df5\u878d\u5165SDLC\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u4e3b\u52a8\u53c2\u4e0e\u5b89\u5168\u9632\u62a4\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8b\u540e\u4fee\u590d\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u8d85\u8fc725\u4e07\u540d\u5f00\u53d1\u8005\u7684\u5185\u90e8\u6570\u636e\u70b9\uff0c\u7ed3\u5408\u591a\u4e2a\u4e3b\u8981\u6765\u6e90\u7684\u805a\u5408\u6570\u636e\uff0c\u5b9a\u4e49\u6700\u4f73\u5b9e\u8df5\u5e76\u5efa\u7acb\u9884\u9632\u6027\u5b89\u5168\u57fa\u51c6\u3002", "result": "\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u5b89\u5168\u8bbe\u8ba1\u57fa\u51c6\u548c\u5b9e\u7528\u7684\u5f00\u53d1\u8005\u57f9\u8bad\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u514d\u8d39\u8d44\u6e90\u548c\u6570\u636e\u9a71\u52a8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u6709\u6548\u63a8\u52a8\u5f00\u53d1\u8005\u9a71\u52a8\u7684\u5b89\u5168\u6587\u5316\u8f6c\u578b\u3002", "topic": "swe application"}}
{"id": "tldr.2511.ab205852", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Fproduct%2Ftrust-agent/1/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/nE2hH5IJKAEDc6PrzQT33P97vp-IUW8oGYZx-VVy49E=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Fproduct%2Ftrust-agent/1/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/nE2hH5IJKAEDc6PrzQT33P97vp-IUW8oGYZx-VVy49E=430", "authors": ["TLDR Newsletter"], "title": "3\ufe0f\u20e3 Free trial of SCW Trust Agent", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Fproduct%2Ftrust-agent/1/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/nE2hH5IJKAEDc6PrzQT33P97vp-IUW8oGYZx-VVy49E=430", "summary": ": Analyzes every code commit by correlating commit data with the contributor's language-specific secure coding proficiency.", "source": "tldr", "AI": {"tldr": "\u5206\u6790\u4ee3\u7801\u63d0\u4ea4\u4e0e\u8d21\u732e\u8005\u8bed\u8a00\u7279\u5b9a\u5b89\u5168\u7f16\u7801\u80fd\u529b\u7684\u5173\u8054", "motivation": "\u901a\u8fc7\u5173\u8054\u63d0\u4ea4\u6570\u636e\u4e0e\u8d21\u732e\u8005\u7684\u5b89\u5168\u7f16\u7801\u719f\u7ec3\u5ea6\uff0c\u8bc6\u522b\u4ee3\u7801\u63d0\u4ea4\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669", "method": "\u5206\u6790\u6bcf\u4e2a\u4ee3\u7801\u63d0\u4ea4\uff0c\u5c06\u63d0\u4ea4\u6570\u636e\u4e0e\u8d21\u732e\u8005\u7684\u8bed\u8a00\u7279\u5b9a\u5b89\u5168\u7f16\u7801\u80fd\u529b\u8fdb\u884c\u5173\u8054\u5206\u6790", "result": "\u5efa\u7acb\u4e86\u4ee3\u7801\u63d0\u4ea4\u4e0e\u8d21\u732e\u8005\u5b89\u5168\u7f16\u7801\u80fd\u529b\u7684\u5173\u8054\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u4ee3\u7801\u63d0\u4ea4\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ee3\u7801\u5b89\u5168\u6027", "topic": "agent analysis"}}
{"id": "tldr.2511.84a07727", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fxai-working-on-grok-code-remote-to-rival-openai%2F%3Futm_source=tldrai/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/SLM8AIN8RKmq7gdJpXKeINtdoMPu0rW1KT3wOdPDw6U=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fxai-working-on-grok-code-remote-to-rival-openai%2F%3Futm_source=tldrai/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/SLM8AIN8RKmq7gdJpXKeINtdoMPu0rW1KT3wOdPDw6U=430", "authors": ["TLDR Newsletter"], "title": "xAI works on Grok Code Remote to rival OpenAI's Codex", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fxai-working-on-grok-code-remote-to-rival-openai%2F%3Futm_source=tldrai/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/SLM8AIN8RKmq7gdJpXKeINtdoMPu0rW1KT3wOdPDw6U=430", "summary": "xAI works on Grok Code Remote to rival OpenAI's Codex (2 minute read) xAI will host a hackathon on December 6-7. Participants will receive early access to experimental models. The company is working on a feature called Grok Code Remote to align with industry trends towards cloud-based code execution. It will allow users to run code in remote environments directly from the web.", "source": "tldr", "AI": {"tldr": "xAI\u6b63\u5728\u5f00\u53d1Grok Code Remote\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u5728\u8fdc\u7a0b\u73af\u5883\u4e2d\u76f4\u63a5\u8fd0\u884c\u4ee3\u7801\uff0c\u5e76\u5c06\u572812\u67086-7\u65e5\u4e3e\u529e\u9ed1\u5ba2\u9a6c\u62c9\u677e\uff0c\u53c2\u4e0e\u8005\u53ef\u83b7\u5f97\u5b9e\u9a8c\u6a21\u578b\u7684\u65e9\u671f\u8bbf\u95ee\u6743\u9650\u3002", "motivation": "\u4e0e\u884c\u4e1a\u5411\u57fa\u4e8e\u4e91\u7684\u4ee3\u7801\u6267\u884c\u8d8b\u52bf\u4fdd\u6301\u4e00\u81f4\uff0c\u5bf9\u6807OpenAI\u7684Codex\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u4e91\u7aef\u4ee3\u7801\u8fd0\u884c\u4f53\u9a8c\u3002", "method": "\u5f00\u53d1Grok Code Remote\u529f\u80fd\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u76f4\u63a5\u4ece\u7f51\u9875\u5728\u8fdc\u7a0b\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7801\u3002", "result": "\u8ba1\u5212\u572812\u6708\u4e3e\u529e\u9ed1\u5ba2\u9a6c\u62c9\u677e\uff0c\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u5b9e\u9a8c\u6a21\u578b\u7684\u65e9\u671f\u8bbf\u95ee\u6743\u9650\u3002", "conclusion": "xAI\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e91\u7aef\u4ee3\u7801\u6267\u884c\u529f\u80fd\uff0c\u4ee5\u5728\u4ee3\u7801AI\u9886\u57df\u4e0eOpenAI\u7ade\u4e89\u3002", "topic": "swe application"}}
{"id": "tldr.2511.b0f64903", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fw55WVG/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/-97H79I3Kfk9Del0eh1KEokWE0PZS2MTbrynIvEhk3E=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fw55WVG/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/-97H79I3Kfk9Del0eh1KEokWE0PZS2MTbrynIvEhk3E=430", "authors": ["TLDR Newsletter"], "title": "AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model", "comment": "Source: TLDR Newsletter, Date: 2025-11-11, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fw55WVG/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/-97H79I3Kfk9Del0eh1KEokWE0PZS2MTbrynIvEhk3E=430", "summary": "AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model (Reddit Thread) Moonshot AI's Kimi K2 is a Mixture-of-Experts model with 32 billion activated parameters and 1 trillion total parameters. It achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models and also has agentic abilities. Kimi-K2-Base and Kimi-K2-Instruct were recently open sourced. This thread contains a discussion with the team at Moonshot AI.", "source": "tldr", "AI": {"tldr": "Moonshot AI\u53d1\u5e03\u4e86Kimi K2\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u6709320\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c1\u4e07\u4ebf\u603b\u53c2\u6570\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u5728\u77e5\u8bc6\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5177\u5907\u667a\u80fd\u4f53\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u77e5\u8bc6\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5177\u5907\u667a\u80fd\u4f53\u80fd\u529b\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u63a8\u52a8AI\u6280\u672f\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6784\u5efa\u62e5\u6709320\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c1\u4e07\u4ebf\u603b\u53c2\u6570\u7684\u5927\u89c4\u6a21\u6a21\u578b\u3002", "result": "Kimi K2\u5728\u975e\u601d\u8003\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u524d\u6cbf\u77e5\u8bc6\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5177\u5907\u667a\u80fd\u4f53\u80fd\u529b\u3002", "conclusion": "Kimi K2\u6a21\u578b\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684AI\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "tldr.2511.04867092", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbecca.ooo%2Fblog%2Fvertical-integration%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/fOV3ZFfm13ttf8tuRsuzXP6AJm2OvTkUgatxJEgKNQQ=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbecca.ooo%2Fblog%2Fvertical-integration%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/fOV3ZFfm13ttf8tuRsuzXP6AJm2OvTkUgatxJEgKNQQ=431", "authors": ["TLDR Newsletter"], "title": "Vertical Integration is the Only Thing That Matters", "comment": "Source: TLDR Newsletter, Date: 2025-11-12, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbecca.ooo%2Fblog%2Fvertical-integration%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/fOV3ZFfm13ttf8tuRsuzXP6AJm2OvTkUgatxJEgKNQQ=431", "summary": "Vertical Integration is the Only Thing That Matters (10 minute read) Vertical integration, which means tight integration between different tools in a development stack, is necessary for developer productivity. Examples of such vertically integrated workflows include automated build artifact sharing and interactive debugging across languages. While building a vertically integrated development environment as a product is challenging, the glue code connecting different tools is where the real va...", "source": "tldr", "AI": {"tldr": "\u5782\u76f4\u96c6\u6210\uff08\u5de5\u5177\u6808\u4e2d\u4e0d\u540c\u5de5\u5177\u7684\u7d27\u5bc6\u96c6\u6210\uff09\u5bf9\u4e8e\u5f00\u53d1\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6784\u5efa\u5de5\u4ef6\u5171\u4eab\u548c\u8de8\u8bed\u8a00\u4ea4\u4e92\u5f0f\u8c03\u8bd5\u7b49\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u4ef7\u503c\u3002", "motivation": "\u63a2\u8ba8\u5f00\u53d1\u5de5\u5177\u6808\u4e2d\u5782\u76f4\u96c6\u6210\u7684\u91cd\u8981\u6027\uff0c\u8ba4\u4e3a\u8fd9\u662f\u63d0\u5347\u5f00\u53d1\u6548\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5782\u76f4\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u7a0b\u793a\u4f8b\uff08\u5982\u81ea\u52a8\u5316\u6784\u5efa\u5de5\u4ef6\u5171\u4eab\u3001\u8de8\u8bed\u8a00\u4ea4\u4e92\u5f0f\u8c03\u8bd5\uff09\u6765\u9610\u8ff0\u5176\u4ef7\u503c\u3002", "result": "\u5782\u76f4\u96c6\u6210\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u867d\u7136\u6784\u5efa\u5782\u76f4\u96c6\u6210\u7684\u5f00\u53d1\u73af\u5883\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u8fde\u63a5\u4e0d\u540c\u5de5\u5177\u7684\u80f6\u6c34\u4ee3\u7801\u662f\u771f\u6b63\u4ef7\u503c\u6240\u5728\u3002", "conclusion": "\u5782\u76f4\u96c6\u6210\u662f\u5f00\u53d1\u5de5\u5177\u6808\u4e2d\u552f\u4e00\u771f\u6b63\u91cd\u8981\u7684\u56e0\u7d20\uff0c\u5bf9\u4e8e\u5f00\u53d1\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "topic": "swe application"}}
{"id": "tldr.2511.4fe6c2c7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgoogle%2Fadk-go%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/uEnZZiWWSf4stuQQJYLD4vvu126cGAzE3zq3AsBIMW0=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgoogle%2Fadk-go%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/uEnZZiWWSf4stuQQJYLD4vvu126cGAzE3zq3AsBIMW0=431", "authors": ["TLDR Newsletter"], "title": "ADK for Go", "comment": "Source: TLDR Newsletter, Date: 2025-11-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgoogle%2Fadk-go%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/uEnZZiWWSf4stuQQJYLD4vvu126cGAzE3zq3AsBIMW0=431", "summary": "ADK for Go (GitHub Repo) The Agent Development Kit (ADK) is an open-source, code-first Go toolkit designed for building, evaluating, and deploying AI agents. It applies software development principles to AI agent creation and makes the orchestration of agent workflows easier. It is model and deployment-agnostic, and has a tool ecosystem with modular multi-agent systems.", "source": "tldr", "AI": {"tldr": "ADK for Go \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee3\u7801\u4f18\u5148\u7684 Go \u5de5\u5177\u5305\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u90e8\u7f72 AI \u4ee3\u7406\uff0c\u5c06\u8f6f\u4ef6\u5f00\u53d1\u539f\u5219\u5e94\u7528\u4e8e AI \u4ee3\u7406\u521b\u5efa\uff0c\u7b80\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7f16\u6392\u3002", "motivation": "\u5c06\u8f6f\u4ef6\u5f00\u53d1\u539f\u5219\u5e94\u7528\u4e8e AI \u4ee3\u7406\u521b\u5efa\uff0c\u7b80\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u7f16\u6392\uff0c\u63d0\u4f9b\u6a21\u578b\u548c\u90e8\u7f72\u65e0\u5173\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee3\u7801\u4f18\u5148\u7684 Go \u5de5\u5177\u5305\uff0c\u652f\u6301\u6a21\u5757\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f7f\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u7f16\u6392\u66f4\u52a0\u5bb9\u6613\u3002", "result": "\u521b\u5efa\u4e86 ADK for Go\uff0c\u4e00\u4e2a\u6a21\u578b\u548c\u90e8\u7f72\u65e0\u5173\u7684\u5de5\u5177\u5305\uff0c\u5177\u6709\u6a21\u5757\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "ADK for Go \u6210\u529f\u5730\u5c06\u8f6f\u4ef6\u5f00\u53d1\u539f\u5219\u5e94\u7528\u4e8e AI \u4ee3\u7406\u521b\u5efa\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\u5305\u6765\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u90e8\u7f72 AI \u4ee3\u7406\u3002", "topic": "code agent"}}
{"id": "tldr.2511.eb55dbc2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdynalang.github.io%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/HFvEaxmdo9f4-a_JSs32OEDxwA1i4-gc49DZ0P_YCDs=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdynalang.github.io%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/HFvEaxmdo9f4-a_JSs32OEDxwA1i4-gc49DZ0P_YCDs=431", "authors": ["TLDR Newsletter"], "title": "Learning to Model the World with Language", "comment": "Source: TLDR Newsletter, Date: 2025-11-12, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdynalang.github.io%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/HFvEaxmdo9f4-a_JSs32OEDxwA1i4-gc49DZ0P_YCDs=431", "summary": "Learning to Model the World with Language (7 minute read) Dynalang is an agent that learns to understand and uses multiple types of language by predicting future text, images, and rewards within a multimodal world model.", "source": "tldr", "AI": {"tldr": "Dynalang\u662f\u4e00\u4e2a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u6587\u672c\u3001\u56fe\u50cf\u548c\u5956\u52b1\u6765\u5b66\u4e60\u7406\u89e3\u548c\u8fd0\u7528\u591a\u79cd\u8bed\u8a00\u7c7b\u578b\u7684\u667a\u80fd\u4f53", "motivation": "\u6784\u5efa\u80fd\u591f\u7406\u89e3\u548c\u8fd0\u7528\u591a\u79cd\u8bed\u8a00\u7c7b\u578b\u7684\u667a\u80fd\u4f53\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u4ea4\u4e92", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u7684\u6587\u672c\u3001\u56fe\u50cf\u548c\u5956\u52b1\u6765\u8bad\u7ec3\u667a\u80fd\u4f53", "result": "\u5f00\u53d1\u51fa\u4e86Dynalang\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u5b66\u4e60\u7406\u89e3\u548c\u8fd0\u7528\u591a\u79cd\u8bed\u8a00\u7c7b\u578b", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u591a\u6a21\u6001\u4fe1\u606f\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\u7406\u89e3\u548c\u8fd0\u7528\u8bed\u8a00", "topic": "agent analysis"}}
{"id": "wechat.2511.618bff84", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzODE5MzI3MA==&mid=2247483705&idx=1&sn=c5ddb3c8afa34f1fb82d1e3090a643ac&chksm=f1e2894243c25cf6ca3a6af202828181a0b1d18e3975d94379b6365a857a92dfaa63c213de3c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzODE5MzI3MA==&mid=2247483705&idx=1&sn=c5ddb3c8afa34f1fb82d1e3090a643ac&chksm=f1e2894243c25cf6ca3a6af202828181a0b1d18e3975d94379b6365a857a92dfaa63c213de3c#rd", "authors": ["\u5927\u6a21\u578b\u51ef\u51ef"], "title": "\u5c31\u5728\u6628\u5929\uff01\uff01 Google\u65b0\u53d1\u5e0354\u9875Agent\u5f00\u53d1\u6307\u5357\uff1a\u4e94\u5c42\u67b6\u6784\u8be6\u89e3<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-11-13 09:49:34", "summary": "google\u65b0\u53d1\u5e0354\u9875 agent\u5f00\u53d1\u6307\u5357\uff0c \u4e94\u5c42\u67b6\u6784\u8be6\u89e3 agentic ai google introduction to agents google table of contents from predictive ai to autonomous agents introduction to ai agents------_ ...... .. 8 the agentic problem-solving process 5\uff0c\uff0c10 a taxonomy of agentic systems.----. -....-", "AI": {"tldr": "google\u65b0\u53d1\u5e0354\u9875 agent\u5f00\u53d1\u6307\u5357\uff0c \u4e94\u5c42\u67b6\u6784\u8be6\u89e3 agentic ai google introduction to agents google table of contents from predictive ai to autonomous agents introduction to ai agents------_ ...... .. 8 the agentic proble...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.4ac9d2fc", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490171&idx=1&sn=c5b854503e6fa4e6c60c9f28eae3172f&chksm=9a771cf46a46fbf44b86afff6b9b6a752e5bb1e0f4eb22b6462900434e0310850d662da7f1d1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490171&idx=1&sn=c5b854503e6fa4e6c60c9f28eae3172f&chksm=9a771cf46a46fbf44b86afff6b9b6a752e5bb1e0f4eb22b6462900434e0310850d662da7f1d1#rd", "authors": ["\u7a76\u6a21\u667a"], "title": "5\u5f20\u52a8\u56fe\u770b\u61c2\u6784\u5efa<em class=\"highlight\">Agentic</em> AI \u7684\u4e3b\u6d41\u8bbe\u8ba1\u6a21\u5f0f", "comment": "Source: WeChat, Published: 2025-11-13 09:27:33", "summary": "Agentic AI\uff08\u667a\u80fd\u4f53\uff09\u7684\u5d1b\u8d77\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u8bc4\u4f30\u3001\u89c4\u5212\u4e0e\u534f\u4f5c\u7b49\u884c\u4e3a\uff0c\u4f7f\u6a21\u578b\u5177\u5907\u4e86\u6301\u7eed\u6f14\u8fdb\u3001\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u672c\u6587\u5c06\u6df1\u5165\u63a2\u8ba8\u6784\u5efaAgentic AI \u76845\u79cd\u4e3b\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u6df1\u5165\u62c6\u89e3\u5176\u8fd0\u4f5c\u903b\u8f91\u3001\u5178\u578b\u5e94\u7528\u573a\u666f\u4e0e\u6838\u5fc3\u4ef7\u503c\u3002", "AI": {"tldr": "Agentic AI\uff08\u667a\u80fd\u4f53\uff09\u7684\u5d1b\u8d77\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u8bc4\u4f30\u3001\u89c4\u5212\u4e0e\u534f\u4f5c\u7b49\u884c\u4e3a\uff0c\u4f7f\u6a21\u578b\u5177\u5907\u4e86\u6301\u7eed\u6f14\u8fdb\u3001\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u672c\u6587\u5c06\u6df1\u5165\u63a2\u8ba8\u6784\u5efaAgentic AI \u76845\u79cd\u4e3b\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u6df1\u5165\u62c6\u89e3\u5176\u8fd0\u4f5c\u903b\u8f91\u3001\u5178\u578b\u5e94\u7528\u573a\u666f\u4e0e\u6838\u5fc3\u4ef7\u503c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.10ae1536", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzODExNDE4NQ==&mid=2247483680&idx=1&sn=0c02ffc11f56c60c6eb5f2bdfc8b24d6&chksm=f1ee3a9c9df7998511cee94d55d377ae03a999717a827a3aa676b5b405b1c8839d32f94c3375#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzODExNDE4NQ==&mid=2247483680&idx=1&sn=0c02ffc11f56c60c6eb5f2bdfc8b24d6&chksm=f1ee3a9c9df7998511cee94d55d377ae03a999717a827a3aa676b5b405b1c8839d32f94c3375#rd", "authors": ["\u51fa\u6d77\u62a4\u822a\u8005"], "title": "\u7528\u4eba\u7c7b\u7684\u201c\u6e29\u5ea6\u201d\u62e5\u62b1AI\uff01\u4eba\u673a\u5171\u9a7e\u65f6\u4ee3\uff0c<em class=\"highlight\">Agentic</em> AI\u5982\u4f55\u91cd\u5851\u5ba2\u670d\u7684\u201c\u5171\u60c5\u529b\u8fb9\u754c\u201d\uff1f", "comment": "Source: WeChat, Published: 2025-11-13 08:25:29", "summary": "\u4ec0\u4e48\u662f Agentic AI\uff1fAgentic AI\uff0c\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u6709\u201c\u6267\u884c\u6743\u201d\u7684AI\u3002\u5b83\u4e0d\u518d\u662f\u4f20\u7edf\u5ba2\u670d\u673a\u5668\u4eba\u90a3\u6837\u53ea\u80fd\u88ab\u52a8\u5730\u201c\u95ee\u7b54\u201d\u6216\u201c\u63d0\u4f9b\u4fe1\u606f\u201d\uff0c\u800c\u662f\u80fd\u591f\u50cf\u4eba\u7c7b\u7279\u5de5\u4e00\u6837\uff0c\u63a5\u6536\u4e00\u4e2a\u6a21\u7cca\u7684\u76ee\u6807\uff08\u5982\u201c\u6211\u9700\u8981\u9000\u8d27\u5e76\u6362\u6210\u9ed1\u8272\u6b3e\u201d\uff09\uff0c\u7136\u540e\u81ea\u4e3b", "AI": {"tldr": "\u4ec0\u4e48\u662f Agentic AI\uff1fAgentic AI\uff0c\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u6709\u201c\u6267\u884c\u6743\u201d\u7684AI\u3002\u5b83\u4e0d\u518d\u662f\u4f20\u7edf\u5ba2\u670d\u673a\u5668\u4eba\u90a3\u6837\u53ea\u80fd\u88ab\u52a8\u5730\u201c\u95ee\u7b54\u201d\u6216\u201c\u63d0\u4f9b\u4fe1\u606f\u201d\uff0c\u800c\u662f\u80fd\u591f\u50cf\u4eba\u7c7b\u7279\u5de5\u4e00\u6837\uff0c\u63a5\u6536\u4e00\u4e2a\u6a21\u7cca\u7684\u76ee\u6807\uff08\u5982\u201c\u6211\u9700\u8981\u9000\u8d27\u5e76\u6362\u6210\u9ed1\u8272\u6b3e\u201d\uff09\uff0c\u7136\u540e\u81ea\u4e3b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.438ba598", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDUxMjkxOQ==&mid=2247489345&idx=1&sn=ab182089d275af6347eafeed607a4c13&chksm=c557e80de6ae2b4d688b93acc737f28de4d59ff38979e1c5889088185d2ef2164b71163ac9df#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDUxMjkxOQ==&mid=2247489345&idx=1&sn=ab182089d275af6347eafeed607a4c13&chksm=c557e80de6ae2b4d688b93acc737f28de4d59ff38979e1c5889088185d2ef2164b71163ac9df#rd", "authors": ["AAIA\u5177\u8eab\u667a\u80fd\u4e0e\u7a7a\u95f4\u667a\u80fd\u5206\u4f1a"], "title": "\u3010\u706b\u7206\u5168\u7f51\u3011AI\u300e<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u300f\u5728\u672a\u6765\u4ea7\u4e1a\u521b\u65b0\u4e0a\u7684\u524d\u6cbf\u5e94\u7528\u4e0e\u53d1\u5c55\u8d8b\u52bf", "comment": "Source: WeChat, Published: 2025-11-13 05:15:34", "summary": "- Evolution\uff08\u6f14\u5316\uff09\u4e8c\u3001Agent vs. Agentic AI\uff1a\u533a\u522b\u4e0e\u6f14\u8fdb\u7279\u5f81\u7279\u5f81 Al Agent Agentic Al\u81ea\u4e3b\u6027 \u8f83\u4f4e \u9ad8\u534f\u4f5c\u80fd\u529b \u5355\u70b9 \u591a\u667a\u80fd\u4f53\u534f\u540c\u8bb0\u5fc6\u80fd\u529b \u65e0/\u5f31 \u6301\u4e45\u8bb0\u5fc6\u63a8\u7406\u4e0e\u89c4\u5212 \u7b80\u5355 \u9ad8\u7ea7\u63a8\u7406\u4e0e\u4efb\u52a1\u5206\u89e3", "AI": {"tldr": "- Evolution\uff08\u6f14\u5316\uff09\u4e8c\u3001Agent vs. Agentic AI\uff1a\u533a\u522b\u4e0e\u6f14\u8fdb\u7279\u5f81\u7279\u5f81 Al Agent Agentic Al\u81ea\u4e3b\u6027 \u8f83\u4f4e \u9ad8\u534f\u4f5c\u80fd\u529b \u5355\u70b9 \u591a\u667a\u80fd\u4f53\u534f\u540c\u8bb0\u5fc6\u80fd\u529b \u65e0/\u5f31 \u6301\u4e45\u8bb0\u5fc6\u63a8\u7406\u4e0e\u89c4\u5212 \u7b80\u5355 \u9ad8\u7ea7\u63a8\u7406\u4e0e\u4efb\u52a1\u5206\u89e3", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.1ef37d69", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247499296&idx=1&sn=53c2a4d26a4c06993c71c7825cc6b1cd&chksm=c3d6b1269a2044d9dd7c266b2b7c0061aeac541633fa2f4a615d3ceda386f9ee3100d469c68a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247499296&idx=1&sn=53c2a4d26a4c06993c71c7825cc6b1cd&chksm=c3d6b1269a2044d9dd7c266b2b7c0061aeac541633fa2f4a615d3ceda386f9ee3100d469c68a#rd", "authors": ["PaperAgent"], "title": "\u5fae\u8f6f <em class=\"highlight\">Agentic</em> \u7ec4\u7ec7\uff1a\u4e0b\u4e00\u4ee3 AI \u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-11-13 03:51:19", "summary": "\u88681\uff1aAgentic Organization\u6982\u5ff5\u4e0e\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u4f18\u96c5\u7c7b\u6bd4\u56db\u5927\u52a8\u4f5c\u6807\u7b7e\u6574\u4e2a\u7cfb\u7edf\u901a\u8fc7\u56db\u4e2a\u7b80\u5355\u7684\u6587\u672c\u6807\u7b7e\u5b9e\u73b0\u590d\u6742\u534f\u540c\uff1a\u5b50\u4efb\u52a1\u63cf\u8ff0\uff1a\u7ec4\u7ec7\u8005\u5411\u7a7a\u95f2\u5de5\u4ebai\u5206\u914d\u5b50\u67e5\u8be2", "AI": {"tldr": "\u88681\uff1aAgentic Organization\u6982\u5ff5\u4e0e\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u4f18\u96c5\u7c7b\u6bd4\u56db\u5927\u52a8\u4f5c\u6807\u7b7e\u6574\u4e2a\u7cfb\u7edf\u901a\u8fc7\u56db\u4e2a\u7b80\u5355\u7684\u6587\u672c\u6807\u7b7e\u5b9e\u73b0\u590d\u6742\u534f\u540c\uff1a\u5b50\u4efb\u52a1\u63cf\u8ff0\uff1a\u7ec4\u7ec7\u8005\u5411\u7a7a\u95f2\u5de5\u4ebai\u5206\u914d\u5b50\u67e5\u8be2", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.77bf77b9", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483786&idx=1&sn=73e4034061da41ac757c3f9705ff34d1&chksm=f16824378479802aa4940abdb73f6ccc7f1f3d1dbf0a997ba4338322c2e5b71fdbd7bc3f5633#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483786&idx=1&sn=73e4034061da41ac757c3f9705ff34d1&chksm=f16824378479802aa4940abdb73f6ccc7f1f3d1dbf0a997ba4338322c2e5b71fdbd7bc3f5633#rd", "authors": ["EasyShip.AI"], "title": "\u4e00\u6587\u5168\u89e3\u6790\uff01<em class=\"highlight\">Agentic</em> AI\u8bb0\u5fc6\u673a\u5236\u76849\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff01", "comment": "Source: WeChat, Published: 2025-11-13 03:14:21", "summary": "\u5728\u8fd9\u7bc7\u535a\u5ba2\u4e2d\uff0c\u6211\u4eec\u5c06\u7f16\u5199\u5e76\u8bc4\u4f309\u79cd\u9488\u5bf9AI\u4ee3\u7406\u7684\u4ece\u521d\u7ea7\u5230\u9ad8\u7ea7\u7684\u5185\u5b58\u4f18\u5316\u6280\u672f\u3002in this blog\uff0c we will code and evaluate 9 beginner-to-advanced memory optimization techniques for ai agents. you will learn how to apply each technique\uff0c along with their advantages and dra", "AI": {"tldr": "\u5728\u8fd9\u7bc7\u535a\u5ba2\u4e2d\uff0c\u6211\u4eec\u5c06\u7f16\u5199\u5e76\u8bc4\u4f309\u79cd\u9488\u5bf9AI\u4ee3\u7406\u7684\u4ece\u521d\u7ea7\u5230\u9ad8\u7ea7\u7684\u5185\u5b58\u4f18\u5316\u6280\u672f\u3002in this blog\uff0c we will code and evaluate 9 beginner-to-advanced memory optimization techniques for ai agents. you will learn how to apply each technique\uff0c along wit...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.9a098c83", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzI1NDI3MA==&mid=2247485150&idx=1&sn=bf07952de56cd3f5e0b33763cf21f2a0&chksm=fe903492ad0d782a9ae0aa51e7c46eff0fe65cadaafe0c3afdefaba5c3229626d31923f7eb74#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzI1NDI3MA==&mid=2247485150&idx=1&sn=bf07952de56cd3f5e0b33763cf21f2a0&chksm=fe903492ad0d782a9ae0aa51e7c46eff0fe65cadaafe0c3afdefaba5c3229626d31923f7eb74#rd", "authors": ["AI\u836f\u7269\u8bbe\u8ba1\u5b9e\u9a8c\u5ba4"], "title": "\u836f\u7269\u7814\u53d1\u8fdb\u5165 \u201c\u5c0f\u65f6\u4ee3\u201d\uff1a<em class=\"highlight\">Agentic</em> AI \u843d\u5730\u5143\u5e74\uff0c\u6848\u4f8b + \u67b6\u6784\u5b9a\u4e49\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-11-13 02:01:32", "summary": "\u7cfb\u7edf\u68b3\u7406\u4e86 agentic AI \u7684\u6982\u5ff5\u3001\u5e38\u89c1\u67b6\u6784\u4e0e\u5de5\u5177\u7ec4\u5408\uff0c\u5e76\u7528\u591a\u4e2a\u771f\u5b9e\u9879\u76ee\u5c55\u793a\u4e86\u5b83\u5728\u6587\u732e\u7efc\u5408\u3001\u6bd2\u7406\u8bc4\u4f30\u3001\u81ea\u52a8\u5316\u5b9e\u9a8c\u3001\u5206\u5b50\u5408\u6210\u3001\u7f55\u89c1\u75c5\u7528\u836f\u91cd\u5b9a\u4f4d\u4e0e\u5546\u4e1a\u51b3\u7b56\u4e2d\u7684\u6548\u679c\uff0c\u6838\u5fc3\u7ed3\u8bba\u662f\u8bb8\u591a\u8fc7\u53bb\u9700\u8981\u6570\u5468\u5230\u6570\u6708\u7684\u73af\u8282\u88ab\u538b\u7f29\u5230\u5c0f\u65f6", "AI": {"tldr": "\u7cfb\u7edf\u68b3\u7406\u4e86 agentic AI \u7684\u6982\u5ff5\u3001\u5e38\u89c1\u67b6\u6784\u4e0e\u5de5\u5177\u7ec4\u5408\uff0c\u5e76\u7528\u591a\u4e2a\u771f\u5b9e\u9879\u76ee\u5c55\u793a\u4e86\u5b83\u5728\u6587\u732e\u7efc\u5408\u3001\u6bd2\u7406\u8bc4\u4f30\u3001\u81ea\u52a8\u5316\u5b9e\u9a8c\u3001\u5206\u5b50\u5408\u6210\u3001\u7f55\u89c1\u75c5\u7528\u836f\u91cd\u5b9a\u4f4d\u4e0e\u5546\u4e1a\u51b3\u7b56\u4e2d\u7684\u6548\u679c\uff0c\u6838\u5fc3\u7ed3\u8bba\u662f\u8bb8\u591a\u8fc7\u53bb\u9700\u8981\u6570\u5468\u5230\u6570\u6708\u7684\u73af\u8282\u88ab\u538b\u7f29\u5230\u5c0f\u65f6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.1425b47d", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDExMjE2OA==&mid=2247487496&idx=3&sn=c229eb371b3097da06a38d6fb04400f6&chksm=c539e1a05ba384ce1b7f2b9215b0c7a775eabda9b456970ea7b4e1d05bf03545be4e95682ac7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDExMjE2OA==&mid=2247487496&idx=3&sn=c229eb371b3097da06a38d6fb04400f6&chksm=c539e1a05ba384ce1b7f2b9215b0c7a775eabda9b456970ea7b4e1d05bf03545be4e95682ac7#rd", "authors": ["\u8c08\u8c08\u51fa\u6d77"], "title": "\u8c08\u8c08\u51fa\u6d77 | \u9648\u5955\u5f3a\u2014\u2014<em class=\"highlight\">Agentic</em> AI\u4e0eAI Agents\uff1a\u4eba\u5de5\u667a\u80fd\u7684\u4e0b\u4e00\u573a\u9769\u547d", "comment": "Source: WeChat, Published: 2025-11-13 01:21:55", "summary": "\u6362\u8a00\u4e4b\uff0cAgentic AI \u4e0d\u53ea\u662f\u201c\u56de\u7b54\u95ee\u9898\u201d\u7684\u673a\u5668\uff0c\u800c\u662f\u80fd\u591f\u201c\u5bfb\u627e\u7b54\u6848\u3001\u5224\u65ad\u4f18\u5148\u7ea7\u3001\u534f\u8c03\u8d44\u6e90\u201d\u7684\u201c\u884c\u52a8\u578b\u667a\u80fd\u4f53\u201d\u3002\u5982\u679c\u8bf4AI Agent\u662f\u4e00\u4f4d\u80fd\u5e72\u7684\u52a9\u7406\uff0c\u90a3\u4e48Agentic AI\u5219\u66f4\u50cf\u662f\u4e00\u4f4d\u5177\u5907\u5224\u65ad\u529b\u4e0e\u8d23\u4efb\u611f\u7684\u9879\u76ee\u7ecf\u7406\u3002", "AI": {"tldr": "\u6362\u8a00\u4e4b\uff0cAgentic AI \u4e0d\u53ea\u662f\u201c\u56de\u7b54\u95ee\u9898\u201d\u7684\u673a\u5668\uff0c\u800c\u662f\u80fd\u591f\u201c\u5bfb\u627e\u7b54\u6848\u3001\u5224\u65ad\u4f18\u5148\u7ea7\u3001\u534f\u8c03\u8d44\u6e90\u201d\u7684\u201c\u884c\u52a8\u578b\u667a\u80fd\u4f53\u201d\u3002\u5982\u679c\u8bf4AI Agent\u662f\u4e00\u4f4d\u80fd\u5e72\u7684\u52a9\u7406\uff0c\u90a3\u4e48Agentic AI\u5219\u66f4\u50cf\u662f\u4e00\u4f4d\u5177\u5907\u5224\u65ad\u529b\u4e0e\u8d23\u4efb\u611f\u7684\u9879\u76ee\u7ecf\u7406\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.2f3ef349", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484293&idx=1&sn=ffc09cbdf9348193b954982e4138a5ee&chksm=c09d12087f379221a9ca0241fbdacb7c317b396dc11b635ea747ddedc5e028119ed35f0e20be#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484293&idx=1&sn=ffc09cbdf9348193b954982e4138a5ee&chksm=c09d12087f379221a9ca0241fbdacb7c317b396dc11b635ea747ddedc5e028119ed35f0e20be#rd", "authors": ["AI Lab Dev"], "title": "<em class=\"highlight\">Agentic</em>21\u79cd\u8bbe\u8ba1\u6a21\u5f0f5-Tool Use (Function Calling)", "comment": "Source: WeChat, Published: 2025-11-13 00:12:18", "summary": "\u7528\u4f8b\uff1a\u7535\u5b50\u5546\u52a1\u5ba2\u670dAgentTools\uff1a\u901a\u8fc7API\u8c03\u7528\u6765\u67e5\u8be2\u4ea7\u54c1\u5e93\u5b58\u3001\u83b7\u53d6\u8ba2\u5355\u72b6\u6001\u6216\u5904\u7406\u4ed8\u6b3e\u3002Agent Flow\uff1a\u5f53\u7528\u6237\u8be2\u95ee\u201cX\u4ea7\u54c1\u6709\u5e93\u5b58\u5417\uff1f\u201d\u65f6\uff0cLLM\u4f1a\u901a\u8fc7API\u8c03\u7528\u5e93\u5b58\u67e5\u8be2\u7cfb\u7edf\uff1b", "AI": {"tldr": "\u7528\u4f8b\uff1a\u7535\u5b50\u5546\u52a1\u5ba2\u670dAgentTools\uff1a\u901a\u8fc7API\u8c03\u7528\u6765\u67e5\u8be2\u4ea7\u54c1\u5e93\u5b58\u3001\u83b7\u53d6\u8ba2\u5355\u72b6\u6001\u6216\u5904\u7406\u4ed8\u6b3e\u3002Agent Flow\uff1a\u5f53\u7528\u6237\u8be2\u95ee\u201cX\u4ea7\u54c1\u6709\u5e93\u5b58\u5417\uff1f\u201d\u65f6\uff0cLLM\u4f1a\u901a\u8fc7API\u8c03\u7528\u5e93\u5b58\u67e5\u8be2\u7cfb\u7edf\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.055ff87e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjYyODQ4NA==&mid=2247486416&idx=1&sn=2c542ffd9a47c1caecf4834fdc880c2d&chksm=faeb4fa9ea382b1ea02982786c5af9cc9e72899bdc4258df98a57495b2df0f42f0eaba7d23ce#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjYyODQ4NA==&mid=2247486416&idx=1&sn=2c542ffd9a47c1caecf4834fdc880c2d&chksm=faeb4fa9ea382b1ea02982786c5af9cc9e72899bdc4258df98a57495b2df0f42f0eaba7d23ce#rd", "authors": ["\u6570\u636e\u62fe\u5149\u8005"], "title": "AI\u90a3\u4e9b\u8da3\u4e8b\u7cfb\u5217108\uff1a\u4e00\u6587\u8f7b\u677e\u8bfb\u61c2 LLaMA \u7cfb\u5217<em class=\"highlight\">\u6a21\u578b</em>\uff1a\u4ece Meta \u5f00\u6e90\u7206\u6b3e\u5230 AI \u751f\u6001\u57fa\u77f3", "comment": "Source: WeChat, Published: 2025-11-13 12:51:30", "summary": "\u5b83\u7684\u51fa\u73b0\uff0c\u76f4\u63a5\u6253\u7834\u4e86 \u201c\u5927\u6a21\u578b = \u95ed\u6e90\u201d \u7684\u56fa\u6709\u8ba4\u77e5 \u2014\u2014 \u5728\u6b64\u4e4b\u524d\uff0c\u53ea\u6709 OpenAI\u3001\u8c37\u6b4c\u7b49\u5de8\u5934\u80fd\u73a9\u5f97\u8f6c\u5927\u6a21\u578b\uff0c\u666e\u901a\u5f00\u53d1\u8005\u8fde\u63a5\u89e6\u7684\u673a\u4f1a\u90fd\u6ca1\u6709\u3002", "AI": {"tldr": "\u5b83\u7684\u51fa\u73b0\uff0c\u76f4\u63a5\u6253\u7834\u4e86 \u201c\u5927\u6a21\u578b = \u95ed\u6e90\u201d \u7684\u56fa\u6709\u8ba4\u77e5 \u2014\u2014 \u5728\u6b64\u4e4b\u524d\uff0c\u53ea\u6709 OpenAI\u3001\u8c37\u6b4c\u7b49\u5de8\u5934\u80fd\u73a9\u5f97\u8f6c\u5927\u6a21\u578b\uff0c\u666e\u901a\u5f00\u53d1\u8005\u8fde\u63a5\u89e6\u7684\u673a\u4f1a\u90fd\u6ca1\u6709\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.19ce0dc7", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzMjE4NzgyMg==&mid=2247483748&idx=1&sn=724bd4b2a941fee4aa271af99bd6f3c7&chksm=f145bf355ccd064b14afa181ed638f7e6d0fcbb81ccb57d442c8de22002592299257f6dd001c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzMjE4NzgyMg==&mid=2247483748&idx=1&sn=724bd4b2a941fee4aa271af99bd6f3c7&chksm=f145bf355ccd064b14afa181ed638f7e6d0fcbb81ccb57d442c8de22002592299257f6dd001c#rd", "authors": ["\u7334\u7684\u81ea\u6211\u4fee\u517b"], "title": "\u56fd\u5185AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u201c\u6218\u56fd\u6740\u201d\uff1a\u8c01\u4f1a\u662f\u8d62\u5bb6\uff1f", "comment": "Source: WeChat, Published: 2025-11-13 10:46:24", "summary": "\u5728\u4eca\u65e5\u7684 2025 \u767e\u5ea6\u4e16\u754c\u5927\u4f1a\u4e0a\uff0c\u767e\u5ea6\u521b\u59cb\u4eba\u674e\u5f66\u5b8f\u6b63\u5f0f\u53d1\u5e03\u6587\u5fc3\u5927\u6a21\u578b 5.0\u3002\u53c2\u6570\u89c4\u6a21\u8d85 2.4 \u4e07\u4ebf\uff0c\u539f\u751f\u5168\u6a21\u6001\u7406\u89e3\u3001\u521b\u610f\u5199\u4f5c\u3001\u667a\u80fd\u4f53\u89c4\u5212\u3001\u6307\u4ee4\u9075\u5faa\u7b49\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "AI": {"tldr": "\u5728\u4eca\u65e5\u7684 2025 \u767e\u5ea6\u4e16\u754c\u5927\u4f1a\u4e0a\uff0c\u767e\u5ea6\u521b\u59cb\u4eba\u674e\u5f66\u5b8f\u6b63\u5f0f\u53d1\u5e03\u6587\u5fc3\u5927\u6a21\u578b 5.0\u3002\u53c2\u6570\u89c4\u6a21\u8d85 2.4 \u4e07\u4ebf\uff0c\u539f\u751f\u5168\u6a21\u6001\u7406\u89e3\u3001\u521b\u610f\u5199\u4f5c\u3001\u667a\u80fd\u4f53\u89c4\u5212\u3001\u6307\u4ee4\u9075\u5faa\u7b49\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.117576e6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247842352&idx=4&sn=55010d957c3b6835c0f97af22cc50327&chksm=e9af37245570963bd71186cdf96dd113a7a56c4998e5e1e0605ee8e5d30cdffd775e6bea6cfd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247842352&idx=4&sn=55010d957c3b6835c0f97af22cc50327&chksm=e9af37245570963bd71186cdf96dd113a7a56c4998e5e1e0605ee8e5d30cdffd775e6bea6cfd#rd", "authors": ["\u91cf\u5b50\u4f4d"], "title": "\u4e00\u4e2a\u6a21\u578b\u8bfb\u61c2\u6240\u6709\u533b\u5b66\u6570\u636e\uff0cHulu-Med\u63a2\u7d22\u533b\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5f00\u6e90\u65b0\u8303\u5f0f | \u6d59\u5927x\u4e0a\u4ea4xUIUC", "comment": "Source: WeChat, Published: 2025-11-13 09:21:51", "summary": "\u800c\u4e14\u4f5c\u4e3a\u5f00\u6e90\u6a21\u578b\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u5747\u6765\u81ea\u516c\u5f00\u533b\u5b66\u6570\u636e\u96c6\u53ca\u81ea\u7814\u5408\u6210\u6570\u636e\uff0c\u4e0d\u4ec5\u80fd\u5927\u5e45\u5ea6\u964d\u4f4eGPU\u8bad\u7ec3\u6210\u672c\uff0c\u66f4\u662f\u572830\u9879\u6743\u5a01\u8bc4\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5ab2\u7f8eGPT-4.1\u7b49\u95ed\u6e90\u6a21\u578b\u7684\u4f18\u5f02\u6027\u80fd\u3002", "AI": {"tldr": "\u800c\u4e14\u4f5c\u4e3a\u5f00\u6e90\u6a21\u578b\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u5747\u6765\u81ea\u516c\u5f00\u533b\u5b66\u6570\u636e\u96c6\u53ca\u81ea\u7814\u5408\u6210\u6570\u636e\uff0c\u4e0d\u4ec5\u80fd\u5927\u5e45\u5ea6\u964d\u4f4eGPU\u8bad\u7ec3\u6210\u672c\uff0c\u66f4\u662f\u572830\u9879\u6743\u5a01\u8bc4\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5ab2\u7f8eGPT-4.1\u7b49\u95ed\u6e90\u6a21\u578b\u7684\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
