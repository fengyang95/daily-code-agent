<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [wechat.article](#wechat.article) [Total: 17]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 20]
- [tldr.article](#tldr.article) [Total: 12]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

TL;DR: 提出自我重写框架，通过让模型重写自身推理文本并从中学习，改善推理过程质量，在保持GRPO奖励信号的同时提升准确率并显著缩短推理长度。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习仅关注最终正确性的奖励无法提供对内部推理过程的详细监督，导致推理质量不佳，出现过度思考、思考不足、冗余思考和思维混乱等问题。

Method: 采用选择性重写方法，仅对模型一致正确的"简单"样本进行重写，将重写和原始生成编译在单个批次中，保持强化学习算法的可扩展性。

Result: 在准确率-长度权衡方面，自我重写方法在准确率提升0.6%的同时显著缩短推理长度46%；在内部推理质量方面，LLM作为评判者的评分显著提高7.2分。

Conclusion: 自我重写框架有效改善了推理过程质量，成功缓解了内部推理缺陷，在多种任务和模型规模下验证了其有效性。

Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [2] [Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks](https://arxiv.org/abs/2511.16540)
*Éloïse Benito-Rodriguez,Einar Urdshals,Jasmina Nasufi,Nicky Pochinkov*

Main category: cs.CL

TL;DR: 该研究提出了一种基于LLM激活状态预测文本体裁的框架，使用Mistral-7B模型和scikit-learn分类器，在两种数据集上分别达到98%和71%的F1分数，证明浅层学习模型可以从LLM中推断文本体裁。


<details>
  <summary>Details</summary>
Motivation: 理解LLM对于确保其安全有益部署至关重要，但由于LLM结构难以解释且无法对所有输出进行人工评估，因此需要开发预测性框架。

Method: 使用Mistral-7B模型，基于其激活状态预测文本体裁，采用scikit-learn分类器进行训练和评估。

Result: 在两个数据集上分别获得98%和71%的F1分数，结果持续优于控制任务。

Conclusion: 提供了概念验证，表明文本体裁可以通过浅层学习模型从LLM中推断出来。

Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [3] [高性能<em class="highlight">强化学习</em>权重交换框架 Awex 开源，瞬息千里，举重若轻](http://mp.weixin.qq.com/s?__biz=MzI0Nzc3MTQyMw==&mid=2247536204&idx=1&sn=f9224c857711bfeab1cc7c493c1d079f&chksm=e8fc0544bb5ee81c62091742d7d565c016e86068c2c587eb5d7f84800c421b1449b6f41d2a28#rd)
*蚂蚁技术AntTech*

Main category: wechat.article

TL;DR: 过去几年，强化学习已成为推动大模型边界扩张的核心技术。从 ChatGPT 的 RLHF，到 DeepSeek、Claude、Llama 的后训练体系，无不依赖强化学习让模型更符合人类偏好、具备更强的推理能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 过去几年，强化学习已成为推动大模型边界扩张的核心技术。从 ChatGPT 的 RLHF，到 DeepSeek、Claude、Llama 的后训练体系，无不依赖强化学习让模型更符合人类偏好、具备更强的推理能力。

</details>


### [4] [全面超越GRPO！阿里提出新型<em class="highlight">强化学习</em>框架，可同时增强LLM和MLLM推理能力](http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449997274&idx=1&sn=159ffb0076dfb45056e7b2b0b9e7f99c&chksm=b054ec2e5433340cdfffd33584a22829dd9c8705348a8c5ca6681fac75067e562e1b322c8dc2#rd)
*智猩猩AI*

Main category: wechat.article

TL;DR: （2）课程强化学习按难度对数据集 D 进行排序后，将其划分为 K 个大小相等的连续桶 {B1，B2，…，BK}：其中， q1，…，q∣D∣ 按从易到难的顺序排列。当前的训练子集 Dc 初始化为第一个桶 B1。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （2）课程强化学习按难度对数据集 D 进行排序后，将其划分为 K 个大小相等的连续桶 {B1，B2，…，BK}：其中， q1，…，q∣D∣ 按从易到难的顺序排列。当前的训练子集 Dc 初始化为第一个桶 B1。

</details>


### [5] [「注意力机制+<em class="highlight">强化学习</em>」重磅突破！荣登Science顶级子刊！](http://mp.weixin.qq.com/s?__biz=Mzg3ODQ2NzMwNA==&mid=2247491211&idx=8&sn=d70738cfec5e008310fac500adf886c5&chksm=ce5c24ded72a9f24356d548f63be0abfb5916efaf88facb209b01a95775e57726a424d88824a#rd)
*深度之眼资料库*

Main category: wechat.article

TL;DR: 提出两阶段强化学习训练 pipeline，先在基础地形上初始化地图编码学习，再引入复杂地形与不确定性微调，兼顾泛化能力与鲁棒性。构建端到端的整体控制框架，无需依赖模型预测控制等上层规划模块，直接将感知信息映射为关


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 提出两阶段强化学习训练 pipeline，先在基础地形上初始化地图编码学习，再引入复杂地形与不确定性微调，兼顾泛化能力与鲁棒性。构建端到端的整体控制框架，无需依赖模型预测控制等上层规划模块，直接将感知信息映射为关

</details>


### [6] [DRL最新彩色中文版-《深度<em class="highlight">强化学习</em> 基础、研究与应用》免费书籍分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247572513&idx=2&sn=1a3f20f9063fdcfc14b0b478d19c7fe7&chksm=96a0c35bac9ea0ffa860076ecd055f39c733903680ea9232e591fdd2bddabef2436f1da6a387#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 本书目录 内容截图本书免费pdf下载地址


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本书目录 内容截图本书免费pdf下载地址

</details>


### [7] [「Xiaomi HAD 增强版」三问：什么是<em class="highlight">强化学习</em>？什么是世界模型？体验升级在哪里？](http://mp.weixin.qq.com/s?__biz=MzkyNzU3MDI3Nw==&mid=2247508744&idx=1&sn=76ed7d46de55040b15e8c36fab0af50e&chksm=c388bd7324fca000b601e5aaca4ed87fac9e2b7002a74a32a3c78d9e3053dc5c6347c8c9c7d5#rd)
*小米汽车*

Main category: wechat.article

TL;DR: 做好强化学习的关键有两个，一个是高保真的世界模型，让模型在虚拟环境中也能像在真实道路上一样学习；另一个是高效率的训练框架，能让模型以更快的速度和更高的质量积累经验、持续优化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 做好强化学习的关键有两个，一个是高保真的世界模型，让模型在虚拟环境中也能像在真实道路上一样学习；另一个是高效率的训练框架，能让模型以更快的速度和更高的质量积累经验、持续优化。

</details>


### [8] [建议收藏！一文拆解 <em class="highlight">Agentic</em> AI 的 7 大核心组件！](http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247484117&idx=1&sn=1cb7de00f71a6eb5ce9824d88a87893e&chksm=f1d87c47cbe13fec3e83e505fd240a92f75eeff58d8a1344e1af8ad05c593cbf4cfc55ee24f9#rd)
*AI次生代*

Main category: wechat.article

TL;DR: 执行（Execution）层是 Agentic AI 与数字世界甚至物理世界交互的桥梁。它不仅仅是生成文本，更是通过“工具调用”和“API 调用”来操作外部软件。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 执行（Execution）层是 Agentic AI 与数字世界甚至物理世界交互的桥梁。它不仅仅是生成文本，更是通过“工具调用”和“API 调用”来操作外部软件。

</details>


### [9] [Agent第02集：<em class="highlight">Agentic</em> AI跟多<em class="highlight">智能体</em>的差别](http://mp.weixin.qq.com/s?__biz=MzU2MjQyMTA0Mw==&mid=2247484599&idx=1&sn=64196fef572b01ceea0081e2e8a3ae2d&chksm=fdd1583c6fd3fe0d71217b4038dfd885e1d3342cda958b734f76adef2b76b72d4c77ebcbe86a#rd)
*红星洞察者*

Main category: wechat.article

TL;DR: Agentic AI代表着AI从“工具”到“合作伙伴”的质变——它不再只是听从指令，而是能够独当一面完成任务。二、团队协作艺术：多智能体系统又是什么？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI代表着AI从“工具”到“合作伙伴”的质变——它不再只是听从指令，而是能够独当一面完成任务。二、团队协作艺术：多智能体系统又是什么？

</details>


### [10] [AI洞察 | 红杉中国：“从工具到伙伴”，用<em class="highlight">Agentic</em> AI开启企业数智化转型的“无限游戏”的下一程](http://mp.weixin.qq.com/s?__biz=MzA3NTgwMzkyMw==&mid=2650475358&idx=1&sn=c2cd99c1912eea99b1b6b9b587a5592b&chksm=865ef0cc5bddcc1e4803e16284e099b6f7f75e9c6dc2a2d6c31fe1bb97e3e1be9d6ede516888#rd)
*易米云通*

Main category: wechat.article

TL;DR: 核心时代特征：Agentic AI 开启 “人机伙伴” 新纪元报告最核心的洞察是企业数智化正式迈入 Agentic AI（自治型 AI）时代，标志着 AI 从辅助 “工具” 向协同 “伙伴” 的本质跃迁。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 核心时代特征：Agentic AI 开启 “人机伙伴” 新纪元报告最核心的洞察是企业数智化正式迈入 Agentic AI（自治型 AI）时代，标志着 AI 从辅助 “工具” 向协同 “伙伴” 的本质跃迁。

</details>


### [11] [Google神作，<em class="highlight">Agentic</em>设计终极宝典中文版来了！](http://mp.weixin.qq.com/s?__biz=Mzk5MDkzMjY3NA==&mid=2247486075&idx=1&sn=ed1ba683051beb2ba94400bf59fbf704&chksm=c47f9b3709a158530aea4ca8892be3f8d6403a55273c46fc2a9a15bf7051fb1b095177f6aa6d#rd)
*大模型知知*

Main category: wechat.article

TL;DR: Google神作，Agentic设计终极宝典中文版来了！由谷歌工程师Antonio Gulli撰写，关于构建智能系统的实战指南。全书系统性地总结了21种智能体设计模式，旨在帮助开发者构建可靠、高效的智能系统。结合LangChain、CrewAI、Google Agent Deve


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Google神作，Agentic设计终极宝典中文版来了！由谷歌工程师Antonio Gulli撰写，关于构建智能系统的实战指南。全书系统性地总结了21种智能体设计模式，旨在帮助开发者构建可靠、高效的智能系统。结合LangChain、CrewAI、Google Agent Deve

</details>


### [12] [行业热点丨从 “算力供给” 到 “智能驱动”：<em class="highlight">Agentic</em> HPC 开启创新范式](http://mp.weixin.qq.com/s?__biz=MzkyNzY3MDYwMA==&mid=2247488086&idx=1&sn=3de61936dd6c485eabf4c5aa96832651&chksm=c3bcce7756662b577d3d6cb5393b6d21fdc968a567301ba551a75a8164e4fe57cc4ecd383e9b#rd)
*小安软服*

Main category: wechat.article

TL;DR: Agentic HPC 平台Altair HPCWorks 平台助力企业快速捕捉核心价值，为下一代智能化、自优化高性能计算做好准备。该平台充分利用人工智能辅助功能（包括AI驱动的内存资源预测），精简作业提交流程，优化集群、云及混合计算环境的


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic HPC 平台Altair HPCWorks 平台助力企业快速捕捉核心价值，为下一代智能化、自优化高性能计算做好准备。该平台充分利用人工智能辅助功能（包括AI驱动的内存资源预测），精简作业提交流程，优化集群、云及混合计算环境的

</details>


### [13] [如何用 LangGraph 构建高效的 <em class="highlight">Agentic</em> 系统](http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486698&idx=1&sn=d4e31866439163837e89de5b8237f08b&chksm=c35fc92d43fec81d03c7e574f44280fecb929fb1821f0311ab3f25373aecc93d5d1d93e03007#rd)
*AI大模型观察站*

Main category: wechat.article

TL;DR: 需要 agentic 框架的原因是它能抽象掉许多你不想亲自处理的复杂性：维护 state（状态）。不仅是消息历史，还包括执行 RAG 时收集的所有信息（RAG 保留英文以示专业术语）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 需要 agentic 框架的原因是它能抽象掉许多你不想亲自处理的复杂性：维护 state（状态）。不仅是消息历史，还包括执行 RAG 时收集的所有信息（RAG 保留英文以示专业术语）

</details>


### [14] [从协作到进化：<em class="highlight">Agentic</em> AI 的五大前沿趋势](http://mp.weixin.qq.com/s?__biz=MzUxNDg5NjcxMA==&mid=2247483712&idx=1&sn=6f6da3c174e0596fceeac10156948285&chksm=f845fe2d16dfaf505158163fc11b05c5f1bc08c53e0bbbf486d2d64637067c06ffb23e64cae1#rd)
*MiTang粑粑*

Main category: wechat.article

TL;DR: 2. 复用现有模块：将可复用的子智能体与工具（如金融领域的数据库连接器）适配到新任务。3. 继承平台基础：新智能体自动具备平台的治理、监控与安全能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2. 复用现有模块：将可复用的子智能体与工具（如金融领域的数据库连接器）适配到新任务。3. 继承平台基础：新智能体自动具备平台的治理、监控与安全能力。

</details>


### [15] [2025年度热词来了：AI相关词汇正高频进入我们生活](http://mp.weixin.qq.com/s?__biz=MzAwODE5NDg3NQ==&mid=2651288168&idx=1&sn=4ec32712cb1140eb8c8dbb57484ec503&chksm=814de1f52c22bb7877774efa8f9e4064c48ca4c4c83a3ee343412164e0b1f9659f918dd064ba#rd)
*红杉汇*

Main category: wechat.article

TL;DR: agentic2025年，“agentic”一词入围Dictionary.com年度词汇候选词名单。这原本是一个心理学和社会学术语，指人类独立行动、做出选择和塑造环境的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic2025年，“agentic”一词入围Dictionary.com年度词汇候选词名单。这原本是一个心理学和社会学术语，指人类独立行动、做出选择和塑造环境的能力。

</details>


### [16] [<em class="highlight">Agentic</em>21种设计模式13-Human-in-the-Loop](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484334&idx=1&sn=e1f04af4770d3120e4ca5c0f848b0633&chksm=c012b64294e7c08e0e80afc3a920727e1f0a1927d74fd451a6e25777311842715d3cd8c1130a#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: 4. 人机协作：人类与人工智能代理相互协作，各自发挥自身的优势：Agent可以处理常规的数据处理任务，而人类则负责处理需要创造性思维或复杂决策的问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 4. 人机协作：人类与人工智能代理相互协作，各自发挥自身的优势：Agent可以处理常规的数据处理任务，而人类则负责处理需要创造性思维或复杂决策的问题。

</details>


### [17] [谷歌神作，<em class="highlight">Agentic</em>设计终极宝典来了！](http://mp.weixin.qq.com/s?__biz=MzA4MTk3ODI2OA==&mid=2650364324&idx=1&sn=aefe5338c93844a64be26ac8a1ffbd76&chksm=86f8f9fcbe68ab210b05f962cf3473b865670e68f1aa0344c6c7a630a5cb6c0365d3a29cd2c8#rd)
*机器学习AI算法工程*

Main category: wechat.article

TL;DR: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！

</details>


### [18] [一起来学习 -《吴恩达<em class="highlight">Agentic</em> AI》（四）](http://mp.weixin.qq.com/s?__biz=Mzg2NTEwMDU5NA==&mid=2247484959&idx=1&sn=9e5be34375253b1249630f4b64db4c7e&chksm=cfa2de739436a6e945b5eb19cb7e881ed721f9c10269e83d1ad8fb92a3fe986dc78fc27cd3d7#rd)
*时间的朋友札记*

Main category: wechat.article

TL;DR: 一起来学习 -《吴恩达Agentic AI》（二）：Agentic AI 中的反思模式（Reflection） 一起来学习 -《吴恩达Agentic AI》（三）：Agentic AI 中的工具调用模式（Tool Use）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一起来学习 -《吴恩达Agentic AI》（二）：Agentic AI 中的反思模式（Reflection） 一起来学习 -《吴恩达Agentic AI》（三）：Agentic AI 中的工具调用模式（Tool Use）

</details>


### [19] [Google发布！一文了解21种<em class="highlight">Agentic</em>设计模式](http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505061&idx=1&sn=469d315719ad1aba2e2f0b3862299da6&chksm=9a4ddbd2b94693b16bd2a2b13f4b1e422bd190bbae65034a633c8904dcf26fde59f6ee4467ac#rd)
*关于NLP那些你不知道的事*

Main category: wechat.article

TL;DR: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn](https://arxiv.org/abs/2511.15738)
*Chao Yu,Qixin Tan,Jiaxuan Gao,Shi Yu,Hong Lu,Xinting Yang,Zelai Xu,Yu Wang,Yi Wu,Eugene Vinitsky*

Main category: cs.LG

TL;DR: 论文提出了3D测试时扩展框架，通过上下文长度、批处理和迭代轮次三个维度的扩展来增强推理强化学习模型的测试时推理能力。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展受限于基础模型的有限上下文长度，远小于训练时消耗的token数量，需要探索多维度扩展方法来突破这一限制。

Method: 提出3D测试时扩展框架，整合上下文长度扩展、批处理扩展和迭代轮次扩展三个维度，通过并行采样和迭代自我优化来增强推理质量。

Result: 每个维度都展现出测试时扩展效应但容量有限；三个维度结合显著提升IOI、IMO和CPHO等挑战性测试集的推理性能，并能从人类偏好反馈中获益；该框架可扩展到具身学习领域。

Conclusion: 多维度测试时扩展是突破推理强化学习测试时限制的有效方法，三个维度的协同作用能显著提升推理性能并支持更开放领域的应用。

Abstract: Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.

</details>


### [21] [AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization](https://arxiv.org/abs/2511.15915)
*Genghan Zhang,Shaowei Zhu,Anjiang Wei,Zhenyu Song,Allen Nie,Zhen Jia,Nandita Vijaykumar,Yida Wang,Kunle Olukotun*

Main category: cs.LG

TL;DR: AccelOpt是一个自改进的LLM代理系统，能够自主优化新兴AI加速器的内核，无需专家提供的硬件特定优化知识。通过迭代生成和优化记忆机制，在NKIBench基准测试中显著提升了内核性能。


<details>
  <summary>Details</summary>
Motivation: 消除对专家硬件优化知识的依赖，为新兴AI加速器提供自动化的内核优化解决方案，降低优化成本。

Method: 使用自改进的LLM代理系统，通过迭代生成探索内核优化空间，利用优化记忆机制积累先前遇到的慢-快内核对经验。

Result: 在Trainium 1上平均峰值吞吐量从49%提升到61%，在Trainium 2上从45%提升到59%。使用开源模型时，性能与Claude Sonnet 4相当但成本降低26倍。

Conclusion: AccelOpt证明了LLM代理系统能够有效自主优化AI加速器内核，具有持续改进能力和显著的成本效益。

Abstract: We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\%$ to $61\%$ on Trainium 1 and from $45\%$ to $59\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\times$ cheaper.

</details>


### [22] [Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning](https://arxiv.org/abs/2511.16043)
*Peng Xia,Kaide Zeng,Jiaqi Liu,Can Qin,Fang Wu,Yiyang Zhou,Caiming Xiong,Huaxiu Yao*

Main category: cs.LG

TL;DR: Agent0是一个完全自主的框架，通过多步协同进化和无缝工具集成，无需外部数据即可进化高性能智能体。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖人类策划数据，限制了可扩展性，而现有的自我进化框架受限于模型固有能力和单轮交互，阻碍了涉及工具使用或动态推理的复杂课程开发。

Method: 建立两个从相同基础LLM初始化的智能体之间的共生竞争：课程智能体提出日益具有挑战性的前沿任务，执行智能体学习解决这些任务。通过外部工具集成增强执行能力，反过来促使课程智能体构建更复杂的工具感知任务。

Result: Agent0显著提升了推理能力，将Qwen3-8B-Base模型在数学推理基准上提高了18%，在一般推理基准上提高了24%。

Conclusion: Agent0建立了一个自我强化的循环，持续产生高质量课程，实现了完全自主的智能体进化。

Abstract: Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.

</details>


### [23] [A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms](https://arxiv.org/abs/2511.16475)
*Ali Murtaza Caunhye,Asad Jeewa*

Main category: cs.LG

TL;DR: 比较决策变换器与传统离线强化学习算法在ANT连续控制环境中密集和稀疏奖励设置下的性能表现


<details>
  <summary>Details</summary>
Motivation: 传统离线RL算法在平衡探索与利用方面存在挑战，特别是在不同奖励密度的环境中。决策变换器将离线RL重新定义为序列建模问题，在多个基准测试中表现出色，需要系统比较其与传统方法在不同奖励结构下的性能差异。

Method: 在ANT连续控制环境中进行实证分析，比较决策变换器与传统离线RL算法（如CQL、IQL）在密集和稀疏奖励设置下的表现，评估它们在不同数据质量和奖励密度下的泛化能力。

Result: 决策变换器对奖励密度变化较不敏感，在稀疏奖励场景中的中等专家数据集上表现优异；传统价值方法如IQL在密集奖励设置中表现更好；CQL在不同数据质量下提供平衡性能；决策变换器性能方差较低但计算资源需求显著更高。

Conclusion: 序列建模方法更适合奖励结构不确定或数据质量混合的场景，而基于价值的方法在密集奖励和高质量演示设置中仍具有竞争力。

Abstract: The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.

</details>


### [24] [Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense](https://arxiv.org/abs/2511.16483)
*Sayak Mukherjee,Samrat Chatterjee,Emilie Purvine,Ted Fujimoto,Tegan Emerson*

Main category: cs.LG

TL;DR: 提出基于大语言模型的奖励设计方法，用于在深度强化学习驱动的网络攻防模拟环境中生成自主防御策略。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态环境中为自主网络攻防学习代理设计奖励对领域专家来说具有挑战性。

Method: 创建多个攻防代理角色，反映代理行为的异质性，首先为大语言模型提供网络模拟环境上下文信息，生成LLM引导的奖励设计，然后在DRL驱动的攻防模拟环境中使用这些奖励结构来学习网络防御策略集合。

Result: 结果表明LLM引导的奖励设计能够产生针对多样化对抗行为的有效防御策略。

Conclusion: LLM引导的奖励设计方法能够有效生成网络防御策略，应对不同的攻击行为。

Abstract: Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.

</details>


### [25] [Evolution Strategies at the Hyperscale](https://arxiv.org/abs/2511.16652)
*Bidipta Sarkar,Mattie Fellows,Juan Agustin Duque,Alistair Letcher,Antonio León Villares,Anya Sims,Dylan Cope,Jarek Liesen,Lukas Seier,Theo Wolf,Uljad Berdica,Alexander David Goldie,Aaron Courville,Karin Sevegnani,Shimon Whiteson,Jakob Nicolaus Foerster*

Main category: cs.LG

TL;DR: EGGROLL是一种进化策略算法，通过低秩学习实现大规模神经网络的高效优化，显著降低计算和内存成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统进化策略在大规模神经网络优化中计算和内存成本过高，需要解决这一可扩展性问题。

Method: 使用低秩矩阵扰动替代全秩扰动，通过生成随机低秩矩阵A和B来形成低秩扰动AB⊤，显著减少存储和计算需求。

Result: 实验表明EGGROLL在保持ES性能的同时速度更快，在LLM推理中与GRPO竞争，并能稳定预训练纯整数数据类型的非线性循环语言模型。

Conclusion: EGGROLL成功解决了大规模进化策略优化的可扩展性问题，为大规模神经网络优化提供了高效解决方案。

Abstract: We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.

</details>


### [26] [Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter](https://arxiv.org/abs/2511.16665)
*Qinghao Hu,Shang Yang,Junxian Guo,Xiaozhe Yao,Yujun Lin,Yuxian Gu,Han Cai,Chuang Gan,Ana Klimovic,Song Han*

Main category: cs.LG

TL;DR: TLT系统通过自适应推测解码加速推理强化学习训练，解决了长尾响应分布导致的效率瓶颈问题，实现了1.7倍端到端训练加速且保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的推理模型在RL训练中存在长尾响应分布问题，少数极长响应主导执行时间，造成资源浪费和成本增加。

Method: 提出TLT系统，包含两个协同组件：自适应草稿模型（在空闲GPU上持续训练以保持与目标模型对齐）和自适应执行引擎（维护内存高效的CUDAGraph池并自适应选择SD策略）。

Result: 评估显示TLT实现了超过1.7倍的端到端RL训练加速，保持模型精度，并产生高质量草稿模型作为免费副产品。

Conclusion: TLT成功解决了推理RL训练中的效率瓶颈，为复杂问题求解提供了高效的训练方案。

Abstract: The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems](https://arxiv.org/abs/2511.15715)
*Yash Raj Singh*

Main category: cs.AI

TL;DR: 提出Graph-Memoized Reasoning框架，通过图结构记忆表示、存储和重用推理工作流，减少重复计算，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现代LLM推理系统经常在不同任务中重复计算相似的推理步骤，浪费计算资源，增加推理延迟，限制可重现性。需要持久化推理机制来回忆和重用先前的计算轨迹。

Method: 引入图记忆推理框架，将推理工作流表示为图结构记忆，通过结构和语义相似性检索过去的决策图，实现跨新推理任务的子图组合重用。

Result: 提出了最小化总推理成本并正则化存储与生成工作流不一致性的优化目标，为智能系统中的效率-一致性权衡提供理论基础。

Conclusion: 该框架为可解释、成本高效和自我改进的推理架构奠定了基础，向大规模智能系统中的持久内存迈进一步。

Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.

</details>


### [28] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: 提出了MACIE框架，结合结构因果模型、干预反事实和Shapley值，为多智能体强化学习系统提供全面解释，解决个体贡献归因、涌现行为量化和复杂交互理解问题。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体强化学习系统在安全关键应用中广泛使用，理解智能体决策原因和集体行为实现方式变得至关重要。现有可解释AI方法在多智能体环境中表现不佳。

Method: MACIE框架结合结构因果模型、干预反事实和Shapley值，通过干预归因分数评估个体因果贡献，使用协同指标分离集体效应与个体贡献，并生成自然语言叙述提供可操作解释。

Result: 在四种MARL场景中测试显示：准确的结果归因（平均φ_i=5.07，标准差<0.05），在合作任务中检测到正向涌现（协同指数高达0.461），计算效率高（CPU上每个数据集0.79秒）。

Conclusion: MACIE独特地结合了因果严谨性、涌现量化和多智能体支持，同时保持实时使用的实用性，代表了向可解释、可信赖和可问责的多智能体AI迈出的一步。

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [29] [ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset](https://arxiv.org/abs/2511.15718)
*Chen Yang,Ran Le,Yun Xing,Zhenwei An,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang*

Main category: cs.AI

TL;DR: ToolMind是一个大规模、高质量的LLM工具代理数据集，包含16万合成数据和20万增强开源数据，通过多智能体框架生成，并采用细粒度的轮次级过滤确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有工具代理数据稀缺且验证不够精细，大多只在轨迹级别验证，忽略了轮次级错误可能导致的训练误差传播问题。

Method: 基于参数相关性构建函数图，使用多智能体框架模拟用户-助手-工具交互，并采用轮次级过滤移除错误或次优步骤。

Result: 在ToolMind上微调的模型在多个基准测试中相比基线有显著提升。

Conclusion: ToolMind通过高质量数据合成和精细验证解决了LLM代理训练中的数据质量问题，有效提升了模型性能。

Abstract: Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.

</details>


### [30] [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](https://arxiv.org/abs/2511.15752)
*Hanzhi Yan,Qin Lu,Xianqiao Wang,Xiaoming Zhai,Tianming Liu,He Li*

Main category: cs.AI

TL;DR: 该论文提出结合LLM和AI代理开发生物力学教育助手，通过RAG提升概念问题回答性能，通过多代理系统解决需要多步推理的计算问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在专业领域应用中存在的知识鸿沟问题，以及在需要多步推理的复杂问题上性能下降的挑战，旨在提升本科生在生物力学课程中的学习效果。

Method: 构建双模块框架：1) 应用RAG提升LLM对概念性真/假问题的回答特异性和逻辑一致性；2) 构建多代理系统解决需要多步推理和代码执行的计算导向问题。

Result: RAG显著提升了LLM在概念问题上的性能和稳定性，超越原始模型；多代理系统能够执行多步推理、推导方程、执行代码并生成可解释的解决方案。

Conclusion: RAG和多代理系统在增强LLM在工程专业课程中的性能方面具有潜力，为开发工程教育智能辅导系统提供了有前景的方向。

Abstract: While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.

</details>


### [31] [Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response](https://arxiv.org/abs/2511.15755)
*Philip Drammeh*

Main category: cs.AI

TL;DR: 多智能体编排相比单智能体方法在事件响应中实现了100%可操作建议率，质量方差为零，是LLM事件响应生产就绪的必要条件


<details>
  <summary>Details</summary>
Motivation: 单智能体方法生成模糊、不可用的建议，无法满足生产系统事件响应的需求

Method: 开发MyAntFarm.ai容器化框架，通过348次对照试验比较单智能体与多智能体系统在相同事件场景下的表现

Result: 多智能体系统实现100%可操作建议率，而单智能体仅为1.7%；在行动特异性上提升80倍，解决方案正确性提升140倍；质量方差为零

Conclusion: 多智能体编排不是性能优化，而是基于LLM的事件响应实现生产就绪的必要条件

Abstract: Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.

</details>


### [32] [An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models](https://arxiv.org/abs/2511.16383)
*Alexander Zadorojniy,Segev Wasserkrug,Eitan Farchi*

Main category: cs.AI

TL;DR: 提出了一种基于智能体的优化模型自动验证方法，将软件测试技术扩展到优化建模领域，通过多个智能体协作生成测试API、测试用例和模型变异来验证LLM生成的优化模型是否正确。


<details>
  <summary>Details</summary>
Motivation: 随着使用大语言模型从自然语言描述生成优化模型越来越流行，如何验证生成的模型是否正确且满足需求成为一个重要开放问题。

Method: 基于智能体的验证框架，包含多个智能体：首先生成问题级测试API，然后利用该API生成测试用例，最后生成优化模型特定的变异来评估测试套件的故障检测能力。

Result: 实验表明该智能体集成方法在突变覆盖率方面提供了高质量的验证。

Conclusion: 该方法成功将软件测试技术扩展到优化建模领域，能够有效验证LLM生成的优化模型的正确性。

Abstract: Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.

</details>


### [33] [Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions](https://arxiv.org/abs/2511.15830)
*Stéphane Aroca-Ouellette,Ian Berlot-Attwell,Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Tongqi Zhu,Herin Kang,Kaheer Suleman,Sam Pasupalak*

Main category: cs.AI

TL;DR: 该论文提出了Mini Amusement Parks (MAPs)模拟器，用于评估AI代理在复杂业务环境中的决策能力，发现人类表现远超当前最先进的LLM代理。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在现实世界决策中面临多个相互关联的挑战，如开放目标优化、稀疏经验学习、长时程规划和空间推理等，但现有基准测试孤立评估这些能力，无法全面评估整体决策能力。

Method: 开发了MAPs游乐园模拟器，统一了环境建模、不确定性下的长期后果预测和复杂业务战略运营等挑战，并提供人类基准和LLM代理的全面评估。

Result: 人类在简单模式下表现优于AI系统6.5倍，在中等模式下优于9.8倍。分析揭示了AI在长时程优化、样本高效学习、空间推理和世界建模方面的持续弱点。

Conclusion: MAPs通过在一个环境中统一这些挑战，为评估具有适应性决策能力的代理提供了新的基准基础。

Abstract: Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs

</details>


### [34] [Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs](https://arxiv.org/abs/2511.15921)
*Chelsea Zou,Yiheng Yao,Basant Khalil*

Main category: cs.AI

TL;DR: 开发了一个基于细粒度不确定性信号的LLM自我修正框架，通过置信度对齐和词元级熵峰值检测幻觉，使用强化学习改进推理过程的准确性和忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖最终答案正确性，无法检测多步推理中的幻觉问题，需要实时检测和缓解不可靠推理的方法。

Method: 使用自我评估置信度对齐和词元级熵峰值作为不确定性信号，设计复合奖励函数，通过强化学习策略改进模型的生成行为。

Result: 实验表明该方法提高了最终答案准确性和推理校准度，消融实验验证了各信号的有效性。

Conclusion: 提出的自我修正框架能有效检测和缓解LLM在多步推理中的幻觉问题，提高推理过程的忠实度和准确性。

Abstract: This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.

</details>


### [35] [JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation](https://arxiv.org/abs/2511.15958)
*Zhenyu Bi,Gaurav Srivastava,Yang Li,Meng Lu,Swastik Roy,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: JudgeBoard是一个新颖的评估管道，直接查询模型来评估候选答案的正确性，无需额外的答案比较。提出了MAJ多智能体评估框架，通过多个交互的小语言模型协作来近似大语言模型的判断准确性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在判断答案正确性方面的能力与大语言模型相比尚不清楚，现有基于比较的评估方法难以实现细粒度和可扩展的推理输出评估。

Method: 构建任务特定的评估排行榜，使用基于准确性的排名和Elo评级系统。提出MAJ框架，利用多个具有不同推理特征的小语言模型通过协作审议来提升判断性能。

Result: 实验结果显示小语言模型与大语言模型在独立判断任务中存在显著性能差距，但MAJ框架显著提高了小语言模型的可靠性和一致性。在MATH数据集上，使用较小模型作为骨干的MAJ表现与较大模型相当甚至更好。

Conclusion: 多智能体小语言模型系统在判断任务中可能匹配或超越大语言模型的性能，对可扩展和高效评估具有重要意义。

Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.

</details>


### [36] [KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy](https://arxiv.org/abs/2511.15974)
*Zhe Li,Yehan Qiu,Yujie Chen,Xiang Zhou*

Main category: cs.AI

TL;DR: KRAL是一个低成本、可扩展、保护隐私的范式，通过教师模型推理自动提炼知识和推理轨迹，使用启发式学习进行半监督数据增强，并利用代理强化学习联合增强医学知识和推理能力，显著优于传统RAG和SFT方法。


<details>
  <summary>Details</summary>
Motivation: 临床抗菌治疗需要动态整合病原体特征、宿主因素、抗菌药物药理学特性和感染严重程度，这种复杂性对LLMs在高风险临床决策中的应用构成了根本限制，包括知识差距、数据隐私问题、高部署成本和有限推理能力。

Method: 利用教师模型推理通过答案到问题的反向生成自动提炼知识和推理轨迹，采用启发式学习进行半监督数据增强（减少约80%手动标注需求），并使用代理强化学习联合增强医学知识和推理，同时优化计算和内存效率。

Result: KRAL显著优于传统RAG和SFT方法，在外部开源基准MEDQA上的知识问答能力（Accuracy@1比SFT提高1.8%，比RAG提高3.6%）和推理能力（在外部基准PUMCH Antimicrobial上的Pass@1比SFT提高27%，比RAG提高27.2%），仅需SFT长期训练成本的约20%。

Conclusion: KRAL是增强本地LLMs临床诊断能力的有效解决方案，能够在复杂医疗决策支持中实现低成本、高安全性的部署。

Abstract: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.

</details>


### [37] [Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis](https://arxiv.org/abs/2511.15992)
*Shahin Zanbaghi,Ryan Rostampour,Farhan Abid,Salim Al Jarmakani*

Main category: cs.AI

TL;DR: 提出了一种结合语义漂移分析和金丝雀基线比较的双重检测方法，用于实时识别被植入后门的LLM，在官方sleeper agent模型上达到92.5%准确率。


<details>
  <summary>Details</summary>
Motivation: LLM可能被植入后门，在特定部署条件下表现出恶意行为，而现有方法无法有效检测这种'潜伏代理'现象。

Method: 使用Sentence-BERT嵌入测量语义偏离安全基线的程度，并结合注入的金丝雀问题监控响应一致性。

Result: 在官方dolphin-llama3-8B sleeper agent模型上，系统达到92.5%准确率、100%精确度和85%召回率，实时检测时间小于1秒。

Conclusion: 该工作填补了AI部署中的关键安全漏洞，证明基于嵌入的检测可以有效识别欺骗性模型行为而不牺牲部署效率。

Abstract: Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as "sleeper agents." Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.

</details>


### [38] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: 提出了一种结合CNN-LSTM时间序列预测和多智能体深度强化学习的混合架构，用于云边缘工作负载资源管理的主动解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的云边缘工作负载资源管理过于被动，依赖静态阈值会导致资源过度配置或性能下降，需要转向主动预测性管理。

Method: 设计混合架构，将CNN-LSTM模型的时间序列预测嵌入到多智能体深度强化学习的智能体状态空间中，使AI管理器能够预见未来并制定长期规划。

Result: 测试表明该系统明显优于传统方法，能够有效解决复杂决策问题，同时平衡成本节约、系统健康和应用程序性能等多个目标。

Conclusion: 通过将预测能力嵌入强化学习状态空间，系统能够预见未来并找到资源管理的最优路径，实现成本效益和性能的平衡。

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [39] [SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent](https://arxiv.org/abs/2511.16108)
*Shiyi Cao,Dacheng Li,Fangzhou Zhao,Shuo Yuan,Sumanth R. Hegde,Connor Chen,Charlie Ruan,Tyler Griggs,Shu Liu,Eric Tang,Richard Liaw,Philipp Moritz,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: SkyRL-Agent是一个用于多轮、长视野智能体训练和评估的高效框架，通过异步调度、轻量级工具集成和灵活后端互操作性，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有智能体训练框架在效率、工具集成和跨框架兼容性方面的不足，开发一个能够支持高效多轮长视野训练的统一框架。

Method: 采用优化的异步管道调度器（1.55倍加速）和基于AST的搜索工具增强训练方法，结合强化学习训练软件工程智能体SA-SWE-32B。

Result: SA-SWE-32B在SWE-Bench Verified上达到39.4% Pass@1，相比之前模型成本降低2倍以上，并能有效泛化到其他智能体任务。

Conclusion: SkyRL-Agent提供了一个高效可扩展的智能体训练框架，通过优化调度和工具集成显著提升了训练效率和性能。

Abstract: We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.
  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.

</details>


### [40] [OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe](https://arxiv.org/abs/2511.16334)
*Kaichen Zhang,Keming Wu,Zuhao Yang,Kairui Hu,Bin Wang,Ziwei Liu,Xingxuan Li,Lidong Bing*

Main category: cs.AI

TL;DR: OpenMMReasoner是一个完全透明的两阶段多模态推理训练方法，包含监督微调(SFT)和强化学习(RL)阶段，在9个多模态推理基准上比Qwen2.5-VL-7B-Instruct基线提升11.6%。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推理取得显著进展，但缺乏透明和可复现的数据整理与训练策略阻碍了可扩展研究。

Method: 两阶段训练方法：1) SFT阶段使用87.4万样本的冷启动数据集进行逐步验证；2) RL阶段使用7.4万样本跨多个领域进一步优化推理能力。

Result: 在9个多模态推理基准上比Qwen2.5-VL-7B-Instruct基线提升11.6%，证明了数据质量和训练设计对多模态推理性能的关键作用。

Conclusion: 该训练方法不仅超越了强基线，还为未来大规模多模态推理研究奠定了坚实的实证基础，所有代码、流程和数据均已开源。

Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.

</details>


### [41] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM框架用专家评估器团队替代单一黑盒奖励模型，通过分解偏好评估到领域特定代理来提升RLHF的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型难以同时优化多个可能冲突的偏好维度（如事实性、帮助性、安全性），且评分透明度有限。

Method: 将偏好评估分解为领域特定代理产生部分信号，结合全局评估器，通过中央聚合器融合信号，使用基于优势的更新优化策略。

Result: CRM和rewardBench提供了更透明的奖励建模和更稳定优化的实用模块化路径。

Conclusion: 该框架为多视角奖励塑造提供了可行方案，无需额外人工标注。

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [42] [D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2511.16590)
*Sen Chen,Tong Zhao,Yi Bin,Fei Ma,Wenqi Shao,Zheng Wang*

Main category: cs.AI

TL;DR: 提出了D-GARA动态基准测试框架，用于评估Android GUI代理在真实世界异常情况下的鲁棒性，包含多种常见异常类型如权限对话框、电池警告等。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的数据集和基准测试大多是静态和理想化的，无法反映真实世界环境的复杂性和不可预测性，特别是异常情况的存在。

Method: 开发了D-GARA动态基准测试框架，引入多样化的真实世界异常类型，构建并标注了包含常用Android应用和嵌入异常的基准测试集。

Result: 实验显示最先进的GUI代理在异常丰富环境中性能显著下降，凸显了鲁棒性学习的必要性。

Conclusion: D-GARA框架是模块化和可扩展的，支持新任务、异常类型和交互场景的无缝集成，以满足特定评估目标。

Abstract: Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.

</details>


### [43] [Distributed Agent Reasoning Across Independent Systems With Strict Data Locality](https://arxiv.org/abs/2511.16292)
*Daniel Vaughan,Kateřina Vaughan*

Main category: cs.AI

TL;DR: 该论文展示了一个分布式系统中智能体间仅通过自然语言消息进行通信的概念验证原型，无需共享标识符、结构化模式或集中式数据交换。


<details>
  <summary>Details</summary>
Motivation: 探索多个组织（诊所、保险公司、专科网络）如何通过假名化案例令牌、本地数据查找和受控操作边界进行安全协作，实现分布式推理。

Method: 使用Orpius平台进行多智能体编排、工具执行和隐私保护通信，通过OperationRelay调用交换简洁的自然语言摘要，每个智能体仅操作自己的本地数据。

Result: 成功演示了可行性，诊所生成HMAC假名令牌，保险公司评估覆盖规则并咨询专科智能体，专科智能体返回适当性建议，所有过程不涉及患者身份信息。

Conclusion: 该原型突出了支持专门智能体间分布式推理的架构模式、隐私考量和通信流程，同时保持每个组织的数据本地化，为去中心化多智能体系统的未来研究奠定基础。

Abstract: This paper presents a proof-of-concept demonstration of agent-to-agent communication across distributed systems, using only natural-language messages and without shared identifiers, structured schemas, or centralised data exchange. The prototype explores how multiple organisations (represented here as a Clinic, Insurer, and Specialist Network) can cooperate securely via pseudonymised case tokens, local data lookups, and controlled operational boundaries.
  The system uses Orpius as the underlying platform for multi-agent orchestration, tool execution, and privacy-preserving communication. All agents communicate through OperationRelay calls, exchanging concise natural-language summaries. Each agent operates on its own data (such as synthetic clinic records, insurance enrolment tables, and clinical guidance extracts), and none receives or reconstructs patient identity. The Clinic computes an HMAC-based pseudonymous token, the Insurer evaluates coverage rules and consults the Specialist agent, and the Specialist returns an appropriateness recommendation.
  The goal of this prototype is intentionally limited: to demonstrate feasibility, not to provide a clinically validated, production-ready system. No clinician review was conducted, and no evaluation beyond basic functional runs was performed. The work highlights architectural patterns, privacy considerations, and communication flows that enable distributed reasoning among specialised agents while keeping data local to each organisation. We conclude by outlining opportunities for more rigorous evaluation and future research in decentralised multi-agent systems.

</details>


### [44] [Trustworthy AI in the Agentic Lakehouse: from Concurrency to Governance](https://arxiv.org/abs/2511.16402)
*Jacopo Tagliabue,Federico Bianchi,Ciro Greco*

Main category: cs.AI

TL;DR: 论文提出Bauplan设计，重新实现湖仓中的数据与计算隔离，为智能体工作流提供正确性和信任保证。


<details>
  <summary>Details</summary>
Motivation: 大多数企业认为智能体不够可信来处理生产数据，传统湖仓不适合智能体访问模式，需要解决基础设施问题。

Method: 借鉴数据库MVCC的操作类比，提出面向智能体的Bauplan设计，在湖仓中重新实现数据和计算隔离。

Result: 分享了Bauplan中自愈管道的参考实现，无缝结合智能体推理与所需的正确性和信任保证。

Conclusion: 通过解决基础设施问题，设计围绕事务的湖仓，可以为可信的智能体工作流铺平道路。

Abstract: Even as AI capabilities improve, most enterprises do not consider agents trustworthy enough to work on production data. In this paper, we argue that the path to trustworthy agentic workflows begins with solving the infrastructure problem first: traditional lakehouses are not suited for agent access patterns, but if we design one around transactions, governance follows. In particular, we draw an operational analogy to MVCC in databases and show why a direct transplant fails in a decoupled, multi-language setting. We then propose an agent-first design, Bauplan, that reimplements data and compute isolation in the lakehouse. We conclude by sharing a reference implementation of a self-healing pipeline in Bauplan, which seamlessly couples agent reasoning with all the desired guarantees for correctness and trust.

</details>


### [45] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: 提出了DPPO（Deliberate Practice Policy Optimization）元认知训练框架，通过交替使用监督微调和强化学习来解决具身智能中的数据瓶颈和算法效率问题，在Pelican-VL 1.0模型上实现了20.3%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能系统中的两个主要挑战：真实世界数据稀缺昂贵的数据瓶颈，以及现有方法资源消耗大的算法效率问题。

Method: DPPO元认知训练框架，动态交替使用监督微调（能力扩展）和强化学习（技能精炼），自动识别弱点并进行针对性资源分配。

Result: 训练出的Pelican-VL 1.0具身视觉语言模型相比基础模型性能提升20.3%，在100B参数规模上超越开源模型10.6%。

Conclusion: DPPO是首个系统性地缓解数据和资源瓶颈的框架，使社区能够高效构建通用具身智能体。

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [46] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: 该论文提出了一个包含28个认知元素的分类法，分析了人类与LLM在推理结构上的系统性差异，并开发了测试时推理指导方法，在复杂问题上提升了60%的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在复杂问题上表现出色，但在简单变体上却失败，这表明它们通过不同于人类推理的机制获得正确输出。研究旨在理解LLM与人类推理的根本差异。

Method: 构建了28个认知元素的分类法，分析了17个模型的17万条推理轨迹和54条人类思考轨迹，并对1,598篇LLM推理论文进行了元分析。

Result: 发现人类使用层次嵌套和元认知监控，而模型依赖浅层前向链；研究社区关注易量化行为而忽视元认知控制；开发的推理指导方法在复杂问题上提升60%性能。

Conclusion: 通过连接认知科学和LLM研究，为开发基于原则性认知机制而非脆弱捷径的模型奠定了基础，为改进模型能力和大规模测试人类认知理论开辟了新方向。

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [47] [Gemini 3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts%2Fgemini%2Fgemini-3%2F%3Futm_source=tldrai/1/0100019a9c78f63b-54f89f7f-1634-4028-bd52-80957d6ddb20-000000/ZP1CS-3e7U8E8K2rURG4XvEyb2Yu8ufIaottIl4iMfQ=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gemini 3在多个主要基准测试中取得最先进性能，包括LMArena上的1501 Elo、GPQA Diamond的91.9%和MMMU-Pro的81%得分。模型包含增强推理的Deep Think模式，并伴随推出新的智能编码IDE Antigravity。


<details>
  <summary>Details</summary>
Motivation: 开发更强大的AI模型以在各种基准测试中实现突破性性能，特别是在推理和编码能力方面。

Method: Gemini 3模型采用先进架构，包含Deep Think模式用于增强推理能力，同时推出Antigravity IDE支持智能编码。

Result: 在LMArena上获得1501 Elo突破性成绩，GPQA Diamond达到91.9%，MMMU-Pro达到81%，在大多数主要基准测试中达到最先进水平。

Conclusion: Gemini 3在多个关键基准测试中表现出色，特别是在推理和编码任务上，Deep Think模式和Antigravity IDE进一步增强了其能力。

Abstract: Gemini 3 (12 minute read) Gemini 3 achieves state-of-the-art performance on most major benchmarks, including a breakthrough 1501 Elo on LMArena, 91.9% on GPQA Diamond, and 81% on MMMU-Pro. The model includes a Deep Think mode for enhanced reasoning and coincides with the launch of a new agentic coding IDE called Antigravity.

</details>


### [48] [How Dash uses context engineering for smarter AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdata/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/b46RO3qY0PUIv50oIcHfmfnIMLXSDwzT1ne0t_WrX0o=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dropbox通过将多个检索工具整合为统一的Dash Search工具、使用知识图谱过滤结果、将复杂子任务委托给专门搜索代理，提升了Dash的智能代理性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理中工具泛滥、上下文过载和token使用不平衡的问题，提高检索效率和准确性。

Method: 采用三种上下文工程策略：统一检索工具、运行时知识图谱过滤、委托复杂子任务给专门搜索代理。

Result: 减少了噪音和工具蔓延，防止了上下文过载，平衡了token使用。

Conclusion: 上下文工程策略有效提升了AI代理的性能和效率。

Abstract: How Dash uses context engineering for smarter AI (5 minute read) Dropbox improved Dash's agentic performance by consolidating many retrieval tools into a single unified “Dash Search” tool, filtering results at runtime using a knowledge graph to deliver only highly relevant context, and delegating complex subtasks like query construction to a specialized search agent. These three context-engineering strategies reduce noise and tool sprawl, prevent context overload, and balance token usage, cos...

</details>


### [49] [How Can You Identify an Agentic AI Use Case?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FZrKhQy/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/tgxg6gt44iQfKZYRtewMj0j3iSRiYYnsyUgoILjnJds=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文讨论了如何识别适合使用智能AI代理的用例，指出智能AI可以自动化复杂、推理密集的任务，这些任务通常具有重复性、依赖专家知识或涉及分散/非结构化数据。


<details>
  <summary>Details</summary>
Motivation: 动机是帮助组织识别哪些业务场景适合部署智能AI代理，以显著减少人工工作量，提高效率。

Method: 方法包括明确界定任务范围、定义良好的工具（可能包含子代理），以及投入足够的文档准备工作来消除歧义。

Result: 通过这种方法，智能AI可以显著减少复杂推理任务中的人力投入。

Conclusion: 结论是智能AI在明确界定范围、工具定义清晰且文档充分的情况下，能够有效自动化复杂任务。

Abstract: How Can You Identify an Agentic AI Use Case? (10 minute read) Agentic AI can automate complex, reasoning-heavy tasks that are repetitive, expert-dependent, or involve scattered/unstructured data, dramatically cutting human effort, provided the scope is clearly bounded, tools are well-defined (potentially with subagents), and sufficient upfront documentation is invested to eliminate ambiguity and prevent incomplete automation.

</details>


### [50] [Building more with GPT-5.1-Codex-Max](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbLpQcx/1/0100019aa1232533-b367583d-d3f5-44c8-afa7-fe2e2196da48-000000/Xy51UUT2ZFX2mmjvI4qQaieUdhvoiIyvr8AXohQ-te4=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.1-Codex-Max是OpenAI推出的新型智能编码代理模型，具有更快的速度、更高的智能水平和更好的token效率，专为长期、详细的工作设计。


<details>
  <summary>Details</summary>
Motivation: 开发更高效、更智能的编码代理模型，以提升软件开发周期的各个阶段效率。

Method: 基于GPT-5.1架构构建的Codex模型，专注于代理式编码能力，支持长期运行的详细开发工作。

Result: 模型在速度、智能水平和token效率方面都有显著提升，已在Codex平台可用，API访问即将推出。

Conclusion: GPT-5.1-Codex-Max代表了编码代理模型的新前沿，为软件开发提供了更强大的工具。

Abstract: Building more with GPT-5.1-Codex-Max (8 minute read) GPT‑5.1-Codex-Max is a new frontier agentic coding model from OpenAI. It is much faster, more intelligent, and more token-efficient at every stage of the development cycle. The model is built for long-running, detailed work. It is available in Codex today. API access is coming soon.

</details>


### [51] [Building more with GPT-5.1-Codex-Max](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbotDcK/1/0100019aa12af4ab-9a09deb7-c20f-4818-9ff5-1f5d093d3fa3-000000/Xlc72HVKpeCJ76WXeIbplS6UMxIxrARzzDMuWU_V8jM=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出了GPT-5.1-Codex-Max，这是一个基于更新推理基础构建的高级代理编码模型，专为长时间运行的详细软件工程任务设计，能够在多个上下文窗口中操作，支持项目级重构和深度调试会话。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理复杂、长时间运行的软件工程任务的AI编码助手，支持项目规模的重构和深度调试需求。

Method: 基于更新的推理基础构建，支持跨多个上下文窗口操作，专门为长时间运行的软件工程任务优化。

Result: GPT-5.1-Codex-Max模型已在Codex中可用，能够执行项目级重构和深度调试会话。

Conclusion: GPT-5.1-Codex-Max代表了AI编码助手的重要进步，能够处理更复杂的软件工程任务。

Abstract: Building more with GPT-5.1-Codex-Max (8 minute read) OpenAI's GPT-5.1-Codex-Max is an advanced agentic coding model built on an updated reasoning foundation and designed for long-running, detailed software engineering tasks. The model can operate across multiple context windows, enabling project-scale refactors and deep debugging sessions. GPT-5.1-Codex-Max is available in Codex.

</details>


### [52] [Start Building with Gemini 3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemini-3-developers%2F%3Futm_source=tldrdesign/1/0100019aa15f04ad-2bb84835-a9c9-459c-85a1-0b6ef7a10245-000000/ZfZ3jUNZ3qOypx8YK6Rd7lrFbH7svsBsArQOehVEQYQ=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google推出Gemini 3 Pro模型，在AI基准测试中超越竞争对手，特别擅长编码任务和代理工作流，并推出Antigravity开发平台支持多环境AI代理协作。


<details>
  <summary>Details</summary>
Motivation: 开发更智能的AI模型以在基准测试中超越竞争对手，同时专注于提升编码能力和代理工作流效率，为开发者提供更好的AI协作工具。

Method: 基于Gemini 3 Pro模型构建Antigravity开发平台，支持在编辑器、终端和浏览器环境中与自主AI代理协作。

Result: Gemini 3 Pro在主要AI基准测试中表现优异，在编码任务和代理工作流方面表现突出，具备领先的多模态理解能力。

Conclusion: Gemini 3 Pro是目前最智能的模型，在多个领域表现卓越，为开发者提供了强大的AI协作平台。

Abstract: Start Building with Gemini 3 (9 minute read) Google's Gemini 3 Pro, its most intelligent model to date, surpasses competitors on major AI benchmarks while excelling at coding tasks and agentic workflows. The model powers Google Antigravity, a new agentic development platform that enables developers to collaborate with autonomous AI agents across editor, terminal, and browser environments. Gemini 3 Pro also leads in multimodal understanding, including document analysis, spatial reasoning, and ...

</details>


### [53] [ServiceNow AI Agents Can Be Tricked Into Acting Against Each Other via Second-Order Prompts](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F11%2Fservicenow-ai-agents-can-be-tricked.html%3Futm_source=tldrinfosec/1/0100019aa1981f7c-8589c13f-0c81-4868-b154-8a9728d1267a-000000/llPjEFzZ44rV_OtsG-yCIJSxe3FBbEVWf2IjnFE8MpI=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ServiceNow的Now Assist AI代理可以通过二阶提示注入被操纵，攻击者利用代理间传递的指令触发意外行为，实现权限提升、数据窃取或工作流重定向。


<details>
  <summary>Details</summary>
Motivation: 揭示AI代理系统中存在的安全漏洞，特别是通过代理间指令传递进行的二阶提示注入攻击，以提升对AI代理安全性的认识。

Method: 通过二阶提示注入技术，攻击者在代理间传递的指令中嵌入恶意内容，利用代理间的信任关系触发意外行为。

Result: ServiceNow的AI代理可以被操纵执行权限提升、数据窃取和工作流重定向等恶意操作，尽管ServiceNow声称这是预期的代理链行为。

Conclusion: 组织需要监控AI代理间的交互并加强防护措施，以防范二阶提示注入攻击带来的安全风险。

Abstract: ServiceNow AI Agents Can Be Tricked Into Acting Against Each Other via Second-Order Prompts (3 minute read) ServiceNow's Now Assist AI agents can be manipulated through second-order prompt injection, where instructions passed between agents trigger unintended actions. Attackers can use this to escalate privileges, steal data, or redirect workflows. ServiceNow claims the behavior matches expected agent chaining, but organizations are urged to monitor agent interactions and harden guardrails.

</details>


### [54] [The average financial company loses $250M a year on unfinished applications](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 金融公司每年因未完成申请损失2.5亿美元，使用对话AI代理后完成率从30%提升至近60%。


<details>
  <summary>Details</summary>
Motivation: 解决金融公司因未完成申请造成的巨大经济损失，提高申请完成率。

Method: 使用对话AI代理处理申请跟进工作。

Result: 申请完成率从30%提升至近60%，几乎翻倍。

Conclusion: AI代理能显著提高金融申请完成率，减少经济损失。

Abstract: The average financial company loses $250M a year on unfinished applications (Sponsor) Today, hundreds of the largest financial institutions are using conversational AI agents to handle application follow-ups instead. Before, completion rates were 30%. Now, they're almost double that.Crazy what a little AI can do.Don't believe us? Get a free custom agent today.

</details>


### [55] [they're almost double that](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 金融公司通过使用对话AI代理处理申请跟进，将完成率从30%提升至近60%，显著减少了未完成申请造成的损失。


<details>
  <summary>Details</summary>
Motivation: 金融公司每年因未完成申请损失2.5亿美元，需要提高申请完成率以减少经济损失。

Method: 使用对话AI代理来处理申请跟进工作，替代传统的人工处理方式。

Result: 申请完成率从之前的30%提升到近60%，几乎翻倍。

Conclusion: AI代理能显著提高金融申请的处理效率，减少经济损失。

Abstract: The average financial company loses $250M a year on unfinished applications (Sponsor) Today, hundreds of the largest financial institutions are using conversational AI agents to handle application follow-ups instead. Before, completion rates were 30%. Now, they're almost double that.Crazy what a little AI can do.Don't believe us? Get a free custom agent today.

</details>


### [56] [Your AI code reviewer doesn't know what actually breaks. Sentry's does.](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q4-aicodereviewlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019aa1a1335d-85d80474-e9fd-442a-9f64-c334d5e6fdb4-000000/S9Drvqy9STvofwTE_L_geS7F8DY0eL0l2CMvW7Gzhtg=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI代码审查工具Seer基于生产环境错误数据，能够精准识别哪些代码变更会导致实际问题，而不仅仅是通用代码问题


<details>
  <summary>Details</summary>
Motivation: 传统AI代码审查工具只能发现语法错误和风格问题，但无法准确预测哪些变更会在实际生产环境中引发故障。Sentry利用其生产监控数据来解决这一问题

Method: Seer结合Sentry的生产错误监控数据、代码库和提交历史，为AI代码审查提供更准确的上下文信息

Result: 该工具能够基于历史故障数据，精确识别可能导致生产问题的代码变更

Conclusion: 利用生产环境监控数据可以显著提升AI代码审查的准确性和实用性

Abstract: Your AI code reviewer doesn't know what actually breaks. Sentry's does. (Sponsor) Generic AI code review catches typos, style issues, and things that might be a problem. Sentry's AI code reviewer (AKA Seer) knows exactly which changes will cause issues based on what's broken before, in your actual environment. Like most things AI, the secret is better context... → Sentry already monitors your production errors and performance. → Seer AI Code Review uses Sentry's data + your code + commit hist...

</details>


### [57] [Tinkering with prompts can only get you so far.](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=26337115-TLDR%2520tech%2520Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrtech_secondary_1121%26utm_content=tldrtech_secondary_1121/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/W4P6nIdzhyE_x6dU-RasLIZ_37h8VWljzuTJoVQrFls=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该指南介绍了构建成功AI代理的五个发展阶段，强调仅靠调整提示词无法实现可靠结果，需要超越提示词阶段。


<details>
  <summary>Details</summary>
Motivation: 大多数公司停留在调整提示词阶段，导致AI代理无法提供可靠结果，需要系统化的代理管理方法。

Method: 通过五个发展阶段来构建成功的AI代理，从基础提示词调整到更高级的代理管理方法。

Result: 揭示了大多数组织尚未达到高级代理管理阶段，提供了超越提示词的实用指南。

Conclusion: 要构建成功的AI代理，需要超越简单的提示词调整，采用系统化的代理管理方法。

Abstract: Tinkering with prompts can only get you so far. (Sponsor) Most companies get stuck tinkering with prompts and wonder why their agents fail to deliver dependable results. This guide from You.com breaks down the evolution of agent management, revealing the five stages for building a successful AI agent and why most organizations haven't gotten there yet. If you're ready to go beyond the prompt, this is the playbook for you.

</details>


### [58] [How to write a great agents.md: Lessons from over 2,500 repositories](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrnewsletter/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/ob9RbeQ-4wRmElIJIAlHRXITe8e-R3Vlym45Pi-VwqE=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub的agents.md功能允许开发者创建具有自定义指令的代理。大多数代理文件失败是因为过于模糊，而最佳文件为代理提供清晰的工作或角色、确切的运行命令、明确定义的边界以及良好的代码示例。


<details>
  <summary>Details</summary>
Motivation: 分析超过2500个代码库后发现，大多数agents.md文件由于内容模糊而失败，需要为开发者提供创建有效代理文件的指导原则。

Method: 通过分析2500多个GitHub代码库中的agents.md文件，总结成功和失败案例的模式，提炼出创建高质量代理文件的关键要素。

Result: 发现成功的agents.md文件通常包含：清晰的代理角色定义、确切的命令指令、明确的边界限制以及具体的代码风格示例。

Conclusion: 编写有效的agents.md文件需要提供具体、明确的指导，避免模糊描述，确保代理能够准确理解和执行开发者的意图。

Abstract: How to write a great agents.md: Lessons from over 2,500 repositories (12 minute read) GitHub's agents.md feature allows developers to create agents with custom instructions. agents.md is where developers define all of the specifics, provide code style examples, and set clear boundaries of what not to do. Most agent files fail because they're too vague. The best files provide agents with a clear job or persona, exact commands to run, well-defined boundaries to follow, and clear examples of goo...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [A Causal Perspective on Measuring, Explaining and Mitigating Smells in \llm-Generated Code](https://arxiv.org/abs/2511.15817)
*Alejandro Velasco,Daniel Rodriguez-Cardenas,Dipin Khati,David N. Palacio,Luftar Rahman Alif,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文系统性地测量、解释和缓解LLM生成代码中的代码异味倾向，提出PSC概率度量方法，分析生成策略、模型大小、架构和提示设计对代码结构质量的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中应用广泛，但其生成的代码存在结构质量问题，经常复制不良编码实践，引入代码异味。现有研究缺乏对这些问题的系统理解。

Method: 基于PSC概率度量方法，通过因果分析研究生成策略、模型大小、架构和提示设计对代码结构质量的影响，并进行用户研究验证。

Result: 发现提示设计和架构选择对代码异味倾向起决定性作用，提出的缓解策略能有效减少异味发生。用户研究表明PSC能帮助开发者解释模型行为和评估代码质量。

Conclusion: 为将质量感知评估集成到LLM代码生成评估和部署中奠定了基础，PSC信号可以支持人类判断。

Abstract: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.
  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

</details>


### [60] [InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution](https://arxiv.org/abs/2511.16004)
*KeFan Li,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: InfCode是一个对抗性多智能体框架，用于自动化仓库级问题解决，通过测试生成器和代码补丁生成器的对抗性交互迭代优化测试和补丁，在SWE-bench Verified上达到79.4%的性能，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体和流水线的方法依赖不充分的测试，可能导致补丁通过验证但未能修复根本缺陷，需要仓库级推理、准确诊断和强验证信号来解决真实世界软件问题。

Method: 采用对抗性多智能体框架，包含测试补丁生成器、代码补丁生成器和选择器智能体，在容器化环境中迭代优化测试和补丁，支持真实的仓库检查、修改和验证。

Result: 在SWE-bench Lite和SWE-bench Verified上使用DeepSeek-V3和Claude 4.5 Sonnet等模型进行实验，InfCode始终优于强基线，在SWE-bench Verified上达到79.4%的性能，创下新纪录。

Conclusion: InfCode通过对抗性多智能体框架有效解决了仓库级软件问题，建立了新的最先进水平，并已作为开源项目发布。

Abstract: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.

</details>


### [61] [InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution](https://arxiv.org/abs/2511.16005)
*Qingao Dong,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: INFCODE-C++是首个专为C++设计的自主问题解决系统，通过语义代码意图检索和确定性AST结构化查询相结合，在MultiSWE-bench-CPP基准测试中达到25.58%的解决率，比现有最佳代理提升10.85个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要针对Python设计，在C++项目中表现不佳，因为C++的重载标识符、嵌套命名空间、模板实例化和深层控制流结构使得上下文检索和故障定位更加困难。

Method: 结合两种互补的检索机制：语义代码意图检索和确定性AST结构化查询，构建准确的语言感知上下文进行修复。

Result: 在MultiSWE-bench-CPP基准测试中达到25.58%的解决率，比最强现有代理提升10.85个百分点，是MSWE-agent性能的两倍多。

Conclusion: INFCODE-C++强调了多语言软件代理中语言感知推理的必要性，为复杂静态类型生态系统的可扩展LLM驱动修复研究奠定了基础。

Abstract: Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.

</details>


### [62] [The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report](https://arxiv.org/abs/2511.16092)
*Xing Hu,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 33位专家在Shonan Meeting 222上讨论了生成式AI对IDE的影响，探讨了挑战和机遇


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在代码生成、测试、代码审查和程序修复等任务中的卓越表现如何改变IDE中的人机交互

Method: 通过跨领域专家会议讨论，汇集软件工程、人工智能和人机交互领域的33位专家

Result: 识别了生成式AI对IDE带来的挑战和机遇，并讨论了提高抽象级别如何改变人机交互

Conclusion: 生成式AI有潜力通过提高抽象级别来改变IDE中的人机交互方式

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report

</details>


### [63] [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](https://arxiv.org/abs/2511.16224)
*Francesco Salzano,Simone Scalabrino,Rocco Oliveto,Simone Scalabrino*

Main category: cs.SE

TL;DR: 本文对LLM生成的Solidity智能合约代码进行了全面评估，发现在语义相似度高的同时，功能正确性较低（仅20-26%），生成的代码更简单但缺乏验证逻辑，而检索增强生成能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 智能合约在区块链生态中至关重要，但LLM在生成Solidity代码时面临gas消耗、安全性和确定性等独特约束，现有研究缺乏对这些关键功能和非功能属性的全面评估。

Method: 对4个最先进模型在零样本和检索增强生成设置下进行基准测试，使用代码相似度指标、语义嵌入、自动化测试执行、gas分析以及认知和圈复杂度分析进行多维度评估。

Result: LLM生成的代码与真实合约具有高语义相似度，但功能正确性低（仅20-26%），生成的代码更简单、复杂度更低、gas消耗更少，但往往省略了验证逻辑。检索增强生成将功能正确性提升高达45%。

Conclusion: LLM生成的智能合约在语义相似度和功能合理性之间存在显著差距，RAG是强大的增强器，但要实现稳健的生产就绪代码生成仍需专家仔细验证。

Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.

</details>
