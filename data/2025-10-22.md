<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [tldr.article](#tldr.article) [Total: 8]
- [cs.SE](#cs.SE) [Total: 16]
- [wechat.article](#wechat.article) [Total: 35]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: LLM代理在第三方惩罚决策中使用情感指导，比人类更强烈地依赖情感，但缺乏成本敏感性，表现出类似人类早期发展的特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否像人类一样使用情感来指导道德决策，特别是在利他性第三方惩罚情境中。

Method: 在796,100个决策中比较4,068个LLM代理和1,159名人类的行为，使用第三方惩罚任务测试情感对惩罚决策的影响。

Result: LLM使用情感指导惩罚，不公平引发更强负面情感导致更多惩罚，情感报告提示因果性增加惩罚，但LLM缺乏成本敏感性。

Conclusion: LLM表现出情感驱动的道德决策，但机制与人类不同，建议未来模型应整合情感与情境敏感推理以实现人类水平的情感智能。

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [2] [CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections](https://arxiv.org/abs/2510.17921)
*Keuntae Kim,Eunhye Jeong,Sehyeon Lee,Seohee Yoon,Yong Suk Choi*

Main category: cs.CL

TL;DR: CLAWS是一种无需人工评估就能将数学解决方案分类为典型、创意和幻觉的方法，通过利用注意力权重在提示部分和输出中的分布来实现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理任务中的准确性有所提高，但对推理任务中创造性的评估却被忽视，主要因为定义创造性范围和需要人工评估的挑战。

Method: 提出CLAWS方法，利用注意力权重在提示部分和输出中的分布，将数学解决方案分类为典型、创意和幻觉三类。

Result: CLAWS在五个7-8B数学RL模型（DeepSeek、Qwen、Mathstral、OpenMath2和Oreal）上优于五种现有的白盒检测方法，并在4545个数学问题上进行了验证。

Conclusion: CLAWS能够有效定义和分类数学解决方案的创造性，无需人工评估，为推理任务中的创造性评估提供了新方法。

Abstract: Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).

</details>


### [3] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 提出Select-Then-Decompose策略，通过选择、执行和验证三阶段闭环流程，动态选择最适合的任务分解方法，在性能和成本之间实现最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解方法主要关注内存、工具使用和反馈机制，但忽视了性能与成本之间的权衡，需要一种更平衡的解决方案。

Method: 首先对任务分解进行系统分类，识别六种分类方案；然后分析影响分解性能的三个因素；最后提出Select-Then-Decompose策略，包含选择、执行和验证三个阶段。

Result: 在多个基准测试上的综合评估表明，Select-Then-Decompose策略始终位于Pareto前沿，在性能和成本之间实现了最优平衡。

Conclusion: Select-Then-Decompose策略通过动态选择分解方法和结果验证，有效解决了任务分解中性能与成本的权衡问题。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [4] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 本文研究LLM作为评估者时的评分范围偏差问题，并提出通过对比解码方法缓解该偏差，在Spearman相关性上获得显著提升


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者在直接评估中面临可靠性挑战，特别是评分范围偏差问题，即LLM评估结果对预定义评分范围高度敏感

Method: 使用对比解码方法来缓解LLM评估者的评分范围偏差问题

Result: 通过对比解码方法，在不同评分范围内与人类判断的Spearman相关性平均相对提升了11.3%

Conclusion: 对比解码方法能有效缓解LLM评估者的评分范围偏差，提高评估可靠性

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [5] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: MENTOR框架通过结合强化学习和教师引导蒸馏，解决了小语言模型在工具使用能力蒸馏中的泛化问题和奖励稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的监督微调方法泛化能力差，而标准强化学习由于奖励稀疏导致探索效率低和策略次优，需要一种更好的方法来蒸馏大语言模型的工具使用能力到小语言模型。

Method: 提出MENTOR框架，结合强化学习和教师引导蒸馏：使用RL学习更通用的策略，同时利用教师参考轨迹构建密集的复合教师引导奖励来提供细粒度指导。

Result: 大量实验表明，MENTOR相比监督微调和标准稀疏奖励RL基线，显著提高了小语言模型的跨域泛化能力和策略能力。

Conclusion: MENTOR框架通过协同结合强化学习和教师引导蒸馏，有效解决了小语言模型工具使用能力蒸馏中的关键挑战。

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [6] [Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models](https://arxiv.org/abs/2510.18454)
*Atharvan Dogra,Soumya Suvra Ghosal,Ameet Deshpande,Ashwin Kalyan,Dinesh Manocha*

Main category: cs.CL

TL;DR: 该研究评估了LLM幽默生成中的安全问题，发现有害内容（刻板印象和毒性）与幽默评分正相关，在角色提示下进一步放大，揭示了生成器和评估器之间的偏见放大循环。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于创意写作和互动内容，评估其输出安全性变得重要。研究以幽默生成为测试平台，分析趣味性优化如何与有害内容耦合。

Method: 通过联合测量幽默度、刻板性和毒性来评估六个模型，补充信息论指标分析不一致性信号，并在讽刺生成任务上进行外部验证。

Result: 有害输出获得更高的幽默评分，在角色提示下进一步增加；信息论分析显示有害线索扩大预测不确定性；LLM讽刺增加刻板性和毒性，刻板/有毒笑话的幽默评分平均提高10-21%。

Conclusion: LLM幽默生成存在安全风险，有害内容与幽默感知正相关，需要开发更安全的优化方法。

Abstract: Large language models are increasingly used for creative writing and
engagement content, raising safety concerns about the outputs. Therefore,
casting humor generation as a testbed, this work evaluates how funniness
optimization in modern LLM pipelines couples with harmful content by jointly
measuring humor, stereotypicality, and toxicity. This is further supplemented
by analyzing incongruity signals through information-theoretic metrics. Across
six models, we observe that harmful outputs receive higher humor scores which
further increase under role-based prompting, indicating a bias amplification
loop between generators and evaluators. Information-theoretic analyses show
harmful cues widen predictive uncertainty and surprisingly, can even make
harmful punchlines more expected for some models, suggesting structural
embedding in learned humor distributions. External validation on an additional
satire-generation task with human perceived funniness judgments shows that LLM
satire increases stereotypicality and typically toxicity, including for closed
models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor
score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes
marked funny by LLM-based metric and up to $10\%$ more often in generations
perceived as funny by humans.

</details>


### [7] [KAT-Coder Technical Report](https://arxiv.org/abs/2510.18779)
*Zizheng Zhan,Ken Deng,Xiaojiang Zhang,Jinghui Wang,Huaixi Tang,Zhiyi Lai,Haoyang Huang,Wen Xiang,Kun Wu,Wenhao Zhuang,Minglei Zhang,Shaojie Wang,Shangpeng Yan,Kepeng Lei,Zongxian Feng,Huiming Wang,Zheng Lin,Mengtong Li,Mengfei Xie,Yinghan Cui,Xuxing Chen,Chao Wang,Weihao Li,Wenqiang Zhu,Jiarong Zhang,Jingxuan Xu,Songwei Yu,Yifan Yao,Xinping Lei,Han Li,Junqi Xiong,Zuchen Gao,Dailin Li,Haimo Li,Jiaheng Liu,Yuqun Zhang,Junyi Peng,Haotian Zhang,Bin Chen*

Main category: cs.CL

TL;DR: KAT-Coder是一个通过多阶段课程训练的大规模智能代码模型，包括中期训练、监督微调、强化微调和部署适应阶段，旨在解决静态文本训练与动态智能执行之间的差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在智能编码方面取得进展，但静态文本训练与动态实际执行之间仍存在核心挑战，需要弥合这一差距。

Method: 采用四阶段训练课程：中期训练增强推理、规划和反思能力；监督微调构建平衡多编程语言和任务类型的数据集；强化微调引入多真实奖励公式；部署适应阶段使用错误掩码监督微调和树结构轨迹训练。

Result: KAT-Coder实现了稳健的工具使用可靠性、指令对齐和长上下文推理能力，形成了可部署的智能编码代理基础。32B模型KAT-Dev已在HuggingFace开源。

Conclusion: 多阶段训练课程使KAT-Coder能够有效解决智能编码中的训练-执行差距问题，为现实世界智能编码代理提供了可部署的基础。

Abstract: Recent advances in large language models (LLMs) have enabled progress in
agentic coding, where models autonomously reason, plan, and act within
interactive software development workflows. However, bridging the gap between
static text-based training and dynamic real-world agentic execution remains a
core challenge. In this technical report, we present KAT-Coder, a large-scale
agentic code model trained through a multi-stage curriculum encompassing
Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning
(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances
reasoning, planning, and reflection capabilities through a corpus of real
software engineering data and synthetic agentic interactions. The SFT stage
constructs a million-sample dataset balancing twenty programming languages, ten
development contexts, and ten task archetypes. The RFT stage introduces a novel
multi-ground-truth reward formulation for stable and sample-efficient policy
optimization. Finally, the Reinforcement-to-Deployment phase adapts the model
to production-grade IDE environments using Error-Masked SFT and Tree-Structured
Trajectory Training. In summary, these stages enable KAT-Coder to achieve
robust tool-use reliability, instruction alignment, and long-context reasoning,
forming a deployable foundation for real-world intelligent coding agents. Our
KAT series 32B model, KAT-Dev, has been open-sourced on
https://huggingface.co/Kwaipilot/KAT-Dev.

</details>


### [8] [WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection](https://arxiv.org/abs/2510.18798)
*Guanzhong He,Zhen Yang,Jinxin Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WebSeer是一个通过强化学习训练、具有自我反思机制的智能搜索代理，能够在真实网络环境中生成更长的工具使用轨迹，显著提升答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理存在工具使用深度浅和多轮交互中错误累积的问题，需要更智能的搜索代理来改善这些问题。

Method: 构建带有反思模式标注的大规模数据集，设计两阶段训练框架（冷启动和强化学习），在自我反思范式下统一训练，使模型能生成更长、更具反思性的工具使用轨迹。

Result: 使用单个14B模型在HotpotQA和SimpleQA上分别达到72.3%和90.0%的准确率，取得最先进结果，并在分布外数据集上表现出强泛化能力。

Conclusion: WebSeer通过自我反思机制显著扩展了工具使用链并提高了答案准确性，为智能搜索代理的发展提供了有效解决方案。

Abstract: Search agents have achieved significant advancements in enabling intelligent
information retrieval and decision-making within interactive environments.
Although reinforcement learning has been employed to train agentic models
capable of more dynamic interactive retrieval, existing methods are limited by
shallow tool-use depth and the accumulation of errors over multiple iterative
interactions. In this paper, we present WebSeer, a more intelligent search
agent trained via reinforcement learning enhanced with a self-reflection
mechanism. Specifically, we construct a large dataset annotated with reflection
patterns and design a two-stage training framework that unifies cold start and
reinforcement learning within the self-reflection paradigm for real-world
web-based environments, which enables the model to generate longer and more
reflective tool-use trajectories. Our approach substantially extends tool-use
chains and improves answer accuracy. Using a single 14B model, we achieve
state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and
90.0%, respectively, and demonstrate strong generalization to
out-of-distribution datasets. The code is available at
https://github.com/99hgz/WebSeer

</details>


### [9] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: LightMem是一个平衡性能与效率的LLM记忆系统，基于人类记忆模型构建三阶段记忆架构，显著提升准确率同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆系统在动态复杂环境中存在显著的时间和计算开销问题，需要更高效的记忆机制来利用历史交互信息。

Method: 基于Atkinson-Shiffrin人类记忆模型，构建三阶段记忆架构：认知启发的感官记忆快速过滤信息，主题感知的短期记忆组织内容，睡眠时间更新的长期记忆离线整合。

Result: 在LongMemEval基准测试中，LightMem相比基线准确率提升达10.9%，同时token使用减少117倍，API调用减少159倍，运行时间减少12倍以上。

Conclusion: LightMem在保持高性能的同时显著提升了记忆系统的效率，为LLM在动态环境中的实际应用提供了可行的解决方案。

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [10] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 论文揭示了LLM在推理过程中的分层预测动态，提出了"猜测-精炼"框架，显示早期层产生高频词作为统计猜测，后期层基于上下文信息进行精炼。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何在不同层间分配计算资源，揭示其深度使用的结构化模式，为提升计算效率提供洞见。

Method: 追踪多个开源权重模型在推理过程中的中间表示，分析不同层级的预测行为，包括词频分析、词性分析、事实回忆和多选任务分析。

Result: 发现早期层主要预测高频词作为统计猜测，这些猜测在后期层被精炼（>70%）；功能词最早被正确预测；多词答案中首个词需要更多计算深度；模型在前半层识别响应格式，后半层确定最终答案。

Conclusion: LLM的深度使用具有结构化特征，早期层进行统计猜测，后期层进行上下文精炼，这为优化transformer模型的计算效率提供了重要启示。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [11] [Neuro SAN: an open source orchestration framework for multi-agent systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cognizant.com%2Fus%2Fen%2Fai-lab%2Fneuro-san%3Futm_source=tldrai/1/0100019a01c6bc04-ee202968-6f1c-4d55-8a20-ac9f54004ff5-000000/T2nEOtCaDibHMN6qlcqeTLs5HUsw-VElj0ZKKvfOuQo=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Neuro SAN是一个开源的多智能体系统编排框架，允许用户基于描述构建工作多智能体网络，连接API和工具，并在不暴露提示的情况下共享数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决多智能体系统中协调、API集成和数据安全共享的挑战，提供一个开源解决方案来简化多智能体网络的构建和管理。

Method: 使用Cognizant的开源库，基于描述性输入自动生成工作多智能体网络，支持API和工具连接，实现安全的跨智能体数据共享机制。

Result: 开发了一个功能完整的多智能体编排框架，能够有效协调多个智能体，安全处理数据交换，并集成外部工具和API。

Conclusion: Neuro SAN为构建复杂多智能体系统提供了一个实用且安全的开源解决方案，简化了开发流程并增强了系统协作能力。

Abstract: Neuro SAN: an open source orchestration framework for multi-agent systems (Sponsor) Use Cognizant's open-source library to vibe code a working multi-agent network based on a description; connect APIs and tools; share data between agents without exposing prompts. Read the details

</details>


### [12] [Claude Code is unreasonably good at building MVPs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brethorsting.com%2Fblog%2F2025%2F10%2Fclaude-code-is-unreasonably-good-at-building-mvps%2F%3Futm_source=tldrnewsletter/1/0100019a064bbbe2-83d2b17b-df33-4a01-9f19-844f5d0bf208-000000/L1LVMFS-DfaqENBWoChm_0o3lkpm0q5pGOXq_7WRY18=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code在构建MVP方面表现异常出色，在原型验证阶段无需过度担心技术债务和代码质量，重点在于快速测试更多想法。


<details>
  <summary>Details</summary>
Motivation: 在原型和验证阶段，技术债务和代码质量的担忧为时过早，完美代码并非验证产品需求所必需，需要的是快速测试更多想法的能力。

Method: 利用Claude Code进行构建和测试，通过实际用户接触来验证想法。

Result: Claude Code能够帮助人们通过构建和测试来验证产品想法，只有那些经得起实际用户检验的想法才值得重构、加固和扩展。

Conclusion: 在MVP阶段应优先关注想法验证而非代码完美度，Claude Code为此提供了有效工具。

Abstract: Claude Code is unreasonably good at building MVPs (4 minute read) Concerns about technical debt and quality are premature in the prototyping and validation stage. Perfect code isn't required to validate whether people want your product. What is needed is the ability to test more ideas quickly. Claude Code allows people to validate through building and testing. The ideas that survive contact with actual users are the ones worth refactoring, hardening, and scaling.

</details>


### [13] [AI hype is excessive, but its productivity gains are real](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pcloadletter.dev%2Fblog%2Fai-hype-and-productivity%2F%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/XGn2sgyUM--NlqPuwgRwbWkLufI1IEgTua8XOnpo90o=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程工具（如Cursor）通过自动补全和智能面板功能，在代码理解、脚手架搭建和调试方面显著提升开发效率


<details>
  <summary>Details</summary>
Motivation: 作者最初因AI过度炒作而对其持怀疑态度，但在实际使用AI编程工具后改变了看法，希望验证这些工具的实际生产力价值

Method: 通过实际使用Cursor等AI编程工具，特别是其自动补全和智能面板功能，结合清晰的指令来辅助代码开发工作

Result: 发现AI编程工具能有效提升开发效率，特别是在代码理解、脚手架搭建和调试方面，证明了其实际的生产力增益

Conclusion: 尽管AI存在过度炒作，但AI编程工具确实能带来真实的生产力提升，值得开发者采用

Abstract: AI hype is excessive, but its productivity gains are real (3 minute read) This dev initially dismissed AI due to excessive hype because of how companies unrealistically claimed it could solve all problems. However, their perspective changed after using AI-powered coding tools like Cursor, especially the autocomplete feature. They found these tools, especially the agent panel, when used with clear instructions, boosted their productivity by helping with code comprehension, scaffolding, and deb...

</details>


### [14] [Claude Code on the Web](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/CnftcoRcRXcXMBiQAbBcu--Neaj6A_chULQ3rwastss=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出了Claude Code网页版测试功能，允许用户直接在浏览器中委托编码任务给Claude，支持并行执行和实时进度跟踪。


<details>
  <summary>Details</summary>
Motivation: 提供更便捷的云端编码服务，让用户无需本地环境即可完成编码任务，提高开发效率。

Method: 基于浏览器的云服务，在沙盒环境中并行执行编码任务，支持连接GitHub仓库。

Result: 成功推出了Claude Code网页版测试功能，实现了云端编码任务的委托和执行。

Conclusion: Claude Code网页版为用户提供了更便捷的云端编码体验，有望提升开发效率。

Abstract: Claude Code on the Web (2 minute read) Anthropic has launched Claude Code on the web, a beta feature allowing users to delegate coding tasks to Claude directly from their browser. This cloud-based service allows for parallel execution of coding tasks in a sandboxed environment, connecting to GitHub repos and providing real-time progress tracking.

</details>


### [15] [Claude Code on the web](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/GKAtygDUYgsgi9kpjnGD_yK0AIT0OsRudnzgsOdMXxI=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出基于浏览器的Claude Code测试版，支持连接GitHub仓库并同时委派多个跨仓库任务，每个会话在隔离沙箱中运行


<details>
  <summary>Details</summary>
Motivation: 提供更便捷的代码开发体验，让用户能够直接在浏览器中使用Claude Code功能，无需本地安装

Method: 开发浏览器版本，支持GitHub仓库连接，实现多任务并行处理和隔离沙箱环境

Result: 成功推出Claude Code网页版测试版，用户可在浏览器中访问并使用其代码开发功能

Conclusion: 浏览器版Claude Code的推出为用户提供了更灵活、便捷的代码开发工具

Abstract: Claude Code on the web (2 minute read) Anthropic has launched a browser-based version of Claude Code. The beta allows users to connect GitHub repositories and delegate multiple tasks simultaneously across different repositories, with each session running in isolated sandboxes.

</details>


### [16] [Meta AI's app downloads and daily users spiked after launch of ‘Vibes' AI video feed](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F20%2Fmeta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed%2F%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/KuMgoWVrgWuTtLVYu1sVna4dH323qVcOewuZjNn5ETs=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude Code网页应用，旨在将AI编程助手从命令行工具扩展到更广泛的使用场景，用户增长显著。


<details>
  <summary>Details</summary>
Motivation: 将Claude Code从命令行界面工具演进为更易用的网页应用，让开发者能在更多环境中创建AI编程代理。

Method: 推出Claude Code网页应用，面向Pro和Max用户逐步推广。

Result: 自5月广泛发布以来用户增长10倍，现在贡献超过5亿美元收入。

Conclusion: 网页版的推出成功扩展了Claude Code的使用场景和用户基础。

Abstract: Meta AI's app downloads and daily users spiked after launch of ‘Vibes' AI video feed (3 minute read) Anthropic launched a web app for Claude Code yesterday. It is now rolling out to Pro and Max users. Anthropic is attempting to evolve Claude Code beyond a command-line interface tool. It hopes developers will spin up AI coding agents in more places now that Claude Code is on the web. Claude Code has grown 10x in users since its broader launch in May. It now accounts for more than $500 million ...

</details>


### [17] [Agentic AI's OODA Loop Problem](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fschneier.com%2Fblog%2Farchives%2F2025%2F10%2Fagentic-ais-ooda-loop-problem.html%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/xlbhO5-rlvjIq6KBz60Xe1K5WJjr6gdG5r4TbqJq5E8=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理在训练过程中嵌入了不可信的行为者，即使准确解释输入并产生相应输出，也可能完全腐败，仅修复幻觉问题是不够的。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在OODA循环中面临的根本安全问题，指出仅解决幻觉问题不足以确保AI系统的可信性。

Method: 分析AI代理训练过程中嵌入不可信行为者的机制，以及这对OODA循环完整性的影响。

Result: 发现即使AI准确处理输入和输出，系统仍可能因训练过程中的腐败而变得不可信。

Conclusion: 需要超越幻觉修复，从根本上解决AI代理训练过程中的安全漏洞。

Abstract: Agentic AI's OODA Loop Problem (13 minute read) AI agents embed untrusted actors within themselves in their training, so fixing hallucinations is insufficient because even if an AI accurately interprets its inputs and produces corresponding output, it can be fully corrupt.

</details>


### [18] [Tech CEOs say the era of 'code by AI' is here. Some software engineers are skeptical](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.npr.org%2F2025%2F10%2F21%2Fnx-s1-5506141%2Fai-code-software-productivity-claims%3Futm_source=tldrnewsletter/1/0100019a0b72cf0a-693f0a61-2e44-4518-8635-428c0d4e809e-000000/kz5Ahqn5dZTrzRTh8QVxvVTKOUmUy7PPYSJQ23OdfoM=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 科技CEO声称'AI编程'时代已到来，但许多软件工程师对此持怀疑态度，认为没有长期效率提升的证据


<details>
  <summary>Details</summary>
Motivation: 探讨AI编程工具在实际软件开发中的真实效果和工程师的接受程度

Method: 通过采访软件工程师和科技CEO，收集他们对AI编程工具使用体验的看法

Result: 软件工程师普遍对AI编程工具的实际效率提升持怀疑态度，与科技CEO的乐观宣传形成对比

Conclusion: 虽然AI编程工具被大力推广，但在实际开发中尚未证明能带来长期效率提升

Abstract: Tech CEOs say the era of 'code by AI' is here. Some software engineers are skeptical (6 minute read) Many software engineers have seen no evidence of long-term boosts to efficiency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: 开发了一个基于检索增强生成(RAG)的工具执行后反思组件，用于修复kubectl命令执行失败的问题，通过结合LLM反思和领域特定文档来提高命令执行成功率和正确性。


<details>
  <summary>Details</summary>
Motivation: 代理系统调用外部工具时经常因各种语法和语义原因失败，一些语义错误只能在分析工具响应后才能识别和解决，需要开发有效的错误修复机制。

Method: 使用基于大型语言模型的反思组件，结合特定领域检索增强生成(RAG)，利用工具描述文档和故障排除文档来修复工具调用错误，以kubectl命令管理Kubernetes为具体用例。

Result: RAG-based反思修复了kubectl命令，使55%的评估模型更可能成功执行，平均提高36%的正确回答用户查询的可能性。故障排除文档相比官方文档平均提高10%的执行成功率。

Conclusion: 基于RAG的反思方法能有效修复工具调用错误，提高命令执行成功率和正确性，故障排除文档在修复过程中发挥重要作用。

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [20] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: TritonRL是一个专门用于Triton内核生成的领域专用LLM，通过包含监督微调和强化学习的训练框架，解决了数据稀缺和评估标准不完整的问题，在KernelBench上实现了最先进的正确性和加速效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，对自动化高性能系统内核的需求成为加速开发和部署的关键推动因素。Triton内核生成面临数据稀缺和不完整评估标准的独特挑战，容易受到奖励攻击的影响。

Method: 通过监督微调在精选数据集上蒸馏Triton特定知识，然后通过强化学习改进代码质量，采用稳健可验证的奖励和分层奖励分配。RL框架能够检测奖励攻击，并通过细粒度验证和分层奖励分解指导推理轨迹和代码标记。

Result: 在KernelBench上的实验表明，TritonRL实现了最先进的正确性和加速效果，超越了所有其他Triton特定模型。

Conclusion: TritonRL能够生成真正可以替代现有模块的高质量Triton内核，证明了基于RL的训练范式的有效性。

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [21] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: 这篇论文对2022-2024年间31项研究进行了系统文献综述，探讨了利用生成式AI增强代码理解的方法和工具。研究发现尽管GenAI有潜力，但经常产生不准确或模糊的解释，新手程序员在编写有效提示词方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 随着程序员越来越多地依赖生成式AI助手开发代码解决方案，理解GenAI生成的代码变得至关重要。同时，GenAI工具也被用于提供代码解释，这对计算教育提出了新的挑战和机遇。

Method: 采用系统文献综述方法，分析31项2022-2024年间发表的研究，对基于GenAI的方法和工具进行分类，识别研究方法，并总结其有效性的实证评估。

Result: 研究发现GenAI助手经常产生不准确或模糊的解释，新手程序员在编写有效提示词方面存在困难，这阻碍了他们利用GenAI辅助代码理解的能力。

Conclusion: 该综述为计算教育工作者提供了基于证据的指导，指出了GenAI在促进代码理解方面的应用方向，并确定了未来研究的重点。

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [22] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: SpecAgent通过索引时异步探索仓库文件构建推测性上下文，在降低推理延迟的同时提升代码生成质量，相比最佳基线实现9-11%的绝对性能提升。


<details>
  <summary>Details</summary>
Motivation: LLMs在真实软件仓库中表现不佳，因为项目特定API和跨文件依赖关系很重要。现有检索增强方法在推理时注入仓库上下文，但低延迟预算会影响检索质量或用户体验。

Method: 在索引时异步探索仓库文件，构建推测性上下文来预测每个文件的未来编辑，通过索引时异步性实现彻底上下文计算并掩盖延迟。

Result: SpecAgent相比最佳基线实现9-11%的绝对性能提升（48-58%相对提升），同时显著降低推理延迟。

Conclusion: 索引时异步构建推测性上下文的方法能同时改善延迟和代码生成质量，解决了现有基准中的未来上下文泄露问题。

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [23] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: Chart2Code是一个新的多模态模型基准测试，专注于图表理解和代码生成能力，包含三个难度级别：图表复制、图表编辑和长表格到图表生成，共2023个任务。


<details>
  <summary>Details</summary>
Motivation: 从用户驱动视角设计，捕捉真实世界场景，系统性地增加任务难度，推动多模态推理的发展。

Method: 构建包含22种图表类型的层次化基准测试，采用多级评估指标评估代码正确性和图表视觉保真度。

Result: 测试了25个最先进的多模态模型，即使是GPT-5在编辑任务上的平均得分也仅为0.57（代码评估）和0.22（图表质量评估），显示该基准的挑战性。

Conclusion: Chart2Code基准将推动多模态推理的进步，促进开发更鲁棒和通用的多模态模型。

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [24] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: JunoBench是首个针对Python机器学习笔记本的真实崩溃基准数据集，包含111个来自Kaggle笔记本的可重现崩溃案例及其验证修复。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门针对机器学习笔记本的调试工具基准，作者创建了JunoBench来填补这一空白。

Method: 从公开Kaggle笔记本中收集并筛选111个真实崩溃案例，为每个案例提供可验证的修复方案，并构建统一的执行环境确保可重现性。

Result: 创建了覆盖TensorFlow/Keras、PyTorch、Scikit-learn、Pandas、NumPy等主流ML库以及笔记本特定执行顺序问题的基准数据集。

Conclusion: JunoBench为笔记本式机器学习开发中的错误检测、定位和修复提供了现实基准支持。

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [25] [BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI](https://arxiv.org/abs/2510.18131)
*Chengquan Guo,Yuzhou Nie,Chulin Xie,Zinan Lin,Wenbo Guo,Bo Li*

Main category: cs.SE

TL;DR: BlueCodeAgent是一个端到端的蓝队代理，通过自动化红队生成多样化风险实例，结合宪法和代码分析进行多级防御，在代码安全任务中显著优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成中的应用增多，安全风险日益突出。目前研究主要集中在红队测试，而蓝队防御方面进展有限，需要有效的语义理解来区分安全与不安全代码。

Method: 提出BlueCodeAgent框架，集成红队和蓝队：红队生成多样化风险实例，蓝队代理利用这些实例通过宪法和代码分析检测已知和未知风险场景，采用动态分析减少误报。

Result: 在三个代表性代码相关任务（偏见指令检测、恶意指令检测、漏洞代码检测）中，BlueCodeAgent相比基础模型和基于安全提示的防御方法取得显著提升，在四个数据集的三个任务中平均F1分数提高了12.7%。

Conclusion: 红队测试通过持续识别新漏洞来增强蓝队防御性能，BlueCodeAgent能够总结可操作的宪法来增强上下文感知的风险检测能力。

Abstract: As large language models (LLMs) are increasingly used for code generation,
concerns over the security risks have grown substantially. Early research has
primarily focused on red teaming, which aims to uncover and evaluate
vulnerabilities and risks of CodeGen models. However, progress on the blue
teaming side remains limited, as developing defense requires effective semantic
understanding to differentiate the unsafe from the safe. To fill in this gap,
we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated
red teaming. Our framework integrates both sides: red teaming generates diverse
risky instances, while the blue teaming agent leverages these to detect
previously seen and unseen risk scenarios through constitution and code
analysis with agentic integration for multi-level defense. Our evaluation
across three representative code-related tasks--bias instruction detection,
malicious instruction detection, and vulnerable code detection--shows that
BlueCodeAgent achieves significant gains over the base models and safety
prompt-based defenses. In particular, for vulnerable code detection tasks,
BlueCodeAgent integrates dynamic analysis to effectively reduce false
positives, a challenging problem as base models tend to be over-conservative,
misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average
12.7\% F1 score improvement across four datasets in three tasks, attributed to
its ability to summarize actionable constitutions that enhance context-aware
risk detection. We demonstrate that the red teaming benefits the blue teaming
by continuously identifying new vulnerabilities to enhance defense performance.

</details>


### [26] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: TestPrune是一个自动化技术，利用回归测试来增强bug复现和补丁验证，通过最小化测试套件来解决LLM调试技术中的上下文限制问题。


<details>
  <summary>Details</summary>
Motivation: 现实项目中的测试套件虽然覆盖率高，但仍无法检测所有bug。回归测试除了确保功能保留外，还可用于调试当前版本，特别是帮助复现新报告的问题和验证补丁。

Method: TestPrune自动最小化回归测试套件，提取高度相关的测试子集，可集成到任何基于代理的bug修复流程中。

Result: 在Otter框架中，TestPrune使问题复现率相对提高6.2%-9.0%；在Agentless框架中，问题解决率相对提高9.4%-12.9%。成本开销极小，每个SWE-Bench实例仅需$0.02-$0.05。

Conclusion: TestPrune能有效提升bug修复管道的性能，成本效益高，可正交改善整体表现。

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [27] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: InspectCoder是首个基于智能代理的程序修复系统，通过交互式调试器控制让LLM进行动态分析，显著提升了代码修复的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自修复方法主要依赖静态语义分析或浅层执行日志，缺乏深入运行时行为分析能力，无法有效诊断复杂逻辑错误的根本原因。

Method: 采用双代理框架，通过战略断点放置、目标状态检查和增量运行时实验，在状态化调试会话中进行自适应动态分析。

Result: 在两个挑战性自修复基准测试中，修复准确率相对提升5.10%-60.37%，bug修复效率提升1.67x-2.24x。

Conclusion: LLM驱动的动态分析在自动化软件工程中具有显著潜力，交互式LLM-调试器系统为代码修复提供了新范式。

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [28] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究通过三个案例研究分析了人类与生成式AI在计算笔记本中编写代码和文档的差异，发现金牌得主以更长更详细的文档为特征，而GenAI笔记本代码质量更高但人类笔记本在结构多样性和创新性方面更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，在竞争环境中区分人类编写与AI生成笔记本的特征变得愈发重要，以探索人类与AI在笔记本活动中的各自优势。

Method: 通过三个案例研究，分析25个代码和文档特征在人类编写和GenAI笔记本中的差异，特别关注Kaggle竞赛中获奖笔记本的特征。

Result: 金牌得主主要特征是更长更详细的文档；GenAI笔记本代码质量更高（代码异味和技术债务指标更好），但人类笔记本在结构多样性、复杂性和问题解决创新性方面表现更佳。

Conclusion: 这项研究为探索如何最大化人类与AI在笔记本中的协作潜力奠定了基础，提出了四个进一步研究的议程。

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [29] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: CodeRL+通过整合执行语义对齐到RLVR训练流程中，解决了LLM代码生成中文本模式与执行语义之间的差距问题，显著提升了代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法仅依赖测试用例的二元通过/失败信号，难以有效建立代码文本表示与执行语义之间的对齐关系，特别是对于代码中的细微逻辑错误。

Method: 提出CodeRL+方法，使模型能够推断变量级执行轨迹，提供执行语义的直接学习信号，可直接利用现有策略rollouts构建执行语义对齐，并与各种RL算法无缝集成。

Result: CodeRL+在pass@1上实现4.6%的相对提升，在代码推理和测试输出生成基准上分别获得15.5%和4.4%的准确率提升，且在不同RL算法和LLM上均表现出强适用性。

Conclusion: CodeRL+有效加强了代码文本表示与底层执行语义之间的对齐，为代码生成任务提供了更有效的训练方法。

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [30] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 提出基于LLM的多智能体系统VAPU，用于自主更新遗留应用程序，相比零样本和单样本学习提示，在低温度设置下能实现相似错误数但更高需求完成率，Python文件更新需求成功率提升达22.5%。


<details>
  <summary>Details</summary>
Motivation: 遗留应用程序包含过时组件，存在兼容性、安全性和可靠性风险，但高昂的资源成本使企业不愿更新。需要成本效益高的自主更新解决方案。

Method: 设计名为VAPU的多智能体系统，模拟软件开发团队不同角色，分阶段更新代码文件。扩展评估从单一LLM到五个LLM，温度参数在0-1之间设置，使用20个开源Python GitHub项目进行测试。

Result: 在低温度设置下，VAPU与ZSL/OSL提示相比错误数相似，但需求完成率更高，具体取决于使用的LLM。Python文件更新需求成功率最高提升22.5%。

Conclusion: 基于LLM的多智能体系统是自主更新遗留应用程序组件的有效解决方案。

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [31] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: WebDevJudge是一个用于评估LLM作为评判者在网页开发任务中性能的系统基准，支持非交互式静态评估和连续交互式动态评估，揭示了LLM评判者与人类专家之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为评判者在动态环境和复杂交互的开放任务中的可靠性，填补现有研究空白。

Method: 构建包含人类偏好标签的网页实现配对数据集，采用结构化且基于查询的评分标准，评估各种评估者（LLM、MLLM、代理工作流）在不同范式和指导机制下的表现。

Result: 实验显示LLM评判者与人类专家存在显著差距，主要源于模型在识别功能等价性、验证任务可行性以及减轻偏见方面的基本限制。

Conclusion: WebDevJudge对LLM作为评判者范式提出了重大挑战，为未来开发更可靠和强大的自动化评估者提供了指导方向。

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [32] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: 提出了CUARewardBench，首个用于评估计算机使用代理奖励模型的基准，包含结果奖励模型和过程奖励模型评估，通过统一的提示集成方法显著提升了奖励模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于脚本的验证器存在可扩展性有限和无法提供逐步评估的问题，而奖励模型在计算机使用代理评估中的有效性尚未充分探索。

Method: 构建了包含10个软件类别和7种代理架构的多样化数据集，通过专家标注确保可靠性，提出统一的提示集成方法进行奖励模型评估。

Result: 揭示了当前计算机使用代理奖励模型的关键局限性，包括视觉推理能力不足和知识缺陷，统一的提示集成方法在结果奖励模型上达到89.8%精确度和93.3%负预测值，在过程奖励模型上达到81.7%精确度和85.1%负预测值。

Conclusion: 统一的提示集成方法显著提升了奖励模型的可靠性，通用视觉语言模型在奖励评估中优于专门的计算机使用代理模型。

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [33] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: AToMIC是一个自动化框架，利用专门的大型语言模型从需求(JIRA工单)和代码变更直接生成Gherkin场景、页面对象和可执行的UI测试脚本，显著减少了移动应用验收测试的手动工作量。


<details>
  <summary>Details</summary>
Motivation: 移动验收测试在现代软件开发中仍然是瓶颈，特别是在使用Flutter等跨平台移动开发框架时。虽然开发者越来越依赖自动化测试工具，但创建和维护验收测试工件仍需要大量手动工作。

Method: AToMIC框架利用专门的大型语言模型，从需求(JIRA工单)和最近的代码变更直接生成Gherkin场景、页面对象和可执行的UI测试脚本。

Result: 在BMW MyBMW应用中测试13个真实问题，覆盖170+屏幕的代码库，AToMIC在标准硬件上每个功能在5分钟内生成可执行测试工件。93.3%的Gherkin场景在生成时语法正确，78.8%的页面对象无需手动编辑即可运行，100%生成的UI测试成功执行。

Conclusion: AToMIC被证实为工业移动项目中简化验收测试创建和维护的可扩展、实用解决方案，所有实践者都报告了时间节省(通常每个功能节省一个完整开发日)并对采用该方法有强烈信心。

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [34] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: EffiReasonTrans是一个训练框架，通过构建推理增强数据集和两阶段训练策略，在提高代码翻译准确性的同时平衡推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码翻译中准确性提升与推理延迟增加之间的权衡问题，满足实际开发流程中需要人工检查的需求。

Method: 首先使用更强的语言模型DeepSeek-R1构建推理增强数据集，然后采用两阶段训练：监督微调推理增强样本，接着使用强化学习进一步优化准确性和推理延迟。

Result: 在六个翻译对上，翻译准确性显著提升（CA最高+49.2%，CodeBLEU最高+27.8%），同时减少生成token数量（最高-19.3%）和降低推理延迟（最高-29.0%）。

Conclusion: EffiReasonTrans框架有效平衡了代码翻译的准确性和效率，并在基于代理的框架中表现出更好的翻译准确性。

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [35] [<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzYzNjA4MzE3MQ==&mid=2247483705&idx=1&sn=6fb1c8aec74cc7bb22b230c863d82cf9&chksm=f1810124934f6febc5cf0a067e0a289e15c8ccfd7fe52f08e3bd2e4f6f79332bb92e70f17e1e#rd)
*西米吹水*

Main category: wechat.article

TL;DR: 详细阐述了自己对强化学习的负面看法。he elaborated in detail his negative views on reinforcement learning.其中核心观点基于他2019年就提出的著名的“苦涩的教训”这个原则。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 详细阐述了自己对强化学习的负面看法。he elaborated in detail his negative views on reinforcement learning.其中核心观点基于他2019年就提出的著名的“苦涩的教训”这个原则。

</details>


### [36] [探路智能体落地“最后一公里”：复现Cursor在线<em class="highlight">强化学习</em>，RLinf-Online团队详解技术实现路径及背后思考](http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247500906&idx=1&sn=51df2f9079a06834b65499902e76c1e5&chksm=c357939b2fe1d01e5b987c4dd826ef49d5579d136cf4800bb71ac9508ed7bf703fc5926f3c85#rd)
*无问芯穹*

Main category: wechat.article

TL;DR: 我们深深认同，强化学习是推动智能体实现能力跃升的关键技术路径，而在线强化学习的意义在于针对生产环境进行持续的自进化，成为智能体进一步发挥能力的利器。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们深深认同，强化学习是推动智能体实现能力跃升的关键技术路径，而在线强化学习的意义在于针对生产环境进行持续的自进化，成为智能体进一步发挥能力的利器。

</details>


### [37] [原创 | 大模型扫盲系列（三）-<em class="highlight">强化学习</em>基础（上）](http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660816&idx=1&sn=e73491aa84bb0054e3ce5dc3579c5915&chksm=e8fac5c56633e89f6430f4b0d7bd76c0b8100ef6b11939f503d78651fe71c6fcc7defc6daef7#rd)
*数据派THU*

Main category: wechat.article

TL;DR: 强化学习：属于机器学习的一个分支，通过"试错"机制学习，核心思想是：智能体（Agent）在环境中采取行动，获得奖励或惩罚，从而学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习：属于机器学习的一个分支，通过"试错"机制学习，核心思想是：智能体（Agent）在环境中采取行动，获得奖励或惩罚，从而学习最优策略。

</details>


### [38] [【NeurIPS 2025】| 山西大学 iDUST LAB 在离线<em class="highlight">强化学习</em>研究中取得重要进展](http://mp.weixin.qq.com/s?__biz=Mzg2NjY2NTk3Ng==&mid=2247484889&idx=2&sn=9d76bbc4703487d4b8d9da26c35a04c1&chksm=cf8df7b3f5ae2867f236e60d2b7c616d114f03d5e93d5b19033875f594c9403f4eddbf832487#rd)
*SXU iDUST Lab*

Main category: wechat.article

TL;DR: 特别适用于小批量甚至单状态输入的强化学习场景。其标准计算方式为：.网络集成：为了进一步增强稳定性并减轻离线强化学习中常见的高估偏差，FANS 在 actor-critic 框架内采用了 M 个独立参数化 critic 网络的集成。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 特别适用于小批量甚至单状态输入的强化学习场景。其标准计算方式为：.网络集成：为了进一步增强稳定性并减轻离线强化学习中常见的高估偏差，FANS 在 actor-critic 框架内采用了 M 个独立参数化 critic 网络的集成。

</details>


### [39] [VL Norm：让<em class="highlight">强化学习</em>更稳、更快的关键一步](http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649505530&idx=1&sn=830dfc14892f13b7b31dd238e3279224&chksm=8352f9202dee44dfe5474b49d396a20a83b21d9ad2bc290f0a3ec2514041a5a8db9f8ce6b8e8#rd)
*微软亚洲研究院*

Main category: wechat.article

TL;DR: 成为强化学习研究的重要方向。由微软亚洲研究院与清华大学联合提出的 VL Norm 方法，针对可验证奖励强化学习（RLVR）中因输出长度波动导致的梯度方差过大问题，给出了理论上无偏且方差最小的解决方案。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 成为强化学习研究的重要方向。由微软亚洲研究院与清华大学联合提出的 VL Norm 方法，针对可验证奖励强化学习（RLVR）中因输出长度波动导致的梯度方差过大问题，给出了理论上无偏且方差最小的解决方案。

</details>


### [40] [智源开源EditScore：为图像编辑解锁在线<em class="highlight">强化学习</em>的无限可能](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650996983&idx=3&sn=4c3e3d70f24ff5b5a61e6bd516bef7a9&chksm=8546184462364132e451785d81914f451bfa03b3800a782b2ee6cf1aa59bcd28850bda7d24da#rd)
*机器之心*

Main category: wechat.article

TL;DR: 团队表示，后续将陆续发布应于 OmniGen2 的强化学习训练代码，以及针对 OmniGen2、Flux-dev-Kontext 和 Qwen-Image-Edit 的 Best-of-N 推理脚本，欢迎社区持续关注。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 团队表示，后续将陆续发布应于 OmniGen2 的强化学习训练代码，以及针对 OmniGen2、Flux-dev-Kontext 和 Qwen-Image-Edit 的 Best-of-N 推理脚本，欢迎社区持续关注。

</details>


### [41] [【博士论文】用于排序与扩散模型的安全、高效与鲁棒<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671489&idx=2&sn=a0f05021c664f517be3349c3be4aa085&chksm=fd0597900239ca6de06c8699cbd993a7c04c08bf72c5d678d0bea7c8800a02bf22c54373e7d1#rd)
*专知*

Main category: wechat.article

TL;DR: 论文的最后一部分探讨了**生成式强化学习（generative RL）中效率与效果之间的权衡。通过对 PPO 与 REINFORCE 的系统性研究，我们提出了Leave-One-Out PPO（LOOP）**算法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 论文的最后一部分探讨了**生成式强化学习（generative RL）中效率与效果之间的权衡。通过对 PPO 与 REINFORCE 的系统性研究，我们提出了Leave-One-Out PPO（LOOP）**算法。

</details>


### [42] [第150期 |  LLM背景下<em class="highlight">强化学习</em>经典算法优化](http://mp.weixin.qq.com/s?__biz=MzkyMTE3MzY1MQ==&mid=2247486903&idx=1&sn=ebc6d6a7266dc7baca036a0443546df3&chksm=c01d58cda4bcd4ae903d358d56db537f9d97ca785c5412bd22c4bb9eecd9454851ab7a529f95#rd)
*暨大经管国家级实验教学示范中心*

Main category: wechat.article

TL;DR: 3.主题：LLM背景下强化学习经典算法优化4.汇报人：张永智（暨大管科）本期活动简介在当下的大语言模型研究背景，如何让模型更好地理解人类意图、学习复杂偏好，已成为模型对齐与强化学习的核心议题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 3.主题：LLM背景下强化学习经典算法优化4.汇报人：张永智（暨大管科）本期活动简介在当下的大语言模型研究背景，如何让模型更好地理解人类意图、学习复杂偏好，已成为模型对齐与强化学习的核心议题。

</details>


### [43] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247720944&idx=1&sn=3853950ae0718834d1c174905c294045&chksm=ce22e619cf98352c6954837b5a5922e280d0ea15205ef301e26087ba57bdc9314b8af7299045#rd)
*arXiv每日学术速递*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [44] [毫无疑问，未来AI界将会是<em class="highlight">强化学习</em>的天下](http://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&mid=2247544454&idx=1&sn=771936fb82936ea467b454126af019bf&chksm=fa543636ab40364675c492dcbb11bd5298ee9ce321fde7875e8a5038e365a319bf03d5365bb3#rd)
*计算机视觉研究院*

Main category: wechat.article

TL;DR: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对某一类明确问题（比如多目标、组合优化），提出新的强化学习应用模式。Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection

</details>


### [45] [大佬开炮：智能体都在装样子，<em class="highlight">强化学习</em>很糟糕，AGI 十年也出不来](http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683630&idx=4&sn=9d2b9b2735f46c0661d77fb3a0690cf7&chksm=cf656d9a6f9a965c59ac3e740631d4df997c788018b70124f6f170f3808657f07367a1f64697#rd)
*自动驾驶之心*

Main category: wechat.article

TL;DR: 强化学习是人工智能的另一个重要领域，大概有两三年甚至四年的时间，每个人都在游戏上进行强化学习。这完全是一个失误。我在 OpenAI 尝试做的事情是，我一直对游戏能否引领 AGI 有点怀疑 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是人工智能的另一个重要领域，大概有两三年甚至四年的时间，每个人都在游戏上进行强化学习。这完全是一个失误。我在 OpenAI 尝试做的事情是，我一直对游戏能否引领 AGI 有点怀疑 。

</details>


### [46] [统一 AMP、DeepMimic、ASE、ADD 的<em class="highlight">强化学习</em>动作模仿平台！MimicKit 框架深度解析](http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488958&idx=1&sn=1f8f06d2a650e0e681692052bb1455a6&chksm=c15ecff26f5dfccf459d0324ac98d522bbd304e35a25c56e1b7adafe37618e64632d21dba9da#rd)
*具身智能研究室*

Main category: wechat.article

TL;DR: mimickit 的算法核心，是在强化学习框架下进行动作模仿。其目标不再是让智能体“通过奖励自己摸索”，而是通过数据学习人类的运动分布。在传统 RL 中，智能体依靠稀疏奖励和大量采样学会任务；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: mimickit 的算法核心，是在强化学习框架下进行动作模仿。其目标不再是让智能体“通过奖励自己摸索”，而是通过数据学习人类的运动分布。在传统 RL 中，智能体依靠稀疏奖励和大量采样学会任务；

</details>


### [47] [【专题】AI领域中的“<em class="highlight">强化学习</em>”相关研究-2025年9-10月](http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488086&idx=1&sn=bdb6e6ed5bf72b1f141f9c2a33d2d6d8&chksm=c2f7831ac1017c830f9db97f8fcab78fde3b128365de97ee83d35edb6c9a38d7c423de050c02#rd)
*AI新文*

Main category: wechat.article

TL;DR: 原文链接 时滞微分对策欺骗资源配置的深度强化学习方法 原标题：A Deep Reinforcement Learning Approach to Time Delay Differential Game Deception Resource Deployment作者：Weizhen He；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 原文链接 时滞微分对策欺骗资源配置的深度强化学习方法 原标题：A Deep Reinforcement Learning Approach to Time Delay Differential Game Deception Resource Deployment作者：Weizhen He；

</details>


### [48] [11 月 22 日，我想和你聊聊「AI 写<em class="highlight">代码</em>」这件事](http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487544&idx=1&sn=42bf26fe4b843976cee48db476b85735&chksm=a7e29cc74249b001354e80d10f658ce64a9b8ce46bc245fcb87d3c8185abf861880571c62a17#rd)
*云谦和他的朋友们*

Main category: wechat.article

TL;DR: 区别其他的 code agent，他也有自己的一些特性，比如二开友好、spec driven、多端等。neovate heovate tips to getting started： 1. input a task。2. /init to create aneovate.md file。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 区别其他的 code agent，他也有自己的一些特性，比如二开友好、spec driven、多端等。neovate heovate tips to getting started： 1. input a task。2. /init to create aneovate.md file。

</details>


### [49] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（10.21）part3](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483746&idx=1&sn=48a23ec1bab148828f8df22170cc9596&chksm=e99570bafd039320116f0b859dc5d403fcdcf9144437dd3d6a8b45ec388a332910cb03f47f73#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: Anthropic Claude Code Web 版与沙箱平台（Agent/Release）DeepSeekOCR 模型与视觉记忆压缩（Code LLM 研究）Karpathy nanochat：可黑客化的小型聊天模型（Code LLM 研究）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Anthropic Claude Code Web 版与沙箱平台（Agent/Release）DeepSeekOCR 模型与视觉记忆压缩（Code LLM 研究）Karpathy nanochat：可黑客化的小型聊天模型（Code LLM 研究）

</details>


### [50] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（10.21）part 2](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483741&idx=1&sn=e72e5c62222db42290de8230d20902a4&chksm=e9d3addc0eb42c0e1e5a237336efc7d19a3186aa30e5d2758220fddb5565f516cc5995d19c08#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: Codev：多代理编码平台实测（Agent / Startup）GitHub：9 个 MCP 开源项目（Release）ScaleRL：RL 后训练 S 型曲线（Code LLM Research）Windsurf Editor 1.12.21 补丁（Release）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Codev：多代理编码平台实测（Agent / Startup）GitHub：9 个 MCP 开源项目（Release）ScaleRL：RL 后训练 S 型曲线（Code LLM Research）Windsurf Editor 1.12.21 补丁（Release）

</details>


### [51] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（10.21）part 1](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483736&idx=1&sn=165b97cee7e74a7e1e82fbb35dc9d0af&chksm=e95eaad8738413517ce1516c03b604785ebc8816707d282c284c00df9cfc24d471de352d83fa#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 1|【类型：code llm 研究】— 自改善代理框架 ace（et：20251018；窗口：36h）来源：InfoQ – Robert Krzaczyński（ET 20251018）｜链接：https：//www.infoq.com/news/2025/10/agentic-context-eng/｜一句话结论：ACE 用“生成反思整理”三元结构对上下文做


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1|【类型：code llm 研究】— 自改善代理框架 ace（et：20251018；窗口：36h）来源：InfoQ – Robert Krzaczyński（ET 20251018）｜链接：https：//www.infoq.com/news/2025/10/agentic-context-eng/｜一句话结论：ACE 用“生成反思整理”三元结构对上下文做

</details>


### [52] [最新综述！70+页论文，系统梳理<em class="highlight">Agentic</em>构建范式转变](http://mp.weixin.qq.com/s?__biz=MzI1MzEwMzIwOQ==&mid=2247503965&idx=1&sn=d13fe477cb5186c81d45d435d1456795&chksm=e88431ef9c4c4503a6404b8dab306e0f3ccfeb6ecf6be48992a7570d96fd5e5f25aa3518fd01#rd)
*青稞AI*

Main category: wechat.article

TL;DR: Beyond Pipelines： A Survey of the Paradigm Shift toward Model-Native Agentic AI. J. ACM 1， 1 （October 2025），Jinlin Xiao，Jiarun Han， Beijing Jiaotong University， Beijing， China，Xiaoyi Chen，Shuyu Wei， B...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Beyond Pipelines： A Survey of the Paradigm Shift toward Model-Native Agentic AI. J. ACM 1， 1 （October 2025），Jinlin Xiao，Jiarun Han， Beijing Jiaotong University， Beijing， China，Xiaoyi Chen，Shuyu Wei， Beijing Jiaotong University， Beijing， China，

</details>


### [53] [麦肯锡：<em class="highlight">代理</em>式组织 (<em class="highlight">Agentic</em> Organization) AI时代的下一个范式](http://mp.weixin.qq.com/s?__biz=MzkzODkwODYwMA==&mid=2247486162&idx=1&sn=d5b175a4d42b33c0f1d293f139a7d345&chksm=c37e47b605bb02a59c32b5d7ac4d14a98efcf920f3642aecbfe301a66a3f406141c9bead42f0#rd)
*科叔AI进化记*

Main category: wechat.article

TL;DR: com/capabilities/people-and-organizational-performance/our-insights/the-agentic-organization-contours-of-the-next-paradigm-for-the-ai-era#/


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: com/capabilities/people-and-organizational-performance/our-insights/the-agentic-organization-contours-of-the-next-paradigm-for-the-ai-era#/

</details>


### [54] [Open Agent Summit | 聚焦 <em class="highlight">Agentic</em> AI的盛会](http://mp.weixin.qq.com/s?__biz=MzU1Mzc4MjA1NQ==&mid=2247487007&idx=3&sn=0b0250c8086a9ed5e2370201759d6b40&chksm=fa00ae2a38275fcc9b8dc9976ea5533444785febc955315021ab6707315489217b433b84995c#rd)
*开源时刻*

Main category: wechat.article

TL;DR: The AI Scientist-v2： Workshop-Level Automated Scientific Discovery via Agentic Tree Search - Yutaro Yamada， Sakanaldea generation tree-based experimentation paper write-up llm idea/plan。1. preliminary...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: The AI Scientist-v2： Workshop-Level Automated Scientific Discovery via Agentic Tree Search - Yutaro Yamada， Sakanaldea generation tree-based experimentation paper write-up llm idea/plan。1. preliminary ldea investigatione plotting + innovation vlm feedback [write to exp. logl [select best nodel

</details>


### [55] [<em class="highlight">Agentic</em> 框架系列（二）：主流生态对比](http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484067&idx=1&sn=894fe0f0207081008c6c1dc5e7a2dbeb&chksm=fe897927cffa006ec249cbf5ef3e1f15565e1a1432087df46ec11db71809c42baee49afb7300#rd)
*AI 知行社 Lab*

Main category: wechat.article

TL;DR: 在第一篇中我们了解了 Agentic 框架的核心理念——让大模型具备「计划、执行、反思与协作」的能力。本篇，我们聚焦目前最活跃的四大 Agentic 框架生态：LangGraph、AutoGen、CrewAI、MCP。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在第一篇中我们了解了 Agentic 框架的核心理念——让大模型具备「计划、执行、反思与协作」的能力。本篇，我们聚焦目前最活跃的四大 Agentic 框架生态：LangGraph、AutoGen、CrewAI、MCP。

</details>


### [56] [企业广泛采用 <em class="highlight">Agentic</em> AI，但领导层理解滞后](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651259717&idx=4&sn=72aa398887ced46784cc3437e2d2c8ec&chksm=bcaef9ea3caad84f220e099e0598926a573848e9b03e792e76987b5191cef38191d323e337f1#rd)
*InfoQ*

Main category: wechat.article

TL;DR: https：//www.infoq.com/news/2025/10/agentic-ai-adoption-leadership/声明：本文为 InfoQ 翻译，未经许可禁止转载。点击底部阅读原文访问 InfoQ 官网，获取更多精彩内容！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: https：//www.infoq.com/news/2025/10/agentic-ai-adoption-leadership/声明：本文为 InfoQ 翻译，未经许可禁止转载。点击底部阅读原文访问 InfoQ 官网，获取更多精彩内容！

</details>


### [57] [云栖大会重磅发布：阿里云AI搜索迈向<em class="highlight">Agentic</em>时代，四大技术亮点重塑智能检索](http://mp.weixin.qq.com/s?__biz=MzA4MjYyOTQ0Ng==&mid=2648402951&idx=1&sn=66666c979daa0b733e21a0e5d49444b1&chksm=868e5b9c9fae341aed59ffafd4aaac661e7692a9f27ea98672ac8ad63d11a4c841c585e9ead5#rd)
*聚搜*

Main category: wechat.article

TL;DR: agentic search的核心在于通过大模型自主决策“何时、何地、如何”执行搜索任务，实现复杂任务规划。阿里云AI搜索开放平台已实现三阶段技术的融合，在保留传统搜索高效性的同时，通过大模型赋能动态任务执行，为客户提供


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic search的核心在于通过大模型自主决策“何时、何地、如何”执行搜索任务，实现复杂任务规划。阿里云AI搜索开放平台已实现三阶段技术的融合，在保留传统搜索高效性的同时，通过大模型赋能动态任务执行，为客户提供

</details>


### [58] [搭建AI“梦之队”：你必须了解的5个自主<em class="highlight">智能体</em>（<em class="highlight">Agentic</em> AI）惊人真相](http://mp.weixin.qq.com/s?__biz=MzY0MDAxMDMwNA==&mid=2247483684&idx=1&sn=f016acf1b5db2756aff8827f40682fed&chksm=f1b9f9a99cc6e61c5a562fd22be1216fde267217d162d65d22b2b80692da1098591b279baefa#rd)
*时雨随想*

Main category: wechat.article

TL;DR: 1. AI 自我反思：智能体也会“照镜子”提升自己对于任何追求高质量输出的系统而言，可靠性都是一个核心挑战。单一提示模型的一个主要局限就在于其“一次性”生成内容，缺乏自我纠错机制。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1. AI 自我反思：智能体也会“照镜子”提升自己对于任何追求高质量输出的系统而言，可靠性都是一个核心挑战。单一提示模型的一个主要局限就在于其“一次性”生成内容，缺乏自我纠错机制。

</details>


### [59] [一文了解<em class="highlight">Agentic</em> RL领域最新进展](http://mp.weixin.qq.com/s?__biz=Mzk3NTc5ODE2OQ==&mid=2247483875&idx=3&sn=24aceb54ebaed1c422ce1755bf5d7b7d&chksm=c5cb2de7ff104068714011bf93e696168fb8e81a627fe6cbd9d06cc702dbe3c1f709665c651a#rd)
*枭龙云技术团队*

Main category: wechat.article

TL;DR: 今年Agentic的概念特别火，本质上就是让一个LLM能做到边想边搜边做，不对的话就自己反思再边想边搜边做，一直到任务完成开始。从去年开始我们就有在做training-free的各种Agentic的技术（像Agentic Rag等等），这几天有空就稍微总


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今年Agentic的概念特别火，本质上就是让一个LLM能做到边想边搜边做，不对的话就自己反思再边想边搜边做，一直到任务完成开始。从去年开始我们就有在做training-free的各种Agentic的技术（像Agentic Rag等等），这几天有空就稍微总

</details>


### [60] [投资<em class="highlight">Agentic</em> AI是面向未来供应链的战略举措](http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247508540&idx=1&sn=f5847cc1fee220aa7e5c06efdb912173&chksm=e95fc7752f3c018b128950c4fe450d2147429a0c1be44a72439ff03fa80ed00560ba5681bc6a#rd)
*开源报告*

Main category: wechat.article

TL;DR: agentic ai：从自动化到自主的旅程。罗戈研究 从数字化雄心到自主化现实。从传统聊天机器人和 rpa 到自主代 机器人流程自动化（rpa）到自主代理流程自动化（apa） 理--迈向自主的旅程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai：从自动化到自主的旅程。罗戈研究 从数字化雄心到自主化现实。从传统聊天机器人和 rpa 到自主代 机器人流程自动化（rpa）到自主代理流程自动化（apa） 理--迈向自主的旅程。

</details>


### [61] [行业洞见 | 麦肯锡：人工智能<em class="highlight">agentic</em> AI 有望为生命科学企业转型提供新机遇（上）](http://mp.weixin.qq.com/s?__biz=MzkyNTQwMzAzOA==&mid=2247494312&idx=1&sn=b6afc7ed3f2798f3871717d5373c9ba7&chksm=c0ed2b72843abfbc1f93e146e4dc62e55f01ae7e5de861b832a6cfc1989704791b5a29a1dfef#rd)
*Boom Health*

Main category: wechat.article

TL;DR: 5.4 p.p.！increased productivity，reduced vendor spend，within a time horizon of revenue pulled forward and optimized operations 3 to 5 years within 3-to-5 year time frame， with potential for incremental...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 5.4 p.p.！increased productivity，reduced vendor spend，within a time horizon of revenue pulled forward and optimized operations 3 to 5 years within 3-to-5 year time frame， with potential for incremental 1% to 3% cagr above baseline growth note： numbers do not include physical ai or growth fr

</details>


### [62] [业界首款专为AI<em class="highlight">智能体</em>打造的首款免费数据库<em class="highlight">Agentic</em> Postgres，来了解一下](http://mp.weixin.qq.com/s?__biz=MzAxNjk4MzQxNw==&mid=2247488407&idx=1&sn=e9574b4f094256daedbcddf97a8df5e7&chksm=9a6ea0a13513aca88c9861a503f99e20a1dd27055d057f7c571d596f807d01576b60b1dd47d8#rd)
*架构师之道*

Main category: wechat.article

TL;DR: Agentic Postgres是数据库向“AI原生”演进的典型探索，其核心价值在于将智能体的“能力需求”转化为数据库的“内置功能”，而非简单叠加AI插件。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic Postgres是数据库向“AI原生”演进的典型探索，其核心价值在于将智能体的“能力需求”转化为数据库的“内置功能”，而非简单叠加AI插件。

</details>


### [63] [“智能前沿观察”系列 | 从AI Agent到<em class="highlight">Agentic</em> AI—“智慧工具”向“智能引擎”的质变跃迁](http://mp.weixin.qq.com/s?__biz=MzU1NTY5ODYxOA==&mid=2247513550&idx=1&sn=5e273b9f691a435cb962ba8099bf6817&chksm=fa006d50d367cf93677197c021ae47dbdbda219d00df3f0952f077830a0b3923b539bf64693b#rd)
*长三角一体化*

Main category: wechat.article

TL;DR: 未来，当Agentic AI自主创新能力日益成熟，将极大改变企业、经济组织形态，涌现出更多“一人独角兽”、“超级个体”等新经济形态；超级APP等网络入口、流量入口也将被Agentic AI所替代。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 未来，当Agentic AI自主创新能力日益成熟，将极大改变企业、经济组织形态，涌现出更多“一人独角兽”、“超级个体”等新经济形态；超级APP等网络入口、流量入口也将被Agentic AI所替代。

</details>


### [64] [一文读懂<em class="highlight">Agentic</em> AI 与 AI Agent的核心区别](http://mp.weixin.qq.com/s?__biz=MzkwMDQ3MjU3Mw==&mid=2247546311&idx=1&sn=1d042bb278c0b145a5cd64fe60314d1a&chksm=c152d918b78e6b42449427a3b2e78ec5b5ec1aa501c9a06db2de27bfbdab42fa9eaf338f3639#rd)
*鹏博士研究院*

Main category: wechat.article

TL;DR: Agentic AIAgentic AI 是以复杂目标达成为核心，统筹多个 AI Agents 及工具的分布式智能体系，其技术本质是动态目标优化器，无需人工定义每一步操作，仅需明确最终目标，系统即可自主规划路径、协调资源、调整策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AIAgentic AI 是以复杂目标达成为核心，统筹多个 AI Agents 及工具的分布式智能体系，其技术本质是动态目标优化器，无需人工定义每一步操作，仅需明确最终目标，系统即可自主规划路径、协调资源、调整策略。

</details>


### [65] [🤖 从 Agent 到 <em class="highlight">Agentic</em>：AI <em class="highlight">智能体</em>的“觉醒时刻”](http://mp.weixin.qq.com/s?__biz=MzE5MTE3ODU4MQ==&mid=2247485015&idx=1&sn=e0985b924ec6959381e7a7f2dbf66c39&chksm=9736cc1fe15627353b1fcc6a0945f854080edc2d7477e740fe3d6969f596ac7c042d710c456c#rd)
*洞见畏来*

Main category: wechat.article

TL;DR: Agentic 是 Agent 的进化形态。它不只是一个被指挥的工具，而是一个能自主做决策、不断自我改进的智能体。举几个例子：AutoGPT会根据高层目标自动拆解任务；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 是 Agent 的进化形态。它不只是一个被指挥的工具，而是一个能自主做决策、不断自我改进的智能体。举几个例子：AutoGPT会根据高层目标自动拆解任务；

</details>


### [66] [90%的团队都在用的 <em class="highlight">Agentic</em> AI 设计模式解析](http://mp.weixin.qq.com/s?__biz=MzU4OTY4MDU4MQ==&mid=2247489231&idx=1&sn=8122ca60e1fe62e52dcc21dadfb25749&chksm=fc12c1fb374f29cf3808c4ae3d50618a09a80d580fbce2d67925d192b835c17292879f0e1320#rd)
*大模型之路*

Main category: wechat.article

TL;DR: 一、什么是 Agentic AI？Agentic AI 是一种具备一定自主能力的 LLM 应用，它能够：观察（Observe）：理解用户请求及相关上下文（如文档、日志等）思考（Think）：判断下一步最佳行动


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、什么是 Agentic AI？Agentic AI 是一种具备一定自主能力的 LLM 应用，它能够：观察（Observe）：理解用户请求及相关上下文（如文档、日志等）思考（Think）：判断下一步最佳行动

</details>


### [67] [一图了解<em class="highlight">Agentic</em> AI宇宙](http://mp.weixin.qq.com/s?__biz=MzkyNDc1NjMzMw==&mid=2247488209&idx=1&sn=470744b81146722870b94dcd4a66f157&chksm=c098a1fd0666bed1cdcd767ff596a945ab6b24d67d4695c18e35b50a2b1b0e2b14df22fc8206#rd)
*辉少-企业级AGI探索者*

Main category: wechat.article

TL;DR: 7. 最外层：Agentic AI（智能体 AI）「高级能力与安全」这是 AI 智能体的前沿进化方向，聚焦 “更复杂、更可靠、更安全的自主系统”：协议与角色：Agent Protocols （MCP；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 7. 最外层：Agentic AI（智能体 AI）「高级能力与安全」这是 AI 智能体的前沿进化方向，聚焦 “更复杂、更可靠、更安全的自主系统”：协议与角色：Agent Protocols （MCP；

</details>


### [68] [<em class="highlight">Agentic</em> AI: Part 4 - <em class="highlight">智能体</em>框架](http://mp.weixin.qq.com/s?__biz=MjM5Nzc3MjI3MA==&mid=2648479142&idx=1&sn=e9d886232c3961d2043f847301b82bb4&chksm=bfe3f1376887927483de776b87e67dc1f2da1551667130b4f370711dd8093bae89ed8b65c737#rd)
*机器AI学习 数据AI挖掘*

Main category: wechat.article

TL;DR: ai模型：大脑框架提供了结构，但智能体仍然需要智能。这来自于ai模型：用于阅读文档的语言模型。用于模式识别的嵌入模型。用于确保安全性和合规性的防护措施。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai模型：大脑框架提供了结构，但智能体仍然需要智能。这来自于ai模型：用于阅读文档的语言模型。用于模式识别的嵌入模型。用于确保安全性和合规性的防护措施。

</details>


### [69] [<em class="highlight">Agentic</em>工作流中函数调用的实战：构建智能工业质检系统](http://mp.weixin.qq.com/s?__biz=MzE5ODEyMTI4MA==&mid=2247484393&idx=1&sn=53dd3868aedbbd98f07346a8bfb2ed69&chksm=97fc7ee0e8a36772a5d49002ee9d8193f5c086bbf2624fa524174a129b02e9a2013d08a9f513#rd)
*大模型RAG和Agent技术实践*

Main category: wechat.article

TL;DR: Agentic工作流 （Agentic Workflow）： 一个混合系统，既包含预定义的执行路径，也包含由Agent动态决策的路径。检索增强生成 （RAG）： 从外部数据源（如数据库、文档）检索信息，并将其作为上下文提供给LLM，以生成更准确、更具


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic工作流 （Agentic Workflow）： 一个混合系统，既包含预定义的执行路径，也包含由Agent动态决策的路径。检索增强生成 （RAG）： 从外部数据源（如数据库、文档）检索信息，并将其作为上下文提供给LLM，以生成更准确、更具

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出了一个仅使用LLM合成智能体数据的统一框架，无需人工监督，能够生成包含任务规范、工具定义、策略伪代码、自然语言交互和执行轨迹的完整交互记录。


<details>
  <summary>Details</summary>
Motivation: 收集智能体数据成本高昂且难以扩展，需要一种可扩展的方法来生成结构化交互记录以支持智能体LLM的开发。

Method: 使用模块化流水线生成符合严格语法和语义约束的交互记录，包括约束生成格式、JSON模式验证和基于评判的过滤机制。

Result: 框架能够生成机器可解析且输入、输出和工具调用之间保持忠实对齐的完整交互记录，支持多任务和多轮交互。

Conclusion: 该框架为手动收集提供了可复现的LLM-only替代方案，推动了能够进行稳健工具使用的智能体LLM的发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [71] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 提出了一种多智能体口头强化学习算法，通过动态构建和优化协作结构来提升多智能体推理能力，在数学推理、创意写作等任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统要么采用预定义结构，要么依赖多数投票或圆桌辩论，这会压制正确但非主导的智能体贡献。作者假设有效的智能体通信对多智能体推理至关重要。

Method: 提出多智能体口头强化学习算法，定义动作空间和反馈机制来评估辩论过程中的通信鲁棒性和连贯性，最终通过多数投票做出决策。

Result: 在数学推理、创意写作、科学推理和数值排序等任务上的实验表明，该方法显著优于单智能体提示方法和最先进的多智能体框架。

Conclusion: 动态优化的多智能体协作结构能够有效提升复杂推理任务的性能，验证了智能体通信质量在多智能体系统中的重要性。

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [72] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt是一个端到端管道，通过硬提示压缩和轻量级文件级数据压缩相结合，显著降低LLM代理工作流中的令牌使用和推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代理工作流中运行时成本高昂，需要处理冗长的提示和丰富的数据流，因此需要有效的压缩方法来降低成本。

Method: 结合自信息评分和基于依赖关系的短语分组来修剪提示中的低信息令牌，同时对附加文档应用n-gram缩写和数值列的统一量化。

Result: 在TAT-QA和FinQA等基准数据集上，令牌使用量和推理成本减少高达60%，同时保持输出质量（Claude-3.5-Sonnet和GPT-4.1-Mini的准确率下降小于5%）。

Conclusion: CompactPrompt为更精简的生成AI管道奠定了基础，能够可视化实时压缩决策并量化成本-性能权衡。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [73] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent是一种评估驱动的方法，通过协调操作简化小语言模型的数据增强过程，发现验证数据中的失败模式并制定针对性策略来减少泛化差距。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署成本和延迟方面具有优势，但在复杂领域特定任务上的准确性往往落后于大型模型。监督微调需要大量手动数据准备和迭代优化工作。

Method: PaDA-Agent通过评估发现验证数据中的失败模式，制定针对性的数据增强策略，直接减少泛化差距，而不是仅关注模型训练错误。

Result: 实验结果显示，在Llama 3.2 1B Instruct模型微调中，该方法相比最先进的基于LLM的数据增强方法有显著改进。

Conclusion: PaDA-Agent提供了一种有效的评估驱动数据增强方法，能够显著提升小语言模型在复杂任务上的性能。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [74] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出了Saber算法，一种无需训练的高效采样方法，用于提升扩散语言模型在代码生成任务中的推理速度和输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成中存在推理速度与输出质量的关键权衡问题，减少采样步骤会导致性能急剧下降。

Method: Saber算法基于两个关键洞察：1）随着代码上下文的建立可以自适应加速；2）需要回溯机制来反转生成的标记。

Result: 在多个主流代码生成基准测试中，Saber将Pass@1准确率平均提升1.9%，同时实现平均251.4%的推理加速。

Conclusion: 通过利用扩散语言模型的固有优势，该工作显著缩小了与自回归模型在代码生成中的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [75] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: 提出了AgentChangeBench基准测试，专门评估工具增强语言模型代理在多轮对话中应对目标变化的能力，包含2,835个任务序列和四种评估指标。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多轮交互经常出现目标变化，但现有基准主要评估静态目标或一次性工具使用，缺乏对动态目标适应能力的测试。

Method: 构建包含2,835个任务序列的基准测试，涵盖三个企业领域，使用四种互补指标：任务成功率、工具使用效率、工具调用冗余率和目标转移恢复时间。

Result: 评估显示前沿模型表现差异显著：GPT-4o在航班预订任务中达到92.2%恢复率，而Gemini仅为48.6%；零售任务参数有效性接近完美但冗余率超过80%。

Conclusion: 高原始准确率并不代表在动态目标下的鲁棒性，必须明确测量恢复时间和冗余率。该基准为诊断和改进企业环境中代理的韧性提供了可复现的测试平台。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [76] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 本文研究了RL后训练对LLM推理过程的影响，发现虽然RL提高了推理轨迹的局部连贯性，但这并不保证最终答案的正确性或逻辑有效性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法通常统一处理所有token而不考虑token级优势，主要基于最终答案正确性评估性能，却声称RL后训练改善了推理轨迹。这促使我们研究RL后训练对未直接激励的中间token的影响。

Method: 使用GRPO算法和Qwen-2.5-0.5B模型在GSM8K数据集上进行实验，引入基于一阶逻辑的轨迹连贯性度量来捕捉推理步骤的一致性。

Result: RL后训练总体上提高了轨迹连贯性，在基础模型失败但RL模型成功的问题上改善最显著。但RL增强了局部连贯性而不一定产生有效或正确的解决方案。

Conclusion: 改进的推理步骤局部连贯性不能保证最终答案正确性，声称RL改善推理的主张需要谨慎审视，因为这种改善可能基于轨迹连贯性的提升，而不一定转化为完全有效的数学证明。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [77] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 该研究测试了八个前沿大语言模型在开放式但规则约束的任务上的自我反思能力，发现模型反思仅带来有限的改进，且经常重复相同的约束违反，表明当前LLM的反思缺乏人类目标驱动的主动监控机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验当前大语言模型的反思能力是否与人类反思推理功能等效，特别是在开放式但规则约束的任务中，而非仅依赖于明确正确信号的封闭任务。

Method: 方法是通过让模型生成科学测试项目，然后基于自身批评进行修订，评估八个前沿模型在开放式规则约束任务上的表现。

Result: 结果显示首次尝试表现很差（通常4个要求项目中零有效项目，均值≈1），反思仅带来适度改进（同样≈1）。模型经常重复相同的约束违反，改进主要来自偶然产生有效项目而非错误检测和原则性修复。

Conclusion: 结论是当前LLM反思缺乏功能证据证明其具有人类那种主动、目标驱动的监控机制，可靠性能需要外部结构来强制执行约束。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [78] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: Genesis是一个针对网络代理攻击的智能框架，通过遗传算法和动态策略发现来持续改进攻击效果，在多种网络任务中表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂网络任务中的广泛应用，安全风险日益凸显。现有红队方法主要依赖手动策略或静态离线模型，难以捕捉网络代理的行为模式，无法适应多样化环境。

Method: 提出Genesis框架，包含三个模块：攻击者（结合遗传算法和混合策略表示生成对抗注入）、评分器（评估目标网络代理响应）、策略师（从交互日志中动态发现有效策略并构建持续增长策略库）。

Result: 在多种网络任务上的广泛实验表明，该框架能够发现新颖策略，并持续优于现有攻击基线方法。

Conclusion: Genesis框架通过动态策略发现和持续优化机制，有效解决了网络代理攻击中的泛化问题，为网络安全研究提供了新思路。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [79] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出MASMP框架，通过状态机提示和记忆机制解决LLM在实时策略游戏中的幻觉和决策碎片化问题，在星际争霸II中达到60%胜率。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM智能体在实时策略游戏中存在的幻觉问题和决策碎片化问题，提升长期战术一致性。

Method: 结合自然语言驱动的状态机架构和轻量级记忆模块，通过提示引导LLM模拟有限状态机和行为树，跨决策周期保持战略变量。

Result: 在星际争霸II中对战最强内置AI(Lv7)达到60%胜率，远超基线方法(0%)，成功解决"知-行差距"问题。

Conclusion: 建立了结合神经和符号AI在复杂决策中的新范式，同时保持了LLM的语义理解能力和状态机般的可靠性。

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [80] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出了一种名为异构对抗游戏（HAP）的对抗性自动课程学习框架，通过教师-学生交互的极小极大优化来生成自适应课程，解决了传统自博弈在非对称学习场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈框架在零和竞争环境中利用智能体对称性，但在具有固有不对称性的开放式学习场景中表现不足。人类教学系统展示了非对称教学框架的优势，需要将其操作化到能够自主合成适当课程的人工系统中。

Method: HAP框架将教师-学生交互形式化为极小极大优化，任务生成教师和问题解决学生通过对抗动态共同进化，建立双向反馈系统，教师根据实时学习者表现指标持续重新校准任务复杂度。

Result: 在多任务学习领域的实验验证表明，该框架与最先进基线达到性能相当，同时生成的课程提高了人工智能体和人类受试者的学习效率。

Conclusion: HAP框架成功实现了非对称自适应教学机制在人工系统中的操作化，为开放式学习提供了有效的自动课程生成方法。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [81] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出了Med-VRAgent框架，结合视觉引导、自我奖励和蒙特卡洛树搜索，通过PPO微调提升视觉语言模型在医学视觉推理中的性能


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学推理中存在幻觉、模糊描述、逻辑不一致和定位差等问题，需要改进

Method: 基于视觉引导和自我奖励范式，结合蒙特卡洛树搜索，使用PPO目标微调视觉语言模型

Result: 在多个医学VQA基准测试中优于现有方法

Conclusion: Med-VRAgent框架有效提升了医学视觉推理能力

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [82] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: PlanU是一种基于LLM的规划方法，通过将蒙特卡洛树搜索中的节点回报建模为分位数分布来捕捉不确定性，并引入带有好奇心因子的上置信界限分数来平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性环境下的决策任务中表现不佳，主要面临LLM不确定性和环境不确定性两大挑战。现有方法要么只关注LLM不确定性，要么不适用于需要与环境交互的多步决策任务。

Method: 将MCTS中每个节点的回报建模为分位数分布，使用一组分位数表示回报分布；引入UCC分数来估计MCTS节点的不确定性，平衡探索与利用。

Result: 通过大量实验验证了PlanU在LLM不确定性决策任务中的有效性。

Conclusion: PlanU能够有效处理LLM决策中的不确定性问题，在不确定性环境下表现出色。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [83] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出了一个用于多轮社交对话中LLM代理的概率意图建模框架，通过维护对伙伴潜在意图的信念分布来提升对话策略的适应性。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM代理在多轮社交对话中理解和适应伙伴意图的不确定性，需要一种能够动态建模和更新意图信念的方法。

Method: 使用概率意图建模框架，通过上下文先验初始化信念分布，并在每轮对话后通过似然估计动态更新，为策略提供额外的上下文基础。

Result: 在SOTOPIA环境中的初步实验显示，该框架在SOTOPIA-All上提高了9.0%的总体得分，在SOTOPIA-Hard上提高了4.1%，甚至略微超过了直接观察伙伴意图的oracle代理。

Conclusion: 概率意图建模可以为开发具有社会智能的LLM代理做出贡献。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [84] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，评估视觉语言模型的多模态决策和主动信息寻求能力，包含直接控制和工具辅助控制两种模式。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在真实客户端环境中，从原始截图到低层级动作的映射以及决定何时寻求指导的能力仍然不足，需要建立评估标准。

Method: 设计了八个战斗任务和两种控制模式：直接控制（仅截图输入，输出低级操作）和工具辅助控制（提供OCR文本化观察）。还包括ask-or-act诊断来评估信息寻求行为。

Result: 基准测试结果显示在直接控制模式下存在显著的感知-控制差距，而明智的信息寻求与改进的成功率相关。

Conclusion: StarBench为真实客户端游戏中的主动信息寻求和多模态决策提供了可复现的评估标准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [85] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 提出了Crucible系统，使用LLM驱动的多级专家模拟来调优算法，并定义了量化评估算法调优潜力的正式指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法在理想或默认配置下的性能，忽视了调优潜力这一关键方面。

Method: 采用LLM驱动的多级专家模拟方法，通过模拟不同场景的专家调优过程来评估算法调优潜力。

Result: 实验结果显示Crucible能够系统量化不同算法的可调优空间，并在真实部署中得到验证。

Conclusion: Crucible为算法分析和设计提供了新的维度，最终能够带来性能提升。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [86] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla是一个端到端的智能体框架，将模拟器构建视为文本计算图中的代码实例优化，通过损失驱动循环实现代码合成、执行、评估和修复。


<details>
  <summary>Details</summary>
Motivation: 将脆弱的提示管道转换为可重复、约束感知的模拟器代码生成，统一多智能体编排与损失对齐的优化视图。

Method: 在文本计算图中嵌入专门的LLM驱动智能体作为图节点，工作流管理器执行损失驱动循环，优化器执行文本梯度下降，保留人机交互用于任务规范确认。

Result: 在三个CPS任务（用户建模、口罩采用和个人移动性）中实现了最先进的整体准确率。

Conclusion: SOCIA-Nabla通过统一多智能体编排与损失对齐的优化视图，实现了跨领域和模拟粒度的可扩展模拟器代码生成。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [87] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个基于强化学习的对话驱动检索框架，通过生成二进制问题序列来高效缩小搜索空间，无需大规模标注对话数据。


<details>
  <summary>Details</summary>
Motivation: 解决信息检索中用户查询模糊性的问题，传统对话式检索系统缺乏明确的提问策略来获取最有价值的信息。

Method: 使用强化学习训练代理生成二进制问题序列，通过优化提问策略来高效缩小搜索空间。

Result: 在结构化任务中性能接近理论最优的二分搜索，在非结构化任务中显著优于基线方法。

Conclusion: SherlockLLM是一个鲁棒高效的解决方案，能够学习高度有效的信息寻求对话策略。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [88] [GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing](https://arxiv.org/abs/2510.17843)
*Zongze Wu,Yani Guo,Churong Liang,Runnan Li*

Main category: cs.LG

TL;DR: GRETEL通过执行验证解决工具检索中的语义-功能差距问题，显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 当前基于语义相似度的工具检索方法存在语义-功能差距问题，经常检索到文本相关但功能不可用的工具

Method: 引入GRETEL系统，通过沙盒化的计划-执行-评估循环对语义检索的候选工具进行实证验证

Result: 在ToolBench基准测试中，Pass Rate从0.690提升到0.826，Recall从0.841提升到0.867，NDCG从0.807提升到0.857

Conclusion: 基于执行的验证比单纯语义相似度提供更可靠的工具选择基础，能实现更鲁棒的智能体性能

Abstract: Despite remarkable advances in Large Language Model capabilities, tool
retrieval for agent-based systems remains fundamentally limited by reliance on
semantic similarity, which fails to capture functional viability. Current
methods often retrieve textually relevant but functionally inoperative tools
due to parameter mismatches, authentication failures, and execution
constraints--a phenomenon we term the semantic-functional gap. We introduce
GRETEL, to address this gap through systematic empirical validation. GRETEL
implements an agentic workflow that processes semantically retrieved candidates
through sandboxed plan-execute-evaluate cycles, generating execution-grounded
evidence to distinguish truly functional tools from merely descriptive matches.
Our comprehensive evaluation on the ToolBench benchmark demonstrates
substantial improvements across all metrics: Pass Rate (at 10) increases from
0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10)
rises from 0.807 to 0.857.. These results establish that execution-based
validation provides a more reliable foundation for tool selection than semantic
similarity alone, enabling more robust agent performance in real-world
applications.

</details>


### [89] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: COMPASS是一种无需外部监督的测试时奖励机制，通过整合双校准答案奖励和决定性路径奖励，在无标签数据上实现强化学习，显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法严重依赖人工标注的偏好数据或标签数据集进行奖励建模，存在可扩展性瓶颈。本文探索在无标签数据上进行强化学习，让模型从连续经验流中自主学习。

Method: 提出COMPASS框架，包含两个互补组件：双校准答案奖励（DCAR）通过置信度和可信度校准建立可信伪标签来稳定训练；决定性路径奖励（DPR）直接优化推理过程质量，超越仅基于结果的监督。

Result: 大量实验表明，COMPASS在不同推理任务和模型架构上实现了显著且一致的性能提升，推动了大语言模型从连续经验中学习的更可扩展方向。

Conclusion: COMPASS通过联合强化可信共识答案和高度决定性推理链，系统性地增强了模型的分析能力，为大语言模型的无监督强化学习提供了有效解决方案。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [90] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: UniRL-Zero是一个统一的强化学习框架，通过单一模型提升多模态语言模型的理解推理能力、扩散模型的多媒体生成能力以及它们之间的有益交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态理解和生成模型通常独立训练，缺乏统一的强化学习框架来协同优化这些能力。本文旨在构建一个统一的强化学习框架，促进理解与生成能力的协同提升。

Method: 提出了UniRL-Zero统一强化学习框架，定义了六种统一模型强化学习场景，为统一理解和生成模型的强化学习提供系统性基准。

Result: 开发了一个开源框架，能够在一个统一模型中同时提升多模态语言模型的理解推理能力和扩散模型的多媒体生成能力。

Conclusion: UniRL-Zero为统一理解和生成模型的强化学习提供了系统性的基准和框架，促进了多模态AI能力的协同发展。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [91] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: 提出了自适应发散正则化策略优化(ADRPO)，通过基于优势估计自动调整正则化强度来解决生成模型强化学习微调中的探索-利用平衡问题，在文本到图像生成和LLM微调中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定发散正则化，导致两难困境：强正则化保留模型能力但限制奖励优化，弱正则化可实现更好对齐但风险不稳定或奖励黑客攻击。

Method: ADRPO根据优势估计自动调整正则化强度：对高价值样本减少正则化，对差样本应用更强正则化，使策略能根据数据质量在探索和积极利用间导航。使用Wasserstein-2正则化进行流匹配生成模型。

Result: 在文本到图像生成中，ADRPO在语义对齐和多样性方面优于DPO和ORW-CFM-W2等方法；2B参数SD3模型在属性绑定、语义一致性等方面超越4.8B和12B参数模型；在LLM和多模态推理模型中也表现出色，7B模型超越Gemini 2.5 Pro和GPT-4o Audio。

Conclusion: ADRPO为不同生成架构和模态的探索-利用挑战提供了有效的即插即用解决方案，能够通过主动探索逃离局部最优，实现优越的逐步推理能力。

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [92] [SPACeR: Self-Play Anchoring with Centralized Reference Models](https://arxiv.org/abs/2510.18060)
*Wei-Jer Chang,Akshay Rangesh,Kevin Joseph,Matthew Strong,Masayoshi Tomizuka,Yihan Hu,Wei Zhan*

Main category: cs.LG

TL;DR: SPACeR框架结合预训练tokenized自回归运动模型和自玩强化学习，在自动驾驶仿真中实现高效、人类化的多智能体行为，比大型生成模型快10倍且参数少50倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，基于扩散或tokenized模型的模仿学习能产生真实行为但计算昂贵且推理慢，而自玩强化学习虽高效但容易偏离人类驾驶规范。需要结合两者优势。

Method: 使用预训练tokenized自回归运动模型作为集中式参考策略，指导去中心化自玩强化学习，通过似然奖励和KL散度将策略锚定在人类驾驶分布上。

Result: 在Waymo Sim Agents Challenge中表现优异，推理速度比大型生成模型快10倍，参数规模小50倍，同时在闭环自我规划评估中能有效测量规划器质量。

Conclusion: SPACeR建立了测试自动驾驶策略的新范式，通过快速可扩展的交通仿真实现高效评估。

Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency,
but also realistic, human-like behaviors that are socially aware and
predictable. Achieving this requires sim agent policies that are human-like,
fast, and scalable in multi-agent settings. Recent progress in imitation
learning with large diffusion-based or tokenized models has shown that
behaviors can be captured directly from human driving data, producing realistic
policies. However, these models are computationally expensive, slow during
inference, and struggle to adapt in reactive, closed-loop scenarios. In
contrast, self-play reinforcement learning (RL) scales efficiently and
naturally captures multi-agent interactions, but it often relies on heuristics
and reward shaping, and the resulting policies can diverge from human norms. We
propose SPACeR, a framework that leverages a pretrained tokenized
autoregressive motion model as a centralized reference policy to guide
decentralized self-play. The reference model provides likelihood rewards and KL
divergence, anchoring policies to the human driving distribution while
preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our
method achieves competitive performance with imitation-learned policies while
being up to 10x faster at inference and 50x smaller in parameter size than
large generative models. In addition, we demonstrate in closed-loop ego
planning evaluation tasks that our sim agents can effectively measure planner
quality with fast and scalable traffic simulation, establishing a new paradigm
for testing autonomous driving policies.

</details>


### [93] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: 提出了一种可靠强化学习方法，通过最大化累积回报超过给定阈值的概率来确保策略可靠性，而非传统RL的期望回报最大化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要确保策略不仅具有高平均性能，还要有成功概率保证，如路由、资源分配等风险决策场景。

Method: 通过状态增强表示将可靠RL问题重新表述为标准RL问题，可直接使用现有RL算法如Q-learning、Dueling Double DQN。

Result: 理论证明两种表述的等价性，数值实验显示该方法在可靠路由问题中能有效平衡效率和可靠性。

Conclusion: 可靠RL框架适用于随机和安全关键环境，无需开发全新算法框架即可获得可靠策略。

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [94] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 本文证明了在强化学习中，使用足够宽松的安全过滤器强制执行安全约束不会降低渐近性能，实现了安全性和性能的完全分离。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在安全关键场景中缺乏正式安全保证的问题，消除安全过滤会牺牲性能的普遍认知。

Method: 提出安全关键马尔可夫决策过程(SC-MDP)和过滤MDP框架，证明在过滤MDP中学习既安全又保持标准RL收敛性。

Result: 在Safety Gymnasium基准测试中实现了零违规训练，最终性能匹配或超过无过滤基线。

Conclusion: 为安全强化学习提供了简单原则性方法：使用最宽松的安全过滤器进行训练和部署。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [95] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: 提出Customized-GRPO框架，通过Synergy-Aware Reward Shaping和Time-Aware Dynamic Weighting解决主题驱动图像生成中身份保持与提示遵循的权衡问题，避免竞争性退化。


<details>
  <summary>Details</summary>
Motivation: 解决在线强化学习中简单线性奖励聚合导致的冲突梯度信号和与扩散过程时间动态不匹配的问题。

Method: 提出Customized-GRPO框架，包含SARS（非线性惩罚冲突奖励信号并放大协同信号）和TDW（根据时间动态调整优化压力）。

Result: 显著优于朴素GRPO基线，成功缓解竞争性退化，在保持关键身份特征的同时准确遵循复杂文本提示。

Conclusion: Customized-GRPO能够更好地平衡身份保持和提示遵循，生成更高质量的图像。

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [96] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文分析了无折扣总奖励设置下的策略梯度方法，针对γ=1的情况建立了收敛理论，通过引入瞬态访问量度替代传统状态访问量度。


<details>
  <summary>Details</summary>
Motivation: 现代基于策略的强化学习算法（特别是大语言模型中的策略梯度方法）使用无折扣总奖励设置（γ=1），但现有理论大多假设γ<1，导致理论不适用。

Method: 基于两个关键洞察：(1) MDP状态在严格正概率策略下的循环-瞬态分类不变性；(2) 用瞬态访问量度替代传统状态访问量度。

Result: 为无折扣期望总奖励无限时域MDPs的策略梯度方法提供了理论分析框架。

Conclusion: 建立了γ=1情况下的策略梯度理论，填补了现有理论与实际应用之间的空白。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [97] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 提出Uncertain Safety Critic (USC)方法，通过不确定性感知的调制和精炼来改进critic训练，在保持任务性能的同时显著减少安全违规。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在安全探索方面存在平衡问题：严格的安全约束会损害任务性能，而优先奖励的方法则频繁违反安全约束，产生平坦的成本梯度景观阻碍策略改进。

Method: USC方法将不确定性感知的调制和精炼集成到critic训练中，在不确定和高成本区域集中保守性，同时在安全区域保持锐利梯度。

Result: 实验显示USC将安全违规减少约40%，同时保持竞争性或更高的奖励，预测成本梯度与真实成本梯度之间的误差减少约83%。

Conclusion: USC打破了安全与性能之间的传统权衡，为可扩展的安全强化学习铺平了道路。

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [98] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: COPPOL是一种基于共形预测的感知到策略学习方法，为语义分割提供有限样本安全保证，生成带有严格漏检界限的校准危险地图，用于下游强化学习规划。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中，现有方法假设完美的危险检测能力，而不确定性感知方法缺乏有限样本保证，需要同时实现准确的危险感知和原则性的不确定性处理。

Method: 集成分布无关的有限样本安全保证到语义分割中，生成校准的危险地图，这些地图为下游RL规划提供风险感知成本场。

Result: 在两个卫星基准测试中，COPPOL将危险覆盖率提高最多6倍，实现近乎完全的不安全区域检测，同时减少导航中的危险违规约50%。

Conclusion: 该方法对分布偏移保持鲁棒性，同时保持安全性和效率。

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [99] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 本文研究了从后门教师模型进行知识蒸馏的安全风险，提出了一种新的可转移后门技术T-MTB，证明了后门可以通过知识蒸馏传递到学生模型。


<details>
  <summary>Details</summary>
Motivation: 由于教师模型可能来自不可信方，知识蒸馏可能带来意想不到的安全风险，现有LLM后门方法选择的触发词在正常上下文中很少出现，低估了知识蒸馏的安全风险。

Method: 提出T-MTB技术，构建由多个特定标记组成的复合后门触发器，这些标记在预期的蒸馏数据集中经常单独出现，使后门能够传递到学生模型。

Result: 使用T-MTB在两个攻击场景（越狱和内容调制）和四个LLM模型家族中证明了可转移后门的安全风险。

Conclusion: 知识蒸馏存在严重的安全风险，后门可以从教师模型传递到学生模型，需要重新评估知识蒸馏的安全性。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [100] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 本文系统比较了监督微调(SFT)和强化学习(RL)两种后训练方法在语言模型中的遗忘模式，发现RL比SFT导致更少的灾难性遗忘，同时保持相当或更好的目标任务性能。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型后训练过程中的灾难性遗忘问题，为缓解该现象提供指导原则。

Method: 通过实验比较SFT和RL在不同语言模型家族(Llama、Qwen)和任务(指令跟随、常识知识、数学推理)中的遗忘模式，并建立简化理论模型分析原因。

Result: RL在所有实验设置中都表现出比SFT更少的遗忘，同时目标任务性能相当或更好。研究发现RL的在线数据使用是其抗遗忘性的关键因素。

Conclusion: RL的后训练方法比SFT更能有效缓解灾难性遗忘，近似在线数据的使用可以更高效地实现这一目标。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


### [101] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 该论文对推理大语言模型(RLLM)的服务性能进行了全面研究，发现RLLM在内存使用、请求处理、运行时间和领域偏好方面与传统LLM存在显著差异，并验证了现有推理优化技术在RLLM上的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然RLLM在复杂推理任务上表现出色，但其服务性能和部署行为尚未被充分研究，这阻碍了RLLM在真实场景中的实际应用。

Method: 首先进行试点研究比较RLLM与传统LLM的服务性能，然后调查现有推理优化技术对RLLM的有效性，最后使用Gamma分布建模的真实工作负载进行评估。

Result: 发现RLLM具有显著的内存使用和波动、拖尾请求、自适应运行时间和领域偏好等特点；模型量化和推测解码能提升系统效率且对精度影响小，而前缀缓存和KV缓存量化可能降低小RLLM的精度或性能。

Conclusion: 研究揭示了RLLM独特的服务特性，为RLLM推理服务的优化提供了重要见解，有助于推动RLLM在研究和工业界的实际部署。

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [102] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 该论文提出了一种处理多步预测增强MDP的新方法，通过贝叶斯价值函数和Bellman-Jensen Gap分析来解决传统RL在多步预测场景下的维度灾难问题，并开发了BOLA算法实现离线学习和在线适应的分离。


<details>
  <summary>Details</summary>
Motivation: 传统RL假设基于单步转移模型的MDP，但在许多实际应用中（如能源管理和股票投资），智能体可以访问多步未来状态预测，这为决策提供了额外优势。然而，多步预测天然高维，直接嵌入MDP会导致状态空间指数爆炸和维度灾难。

Method: 提出三个关键创新：1）贝叶斯价值函数来表征最优预测感知策略；2）Bellman-Jensen Gap分析来表征不完美预测的价值；3）BOLA算法（贝叶斯离线学习与在线适应），将离线贝叶斯价值学习与轻量级在线适应分离的两阶段基于模型的RL算法。

Result: 证明了BOLA即使在预测不完美的情况下也保持样本效率，并在合成MDP和真实世界风能存储控制问题上验证了理论和算法。

Conclusion: 该工作为处理多步预测增强的MDP提供了理论工具和实用算法，解决了传统RL在多步预测场景下的局限性。

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [103] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: 提出了一种简单的在线监督微调(OSFT)方法，让LLM生成自己的响应并立即在这些自生成数据上进行微调，无需奖励信号即可提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法需要复杂的奖励设计，作者希望开发一种更简单高效的训练范式来提升LLM的推理能力。

Method: OSFT方法：模型生成响应后立即在自生成数据上进行微调，默认只需一次rollout，无需奖励信号。

Result: 在数学推理任务上，OSFT达到了与GRPO等强RL方法相当的性能，且更高效稳健。

Conclusion: OSFT通过促进模型从预训练中学到的潜在偏好知识来提升推理能力，为复杂奖励训练范式提供了高效替代方案。

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [104] [Search Self-play: Pushing the Frontier of Agent Capability without Supervision](https://arxiv.org/abs/2510.18821)
*Hongliang Lu,Yuhang Wen,Pengyu Cheng,Ruijin Ding,Haotian Xu,Jiaqi Guo,Chutian Wang,Haonan Chen,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出搜索自博弈（SSP）方法，让LLM同时作为任务提出者和问题解决者，通过竞争与合作共同进化搜索能力，无需人工监督即可显著提升搜索代理性能。


<details>
  <summary>Details</summary>
Motivation: 传统的可验证奖励强化学习（RLVR）依赖精心设计的任务查询和真实答案，需要大量人工标注，阻碍了在智能体场景下的扩展。现有任务合成方法难以控制生成任务的难度，无法有效支持RL训练。

Method: 采用自博弈训练框架，LLM同时扮演任务提出者（生成有明确真实答案且难度递增的深度搜索查询）和问题解决者（处理搜索查询并输出答案）。通过收集搜索轨迹作为外部知识，使用检索增强生成（RAG）验证查询的可回答性。

Result: SSP在各种基准测试上显著提升了搜索代理的性能，在从零开始和持续RL训练设置下均有效，且无需任何监督。

Conclusion: 搜索自博弈（SSP）为智能体RLVR提供了更高的可扩展性，通过竞争与合作的共同进化机制有效提升了搜索能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.

</details>


### [105] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 提出了一种纯基于价值函数的连续控制框架，通过结构最大化Q函数实现高效稳定的学习，无需单独学习actor网络


<details>
  <summary>Details</summary>
Motivation: 传统基于价值的算法在连续动作空间中计算不可行，而actor-critic方法训练不稳定，需要开发更稳定高效的纯价值方法

Method: 采用结构最大化Q函数的关键架构和算法选择，在标准仿真任务上进行评估

Result: 在性能和数据效率上与最先进基线相当，在约束动作空间环境中表现优于传统actor-critic方法

Conclusion: 证明了纯基于价值的方法在连续控制中的可行性，特别是在非平滑价值函数环境中具有优势

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>
