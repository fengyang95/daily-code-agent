<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.SE](#cs.SE) [Total: 6]
- [wechat.article](#wechat.article) [Total: 12]
- [cs.AI](#cs.AI) [Total: 11]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GMTRouter: Personalized LLM Router over Multi-turn User Interactions](https://arxiv.org/abs/2511.08590)
*Encheng Xie,Yihang Sun,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: GMTRouter是一个基于异构图学习的个性化LLM路由框架，通过将用户-LLM多轮交互建模为异构图，在少量数据下有效学习用户偏好，实现高效个性化路由。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法未能充分个性化，且难以捕捉用户与LLM之间的复杂交互关系。用户偏好数据通常稀缺、嘈杂且格式不一致，限制了仅依赖用户特定数据的方法效果。

Method: 将多轮用户-LLM交互表示为包含用户、LLM、查询和响应四种节点类型的异构图，通过定制化的消息传递机制，在轻量级归纳图学习框架中从少量数据学习用户偏好。

Result: 在多个数据集上持续优于强基线，准确率提高0.9-21.6%，AUC提高0.006-0.309。能够仅用少量数据适应新用户和演化偏好，无需大量微调。

Conclusion: GMTRouter通过异构图建模有效解决了LLM路由中的个性化挑战，在数据稀缺情况下仍能实现高效个性化路由。

Abstract: Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.

</details>


### [2] [Diverse Preference Learning for Capabilities and Alignment](https://arxiv.org/abs/2511.08594)
*Stewart Slocum,Asher Parker-Sartori,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: 本文提出Soft Preference Learning方法，通过解耦KL散度正则化中的熵和交叉熵项，解决对齐算法导致的LLM输出多样性下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有对齐算法（如RLHF和DPO）显著降低了LLM输出的多样性，导致文本结构重复、问题解决方式单一、社会观点范围变窄。

Method: 提出Soft Preference Learning方法，解构KL散度正则化中的熵和交叉熵项，实现对LLM生成多样性的精细控制。

Result: 使用Soft Preference Learning训练的LLM在困难重复采样任务中准确率更高，输出具有更大的语义和词汇多样性，能代表更广泛的社会观点，并改善了logit校准。

Conclusion: Soft Preference Learning是对标准温度缩放的帕累托改进，能有效平衡LLM的对齐与多样性需求。

Abstract: The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.

</details>


### [3] [Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning](https://arxiv.org/abs/2511.08595)
*Joongho Kim,Xirui Huang,Zarreen Reza,Gabriel Grand,Kevin Zhu,Ryan Lagasse*

Main category: cs.CL

TL;DR: SSDP是一种轻量级方法，通过在线语义合并和动态剪枝来减少Tree-of-Thought推理中的语义冗余，实现2.3倍加速，同时保持竞争性准确率。


<details>
  <summary>Details</summary>
Motivation: Tree-of-Thought推理虽然提升了LLM的问题解决能力，但由于语义冗余导致计算成本高昂，需要一种方法来实时识别和剪枝冗余推理路径。

Method: 提出语义相似性动态剪枝(SSDP)框架，将在线语义合并集成到并行化树搜索中，实时聚类和剪枝冗余步骤。

Result: 在GSM8K和MATH500等推理基准测试中，SSDP相比最先进的树搜索基线实现了2.3倍加速，探索节点数减少85-90%，准确率通常保持在最强基线的5%以内。

Conclusion: SSDP为高效、可扩展的LLM推理提供了一种实用方法，显著减少了计算开销。

Abstract: Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.

</details>


### [4] [What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge](https://arxiv.org/abs/2511.08596)
*Arka Dutta,Sujan Dutta,Rijul Magu,Soumyajit Datta,Munmun De Choudhury,Ashiqur R. KhudaBukhsh*

Main category: cs.CL

TL;DR: 提出了一个框架来测试LLMs在对抗性提示下的真实性保真度，发现不同模型对自我生成的谎言表现出不同程度的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题是LLMs在关键领域部署的主要挑战，需要测试模型在对抗性提示下的真实性保真度。

Method: 三步骤框架：1) 让LLM生成特定领域的真假陈述；2) 让LLM验证这些陈述；3) 测试LLM对自身生成谎言的抵抗能力。

Result: 在电影和小说两个封闭领域测试5个主流LLMs，Claude表现最强韧性，GPT和Grok中等，Gemini和DeepSeek最弱。

Conclusion: 随着越来越多人使用LLMs获取信息，这些发现敲响了警钟，模型对自身生成谎言的脆弱性值得关注。

Abstract: Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \texttt{Claude} exhibits strong resilience, \texttt{GPT} and \texttt{Grok} demonstrate moderate resilience, while \texttt{Gemini} and \texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.

</details>


### [5] [OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking](https://arxiv.org/abs/2511.08598)
*Yanhong Li,Tianyang Xu,Kenan Tang,Karen Livescu,David McAllester,Jiawei Zhou*

Main category: cs.CL

TL;DR: OKBench是一个全自动框架，用于按需生成高质量、动态的知识基准，特别关注新闻领域，通过代理框架自动化基准的来源获取、创建、验证和分发。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准无法捕捉动态世界中的演化知识，集中化策划难以跟上LLM的快速发展。需要能够评估LLM在新知识上表现的动态基准。

Method: 提出OKBench框架，使用代理自动化基准生成流程，包括来源获取、创建、验证和分发，重点关注新闻领域以减少与预训练数据的重叠。

Result: 评估了各种开源和专有LLM，发现在新信息面前模型表现出不同行为，检索缩小了大小模型之间的性能差距。

Conclusion: 在演化知识基准上评估LLM非常重要，OKBench能够促进检索增强方法的全面评估。

Abstract: Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.

</details>


### [6] [Structured Uncertainty guided Clarification for LLM Agents](https://arxiv.org/abs/2511.08798)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出SAGE-Agent，通过结构化不确定性建模工具调用参数，在模糊用户指令下实现更高效的工具调用，显著提升任务覆盖率和减少澄清问题


<details>
  <summary>Details</summary>
Motivation: LLM代理在工具调用时面临模糊用户指令导致错误调用和任务失败的问题，需要更有效的不确定性建模方法来优化澄清过程

Method: 将工具参数的结构化不确定性建模为POMDP，使用EVPI目标进行最优问题选择，采用基于方面的成本建模防止冗余，并通过不确定性加权GRPO训练强化学习

Result: 在模糊任务上覆盖率提升7-39%，澄清问题减少1.5-2.7倍；When2Call准确率从36.5%提升至65.2%（3B模型）和36.7%提升至62.9%（7B模型）

Conclusion: 结构化不确定性为工具增强代理提供了原则性高效方法，在现实场景中同时改善任务成功率和交互效率

Abstract: LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\% to 65.2\% (3B model) and 36.7\% to 62.9\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.

</details>


### [7] [Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents](https://arxiv.org/abs/2511.08835)
*Yejin Yoon,Yuri Son,Namyoung So,Minseo Kim,Minsoo Cho,Chanhee Park,Seungshin Lee,Taeuk Kim*

Main category: cs.CL

TL;DR: 提出了TACT数据集用于统一任务导向对话和闲聊，通过结构多样的模式流支持用户和智能体驱动的模式切换，结合DPO训练显著提升了模式转换处理能力和响应质量。


<details>
  <summary>Details</summary>
Motivation: 传统对话系统通常分别处理任务导向对话和闲聊，但真实对话需要在两种模式间自然切换，现有系统缺乏统一建模这种动态转换的能力。

Method: 构建TACT数据集支持模式转换，提出Switch和Recovery评估指标，使用DPO对TACT训练的模型进行优化。

Result: TACT训练的模型在意图检测和模式转换处理上优于基线，DPO优化后达到75.74%联合模式-意图准确率和70.1%胜率优于GPT-4o。

Conclusion: 结构多样的数据与DPO结合能够显著提升对话系统的响应质量和转换控制能力，为更主动和转换感知的对话智能体铺平道路。

Abstract: Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\% joint mode-intent accuracy and a 70.1\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.

</details>


### [8] [BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation](https://arxiv.org/abs/2511.08866)
*Fuyi Yang,Chenchen Ye,Mingyu Derek Ma,Yijia Xiao,Matthew Yang,Wei Wang*

Main category: cs.CL

TL;DR: BioVerge是一个用于生物医学假设生成的基准测试和LLM智能体框架，通过结合结构化数据和文本数据，采用ReAct方法进行迭代生成和自评估，显著提升了假设的新颖性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统生物医学假设生成方法依赖单一数据类型或预定义模式，限制了发现新颖复杂关系的能力。LLM智能体在信息检索、推理和生成方面有潜力，但缺乏标准化数据集和执行环境。

Method: 提出BioVerge基准测试和BioVerge Agent框架，使用ReAct方法，包含生成和评估模块，迭代产生和自评估假设提案，结合结构化数据和PubMed文献文本数据。

Result: 实验发现：1）不同智能体架构影响探索多样性和推理策略；2）结构化和文本信息源各自提供独特关键上下文；3）自评估显著提升假设的新颖性和相关性。

Conclusion: BioVerge为生物医学假设生成提供了标准化环境，LLM智能体框架能够有效探索科学知识前沿，结合多种数据源和自评估机制可产生更优质的假设。

Abstract: Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.

</details>


### [9] [MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique](https://arxiv.org/abs/2511.09067)
*Gailun Zeng,Ziyang Luo,Hongzhan Lin,Yuchen Tian,Kaixin Li,Ziyang Gong,Jianxiong Guo,Jing Ma*

Main category: cs.CL

TL;DR: 提出了MM-CRITIC基准，用于评估大型多模态模型的批判能力，涵盖基础、修正和比较三个维度，包含8种任务类型和500多个任务，共4471个样本。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在图像描述和视觉推理等任务中能力不断增强，但其多模态批判能力研究仍不足，这对于模型自我改进和作为可靠AI助手至关重要。

Method: 构建MM-CRITIC基准，整合专家指导的参考答案到评分标准中，使用GPT-4o标注响应并生成参考批判，作为可信判断的锚点。

Result: 实验验证了MM-CRITIC的有效性，全面评估了主流多模态模型的批判能力，揭示了响应质量与批判能力之间的相关性，以及不同评估维度下批判难度的变化。

Conclusion: MM-CRITIC为多模态模型批判能力提供了可靠的评估基准，揭示了当前模型的批判能力现状和挑战。

Abstract: The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.

</details>


### [10] [mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.09339)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

TL;DR: 提出了mmJEE-Eval多模态双语基准，基于印度JEE Advanced考试题目，用于评估视觉语言模型的科学推理能力，发现前沿模型与开源模型性能差距显著。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理基准无法有效区分真正的科学推理能力与模式匹配，需要更严格的评估基准。

Method: 构建包含1,460道JEE Advanced考题的多模态双语基准，涵盖物理、化学、数学，评估17个最先进模型。

Result: 前沿VLMs在2025年题目上达到77-84%准确率，开源模型仅37-45%，当增加元认知推理负荷时，GPT-5仅修复5.2%错误。

Conclusion: mmJEE-Eval能有效区分优秀训练和推理方法，其难度源于复杂性和推理深度而非记忆。

Abstract: Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io

</details>


### [11] [Self-Correcting Large Language Models: Generation vs. Multiple Choice](https://arxiv.org/abs/2511.09381)
*Hossein A. Rahmani,Satyapriya Krishna,Xi Wang,Mohammadmehdi Naghiaei,Emine Yilmaz*

Main category: cs.CL

TL;DR: 该论文系统比较了LLM在开放式生成和多项选择两种范式下的自我修正机制，发现不同任务结构对自我修正效果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM自我修正机制在不同任务范式（开放式生成vs多项选择）中的动态差异，为智能体应用设计提供指导。

Method: 在多种自然语言理解和推理任务上，对不同规模和家族的LLM进行系统性实验，比较两种范式的性能趋势和错误修正行为。

Result: 开放式生成受益于重新解释和组合优化的灵活性，而多项选择能利用更清晰的解边界但受限于选项范围。

Conclusion: 自我修正机制设计应考虑任务结构与输出空间的交互作用，这对知识密集型推理和决策导向的LLM应用都有重要意义。

Abstract: Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:
  \textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion](https://arxiv.org/abs/2511.08653)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.LG

TL;DR: CGAR提出了一种基于架构深度的课程学习方法，通过渐进深度课程和分层监督加权，显著提升递归推理模型的训练效率，在保持性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 递归推理模型虽然性能出色，但训练成本高昂（约36 GPU小时/数据集），限制了广泛应用。需要开发更高效的训练方法。

Method: CGAR包含两个核心组件：渐进深度课程（动态调整递归深度）和分层监督加权（对监督步骤应用指数衰减权重），通过架构深度而非数据顺序的课程学习来优化训练。

Result: 在Sudoku-Extreme数据集上，CGAR实现1.71倍训练加速（10.93到6.38小时，成本降低42%），准确率仅下降0.63%（86.65%到86.02%）。

Conclusion: 基于架构深度的课程学习能够高效训练递归推理模型，在提升训练效率的同时保持模型性能，展示了帕累托改进的罕见案例。

Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku

</details>


### [13] [Benevolent Dictators? On LLM Agent Behavior in Dictator Games](https://arxiv.org/abs/2511.08721)
*Andreas Einwiller,Kanishka Ghosh Dastidar,Artur Romazanov,Annette Hautli-Janisz,Michael Granitzer,Florian Lemmerich*

Main category: cs.LG

TL;DR: 提出了LLM-ABS框架来研究LLM智能体在独裁者游戏中的行为，发现系统提示对智能体行为有显著影响，智能体通常表现出强烈的公平偏好。


<details>
  <summary>Details</summary>
Motivation: 质疑现有研究中LLM智能体行为结果的稳健性，特别是忽视系统提示的作用和提示微小变化对结果的敏感性。

Method: 提出LLM-ABS框架，通过(i)探索不同系统提示对模型行为的影响，(ii)使用中性提示变体获得更可靠的智能体偏好洞察，(iii)分析LLM智能体对开放式指令响应的语言特征。

Result: 发现智能体通常表现出强烈的公平偏好，系统提示对其行为有显著影响，从语言角度发现模型以不同方式表达响应。

Conclusion: 尽管提示敏感性仍是持续挑战，但提出的框架为LLM智能体行为研究提供了稳健基础。

Abstract: In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.

</details>


### [14] [TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations](https://arxiv.org/abs/2511.08832)
*Nikunj Gupta,Ludwika Twardecka,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 提出了TIGER方法，通过图嵌入和表示捕获时间信息来增强多智能体强化学习，显式建模智能体间协调结构随时间演化的过程。


<details>
  <summary>Details</summary>
Motivation: 大多数MARL方法依赖静态或每步关系图，忽略了智能体在适应、移动或重组合作策略时自然产生的时间演化交互。捕获这种演化依赖关系对于实现鲁棒和自适应协调至关重要。

Method: TIGER构建MARL智能体的动态时间图，连接当前和历史交互，使用时序注意力编码器聚合结构和时间邻域信息，生成时间感知的智能体嵌入来指导合作策略学习。

Result: 在两个协调密集型基准测试上的广泛实验表明，TIGER在任务性能和样本效率上持续优于多种价值分解和基于图的MARL基线方法。

Conclusion: 通过全面的消融研究揭示了结构和时间因素如何共同塑造MARL中有效的策略学习。

Abstract: In this paper, we propose capturing and utilizing \textit{Temporal Information through Graph-based Embeddings and Representations} or \textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.

</details>


### [15] [Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2511.08922)
*Yunchang Ma,Tenglong Liu,Yixing Lan,Xin Yin,Changxin Zhang,Xinglong Zhang,Xin Xu*

Main category: cs.LG

TL;DR: 提出了DIVO方法，利用扩散模型生成高质量的状态-动作样本，通过基于优势值的二元加权机制实现更精确的数据分布对齐，在离线强化学习中平衡保守性和探索性。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中由于分布外动作导致的价值高估问题，现有扩散模型方法存在过度保守和效率-表达能力不平衡的问题。

Method: 提出DIVO方法，引入基于动作优势值的二元加权机制来指导扩散模型训练，动态筛选高回报潜力动作，实现精确分布对齐和选择性边界扩展。

Result: 在D4RL基准测试中表现优异，在运动任务上实现显著平均回报提升，在具有稀疏奖励挑战的AntMaze领域超越现有方法。

Conclusion: DIVO在离线强化学习中成功平衡了保守性和探索性，通过扩散模型实现了高效策略改进。

Abstract: In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.

</details>


### [16] [Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment](https://arxiv.org/abs/2511.09105)
*Shigeki Kusaka,Keita Saito,Mikoto Kudo,Takumi Tanabe,Akifumi Wachi,Youhei Akimoto*

Main category: cs.LG

TL;DR: 该论文研究了在RLHF/DPO对齐过程中通过翻转偏好标签进行最小成本数据投毒攻击的理论基础，提出了凸优化框架来计算攻击成本的下界和上界，并开发了成本最小化后处理方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实系统中的部署增加，理解其脆弱性变得至关重要。虽然RLHF/DPO对齐过程中的数据投毒攻击已有实证研究，但其理论基础仍不清楚。

Method: 将投毒攻击建模为带线性约束的凸优化问题，推导攻击成本的下界和上界，并提出成本最小化后处理方法减少标签翻转次数。

Result: 实证结果显示，所提出的成本最小化后处理能显著降低投毒成本，特别是在奖励模型特征维度相对于数据集大小较小时效果更明显。

Conclusion: 研究揭示了RLHF/DPO流程中的基本脆弱性，并提供了评估其对低成本投毒攻击鲁棒性的工具。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.

</details>


### [17] [Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks](https://arxiv.org/abs/2511.09114)
*Tim Dudman,Martyn Bull*

Main category: cs.LG

TL;DR: 提出TERLA框架，使用异构图神经网络和固定大小动作空间，使强化学习代理能够泛化到不同拓扑和规模的网络防御任务中，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有网络防御强化学习代理在面对不同拓扑和规模的网络时需要重新训练，难以适应现实世界中网络结构动态变化的需求。

Method: 使用异构图神经网络层生成固定大小的网络状态潜在嵌入，结合固定大小、语义明确且可解释的动作空间，应用于PPO代理模型。

Result: TERLA代理在保持防御性能的同时提高了动作效率，能够部署到不同拓扑和规模的网络段，展示了良好的泛化能力。

Conclusion: TERLA框架成功实现了网络防御代理的拓扑和规模泛化，为实际网络防御应用提供了可行的解决方案。

Abstract: Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.

</details>


### [18] [Enabling Agents to Communicate Entirely in Latent Space](https://arxiv.org/abs/2511.09149)
*Zhuoyun Du,Runze Wang,Huiyu Bai,Zouying Cao,Xiaoyong Zhu,Bo Zheng,Wei Chen,Haochao Ying*

Main category: cs.LG

TL;DR: 提出Interlat（代理间潜在空间通信）范式，利用LLM的最后一个隐藏状态作为心智表征进行直接传输，通过潜在空间推理进一步压缩通信，提升协作问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 自然语言作为LLM代理通信媒介存在根本限制，将丰富的内部潜在状态下采样为离散标记会限制信息传输的深度和细微差别，从而阻碍协作问题解决。

Method: 使用LLM的最后一个隐藏状态作为心智表征进行直接传输（潜在通信），并通过额外的压缩过程在潜在空间中进行推理来进一步压缩通信。

Result: Interlat在实验中优于微调的思维链提示和单代理基线，促进更探索性行为并实现真正利用潜在信息。进一步压缩不仅显著加速推理，还通过高效的信息保留机制保持竞争力。

Conclusion: 这项工作作为完全潜在空间代理间通信的可行性研究，结果突显其潜力，为未来研究提供有价值的见解。

Abstract: While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.

</details>


### [19] [Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization](https://arxiv.org/abs/2511.09173)
*Guojian Wang,Quinson Hon,Xuyang Chen,Lin Zhao*

Main category: cs.LG

TL;DR: DFDT提出了一种解决决策变换器在跨域迁移中轨迹片段拼接问题的紧凑管道，通过两级数据过滤、优势条件令牌和Q引导正则化来恢复拼接性，在多种控制任务中提升了回报和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域策略适应方法通常依赖单一过滤标准选择源轨迹片段，但拼接后仍存在状态结构错位、RTG不可比、动作跳跃等问题，导致RTG令牌失去连续性，影响DT的推理能力。

Method: DFDT通过两级数据过滤器（MMD不匹配用于状态结构对齐，OT偏差用于动作可行性）融合稀缺目标数据和选择性信任的源片段，使用可行性加权的融合分布进行训练，并用优势条件令牌替换RTG令牌，应用Q引导正则化抑制连接点值和动作跳跃。

Result: DFDT在D4RL风格控制任务的重力、运动学和形态变化中，相比强离线RL和序列模型基线，提高了回报和稳定性，并通过令牌拼接和序列语义稳定性分析验证了这些收益。

Conclusion: DFDT通过恢复轨迹片段的拼接性，有效解决了决策变换器在跨域迁移中的挑战，理论分析表明状态值和策略性能差距与MMD不匹配和OT偏差度量相关，且随着这两个度量的缩小而收紧。

Abstract: Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.

</details>


### [20] [Iterated Population Based Training with Task-Agnostic Restarts](https://arxiv.org/abs/2511.09190)
*Alexander Chebykin,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 提出了迭代种群训练(IPBT)，一种自动调整超参数优化中更新间隔的PBT变体，通过重启机制和贝叶斯优化在多个任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有PBT方法中更新间隔这一重要元超参数缺乏有效的设置方法，需要自动化解决方案

Method: IPBT通过任务无关的重启机制重用权重信息，并使用时变贝叶斯优化重新初始化超参数

Result: 在8个图像分类和强化学习任务中，IPBT平均表现优于5种PBT变体和其他HPO算法，无需增加预算或调整超参数

Conclusion: IPBT提供了一种有效的自动调整PBT更新间隔的方法，在多个任务中表现稳定且优于现有方法

Abstract: Hyperparameter Optimization (HPO) can lift the burden of tuning hyperparameters (HPs) of neural networks. HPO algorithms from the Population Based Training (PBT) family are efficient thanks to dynamically adjusting HPs every few steps of the weight optimization. Recent results indicate that the number of steps between HP updates is an important meta-HP of all PBT variants that can substantially affect their performance. Yet, no method or intuition is available for efficiently setting its value. We introduce Iterated Population Based Training (IPBT), a novel PBT variant that automatically adjusts this HP via restarts that reuse weight information in a task-agnostic way and leverage time-varying Bayesian optimization to reinitialize HPs. Evaluation on 8 image classification and reinforcement learning tasks shows that, on average, our algorithm matches or outperforms 5 previous PBT variants and other HPO algorithms (random search, ASHA, SMAC3), without requiring a budget increase or any changes to its HPs. The source code is available at https://github.com/AwesomeLemon/IPBT.

</details>


### [21] [Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization](https://arxiv.org/abs/2511.09219)
*Paul Strang,Zacharie Alès,Côme Bissuel,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了PlanB&B，一种基于模型的强化学习代理，通过学习分支定界动态的内部模型来发现改进的分支策略，在四个标准MILP基准测试中优于先前最先进的RL方法。


<details>
  <summary>Details</summary>
Motivation: 传统MILP求解器使用静态、手工制作的分支启发式方法，希望超越这些方法，学习针对特定MILP分布的分支策略。

Method: 结合蒙特卡洛树搜索和模型强化学习，学习分支定界动态的内部模型，通过规划来改进分支决策。

Result: 在四个标准MILP基准测试中，PlanB&B的性能超过了之前最先进的强化学习方法。

Conclusion: 基于模型的强化学习方法在分支定界求解器中能够有效学习改进的分支策略。

Abstract: Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&B). A key driver influencing B&B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.

</details>


### [22] [A Distributed Training Architecture For Combinatorial Optimization](https://arxiv.org/abs/2511.09261)
*Yuyao Long*

Main category: cs.LG

TL;DR: 提出分布式GNN训练框架解决组合优化问题，通过图分割、子图训练和强化学习实现大规模图上的高效优化


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在处理复杂图时精度有限且扩展性差，全图训练需要加载整个邻接矩阵和嵌入，导致单机内存不足，限制了在大规模场景的应用

Method: 1. 将大图分割成多个小子图；2. 对各子图进行完整训练；3. 使用强化学习根据GNN输出采取行动，学习跨节点约束

Result: 在真实大规模社交网络数据集和合成高复杂度图上实验表明，该框架在解质量和计算效率上优于现有方法，大图实例验证了模型的可扩展性

Conclusion: 分布式GNN框架有效解决了组合优化问题中的扩展性挑战，在大规模图上表现出优越性能

Abstract: In recent years, graph neural networks (GNNs) have been widely applied in tackling combinatorial optimization problems. However, existing methods still suffer from limited accuracy when addressing that on complex graphs and exhibit poor scalability, since full training requires loading the whole adjacent matrix and all embeddings at a time, the it may results in out of memory of a single machine. This limitation significantly restricts their applicability to large-scale scenarios. To address these challenges, we propose a distributed GNN-based training framework for combinatorial optimization. In details, firstly, large graph is partition into several small subgraphs. Then the individual subgraphs are full trained, providing a foundation for efficient local optimization. Finally, reinforcement learning (RL) are employed to take actions according to GNN output, to make sure the restrictions between cross nodes can be learned. Extensive experiments are conducted on both real large-scale social network datasets (e.g., Facebook, Youtube) and synthetically generated high-complexity graphs, which demonstrate that our framework outperforms state-of-the-art approaches in both solution quality and computational efficiency. Moreover, the experiments on large graph instances also validate the scalability of the model.

</details>


### [23] [AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting](https://arxiv.org/abs/2511.09478)
*Renda Li,Hailang Huang,Fei Wei,Feng Xiong,Yong Wang,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出AdaCuRL自适应课程强化学习框架，通过粗到细难度估计和自适应课程调度，解决现有方法在混合难度样本训练中的梯度饥饿和策略退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在混合难度样本训练中存在梯度饥饿和策略退化问题，而传统课程学习方法面临难度不匹配、依赖人工设计和灾难性遗忘等挑战。

Method: AdaCuRL框架整合了粗到细难度估计和自适应课程调度，动态对齐数据难度与模型能力，并采用数据重访机制缓解灾难性遗忘，同时使用自适应参考和稀疏KL策略防止策略退化。

Result: 在多样化推理基准测试中，AdaCuRL在LLMs和MLLMs上均实现了显著的性能提升。

Conclusion: AdaCuRL通过自适应课程强化学习有效解决了现有方法的局限性，在推理任务中展现出优越性能。

Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 开发了一个面向工业PLC编程的低数据领域代码助手，通过RAG、多模型竞争和即时编译验证等技术，在不微调大模型的情况下实现了高质量的IEC 61131-3代码生成。


<details>
  <summary>Details</summary>
Motivation: 工业PLC使用专有代码方言，难以训练代码助手；现有LLM不了解特定功能块和项目代码，且企业不信任云服务商，需要本地化解决方案。

Method: 采用检索增强生成(RAG)、多模型竞争机制、自动错误修正和即时编译验证，通过精细提示工程和定向检索支持低数据领域。

Result: 实现了高质量的代码生成，支持边缘设备使用的小模型微调，提供了代码编译统计和用户评分等全面评估。

Conclusion: RAG支持的代码助手可以在低数据领域有效工作，通过扩展提示工程和定向检索来弥补数据不足。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [25] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: 提出SPLVD方法，通过自步学习动态选择训练数据，模拟人类从易到难的学习过程，显著提升软件漏洞检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法准确率有限，主要原因是训练数据质量低。需要改进训练数据选择策略

Method: SPLVD方法包含专门设计的数据选择器，根据训练阶段动态选择源代码，优先学习简单代码，并在每个训练周期重新计算代码难度并更新选择器

Result: 在三个基准数据集上分别达到89.2%、68.7%和43.5%的F1分数，优于现有方法。在OpenHarmony项目中达到90.9%的最高精度

Conclusion: SPLVD通过自步学习策略有效提升漏洞检测性能，在实际项目中表现出色

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [26] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: AILINKPREVIEWER是一个利用大语言模型生成PR中链接预览的工具，通过分析PR元数据来提供更丰富的上下文信息，帮助提升代码审查效率。


<details>
  <summary>Details</summary>
Motivation: 代码审查中链接通常被丢弃，限制了信息的丰富性并增加了认知负担，需要工具来提供更好的上下文预览。

Method: 使用LLM生成链接预览，比较了三种方法：上下文LLM摘要、非上下文LLM摘要和基于元数据的预览，并在50个GitHub仓库中进行了评估。

Result: 上下文摘要在BLEU、BERTScore和压缩比等指标上表现更好，但用户研究显示大多数参与者更喜欢非上下文摘要，表明存在指标性能与感知可用性之间的权衡。

Conclusion: LLM驱动的链接预览有潜力提升代码审查效率，为开发者和自动化工具提供更丰富的上下文。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [27] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本研究探索使用大型语言模型辅助用例建模，通过集成开源LLM和高级提示工程技术从软件需求中提取参与者和用例，相比传统手动方法可减少60%建模时间且保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 手动创建用例模型耗时费力，在实践中经常被跳过，因此需要自动化方法来提高效率。

Method: 集成开源大型语言模型，运用高级提示工程技术从软件需求中系统提取参与者和用例。

Result: 与手动建模相比，LLM方法减少60%建模时间，同时模型质量与传统方法相当，参与者认为该方法在过程中提供了有价值的指导。

Conclusion: LLM辅助的用例建模方法能显著提高建模效率，为软件工程实践提供有价值的自动化支持。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [28] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 对Claude Code配置文件的实证研究，分析了328个公开项目中的配置结构、内容及软件工程关注点


<details>
  <summary>Details</summary>
Motivation: 了解AI代码助手配置生态系统的结构和内容，这些配置定义了架构约束、编码实践和工具使用策略，但相关研究较少

Method: 收集并分析328个Claude Code公开项目的配置文件，识别指定的软件工程关注点和实践，以及这些关注点在单个文件中的共现模式

Result: 配置文件需要定义广泛的关注点和实践，特别强调指定代理应遵循的架构

Conclusion: 配置文件在AI代码助手中扮演关键角色，需要明确定义各种软件工程关注点，尤其是架构规范

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [29] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain是首个针对软件相关任务的LLM路由器，通过提取可解释概念进行路由决策，在准确性和成本方面优于单个模型，并达到或超越所有黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在软件任务中表现差异显著，但之前的路由工作主要关注通用LLM路由。需要专门针对软件任务的可解释路由方案来提升响应质量并降低成本。

Method: Routesplain从查询中提取人类可解释的概念（如任务类型、领域、推理复杂度），仅基于这些概念进行路由决策，提供可理解且可信的推理过程。

Result: 在8个软件相关任务和16个最先进LLM上的评估显示，Routesplain在准确性和成本方面优于单个模型，并等于或超越所有黑盒基线方法。

Conclusion: Routesplain证明了基于概念的路由在软件任务中的有效性，概念级干预为进一步改进路由器提供了途径。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [30] [就在昨天！！ Google新发布54页Agent开发指南：五层架构详解<em class="highlight">Agentic</em> AI](http://mp.weixin.qq.com/s?__biz=MzYzODE5MzI3MA==&mid=2247483705&idx=1&sn=c5ddb3c8afa34f1fb82d1e3090a643ac&chksm=f1e2894243c25cf6ca3a6af202828181a0b1d18e3975d94379b6365a857a92dfaa63c213de3c#rd)
*大模型凯凯*

Main category: wechat.article

TL;DR: google新发布54页 agent开发指南， 五层架构详解 agentic ai google introduction to agents google table of contents from predictive ai to autonomous agents introduction to ai agents------_ ...... .. 8 the agentic proble...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: google新发布54页 agent开发指南， 五层架构详解 agentic ai google introduction to agents google table of contents from predictive ai to autonomous agents introduction to ai agents------_ ...... .. 8 the agentic problem-solving process 5，，10 a taxonomy of agentic systems.----. -....-

</details>


### [31] [5张动图看懂构建<em class="highlight">Agentic</em> AI 的主流设计模式](http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490171&idx=1&sn=c5b854503e6fa4e6c60c9f28eae3172f&chksm=9a771cf46a46fbf44b86afff6b9b6a752e5bb1e0f4eb22b6462900434e0310850d662da7f1d1#rd)
*究模智*

Main category: wechat.article

TL;DR: Agentic AI（智能体）的崛起，通过引入自我评估、规划与协作等行为，使模型具备了持续演进、与环境交互的能力。本文将深入探讨构建Agentic AI 的5种主流设计模式，深入拆解其运作逻辑、典型应用场景与核心价值。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI（智能体）的崛起，通过引入自我评估、规划与协作等行为，使模型具备了持续演进、与环境交互的能力。本文将深入探讨构建Agentic AI 的5种主流设计模式，深入拆解其运作逻辑、典型应用场景与核心价值。

</details>


### [32] [用人类的“温度”拥抱AI！人机共驾时代，<em class="highlight">Agentic</em> AI如何重塑客服的“共情力边界”？](http://mp.weixin.qq.com/s?__biz=MzYzODExNDE4NQ==&mid=2247483680&idx=1&sn=0c02ffc11f56c60c6eb5f2bdfc8b24d6&chksm=f1ee3a9c9df7998511cee94d55d377ae03a999717a827a3aa676b5b405b1c8839d32f94c3375#rd)
*出海护航者*

Main category: wechat.article

TL;DR: 什么是 Agentic AI？Agentic AI，简单来说，就是有“执行权”的AI。它不再是传统客服机器人那样只能被动地“问答”或“提供信息”，而是能够像人类特工一样，接收一个模糊的目标（如“我需要退货并换成黑色款”），然后自主


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是 Agentic AI？Agentic AI，简单来说，就是有“执行权”的AI。它不再是传统客服机器人那样只能被动地“问答”或“提供信息”，而是能够像人类特工一样，接收一个模糊的目标（如“我需要退货并换成黑色款”），然后自主

</details>


### [33] [【火爆全网】AI『<em class="highlight">智能体</em>』在未来产业创新上的前沿应用与发展趋势](http://mp.weixin.qq.com/s?__biz=Mzk2NDUxMjkxOQ==&mid=2247489345&idx=1&sn=ab182089d275af6347eafeed607a4c13&chksm=c557e80de6ae2b4d688b93acc737f28de4d59ff38979e1c5889088185d2ef2164b71163ac9df#rd)
*AAIA具身智能与空间智能分会*

Main category: wechat.article

TL;DR: - Evolution（演化）二、Agent vs. Agentic AI：区别与演进特征特征 Al Agent Agentic Al自主性 较低 高协作能力 单点 多智能体协同记忆能力 无/弱 持久记忆推理与规划 简单 高级推理与任务分解


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: - Evolution（演化）二、Agent vs. Agentic AI：区别与演进特征特征 Al Agent Agentic Al自主性 较低 高协作能力 单点 多智能体协同记忆能力 无/弱 持久记忆推理与规划 简单 高级推理与任务分解

</details>


### [34] [微软 <em class="highlight">Agentic</em> 组织：下一代 AI 系统](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247499296&idx=1&sn=53c2a4d26a4c06993c71c7825cc6b1cd&chksm=c3d6b1269a2044d9dd7c266b2b7c0061aeac541633fa2f4a615d3ceda386f9ee3100d469c68a#rd)
*PaperAgent*

Main category: wechat.article

TL;DR: 表1：Agentic Organization概念与计算机系统的优雅类比四大动作标签整个系统通过四个简单的文本标签实现复杂协同：子任务描述：组织者向空闲工人i分配子查询


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 表1：Agentic Organization概念与计算机系统的优雅类比四大动作标签整个系统通过四个简单的文本标签实现复杂协同：子任务描述：组织者向空闲工人i分配子查询

</details>


### [35] [一文全解析！<em class="highlight">Agentic</em> AI记忆机制的9种实现方式！](http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483786&idx=1&sn=73e4034061da41ac757c3f9705ff34d1&chksm=f16824378479802aa4940abdb73f6ccc7f1f3d1dbf0a997ba4338322c2e5b71fdbd7bc3f5633#rd)
*EasyShip.AI*

Main category: wechat.article

TL;DR: 在这篇博客中，我们将编写并评估9种针对AI代理的从初级到高级的内存优化技术。in this blog， we will code and evaluate 9 beginner-to-advanced memory optimization techniques for ai agents. you will learn how to apply each technique， along wit...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在这篇博客中，我们将编写并评估9种针对AI代理的从初级到高级的内存优化技术。in this blog， we will code and evaluate 9 beginner-to-advanced memory optimization techniques for ai agents. you will learn how to apply each technique， along with their advantages and dra

</details>


### [36] [药物研发进入 “小时代”：<em class="highlight">Agentic</em> AI 落地元年，案例 + 架构定义新范式](http://mp.weixin.qq.com/s?__biz=MzYyMzI1NDI3MA==&mid=2247485150&idx=1&sn=bf07952de56cd3f5e0b33763cf21f2a0&chksm=fe903492ad0d782a9ae0aa51e7c46eff0fe65cadaafe0c3afdefaba5c3229626d31923f7eb74#rd)
*AI药物设计实验室*

Main category: wechat.article

TL;DR: 系统梳理了 agentic AI 的概念、常见架构与工具组合，并用多个真实项目展示了它在文献综合、毒理评估、自动化实验、分子合成、罕见病用药重定位与商业决策中的效果，核心结论是许多过去需要数周到数月的环节被压缩到小时


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 系统梳理了 agentic AI 的概念、常见架构与工具组合，并用多个真实项目展示了它在文献综合、毒理评估、自动化实验、分子合成、罕见病用药重定位与商业决策中的效果，核心结论是许多过去需要数周到数月的环节被压缩到小时

</details>


### [37] [谈谈出海 | 陈奕强——<em class="highlight">Agentic</em> AI与AI Agents：人工智能的下一场革命](http://mp.weixin.qq.com/s?__biz=Mzk2NDExMjE2OA==&mid=2247487496&idx=3&sn=c229eb371b3097da06a38d6fb04400f6&chksm=c539e1a05ba384ce1b7f2b9215b0c7a775eabda9b456970ea7b4e1d05bf03545be4e95682ac7#rd)
*谈谈出海*

Main category: wechat.article

TL;DR: 换言之，Agentic AI 不只是“回答问题”的机器，而是能够“寻找答案、判断优先级、协调资源”的“行动型智能体”。如果说AI Agent是一位能干的助理，那么Agentic AI则更像是一位具备判断力与责任感的项目经理。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 换言之，Agentic AI 不只是“回答问题”的机器，而是能够“寻找答案、判断优先级、协调资源”的“行动型智能体”。如果说AI Agent是一位能干的助理，那么Agentic AI则更像是一位具备判断力与责任感的项目经理。

</details>


### [38] [<em class="highlight">Agentic</em>21种设计模式5-Tool Use (Function Calling)](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484293&idx=1&sn=ffc09cbdf9348193b954982e4138a5ee&chksm=c09d12087f379221a9ca0241fbdacb7c317b396dc11b635ea747ddedc5e028119ed35f0e20be#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: 用例：电子商务客服AgentTools：通过API调用来查询产品库存、获取订单状态或处理付款。Agent Flow：当用户询问“X产品有库存吗？”时，LLM会通过API调用库存查询系统；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 用例：电子商务客服AgentTools：通过API调用来查询产品库存、获取订单状态或处理付款。Agent Flow：当用户询问“X产品有库存吗？”时，LLM会通过API调用库存查询系统；

</details>


### [39] [AI那些趣事系列108：一文轻松读懂 LLaMA 系列<em class="highlight">模型</em>：从 Meta 开源爆款到 AI 生态基石](http://mp.weixin.qq.com/s?__biz=MzU1NjYyODQ4NA==&mid=2247486416&idx=1&sn=2c542ffd9a47c1caecf4834fdc880c2d&chksm=faeb4fa9ea382b1ea02982786c5af9cc9e72899bdc4258df98a57495b2df0f42f0eaba7d23ce#rd)
*数据拾光者*

Main category: wechat.article

TL;DR: 它的出现，直接打破了 “大模型 = 闭源” 的固有认知 —— 在此之前，只有 OpenAI、谷歌等巨头能玩得转大模型，普通开发者连接触的机会都没有。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它的出现，直接打破了 “大模型 = 闭源” 的固有认知 —— 在此之前，只有 OpenAI、谷歌等巨头能玩得转大模型，普通开发者连接触的机会都没有。

</details>


### [40] [国内AI<em class="highlight">大模型</em>“战国杀”：谁会是赢家？](http://mp.weixin.qq.com/s?__biz=MzYzMjE4NzgyMg==&mid=2247483748&idx=1&sn=724bd4b2a941fee4aa271af99bd6f3c7&chksm=f145bf355ccd064b14afa181ed638f7e6d0fcbb81ccb57d442c8de22002592299257f6dd001c#rd)
*猴的自我修养*

Main category: wechat.article

TL;DR: 在今日的 2025 百度世界大会上，百度创始人李彦宏正式发布文心大模型 5.0。参数规模超 2.4 万亿，原生全模态理解、创意写作、智能体规划、指令遵循等方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在今日的 2025 百度世界大会上，百度创始人李彦宏正式发布文心大模型 5.0。参数规模超 2.4 万亿，原生全模态理解、创意写作、智能体规划、指令遵循等方面均有提升。

</details>


### [41] [一个模型读懂所有医学数据，Hulu-Med探索医学<em class="highlight">大模型</em>开源新范式 | 浙大x上交xUIUC](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247842352&idx=4&sn=55010d957c3b6835c0f97af22cc50327&chksm=e9af37245570963bd71186cdf96dd113a7a56c4998e5e1e0605ee8e5d30cdffd775e6bea6cfd#rd)
*量子位*

Main category: wechat.article

TL;DR: 而且作为开源模型，其训练数据均来自公开医学数据集及自研合成数据，不仅能大幅度降低GPU训练成本，更是在30项权威评测中展现出媲美GPT-4.1等闭源模型的优异性能。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而且作为开源模型，其训练数据均来自公开医学数据集及自研合成数据，不仅能大幅度降低GPU训练成本，更是在30项权威评测中展现出媲美GPT-4.1等闭源模型的优异性能。

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 提出了一个基于向量符号代数的认知合理ARC-AGI求解器，结合系统1直觉和系统2推理，通过神经符号方法实现高效可解释的抽象推理


<details>
  <summary>Details</summary>
Motivation: ARC-AGI作为通用人工智能基准对人类来说很容易，但对最先进AI系统仍极具挑战，受神经科学和心理学启发，希望开发认知合理的求解方法

Method: 使用向量符号代数进行神经符号方法，集成系统1直觉和系统2推理，通过对象中心程序合成实现抽象推理，利用VSA表示抽象对象并指导解决方案搜索

Result: 在ARC-AGI-1-Train上得分10.8%，ARC-AGI-1-Eval上3.0%；在简单基准上表现优秀：Sort-of-ARC得分94.5%，1D-ARC得分83.1%（优于GPT-4且计算成本极低）

Conclusion: 这是首个将VSA应用于ARC-AGI的方法，开发了迄今为止最认知合理的ARC-AGI求解器，展示了神经符号方法在抽象推理任务中的潜力

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [43] [Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning](https://arxiv.org/abs/2511.08749)
*Mehrdad Zakershahrak*

Main category: cs.AI

TL;DR: 提出QDIN架构，将强化学习系统设计为可回答多种环境查询的推理引擎，而非仅专注于奖励最大化。研究发现推理准确性与控制性能存在解耦现象。


<details>
  <summary>Details</summary>
Motivation: 挑战传统RL仅关注奖励最大化的范式，探索将RL系统构建为可回答环境多样性查询的推理引擎，以暴露智能体隐含的环境知识。

Method: 引入查询条件确定性推理网络(QDIN)，为不同类型的查询（策略、可达性、路径、比较）设计专门的神经模块，每种推理模式都有优化架构。

Result: 发现推理准确性与控制性能存在根本性解耦：推理准确率可达近完美水平（99%可达性IoU），而控制性能仍处于次优状态（31%回报）。专用架构在推理任务上优于统一模型和后验提取方法。

Conclusion: 这项工作为从设计之初就作为可查询知识库的RL系统建立了研究议程，对可解释性、验证和人机协作具有重要意义。

Abstract: Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.

</details>


### [44] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 提出了单向认知优化(UCO)方法，通过多轮交互强化学习解决LLM作为智能导师时的两个关键挑战：区分学生真实理解与答案回响，以及动态感知学生认知状态。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育场景中从答案提供者转向智能导师，但现有方法缺乏动态适应能力，无法区分学生真实理解与答案回响，也无法感知学生认知状态变化。

Method: 使用多轮交互强化学习范式，包含两个协同奖励函数：进展奖励捕捉学生认知进步，支架奖励动态识别学生的最近发展区(ZPD)。

Result: 在BigMath和MathTutorBench基准测试中，UCO模型优于所有同等规模模型，性能可与先进闭源模型相媲美。

Conclusion: UCO方法有效解决了LLM作为智能导师时的关键挑战，实现了动态适应学生认知水平的教学能力。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [45] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 论文提出通过结构化多智能体管道实现渐进式增量搜索(GIS)来增强LLM推理能力，并通过递归精炼方法验证了复杂管道模型在政治议题分析中优于简单模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出优秀的流畅性，但研究者仍在努力提升其推理能力。论文基于搜索导向的LLM计算解释，旨在系统性地理解和优化LLM推理过程。

Method: 采用递归精炼(RR)方法，通过自我批评、对抗性压力测试和整合关键反馈的迭代过程实现GIS搜索。设计了简单线性管道与复杂结构化管道的对比实验，使用基于RAG的美国开国元勋人物模型分析当代政治议题。

Result: 复杂模型在所有九个测试案例中均优于简单模型，平均仲裁分数为88.3 vs 71.7。复杂模型的论证在分析深度、结构细微差别和策略框架方面表现更优。

Conclusion: 递归精炼是通过GIS搜索增强LLM推理能力的强大架构特征，结构化多智能体管道能够实现更高质量的推理输出。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [46] [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)
*Elliot Meyerson,Giuseppe Paolo,Roberto Dailey,Hormoz Shahrzad,Olivier Francon,Conor F. Hayes,Xin Qiu,Babak Hodjat,Risto Miikkulainen*

Main category: cs.AI

TL;DR: MAKER系统首次成功实现了超过100万步LLM任务且零错误，通过极端任务分解和微代理机制解决了LLM在长程任务中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长程任务中因持续错误率而无法扩展的问题，突破现有LLM在依赖逻辑步骤较多任务中的局限性。

Method: 采用极端任务分解策略，将任务分解为可由专注微代理处理的子任务，结合高效的多代理投票机制进行错误校正。

Result: 成功实现超过100万步LLM任务零错误执行，理论上可扩展到更高水平。

Conclusion: 大规模分解代理过程（MDAPs）可能比持续改进现有LLM更有效地解决组织和社会层面的问题。

Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.

</details>


### [47] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了下一代自动驾驶车辆在紧急服务中的优化策略，重点分析了从传统强化学习向扩散模型增强强化学习和LLM辅助上下文学习的转变。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在自动驾驶紧急响应中存在样本效率低和动态场景适应性差的问题，需要更先进的AI方法来提升自动驾驶车辆在紧急服务中的性能。

Method: 分析了三种方法：传统强化学习、扩散模型增强强化学习（通过合成数据生成增强策略鲁棒性）和LLM辅助上下文学习（无需重新训练即可快速适应）。

Result: 扩散模型增强强化学习提高了策略鲁棒性但计算成本增加，LLM辅助上下文学习提供了轻量级且可解释的替代方案。

Conclusion: 从生成式AI视角为理解下一代自主紧急响应系统提供了关键框架。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [48] [History-Aware Reasoning for GUI Agents](https://arxiv.org/abs/2511.09127)
*Ziwei Wang,Leyang Yang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: 提出历史感知推理(HAR)框架，通过反思学习增强GUI代理的短期记忆能力，解决现有GUI代理在长程任务中历史交互意识不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在显式推理中表现出较弱的短期记忆，将链式交互视为离散的屏幕理解，缺乏对历史交互的感知，这限制了在GUI自动化中的性能。

Method: 构建反思学习场景、合成定制修正指南、设计混合RL奖励函数，开发HAR-GUI-3B模型，将推理模式从历史无关转变为历史感知。

Result: 在多个GUI相关基准测试中的综合评估表明，该方法具有有效性和泛化能力。

Conclusion: HAR框架通过增强短期记忆和屏幕细节感知，为GUI代理提供了稳定的历史交互意识和可靠的推理能力。

Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.

</details>


### [49] [ProBench: Benchmarking GUI Agents with Accurate Process Information](https://arxiv.org/abs/2511.09157)
*Leyang Yang,Ziwei Wang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: 提出了ProBench，一个包含200多个挑战性GUI任务的移动端基准测试，不仅评估最终状态，还引入过程相关任务和专门的评估方法来精确评估GUI代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅通过检查最终屏幕状态来评估GUI代理，但GUI操作任务包含多个链式步骤，并非所有关键信息都呈现在最后几页中，准确自动捕获过程信息仍是一个挑战。

Method: 引入ProBench基准测试，包含状态相关任务和过程相关任务，设计专门的评估方法，通过过程提供者自动提供准确的过程信息来精确评估代理性能。

Result: 对先进GUI代理的评估揭示了它们在真实世界GUI场景中的显著局限性，这些缺陷在包括大规模通用模型和较小GUI专用模型在内的各种模型中普遍存在。

Conclusion: 详细的错误分析揭示了几个普遍问题，为未来的改进指明了具体方向。

Abstract: With the deep integration of artificial intelligence and interactive technology, Graphical User Interface (GUI) Agent, as the carrier connecting goal-oriented natural language and real-world devices, has received widespread attention from the community. Contemporary benchmarks aim to evaluate the comprehensive capabilities of GUI agents in GUI operation tasks, generally determining task completion solely by inspecting the final screen state. However, GUI operation tasks consist of multiple chained steps while not all critical information is presented in the final few pages. Although a few research has begun to incorporate intermediate steps into evaluation, accurately and automatically capturing this process information still remains an open challenge. To address this weakness, we introduce ProBench, a comprehensive mobile benchmark with over 200 challenging GUI tasks covering widely-used scenarios. Remaining the traditional State-related Task evaluation, we extend our dataset to include Process-related Task and design a specialized evaluation method. A newly introduced Process Provider automatically supplies accurate process information, enabling presice assessment of agent's performance. Our evaluation of advanced GUI agents reveals significant limitations for real-world GUI scenarios. These shortcomings are prevalent across diverse models, including both large-scale generalist models and smaller, GUI-specific models. A detailed error analysis further exposes several universal problems, outlining concrete directions for future improvements.

</details>


### [50] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种训练简洁性奖励模型(CRM)的管道，通过新颖的简洁性奖励函数(CRF)解决大型推理模型中的过度思考问题，在保持准确性的同时显著减少推理步骤长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型如DeepSeek-R1和OpenAI o1经常产生包含冗余或无关推理步骤的冗长响应（过度思考现象），这显著增加了计算成本。现有的长度惩罚方法存在长度崩溃和训练崩溃问题。

Method: 提出训练简洁性奖励模型(CRM)的管道，为推理路径的简洁性评分；引入新颖的简洁性奖励函数(CRF)，明确建立结果奖励与简洁性评分之间的依赖关系。

Result: 在五个数学基准数据集上的实验显示，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应token长度减少，并在Llama和Mistral等其他LLM上具有良好的泛化性。

Conclusion: 该方法通过理论证明和实验验证，能够有效促进更高效和更有效的推理，解决了过度思考问题，同时提高了模型性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [51] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 提出了一个双层可靠性监控框架来解决智能AI系统在运行时的可靠性挑战，包括离群检测层和AI透明度层，为人类操作员提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统虽然具有自主性优势，但在高风险领域应用时存在可靠性不足的问题，需要缓解技术来降低运行时的风险。

Method: 基于智能AI系统特性推导主要可靠性挑战，提出包含离群检测和AI透明度的双层监控框架，为操作员提供干预决策支持。

Result: 建立了一个基础框架来开发缓解技术，减少智能AI系统在运行时因可靠性不确定而产生的风险。

Conclusion: 双层监控框架为解决智能AI系统可靠性问题提供了有效方法，能够支持人类操作员识别不可靠输出并进行干预。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [52] [BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems](https://arxiv.org/abs/2511.09363)
*Ali Taheri,Alireza Taban,Sadegh Soudjani,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 提出基于LLM的代理框架，用于自动合成动态系统的屏障证书，结合自然语言推理和SMT验证，在BarrierBench基准测试中达到90%以上的成功率


<details>
  <summary>Details</summary>
Motivation: 传统屏障证书合成方法存在可扩展性差、依赖模板设计、需要大量人工专业知识等问题，需要探索能否用语言模型捕捉专家推理

Method: 使用LLM驱动的代理框架，通过自然语言推理提出、改进和验证候选证书，结合模板发现和SMT验证，支持屏障-控制器协同合成

Result: 在包含100个动态系统的BarrierBench基准测试中，框架成功生成有效证书的比例超过90%

Conclusion: LLM引导的屏障合成方法有效，检索增强生成和代理协调策略提高了可靠性和性能，为语言推理与形式验证的集成提供了测试平台

Abstract: Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.
  This motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.
  To evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.
  The benchmark is publicly available at https://hycodev.com/dataset/barrierbench

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [53] [Claude Code Infrastructure Showcase](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdiet103%2Fclaude-code-infrastructure-showcase%3Futm_source=tldrai/1/0100019a6e242a70-7748493a-44ba-4eff-a918-88d6e003c985-000000/LKXbz_jfPH0MUPdrLrtHUX9yOIC62Lf7U_tQUD7fI-U=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 这是一个展示Claude代码基础设施的GitHub仓库，包含生产环境测试的自动技能激活、专用代理等基础设施组件


<details>
  <summary>Details</summary>
Motivation: 为Claude代码代理提供生产级别的可靠基础设施，解决技能自动激活、上下文管理、知识保留等实际问题

Method: 构建包含技能锁、专用代理、模块化技能、开发文档等组件的参考基础设施库

Result: 实现了基于上下文的技能自动建议、适时触发、模块化管理和知识保留等功能

Conclusion: 该基础设施为构建可靠的代码代理系统提供了实用的生产级解决方案

Abstract: Claude Code Infrastructure Showcase (GitHub Repo) This repository contains a curated reference library of production-tested Claude Code infrastructure. It contains production-tested infrastructure for auto-activating skills via locks, specialized agents for complex tasks, and more. Using this infrastructure, skills will suggest themselves based on context, hooks will trigger skills at the right time, modular skills stay under context limits, dev docs preserve knowledge across resets, and agen...

</details>


### [54] [Dazl: Vibe code your app, then refine with a visual editor](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdazl.dev%2F%3Futm_source=tldrproduct%26utm_medium=newsletter/1/0100019a72993f41-3407bbaa-f6c4-4341-92ab-88bc7c618096-000000/9ux6Es4pUqNrn0hPWg2k1hHVHy6u4M8vMuXlvjOge0k=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dazl是一个结合生成式AI与可视化编辑的平台，允许用户通过聊天、可视化面板或代码来构建和精炼应用程序。


<details>
  <summary>Details</summary>
Motivation: 为产品制作者提供一个结合AI生成能力和手动编辑的综合性开发平台，简化应用开发流程。

Method: 结合生成式AI与可视化编辑器，支持通过聊天界面、可视化面板或直接代码编辑三种方式开发应用。

Result: 推出了Dazl平台，目前对所有产品制作者开放，并提供1个月免费试用优惠。

Conclusion: Dazl成功创建了一个融合AI生成与手动编辑的开发环境，为应用开发提供了新的可能性。

Abstract: Dazl: Vibe code your app, then refine with a visual editor (Sponsor) Now open to all product makers! Dazl is the first platform to combine genAI with hands-on editing through chat, visual panels, or code. Get 1 month free with coupon: DazlxTLDR (offer valid for the first 100 users, until 11/30).

</details>


### [55] [Getting agents to code is easy. Getting them to follow rules is hard](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feu1.hubs.ly%2FH0pkGMy0%3Futm_source=tldrnewsletter/1/0100019a72a8565c-a1cad75b-bef1-43e6-b99f-b86f4949da1c-000000/1-kich-rh5dfTKuXZW4QHV16XAw8X-flIs8KCPFO3Pk=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Tabnine通过多层上下文技术让AI代码代理遵循架构、标准和合规规则


<details>
  <summary>Details</summary>
Motivation: 虽然让AI代理生成代码很容易，但让它们遵循规则却很困难，这给AI编码带来了风险

Method: 使用多层上下文技术来确保AI代码代理能够尊重架构、标准和合规规则

Result: Tabnine成功弥合了AI代码生成与规则遵循之间的差距

Conclusion: 通过适当的多层上下文方法，可以让AI代码代理在生成代码时可靠地遵循既定规则

Abstract: Getting agents to code is easy. Getting them to follow rules is hard (Sponsor) If AI coding feels risky, you're not wrong. But with multi-layer context, you can get agents to respect your architecture, standards, and compliance rules. See how Tabnine bridges the gap

</details>


### [56] [Free Resources for Scaling Developer-Driven Security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Farticle%2Fsecure-by-design-whitepaper-pdf/2/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/m2EksrhsTV29-OSB8nxzuJ_7nC_yEiXKd93BHPmqcTM=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文提供了三个免费资源来帮助开发者在软件开发生命周期中集成安全实践，包括安全设计基准测试、研究论文和安全编码培训平台。


<details>
  <summary>Details</summary>
Motivation: 将安全实践融入SDLC，使开发者能够主动参与安全防护，而不是依赖事后修复。

Method: 通过收集超过25万名开发者的内部数据点，结合多个主要来源的聚合数据，定义最佳实践并建立预防性安全基准。

Result: 提供了可量化的安全设计基准和实用的开发者培训资源。

Conclusion: 通过提供免费资源和数据驱动的基准测试，能够有效推动开发者驱动的安全文化转型。

Abstract: Free Resources for Scaling Developer-Driven Security (Sponsor) Turn security into an integral part of your SDLC with these 3 free resources from Secure Code Warrior:1️⃣ Benchmarking Secure-by-Design initiatives: A presentation looking at aggregated data derived from multiple primary sources, including internal data points collected from over 250,000 developers. 2️⃣ Secure by Design research paper: Defining best practices, enabling developers, and benchmarking preventative security outcomes. A...

</details>


### [57] [3️⃣ Free trial of SCW Trust Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.securecodewarrior.com%2Fproduct%2Ftrust-agent/1/0100019a733e0e71-a991c177-c757-47f5-9c2f-3cc6ecae3e78-000000/nE2hH5IJKAEDc6PrzQT33P97vp-IUW8oGYZx-VVy49E=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 分析代码提交与贡献者语言特定安全编码能力的关联


<details>
  <summary>Details</summary>
Motivation: 通过关联提交数据与贡献者的安全编码熟练度，识别代码提交中的潜在安全风险

Method: 分析每个代码提交，将提交数据与贡献者的语言特定安全编码能力进行关联分析

Result: 建立了代码提交与贡献者安全编码能力的关联模型

Conclusion: 该方法能够有效识别代码提交中的安全风险，有助于提高代码安全性

Abstract: : Analyzes every code commit by correlating commit data with the contributor's language-specific secure coding proficiency.

</details>


### [58] [xAI works on Grok Code Remote to rival OpenAI's Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fxai-working-on-grok-code-remote-to-rival-openai%2F%3Futm_source=tldrai/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/SLM8AIN8RKmq7gdJpXKeINtdoMPu0rW1KT3wOdPDw6U=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: xAI正在开发Grok Code Remote功能，允许用户在远程环境中直接运行代码，并将在12月6-7日举办黑客马拉松，参与者可获得实验模型的早期访问权限。


<details>
  <summary>Details</summary>
Motivation: 与行业向基于云的代码执行趋势保持一致，对标OpenAI的Codex，提供更便捷的云端代码运行体验。

Method: 开发Grok Code Remote功能，让用户能够直接从网页在远程环境中运行代码。

Result: 计划在12月举办黑客马拉松，为参与者提供实验模型的早期访问权限。

Conclusion: xAI正在积极开发云端代码执行功能，以在代码AI领域与OpenAI竞争。

Abstract: xAI works on Grok Code Remote to rival OpenAI's Codex (2 minute read) xAI will host a hackathon on December 6-7. Participants will receive early access to experimental models. The company is working on a feature called Grok Code Remote to align with industry trends towards cloud-based code execution. It will allow users to run code in remote environments directly from the web.

</details>


### [59] [AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fw55WVG/1/0100019a7346e404-6b4fae15-4f6e-42d6-8220-0e366ad1ed7e-000000/-97H79I3Kfk9Del0eh1KEokWE0PZS2MTbrynIvEhk3E=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Moonshot AI发布了Kimi K2模型，这是一个拥有320亿激活参数和1万亿总参数的专家混合模型，在知识、数学和编程方面达到最先进水平，并具备智能体能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个在知识、数学和编程领域表现优异，同时具备智能体能力的开源模型，推动AI技术发展。

Method: 使用专家混合架构，构建拥有320亿激活参数和1万亿总参数的大规模模型。

Result: Kimi K2在非思考模型中实现了前沿知识、数学和编程的最先进性能，并具备智能体能力。

Conclusion: Kimi K2模型展示了大规模专家混合模型在多个领域的强大能力，为开源社区提供了高质量的AI模型。

Abstract: AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model (Reddit Thread) Moonshot AI's Kimi K2 is a Mixture-of-Experts model with 32 billion activated parameters and 1 trillion total parameters. It achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models and also has agentic abilities. Kimi-K2-Base and Kimi-K2-Instruct were recently open sourced. This thread contains a discussion with the team at Moonshot AI.

</details>


### [60] [Vertical Integration is the Only Thing That Matters](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbecca.ooo%2Fblog%2Fvertical-integration%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/fOV3ZFfm13ttf8tuRsuzXP6AJm2OvTkUgatxJEgKNQQ=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 垂直集成（工具栈中不同工具的紧密集成）对于开发效率至关重要，通过自动化构建工件共享和跨语言交互式调试等示例展示了其价值。


<details>
  <summary>Details</summary>
Motivation: 探讨开发工具栈中垂直集成的重要性，认为这是提升开发效率的关键因素。

Method: 通过分析垂直集成的工作流程示例（如自动化构建工件共享、跨语言交互式调试）来阐述其价值。

Result: 垂直集成能够显著提升开发效率，虽然构建垂直集成的开发环境具有挑战性，但连接不同工具的胶水代码是真正价值所在。

Conclusion: 垂直集成是开发工具栈中唯一真正重要的因素，对于开发效率至关重要。

Abstract: Vertical Integration is the Only Thing That Matters (10 minute read) Vertical integration, which means tight integration between different tools in a development stack, is necessary for developer productivity. Examples of such vertically integrated workflows include automated build artifact sharing and interactive debugging across languages. While building a vertically integrated development environment as a product is challenging, the glue code connecting different tools is where the real va...

</details>


### [61] [ADK for Go](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgoogle%2Fadk-go%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/uEnZZiWWSf4stuQQJYLD4vvu126cGAzE3zq3AsBIMW0=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ADK for Go 是一个开源的、代码优先的 Go 工具包，用于构建、评估和部署 AI 代理，将软件开发原则应用于 AI 代理创建，简化代理工作流程编排。


<details>
  <summary>Details</summary>
Motivation: 将软件开发原则应用于 AI 代理创建，简化代理工作流程的编排，提供模型和部署无关的工具生态系统。

Method: 开发一个开源的、代码优先的 Go 工具包，支持模块化多代理系统，使代理工作流程的编排更加容易。

Result: 创建了 ADK for Go，一个模型和部署无关的工具包，具有模块化多代理系统的工具生态系统。

Conclusion: ADK for Go 成功地将软件开发原则应用于 AI 代理创建，提供了一个有效的工具包来构建、评估和部署 AI 代理。

Abstract: ADK for Go (GitHub Repo) The Agent Development Kit (ADK) is an open-source, code-first Go toolkit designed for building, evaluating, and deploying AI agents. It applies software development principles to AI agent creation and makes the orchestration of agent workflows easier. It is model and deployment-agnostic, and has a tool ecosystem with modular multi-agent systems.

</details>


### [62] [Learning to Model the World with Language](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdynalang.github.io%2F%3Futm_source=tldrwebdev/1/0100019a77f9393c-c30e7cd5-8ec0-4cf8-9832-a3fc91bed46e-000000/HFvEaxmdo9f4-a_JSs32OEDxwA1i4-gc49DZ0P_YCDs=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dynalang是一个通过预测未来文本、图像和奖励来学习理解和运用多种语言类型的智能体


<details>
  <summary>Details</summary>
Motivation: 构建能够理解和运用多种语言类型的智能体，使其能够在多模态环境中进行有效交互

Method: 使用多模态世界模型，通过预测未来的文本、图像和奖励来训练智能体

Result: 开发出了Dynalang智能体，能够学习理解和运用多种语言类型

Conclusion: 通过预测未来多模态信息的方法可以有效训练智能体理解和运用语言

Abstract: Learning to Model the World with Language (7 minute read) Dynalang is an agent that learns to understand and uses multiple types of language by predicting future text, images, and rewards within a multimodal world model.

</details>
