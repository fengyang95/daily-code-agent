<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [tldr.article](#tldr.article) [Total: 10]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 11]
- [wechat.article](#wechat.article) [Total: 12]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: è®ºæ–‡ç ”ç©¶äº†LLMåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„"è¿·å¤±åœ¨ä¸­é—´"ç°è±¡ï¼Œæå‡ºäº†GM-ExtractåŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°æ§åˆ¶å˜é‡æ£€ç´¢æ€§èƒ½ï¼Œå¹¶å¼€å‘äº†ç©ºé—´å’Œè¯­ä¹‰æ£€ç´¢ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶å‘ç°æ•°æ®è¡¨ç¤ºæ–¹å¼æ˜¾è‘—å½±å“æ£€ç´¢æ€§èƒ½ï¼Œç¼“è§£æ–¹æ³•çš„æœ‰æ•ˆæ€§å…·æœ‰é«˜åº¦æƒ…å¢ƒä¾èµ–æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿èŒƒå›´ä¸Šä¸‹æ–‡ä¸­çš„"è¿·å¤±åœ¨ä¸­é—´"ç°è±¡å¯¹åŸºäºæ£€ç´¢çš„LLMåº”ç”¨æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å®é™…åº”ç”¨ç¯å¢ƒä¸­ç ”ç©¶è¿™ä¸€ç°è±¡çš„å½±å“ã€‚

Method: æå‡ºäº†GM-ExtractåŸºå‡†æ•°æ®é›†ï¼Œä½¿ç”¨æ–‡æ¡£æŒ‡æ ‡å’Œå˜é‡æå–æŒ‡æ ‡ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œç³»ç»Ÿè¯„ä¼°äº†7-8Bå‚æ•°æ¨¡å‹åœ¨å¤šæ–‡æ¡£ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶è¿›è¡Œäº†ç¼“è§£æ–¹æ³•çš„æ–‡çŒ®è°ƒç ”å’Œåº”ç”¨æµ‹è¯•ã€‚

Result: ç ”ç©¶å‘ç°ä»…é€šè¿‡æ”¹å˜æ•°æ®åœ¨ä¸Šä¸‹æ–‡çª—å£ä¸­çš„è¡¨ç¤ºæ–¹å¼å°±èƒ½æ˜¾è‘—æ”¹å˜æ£€ç´¢æ€§èƒ½ï¼Œè™½ç„¶æœªå§‹ç»ˆè§‚å¯Ÿåˆ°æ˜æ˜¾çš„Uå½¢æ›²çº¿ï¼Œä½†å‘ç°äº†æ¸…æ™°çš„æ€§èƒ½æ¨¡å¼ã€‚ç¼“è§£æ–¹æ³•çš„æœ‰æ•ˆæ€§é«˜åº¦å¾®å¦™ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹èƒ½æˆåŠŸæå‡æ€§èƒ½ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ã€‚

Conclusion: åœ¨é•¿ä¸Šä¸‹æ–‡æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œæ•°æ®è¡¨ç¤ºæ–¹å¼å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œç¼“è§£ç­–ç•¥çš„æ•ˆç”¨éœ€è¦æ ¹æ®å…·ä½“æƒ…å¢ƒä»”ç»†è¯„ä¼°ã€‚

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [2] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: æå‡ºä¸€ç§ç†µå¼•å¯¼è®­ç»ƒæ¡†æ¶æ¥è§£å†³æ¨ç†æ¨¡å‹å‹ç¼©ä¸­çš„ç†µå†²çªé—®é¢˜ï¼Œå°†æ¨ç†é•¿åº¦å‹ç¼©åˆ°åŸå§‹çš„20%åŒæ—¶ä¿æŒæˆ–æå‡å‡†ç¡®ç‡


<details>
  <summary>Details</summary>
Motivation: å¤§å‹æ¨ç†æ¨¡å‹çš„æ€ç»´é“¾è¾“å‡ºè¿‡é•¿å¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œéƒ¨ç½²å›°éš¾ï¼Œç°æœ‰å‹ç¼©æ–¹æ³•å¿½è§†äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç†µå†²çªç°è±¡

Method: é‡‡ç”¨ç†µå¼•å¯¼è®­ç»ƒæ¡†æ¶ï¼Œå½“ç†µä¸‹é™æ—¶å¼•å¯¼æ¨¡å‹è¿›è¡Œç®€æ´æ¨ç†ï¼Œå½“ç†µä¸Šå‡æ—¶åœ¨ç´§å‡‘æ¨ç†æ¨¡å¼ä¸‹å¼ºåŒ–æ¢ç´¢

Result: åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ–¹æ³•å°†æ¨ç†é•¿åº¦å‹ç¼©åˆ°åŸå§‹çš„20%ï¼ŒåŒæ—¶ä¿æŒç”šè‡³è¶…è¿‡åŸºçº¿å‡†ç¡®ç‡

Conclusion: ç†µå¼•å¯¼è®­ç»ƒèƒ½æœ‰æ•ˆè§£å†³æ¨ç†å‹ç¼©ä¸­çš„ç†µå†²çªé—®é¢˜ï¼Œå®ç°é«˜æ•ˆä¸”é²æ£’çš„æ¨ç†

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [3] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹è¯­è¨€åŒ–æ¦‚ç‡åˆ†å¸ƒæ¥å¢å¼ºç½®ä¿¡åº¦ä¼°è®¡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¼“åŠ±LLMæ·±å…¥æ¨ç†ï¼Œè€ƒè™‘æ‰€æœ‰å€™é€‰ç­”æ¡ˆè€Œéå•ä¸€çŒœæµ‹ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨ç”Ÿæˆè¯­è¨€åŒ–ç½®ä¿¡åº¦ï¼Œä½†æ¨ç†ç­–ç•¥å¦‚ä½•å½±å“ç½®ä¿¡åº¦ä¼°è®¡ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚éœ€è¦ä¸€ç§èƒ½ä¿ƒè¿›æ·±åº¦æ¨ç†çš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ã€‚

Method: é€šè¿‡é¢„æµ‹è¯­è¨€åŒ–æ¦‚ç‡åˆ†å¸ƒæ¥é¼“åŠ±æ·±å…¥æ¨ç†ï¼Œè¦æ±‚LLMè€ƒè™‘ç­”æ¡ˆç©ºé—´ä¸­çš„æ‰€æœ‰å€™é€‰ç­”æ¡ˆï¼Œå¹¶ä»”ç»†åˆ†é…ç½®ä¿¡åº¦åˆ†æ•°ä»¥æ»¡è¶³åˆ†å¸ƒè¦æ±‚ã€‚

Result: è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ— è®ºç­”æ¡ˆç©ºé—´æ˜¯å¦å·²çŸ¥ã€‚å³ä½¿åœ¨å¼ºåŒ–å­¦ä¹ åä»ä¿æŒä¼˜åŠ¿ï¼Œå…¶æ¨ç†æ¨¡å¼ä¸äººç±»æœŸæœ›ä¸€è‡´ã€‚

Conclusion: é¢„æµ‹è¯­è¨€åŒ–æ¦‚ç‡åˆ†å¸ƒæ˜¯æœ‰æ•ˆçš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ï¼Œèƒ½ä¿ƒè¿›æ·±åº¦æ¨ç†å¹¶äº§ç”Ÿä¸äººç±»æœŸæœ›ä¸€è‡´çš„æ¨ç†æ¨¡å¼ã€‚

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [4] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4æ˜¯ä¸€ä¸ªå…¨å›½æ€§çš„åŒ»ç–—AIåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«70å¤šä¸‡ä¸ªä¸“å®¶ç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–24ä¸ªä¸»è¦å’Œ91ä¸ªæ¬¡è¦ä¸“ä¸šï¼Œè¯„ä¼°äº†15ä¸ªå‰æ²¿æ¨¡å‹ã€‚åŸºç¡€LLMå¹³å‡å¾—åˆ†54.1/100ï¼Œå®‰å…¨å’Œä¼¦ç†è¡¨ç°è¾ƒå·®ï¼›å¤šæ¨¡æ€æ¨¡å‹è¡¨ç°æ›´å·®ï¼›åŸºäºç›¸åŒéª¨å¹²çš„æ™ºèƒ½ä½“æ˜¾è‘—æå‡æ€§èƒ½è‡³79.8/100ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä½“çš„å‘å±•ï¼Œéœ€è¦èƒ½å¤Ÿåæ˜ çœŸå®ä¸´åºŠå·¥ä½œæµç¨‹å’Œå®‰å…¨çº¦æŸçš„è¯„ä¼°æ¡†æ¶ã€‚

Method: æ„å»ºåŒ…å«70å¤šä¸‡ä¸ªä¸“å®¶ç­–åˆ’ä»»åŠ¡çš„äº‘åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½ï¼Œä»»åŠ¡ç»è¿‡å¤šé˜¶æ®µç²¾ç‚¼å’Œå¤šè½®ä¸´åºŠåŒ»ç”Ÿè¯„å®¡ï¼Œå¼€æ”¾å¼å›ç­”ç”±æ ¡å‡†åˆ°äººç±»è¯„åˆ†çš„LLM-as-a-judgeè¯„åˆ†ã€‚

Result: åŸºç¡€LLMå¹³å‡å¾—åˆ†54.1/100ï¼ˆæœ€ä½³ï¼šClaude Sonnet 4.5ï¼Œ62.5/100ï¼‰ï¼Œå®‰å…¨å’Œä¼¦ç†å¾—åˆ†è¾ƒä½ï¼ˆ18.4/100ï¼‰ï¼›å¤šæ¨¡æ€æ¨¡å‹è¡¨ç°æ›´å·®ï¼ˆå¹³å‡47.5/100ï¼‰ï¼›æ™ºèƒ½ä½“æ˜¾è‘—æå‡æ€§èƒ½ï¼ˆå¹³å‡79.8/100ï¼‰ã€‚

Conclusion: MedBench v4æ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†å’Œå®‰å…¨æ€§æ–¹é¢çš„æŒç»­å·®è·ï¼ŒåŒæ—¶è¡¨æ˜å…·æœ‰æ²»ç†æ„è¯†çš„æ™ºèƒ½ä½“ç¼–æ’å¯ä»¥æ˜¾è‘—æå‡ä¸´åºŠå‡†å¤‡åº¦è€Œä¸ç‰ºç‰²èƒ½åŠ›ã€‚

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [5] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹LLMæ™ºèƒ½ä½“çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶Agent-R1ï¼Œé€šè¿‡æ‰©å±•MDPæ¡†æ¶æ¥å®šä¹‰LLMæ™ºèƒ½ä½“çš„å…³é”®ç»„ä»¶ï¼Œå¹¶åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMæ™ºèƒ½ä½“åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œç¼ºä¹ä¸“é—¨é’ˆå¯¹LLMæ™ºèƒ½ä½“èƒŒæ™¯çš„RLæ–¹æ³•æ¢ç´¢å’Œçµæ´»å¯æ‰©å±•çš„è®­ç»ƒæ¡†æ¶ã€‚

Method: ç³»ç»Ÿæ€§åœ°æ‰©å±•é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¡†æ¶æ¥å®šä¹‰LLMæ™ºèƒ½ä½“çš„å…³é”®ç»„ä»¶ï¼Œå¹¶å¼€å‘äº†æ¨¡å—åŒ–ã€çµæ´»ä¸”ç”¨æˆ·å‹å¥½çš„è®­ç»ƒæ¡†æ¶Agent-R1ã€‚

Result: åœ¨å¤šè·³é—®ç­”åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒåˆæ­¥éªŒè¯äº†æ‰€ææ–¹æ³•å’Œæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºLLMæ™ºèƒ½ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†ç³»ç»Ÿçš„æ–¹æ³•è®ºå’Œå®ç”¨æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ã€‚

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [6] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½å¤Ÿé€šè¿‡å°†å›¾è¡¨ä½œä¸ºå¯éªŒè¯æ£€æŸ¥ç‚¹ï¼Œå®æ—¶è¯„ä¼°å’Œçº æ­£æ•°æ®åˆ†æé”™è¯¯ï¼Œä»è€Œæå‡ç«¯åˆ°ç«¯è‡ªä¸»ç§‘å­¦å‘ç°èƒ½åŠ›ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿçš„æ•°æ®é©±åŠ¨ç§‘å­¦å‘ç°æ–¹æ³•ç¼ºä¹å®æ—¶é”™è¯¯æ£€æµ‹å’Œçº æ­£æœºåˆ¶ï¼Œå¯¼è‡´æ¨ç†è·¯å¾„é”™è¯¯éš¾ä»¥è¢«å‘ç°å’Œä¿®å¤ã€‚

Method: ä½¿ç”¨VLMä½œä¸ºè¯„åˆ¤è€…ï¼Œæ ¹æ®åŠ¨æ€ç”Ÿæˆçš„é¢†åŸŸç‰¹å®šè¯„åˆ†æ ‡å‡†è¯„ä¼°å›¾è¡¨ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå®æ—¶çº æ­£è‡ªèº«é”™è¯¯å¹¶æŒ‡å¯¼æ¢ç´¢æ€§æ•°æ®åˆ†æã€‚

Result: åœ¨å®‡å®™å­¦å’Œå¤©ä½“åŒ–å­¦æ¡ˆä¾‹ä¸­ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»é”™è¯¯æ¨ç†è·¯å¾„ä¸­æ¢å¤å¹¶é€‚åº”æ–°æ•°æ®é›†ã€‚åœ¨10ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLMå¢å¼ºç³»ç»Ÿè¾¾åˆ°0.7-0.8çš„pass@1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºä»£ç åŸºå‡†ï¼ˆ0.2-0.3ï¼‰å’Œä»£ç åŠ æ–‡æœ¬åŸºå‡†ï¼ˆ0.4-0.5ï¼‰ã€‚

Conclusion: VLMå¼•å¯¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ˜¾è‘—æå‡äº†è‡ªä¸»ç§‘å­¦å‘ç°çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼ŒåŒæ—¶æä¾›äº†å¯å®¡è®¡çš„æ¨ç†è½¨è¿¹ä»¥æé«˜å¯è§£é‡Šæ€§ã€‚

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [7] [SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://arxiv.org/abs/2511.14684)
*Biaojie Zeng,Min Zhang,Juan Zhou,Fengrui Liu,Ruiyang Huang,Xin Lin*

Main category: cs.CL

TL;DR: æå‡ºSMRCæ–¹æ³•ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œè¿‡ç¨‹ç›‘ç£æ¥è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£å­¦ç”Ÿæ•°å­¦æ¨ç†é”™è¯¯ï¼Œåœ¨æ•™è‚²åœºæ™¯ä¸­å®ç°æ•™å¸ˆå¼æŒ‡å¯¼ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹è‡ªæˆ‘çº é”™ï¼Œæ— æ³•æ»¡è¶³æ•™è‚²åœºæ™¯ä¸­éœ€è¦ç³»ç»ŸæŒ‡å¯¼å­¦ç”Ÿè§£é¢˜è¿‡ç¨‹çš„"æ•™å¸ˆå¼"çº é”™éœ€æ±‚ã€‚

Method: å°†å­¦ç”Ÿæ¨ç†å»ºæ¨¡ä¸ºå¤šæ­¥åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¼•å…¥è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¢ç´¢æœ€ä¼˜çº é”™è·¯å¾„ï¼Œåˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢å’Œæœ€ç»ˆç­”æ¡ˆè¯„ä¼°ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡åå‘ä¼ æ’­æœºåˆ¶å®ç°ç»†ç²’åº¦è¿‡ç¨‹ç›‘ç£ã€‚

Result: åœ¨ProcessBenchã€MR-GSM8Kå’Œè‡ªå»ºMSEBæ•°æ®é›†ä¸Šï¼ŒSMRCåœ¨æ•ˆæœå’Œæ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

Conclusion: SMRCæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆçº æ­£å­¦ç”Ÿæ•°å­¦æ¨ç†é”™è¯¯ï¼Œåœ¨æ•™è‚²åº”ç”¨åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚

Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [8] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a922ec46c-59b352f8-f0ca-465d-b3c9-9cf5c6980258-000000/l94jDOvIZvUcEm0CepsT45t8R8SJ4FrWnAgMnE_rUIM=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google Mapsæ¨å‡ºAIå·¥å…·ï¼ŒåŒ…æ‹¬æ„å»ºå™¨ä»£ç†å’ŒMCPæœåŠ¡å™¨ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿä½¿ç”¨åœ°å›¾æ•°æ®åˆ›å»ºäº¤äº’å¼é¡¹ç›®ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ºäº†å¢å¼ºGoogle Mapså¹³å°çš„å¼€å‘èƒ½åŠ›ï¼Œè®©å¼€å‘è€…æ›´å®¹æ˜“åˆ©ç”¨åœ°å›¾æ•°æ®åˆ›å»ºäº¤äº’å¼åº”ç”¨ã€‚

Method: å¼•å…¥AIå·¥å…·ï¼ŒåŒ…æ‹¬æ„å»ºå™¨ä»£ç†å’ŒMCPæœåŠ¡å™¨ï¼Œæä¾›å¼€å‘è€…å‹å¥½çš„æ¥å£æ¥è®¿é—®å’Œæ“ä½œåœ°å›¾æ•°æ®ã€‚

Result: æˆåŠŸå‘å¸ƒäº†æ–°çš„AIå·¥å…·ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåˆ›å»ºåŸºäºåœ°å›¾æ•°æ®çš„äº¤äº’å¼é¡¹ç›®ã€‚

Conclusion: è¿™äº›AIå·¥å…·æ˜¾è‘—æå‡äº†Google Mapså¹³å°çš„å¼€å‘ä½“éªŒå’ŒåŠŸèƒ½æ‰©å±•æ€§ã€‚

Abstract: Google Maps releases new AI tools that let you create interactive projects (2 minute read) Google Maps introduced AI tools, including a builder agent and an MCP server, enabling developers to create interactive projects using Maps data.

</details>


### [9] [WTF isâ€¦ - AI-Native SAST](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparsiya.net%2Fblog%2Fwtf-is-ai-native-sast%2F%3Futm_source=tldrinfosec/1/0100019a92371663-c0ba5cf3-c27d-4490-b944-4de78afc3475-000000/LIOJoi8R_Ef5YA_VvgyV0paVqt97ghKHvIOkno9BrHU=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†AIä½œä¸ºä¼ ç»ŸSASTï¼ˆé™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼‰çš„è¡¥å……ï¼Œé€šè¿‡RAGæŠ€æœ¯æ³¨å…¥ç‰¹å®šæ¼æ´ç±»åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æ˜¾è‘—æ”¹å–„æ£€æµ‹ç»“æœã€‚ä»‹ç»äº†ä¸‰ç§AIä¸SASTç»“åˆçš„æ–¹æ³•ï¼šæç¤º-ä»£ç ã€æç¤º-ä»£ç†å’Œå®šåˆ¶æç¤º-SASTç»“æœæ–¹æ³•ï¼Œå¹¶ä»¥ZeroPathä¸ºä¾‹è¯´æ˜äº†ä»£ç†ã€ä»£ç å›¾å’ŒSAST MCPçš„åº”ç”¨ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸSASTå·¥å…·åœ¨æ£€æµ‹å¤æ‚æ¼æ´æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆæ¥æå‡å®‰å…¨æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

Method: ä½¿ç”¨RAGæŠ€æœ¯ä¸ºç‰¹å®šæ¼æ´ç±»åˆ«æ³¨å…¥ä¸Šä¸‹æ–‡ï¼Œé‡‡ç”¨ä¸‰ç§ç»“åˆæ–¹å¼ï¼šæç¤º-ä»£ç ã€æç¤º-ä»£ç†å’Œå®šåˆ¶æç¤º-SASTç»“æœæ–¹æ³•ã€‚

Result: AIä½œä¸ºSASTçš„è¡¥å……å¯ä»¥æ˜¾è‘—æ”¹å–„æ£€æµ‹ç»“æœï¼ŒZeroPathå±•ç¤ºäº†ä»£ç†ã€ä»£ç å›¾å’ŒSAST MCPçš„å®é™…åº”ç”¨æ•ˆæœã€‚

Conclusion: AIåŸç”ŸSASTæ˜¯å®‰å…¨æ£€æµ‹çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œé€šè¿‡æ™ºèƒ½åŒ–çš„ä¸Šä¸‹æ–‡ç†è§£å’Œåˆ†æèƒ½å¤Ÿå¼¥è¡¥ä¼ ç»Ÿå·¥å…·çš„ä¸è¶³ã€‚

Abstract: WTF isâ€¦ - AI-Native SAST (17 minute read) Leveraging AI as a complement to a traditional SAST can significantly improve results by using RAG to inject context on specific vulnerability classes. AI and SAST solutions can be leveraged using prompt-and-code, prompt-and-agent, or tailored-prompt-and-SAST-result methods. This post examines ZeroPath as an example of an agent, code graph, and SAST MCP.

</details>


### [10] [How Dash uses context engineering for smarter AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/03WKnJXUNZ_HZdVgclNBnARfw5obgrJBVd9C2mXSjJs=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dropboxå°†Dash AIä»ä¼ ç»Ÿæœç´¢å·¥å…·æ¼”å˜ä¸ºèƒ½å¤Ÿè§£é‡Šã€æ€»ç»“å’Œæ“ä½œä¿¡æ¯çš„"ä»£ç†AI"ï¼Œé€šè¿‡é™åˆ¶å·¥å…·å®šä¹‰ã€è¿‡æ»¤ä¸Šä¸‹æ–‡å’Œä¼˜åŒ–æç¤ºå·¥ç¨‹æ¥é¿å…åˆ†æç˜«ç—ªå’Œä¸Šä¸‹æ–‡é€€åŒ–é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸAIç³»ç»Ÿåœ¨æ·»åŠ æ›´å¤šå·¥å…·å’Œä¸Šä¸‹æ–‡æ—¶ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ï¼Œäº§ç”Ÿåˆ†æç˜«ç—ªå’Œ"ä¸Šä¸‹æ–‡é€€åŒ–"é—®é¢˜ï¼Œéœ€è¦å¼€å‘æ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥ã€‚

Method: å¼€å‘äº†ä¸‰ç§å…³é”®ç­–ç•¥ï¼š1) é€šè¿‡å°†å¤šä¸ªæ£€ç´¢é€‰é¡¹æ•´åˆä¸ºå•ä¸€é€šç”¨æœç´¢å·¥å…·æ¥é™åˆ¶å·¥å…·å®šä¹‰ï¼›2) è¿‡æ»¤ä¸Šä¸‹æ–‡ï¼›3) ä¼˜åŒ–æç¤ºå·¥ç¨‹ã€‚

Result: æˆåŠŸå°†Dash AIä»ä¼ ç»Ÿæœç´¢å·¥å…·è½¬å˜ä¸ºèƒ½å¤Ÿè§£é‡Šã€æ€»ç»“å’Œæ“ä½œä¿¡æ¯çš„ä»£ç†AIç³»ç»Ÿã€‚

Conclusion: é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡AIä»£ç†çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œé¿å…å·¥å…·è¿‡è½½å¸¦æ¥çš„è´Ÿé¢æ•ˆåº”ã€‚

Abstract: How Dash uses context engineering for smarter AI (8 minute read) Dropbox evolved its Dash AI system from a traditional search tool into an "agentic AI" that can interpret, summarize, and act on information. Its team discovered that adding more tools and context often led to worse performance due to analysis paralysis and "context rot," so they developed three key strategies: limiting tool definitions by consolidating multiple retrieval options into a single universal search tool, filtering co...

</details>


### [11] [Continuous Claude](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAnandChowdhary%2Fcontinuous-claude%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/IsyrRd2M96J0n_JxqZRXtK87jPfInkkRMiOCFMcBd-E=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Continuous Claudeæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–PRç”Ÿå‘½å‘¨æœŸçš„è„šæœ¬ï¼Œä½¿ç”¨Claude Codeè‡ªåŠ¨æäº¤å˜æ›´ã€åˆ›å»ºPRã€ç­‰å¾…æ£€æŸ¥å’Œå®¡æ ¸ï¼Œç„¶ååˆå¹¶PRï¼Œå¾ªç¯æ‰§è¡Œç›´åˆ°ä»»åŠ¡å®Œæˆ


<details>
  <summary>Details</summary>
Motivation: è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ä¸­çš„PRæµç¨‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜å¼€å‘æ•ˆç‡

Method: ä½¿ç”¨è„šæœ¬è‡ªåŠ¨åŒ–æ•´ä¸ªPRç”Ÿå‘½å‘¨æœŸï¼šæäº¤å˜æ›´åˆ°æ–°åˆ†æ”¯ã€åˆ›å»ºPRã€ç­‰å¾…æ£€æŸ¥å’Œå®¡æ ¸ã€åˆå¹¶PR

Result: å®ç°äº†PRæµç¨‹çš„å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤ŸæŒç»­æ‰§è¡Œç›´åˆ°ä»»åŠ¡å®Œæˆ

Conclusion: Continuous Claudeæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–PRç®¡ç†è§£å†³æ–¹æ¡ˆ

Abstract: Continuous Claude (GitHub Repo) Continuous Claude is a script that automates the pull request lifecycle using Claude Code. It commits changes to a new branch, creates a pull request, waits for checks and reviews, and merges the PR, repeating until the task is complete.

</details>


### [12] [ServiceNow and Figma Launch Strategic Collaboration to Turn Design Vision Into Enterprise Transformation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fservicenow-mcp-integration%2F%3Futm_source=tldrdesign/1/0100019a97ab2d91-54685b40-860f-48e2-8495-506ff187715a-000000/3kvUuMhjfF-xt5XmTQDbfNCiIK7DbAHBu2m2J2ys8jU=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ServiceNowä¸Figmaåˆä½œï¼Œåˆ©ç”¨MCPæŠ€æœ¯å°†è®¾è®¡ç›´æ¥è½¬åŒ–ä¸ºä¼ä¸šçº§åº”ç”¨ï¼Œå‡å°‘80%çš„UIå’Œæ•°æ®æ¨¡å‹å®ç°æ—¶é—´


<details>
  <summary>Details</summary>
Motivation: è§£å†³ä»è®¾è®¡åˆ°ä¼ä¸šåº”ç”¨éƒ¨ç½²çš„è‡ªåŠ¨åŒ–è½¬æ¢é—®é¢˜ï¼Œæé«˜å¼€å‘æ•ˆç‡

Method: ä½¿ç”¨MCPæŠ€æœ¯å’ŒServiceNowçš„Build Agentï¼Œå®ç°Figmaè®¾è®¡åˆ°ä¼ä¸šçº§åº”ç”¨çš„è‡ªåŠ¨åŒ–è½¬æ¢

Result: UIå’Œæ•°æ®æ¨¡å‹å®ç°æ—¶é—´å‡å°‘80%ä»¥ä¸Šï¼Œå…·å¤‡ä¼ä¸šçº§å®‰å…¨æ€§

Conclusion: è¯¥åˆä½œæ˜¾è‘—æå‡äº†ä»è®¾è®¡åˆ°éƒ¨ç½²çš„è‡ªåŠ¨åŒ–æ°´å¹³ï¼Œä¸ºä¼ä¸šåº”ç”¨å¼€å‘å¸¦æ¥æ•ˆç‡çªç ´

Abstract: ServiceNow and Figma Launch Strategic Collaboration to Turn Design Vision Into Enterprise Transformation (3 minute read) ServiceNow and Figma have launched a strategic collaboration utilizing MCP technology, enabling developers to transform Figma designs directly into enterprise-ready applications through ServiceNow's Build Agent. The integration automates the path from design to deployment with enterprise-grade security, reducing initial UI and data model implementation time by over 80%. Use...

</details>


### [13] [3M+ users are now](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lovart.ai%2F%3FsourceId=900187/2/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/jlDjdTUodP2qNE9anPjZb1YhzCB0O5UbYp4WbZ8YVeg=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Lovartæ˜¯ä¸€å®¶è®¾è®¡AIå…¬å¸ï¼Œå¹´æ”¶å…¥è¾¾3000ä¸‡ç¾å…ƒï¼Œæä¾›ç±»ä¼¼Figmaçš„AIè®¾è®¡ç¼–è¾‘åŠŸèƒ½ï¼Œæ‹¥æœ‰300ä¸‡ç”¨æˆ·ï¼Œå¯å°†æç¤ºè¯è½¬æ¢ä¸ºå“ç‰Œå°±ç»ªçš„è§†è§‰å†…å®¹ã€è§†é¢‘å’Œæ¼”ç¤ºæ–‡ç¨¿ã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³ä¼ ç»ŸAIè®¾è®¡å·¥å…·ç¼ºä¹ç²¾ç¡®ç¼–è¾‘æ§åˆ¶çš„é—®é¢˜ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿåœ¨ä¸é‡æ–°ç”Ÿæˆçš„æƒ…å†µä¸‹ç²¾ç¡®ä¿®æ”¹è®¾è®¡å…ƒç´ ã€‚

Method: å¼€å‘äº†AIé©±åŠ¨çš„è®¾è®¡ä»£ç†ï¼Œå…·æœ‰å…ƒç´ ç¼–è¾‘åŠŸèƒ½ï¼ŒåŒ…æ‹¬å®æ—¶å¯ç¼–è¾‘æ–‡æœ¬(LET)å’Œå›¾å±‚åˆ†ç¦»æŠ€æœ¯ã€‚

Result: å–å¾—äº†å•†ä¸šæˆåŠŸï¼Œå¹´æ”¶å…¥3000ä¸‡ç¾å…ƒï¼Œæ‹¥æœ‰300ä¸‡ç”¨æˆ·ï¼Œæˆä¸ºMidjourneyä¹‹åæœ€é‡è¦çš„è®¾è®¡AIçªç ´ã€‚

Conclusion: Lovartçš„è®¾è®¡ä»£ç†åœ¨AIè®¾è®¡é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œé€šè¿‡æä¾›ç²¾ç¡®çš„ç¼–è¾‘æ§åˆ¶åŠŸèƒ½æ»¡è¶³äº†ç”¨æˆ·éœ€æ±‚ã€‚

Abstract: Lovart hits $30M ARR with Figma-like editing for AI designs (Sponsor) Lovart's design agent is the biggest revelation since Midjourney, and it's absolutely exploding: 3M+ users are now turning prompts into brand-ready visuals, videos, and decks. New features drops include: ğŸ§‘â€ğŸ’» Edit Elements is Lovart's new generation of AI-powered design precision. Users have full control over their creations without regenerating from scratch - including Live Editable Text (LET) and Layer Separation - isolate...

</details>


### [14] [RL is even more information inefficient than you thought](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dwarkesh.com%2Fp%2Fbits-per-sample%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/e8QNxDgTlzngBugtNOkYibPqH4fweeQc8EZf2DkIF3o=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: å¼ºåŒ–å­¦ä¹ æ¯”ç›‘ç£å­¦ä¹ çš„ä¿¡æ¯æ•ˆç‡æ›´ä½ï¼Œéœ€è¦æ›´å¤šçš„FLOPsæ¥è·å–å•ä¸ªæ ·æœ¬ï¼Œå› ä¸ºRLéœ€è¦å±•å¼€æ•´ä¸ªæ€è€ƒè½¨è¿¹ï¼ˆæ•°ä¸‡ä¸ªtokenï¼‰æ‰èƒ½è·å¾—æœ€ç»ˆçš„ä¸€ä¸ªå¥–åŠ±ä¿¡å·ï¼Œè€Œé¢„è®­ç»ƒä¸­æ¯ä¸ªtokenéƒ½èƒ½è·å¾—ä¿¡å·ã€‚


<details>
  <summary>Details</summary>
Motivation: æ­ç¤ºå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ åœ¨ä¿¡æ¯æ•ˆç‡ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œè¯´æ˜RLè®­ç»ƒè¿‡ç¨‹ä¸­ä¿¡æ¯å¯†åº¦è¿œä½äºç›‘ç£å­¦ä¹ ã€‚

Method: é€šè¿‡æ¯”è¾ƒRLå’Œç›‘ç£å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ï¼Œåˆ†æRLéœ€è¦å±•å¼€å®Œæ•´æ€è€ƒè½¨è¿¹æ‰èƒ½è·å¾—å¥–åŠ±ä¿¡å·çš„ç‰¹ç‚¹ã€‚

Result: RLçš„ä¿¡æ¯å¯†åº¦åœ¨å¤§éƒ¨åˆ†è®­ç»ƒæ—¶é—´å†…è¿œä½äºç›‘ç£å­¦ä¹ ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æºæ¥è·å–æœ‰æ•ˆè®­ç»ƒä¿¡å·ã€‚

Conclusion: å¼ºåŒ–å­¦ä¹ åœ¨ä¿¡æ¯æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸¥é‡ä¸è¶³ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆRLè®­ç»ƒé€šå¸¸éœ€è¦æ¯”ç›‘ç£å­¦ä¹ æ›´å¤šçš„è®¡ç®—èµ„æºã€‚

Abstract: RL is even more information inefficient than you thought (12 minute read) It takes way more FLOPs to get a single sample in RL than it does in supervised learning. RL requires unrolling a whole thinking trajectory tens of thousands of tokens long to get a single reward signal at the end. In pretraining, you get a signal on every single token you train on. For most of training, the information density per sample is way lower for RL compared to supervised learning.

</details>


### [15] [The Agent Labs Thesis](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.latent.space%2Fp%2Fagent-labs%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/OpF9lkh6d2u0dPGF4s-tnaYbkbJ4SsS181mCPIZ7krw=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Labsä¸“æ³¨äºç ”ç©¶å’Œé”€å”®æ™ºèƒ½ä½“ï¼Œé‡‡ç”¨äº§å“ä¼˜å…ˆå’ŒåŸºäºç»“æœçš„å®šä»·ç­–ç•¥ï¼Œä¸Model Labsçš„æ¨¡å‹ä¼˜å…ˆå’ŒæŒ‰tokenå®šä»·å½¢æˆå¯¹æ¯”ã€‚


<details>
  <summary>Details</summary>
Motivation: æ¢è®¨ä¸åŒç±»å‹AIå®éªŒå®¤ï¼ˆAgent Labs vs Model Labsï¼‰åœ¨å•†ä¸šæ¨¡å¼ã€å®šä»·ç­–ç•¥å’Œä¼°å€¼å‰æ™¯æ–¹é¢çš„å·®å¼‚ã€‚

Method: é€šè¿‡å¯¹æ¯”åˆ†æAgent Labså’ŒModel Labsçš„æ ¸å¿ƒä¸šåŠ¡æ¨¡å¼ã€å®šä»·æœºåˆ¶å’Œç°é‡‘æµç‰¹å¾ã€‚

Result: Agent Labså…·æœ‰æ›´å¥½çš„ç°é‡‘æµç»æµæ€§ï¼Œä½†é€€å‡ºä¼°å€¼å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼›Model Labsçš„ä½¿å‘½å¯èƒ½åœ¨è½¬å˜ã€‚

Conclusion: ä¸¤ç§å®éªŒå®¤æ¨¡å¼å„æœ‰ä¼˜åŠ£ï¼ŒAgent Labsæ³¨é‡äº§å“åŒ–å’Œç»“æœå¯¼å‘ï¼ŒModel Labså¯èƒ½é¢ä¸´ä¸šåŠ¡æ¨¡å¼è°ƒæ•´ã€‚

Abstract: The Agent Labs Thesis (12 minute read) Agent Labs primarily research and sell agents. They put product first and use outcome-based pricing, as opposed to Model Labs, which put models first and price per token. Agent Labs have better cashflow economics, but it might take longer to see exit valuations. The Model Lab mission may be shifting, at least until the next big algorithm shift.

</details>


### [16] [Build a GPT-5.1 Coding Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcookbook.openai.com%2Fexamples%2Fbuild_a_coding_agent_with_gpt-5.1%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/_sxWmBjBf6xXaVoKZUCLAWmTPyvuEt02rsGqEI_2Wsw=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ä½¿ç”¨GPT-5.1å’ŒAgents SDKæ„å»ºç¼–ç ä»£ç†çš„æŒ‡å—ï¼Œæ”¯æŒshellæ‰§è¡Œã€è¡¥ä¸ç¼–è¾‘ã€ç½‘ç»œæœç´¢å’ŒContext7 MCPå®æ—¶æ–‡æ¡£è®¿é—®


<details>
  <summary>Details</summary>
Motivation: ä¸ºäº†åˆ›å»ºåŠŸèƒ½å¼ºå¤§çš„ç¼–ç ä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç¼–ç¨‹ä»»åŠ¡å¹¶è®¿é—®å®æ—¶æ–‡æ¡£èµ„æº

Method: åˆ©ç”¨GPT-5.1æ¨¡å‹å’ŒAgents SDKï¼Œé›†æˆå¤šç§å·¥å…·åŒ…æ‹¬shellæ‰§è¡Œã€è¡¥ä¸ç¼–è¾‘ã€ç½‘ç»œæœç´¢å’ŒContext7 MCP

Result: æˆåŠŸæ„å»ºäº†ä¸€ä¸ªå…·å¤‡å¤šç§ç¼–ç¨‹è¾…åŠ©åŠŸèƒ½çš„ç¼–ç ä»£ç†

Conclusion: GPT-5.1ç»“åˆAgents SDKèƒ½å¤Ÿæœ‰æ•ˆæ„å»ºåŠŸèƒ½ä¸°å¯Œçš„ç¼–ç ä»£ç†

Abstract: Build a GPT-5.1 Coding Agent (19 minute read) This guide walks through building a coding agent using GPT-5.1 and the Agents SDK, leveraging tools like shell execution, patch editing, web search, and Context7 MCP for live documentation access.

</details>


### [17] [Google to enable research automation on Gemini Enterprise](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-to-enable-research-automation-on-gemini-enterprise%2F%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/XULDNy7gv6ytS0Y9y6YgtY7L-_JF8IqyfsQO9q-o9Kc=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: è°·æ­Œå¼€å‘äº†ä¸€ä¸ªæœªå‘å¸ƒçš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆçº¦100ä¸ªä¸»é¢˜æƒ³æ³•ï¼Œå¹¶é€šè¿‡ä»£ç†å›¢é˜Ÿåœ¨é”¦æ ‡èµ›å¼èµ›åˆ¶ä¸­ç«äº‰ä»¥äº§ç”Ÿæœ€ä½³ç»“æœã€‚


<details>
  <summary>Details</summary>
Motivation: æ—¨åœ¨å®ç°ç ”ç©¶è¿‡ç¨‹çš„è‡ªåŠ¨åŒ–ï¼Œæé«˜ç ”ç©¶æ•ˆç‡å’Œåˆ›æ–°æ€§ï¼Œé€šè¿‡å¤šä»£ç†ç«äº‰æœºåˆ¶æ¿€å‘æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚

Method: ç³»ç»Ÿé¦–å…ˆç”Ÿæˆå¤§é‡ä¸»é¢˜æƒ³æ³•ï¼Œç„¶ååˆ›å»ºå¤šä¸ªä»£ç†å›¢é˜Ÿï¼Œè¿™äº›å›¢é˜Ÿåœ¨é”¦æ ‡èµ›å¼èµ›åˆ¶ä¸­ç›¸äº’ç«äº‰ï¼Œæœ€ç»ˆé€‰å‡ºæœ€ä½³ç»“æœã€‚

Result: è¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç ”ç©¶è¿‡ç¨‹ï¼Œé€šè¿‡ç«äº‰æœºåˆ¶äº§ç”Ÿé«˜è´¨é‡çš„ç ”ç©¶æˆæœã€‚

Conclusion: è¯¥è‡ªåŠ¨åŒ–ç ”ç©¶ç³»ç»Ÿå±•ç¤ºäº†é€šè¿‡å¤šä»£ç†ç«äº‰å¯ä»¥æœ‰æ•ˆæå‡ç ”ç©¶è´¨é‡å’Œæ•ˆç‡ã€‚

Abstract: Google to enable research automation on Gemini Enterprise (3 minute read) The unreleased system generates roughly 100 ideas on a topic, then spawns agent teams that compete in a tournament-style bracket for the best result.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench-Agentæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°LLMä»£ç†åœ¨çœŸå®é•¿ä¸Šä¸‹æ–‡è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹ä¸­çš„ç»¼åˆæ¡†æ¶ï¼Œå°†LoCoBenchçš„8000ä¸ªåœºæ™¯æ‰©å±•åˆ°äº¤äº’å¼ä»£ç†ç¯å¢ƒï¼Œè¯„ä¼°å¤šè½®å¯¹è¯ã€å·¥å…·ä½¿ç”¨æ•ˆç‡ã€é”™è¯¯æ¢å¤å’Œæ¶æ„ä¸€è‡´æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰åŸºå‡†å¦‚LoCoBenchä¸»è¦è¯„ä¼°å•è½®é•¿ä¸Šä¸‹æ–‡ä»£ç ç†è§£ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œç¼–ç ä»£ç†æ‰€éœ€çš„å¤šè½®äº¤äº’ã€å·¥å…·ä½¿ç”¨æ¨¡å¼å’Œè‡ªé€‚åº”æ¨ç†èƒ½åŠ›ã€‚

Method: æ‰©å±•LoCoBenchåœºæ™¯ä¸ºäº¤äº’å¼ä»£ç†ç¯å¢ƒï¼Œæä¾›8ä¸ªä¸“ç”¨å·¥å…·ï¼ˆæ–‡ä»¶æ“ä½œã€æœç´¢ã€ä»£ç åˆ†æï¼‰ï¼Œåœ¨10Kåˆ°1Mä»¤ç‰Œçš„ä¸Šä¸‹æ–‡é•¿åº¦èŒƒå›´å†…è¯„ä¼°ï¼Œä½¿ç”¨9ä¸ªæ¶µç›–ç†è§£å’Œæ•ˆç‡ç»´åº¦çš„æŒ‡æ ‡ã€‚

Result: è¯„ä¼°å‘ç°ï¼š(1)ä»£ç†å±•ç°æ˜¾è‘—çš„é•¿ä¸Šä¸‹æ–‡é²æ£’æ€§ï¼›(2)ç†è§£ä¸æ•ˆç‡å­˜åœ¨è´Ÿç›¸å…³çš„æƒè¡¡ï¼›(3)å¯¹è¯æ•ˆç‡åœ¨ä¸åŒæ¨¡å‹é—´å·®å¼‚æ˜¾è‘—ï¼Œç­–ç•¥æ€§å·¥å…·ä½¿ç”¨æ¨¡å¼åŒºåˆ†é«˜æ€§èƒ½ä»£ç†ã€‚

Conclusion: ä½œä¸ºé¦–ä¸ªè½¯ä»¶å·¥ç¨‹é•¿ä¸Šä¸‹æ–‡LLMä»£ç†åŸºå‡†ï¼ŒLoCoBench-Agentä¸ºè¡¡é‡ä»£ç†èƒ½åŠ›ã€è¯†åˆ«æ€§èƒ½å·®è·å’Œæ¨è¿›å¤§è§„æ¨¡è‡ªä¸»è½¯ä»¶å¼€å‘å»ºç«‹äº†ä¸¥æ ¼åŸºç¡€ã€‚

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [19] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuardé€šè¿‡å°†ä»£ç è§†ä¸ºå›¾ç»“æ„å¹¶ä½¿ç”¨é€‰æ‹©æ€§å›¾æ¢ç´¢æ¥æ‰¾åˆ°æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ä¿®å¤ä¸ç¨³å®šæµ‹è¯•æ—¶çš„ä¸Šä¸‹æ–‡é—®é¢˜ï¼Œåœ¨å·¥ä¸šç¯å¢ƒä¸­ä¿®å¤äº†47.6%çš„å¯é‡ç°ä¸ç¨³å®šæµ‹è¯•ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¸ç¨³å®šæµ‹è¯•ä¼šæµªè´¹å¼€å‘è€…æ—¶é—´å¹¶å‡æ…¢å‘å¸ƒå‘¨æœŸï¼Œç°æœ‰åŸºäºLLMçš„æ–¹æ³•åœ¨å·¥ä¸šç¯å¢ƒä¸­å› ä¸Šä¸‹æ–‡é—®é¢˜ï¼ˆæä¾›å¤ªå°‘æˆ–å¤ªå¤šä¸Šä¸‹æ–‡ï¼‰è€Œå¤±è´¥ã€‚

Method: å°†ä»£ç è§†ä¸ºå›¾ç»“æ„ï¼Œä½¿ç”¨é€‰æ‹©æ€§å›¾æ¢ç´¢æ¥æ‰¾åˆ°æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…æä¾›è¿‡å¤šæˆ–è¿‡å°‘çš„ä¿¡æ¯ã€‚

Result: åœ¨å·¥ä¸šä»“åº“çš„çœŸå®ä¸ç¨³å®šæµ‹è¯•ä¸­ä¿®å¤äº†47.6%çš„å¯é‡ç°ä¸ç¨³å®šæµ‹è¯•ï¼Œ51.8%çš„ä¿®å¤è¢«å¼€å‘è€…æ¥å—ï¼Œæ¯”æœ€å…ˆè¿›æ–¹æ³•è‡³å°‘é«˜å‡º22%çš„ä¿®å¤æˆåŠŸç‡ã€‚

Conclusion: FlakyGuardé€šè¿‡é€‰æ‹©æ€§å›¾æ¢ç´¢æœ‰æ•ˆè§£å†³äº†ä¸ç¨³å®šæµ‹è¯•ä¿®å¤ä¸­çš„ä¸Šä¸‹æ–‡é—®é¢˜ï¼Œå¼€å‘è€…è°ƒæŸ¥æ˜¾ç¤º100%è®¤ä¸ºå…¶æ ¹æœ¬åŸå› è§£é‡Šæœ‰ç”¨ã€‚

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [20] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶äº†åœ¨ä»£ç åº“æŒç»­æ¼”åŒ–èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•ä¿æŒä»£ç æœç´¢æ¨¡å‹çš„æ–°é²œåº¦è€Œä¸ä¸¢å¤±å¯¹æ—§ä»£ç çš„è®°å¿†ã€‚æ¯”è¾ƒäº†ä¸‰ç§æ›´æ–°ç­–ç•¥ï¼šå®Œå…¨åˆ·æ–°ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¢é‡å¾®è°ƒï¼Œå‘ç°å¢é‡å¾®è°ƒç»“åˆæ–°æ—§ä»£ç æ··åˆè®­ç»ƒèƒ½è·å¾—æœ€ä½³å¹³è¡¡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°ä»£ä»£ç åº“æŒç»­æ¼”åŒ–å¯¼è‡´è®­ç»ƒæ¨¡å‹å¿«é€Ÿè¿‡æ—¶ï¼Œéœ€è¦åœ¨ä¸ä¸¢å¤±æ—§ä»£ç çŸ¥è¯†çš„å‰æä¸‹ä¿æŒæ¨¡å‹å¯¹æœ€æ–°ä»£ç çš„ç†è§£èƒ½åŠ›ã€‚

Method: å°†ä»£ç æ–°é²œåº¦è§†ä¸ºé¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæ¯”è¾ƒä¸‰ç§æ›´æ–°ç­–ç•¥ï¼šå®Œå…¨é‡æ–°è®­ç»ƒã€ä¸Šä¸‹æ–‡å­¦ä¹ æ³¨å…¥æœ€æ–°å˜æ›´ã€å¢é‡å¾®è°ƒç»“åˆæ–°æ—§ä»£ç æ··åˆè®­ç»ƒä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚

Result: åœ¨Flaskã€SQLAlchemyã€Pandaså’ŒPoetryç­‰é¡¹ç›®ä¸­ï¼Œå¢é‡å¾®è°ƒç»“åˆæ–°æ—§ä»£ç æ··åˆè®­ç»ƒåœ¨æ··åˆæµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ åœ¨æ— æ³•è®­ç»ƒæ—¶æä¾›æœ€å¿«çš„æ–°ä»£ç æå‡ï¼Œå®Œå…¨é‡æ–°è®­ç»ƒåœ¨è¿½æ±‚æœ€é«˜æ–°ä»£ç å‡†ç¡®ç‡æ—¶ä»æ˜¯ä¸Šé™ã€‚

Conclusion: å¢é‡å¾®è°ƒæ˜¯ä¿æŒä»£ç æœç´¢æ¨¡å‹æ–°é²œåº¦çš„æœ€ä½³å¹³è¡¡ç­–ç•¥ï¼Œè€Œä¸åŒæ›´æ–°æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹å„æœ‰ä¼˜åŠ¿ã€‚

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [21] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTesteræ˜¯ä¸€ä¸ªé›†æˆé¡¹ç›®ç‰¹å®šçŸ¥è¯†å’Œæµ‹è¯•é¢†åŸŸçŸ¥è¯†çš„LLMæµ‹è¯•ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é™æ€åˆ†ææå–é¡¹ç›®ç»“æ„å’Œä½¿ç”¨çŸ¥è¯†ï¼Œé‡‡ç”¨æµ‹è¯•é¢†åŸŸçŸ¥è¯†å¼•å¯¼çš„æµ‹è¯•ç”¨ä¾‹è®¾è®¡ä¸æµ‹è¯•æ–¹æ³•ç”Ÿæˆåˆ†ç¦»ç­–ç•¥ï¼Œç»“åˆå¤šè§†è§’æç¤ºæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡æµ‹è¯•ç”Ÿæˆçš„è´¨é‡å’Œå¯ç»´æŠ¤æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çš„åŸºäºLLMçš„è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆæ–¹æ³•åœ¨çœŸå®é¡¹ç›®ä¸­å¾€å¾€éš¾ä»¥ç”Ÿæˆæ—¢æ­£ç¡®åˆå¯ç»´æŠ¤çš„æµ‹è¯•ç”¨ä¾‹ï¼Œéœ€è¦æ›´å¥½çš„çŸ¥è¯†é›†æˆæ¡†æ¶æ¥æå‡æµ‹è¯•è´¨é‡ã€‚

Method: 1) é€šè¿‡é™æ€åˆ†ææå–é¡¹ç›®ç»“æ„å’Œç”¨æ³•çŸ¥è¯†ï¼›2) é‡‡ç”¨æµ‹è¯•é¢†åŸŸçŸ¥è¯†å¼•å¯¼çš„æµ‹è¯•ç”¨ä¾‹è®¾è®¡ä¸æµ‹è¯•æ–¹æ³•ç”Ÿæˆåˆ†ç¦»ç­–ç•¥ï¼›3) å¤šè§†è§’æç¤ºæŠ€æœ¯å¼•å¯¼LLMè€ƒè™‘å¤šæ ·åŒ–æµ‹è¯•å¯å‘å¼æ–¹æ³•ï¼›4) ä½¿ç”¨ç»“æ„åŒ–æ¨¡æ¿ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚

Result: åœ¨å¤šä¸ªå¼€æºé¡¹ç›®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒKTesteråœ¨å…­ä¸ªå…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ‰§è¡Œé€šè¿‡ç‡æå‡5.69%ï¼Œè¡Œè¦†ç›–ç‡æå‡8.83%ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘æ—¶é—´å’Œç”Ÿæˆæ›´å°‘æµ‹è¯•ç”¨ä¾‹ã€‚äººå·¥è¯„ä¼°ä¹Ÿç¡®è®¤KTesterç”Ÿæˆçš„æµ‹è¯•åœ¨æ­£ç¡®æ€§ã€å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢è¯„åˆ†æ›´é«˜ã€‚

Conclusion: KTesteré€šè¿‡é›†æˆé¡¹ç›®ç‰¹å®šçŸ¥è¯†å’Œæµ‹è¯•é¢†åŸŸçŸ¥è¯†ï¼Œæœ‰æ•ˆæå‡äº†LLMæµ‹è¯•ç”Ÿæˆçš„è´¨é‡å’Œå®ç”¨æ€§ï¼Œè¯æ˜äº†çŸ¥è¯†é©±åŠ¨æ¡†æ¶åœ¨è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [22] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºLLMä»£ç†çš„AIå·¥ä½œæµï¼Œç”¨äºå°†é—ç•™çš„Fortranä»£ç è‡ªåŠ¨è½¬æ¢ä¸ºæ€§èƒ½å¯ç§»æ¤çš„Kokkos C++ç¨‹åºï¼Œå®ç°åœ¨å¼‚æ„GPUåŠ é€Ÿæ¶æ„ä¸Šçš„ç°ä»£åŒ–ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€é«˜æ€§èƒ½è®¡ç®—å‘å¼‚æ„GPUåŠ é€Ÿæ¶æ„è½¬å˜ï¼Œè®¸å¤šåŠ é€Ÿå™¨ç¼ºä¹åŸç”ŸFortranç»‘å®šï¼Œéœ€è¦å°†é—ç•™çš„Fortranä»£ç åº“ç°ä»£åŒ–ä»¥å®ç°è·¨ç¡¬ä»¶å¹³å°çš„æ€§èƒ½å¯ç§»æ¤æ€§ã€‚

Method: ä½¿ç”¨ä¸“é—¨çš„LLMä»£ç†åä½œå·¥ä½œæµï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€éªŒè¯ã€ç¼–è¯‘ã€è¿è¡Œã€æµ‹è¯•ã€è°ƒè¯•å’Œä¼˜åŒ–Fortranå†…æ ¸ä¸ºå¯ç§»æ¤çš„Kokkos C++ç¨‹åºã€‚

Result: è¯¥æµæ°´çº¿æˆåŠŸç°ä»£åŒ–äº†ä¸€ç³»åˆ—åŸºå‡†å†…æ ¸ï¼Œåœ¨ç¡¬ä»¶åˆ†åŒºä¸Šç”Ÿæˆäº†æ€§èƒ½å¯ç§»æ¤çš„Kokkosä»£ç ã€‚ä»˜è´¹çš„OpenAIæ¨¡å‹ä»…éœ€å‡ ç¾å…ƒå³å¯æ‰§è¡Œå·¥ä½œæµï¼Œç”Ÿæˆçš„ä¼˜åŒ–ä»£ç è¶…è¶Šäº†FortranåŸºçº¿ï¼Œè€Œå¼€æºæ¨¡å‹é€šå¸¸æ— æ³•ç”ŸæˆåŠŸèƒ½ä»£ç ã€‚

Conclusion: è¿™é¡¹å·¥ä½œè¯æ˜äº†åŸºäºä»£ç†çš„AIåœ¨Fortranåˆ°Kokkosè½¬æ¢ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºè‡ªä¸»ç°ä»£åŒ–é—ç•™ç§‘å­¦åº”ç”¨ç¨‹åºæä¾›äº†é€”å¾„ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒçš„è¶…çº§è®¡ç®—æœºä¸Šå¯ç§»æ¤ä¸”é«˜æ•ˆåœ°è¿è¡Œã€‚

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning](https://arxiv.org/abs/2511.13765)
*Shengjie Sun,Jiafei Lyu,Runze Liu,Mengbei Yan,Bo Liu,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: PROFæ¡†æ¶åˆ©ç”¨LLMä»è‡ªç„¶è¯­è¨€æè¿°å’Œå•ä¸ªä¸“å®¶è½¨è¿¹ç”Ÿæˆå¯æ‰§è¡Œçš„å¥–åŠ±å‡½æ•°ä»£ç ï¼Œé€šè¿‡å¥–åŠ±åå¥½æ’åº(RPR)è‡ªåŠ¨è¯„ä¼°å’Œä¼˜åŒ–å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€ç¯å¢ƒäº¤äº’æˆ–RLè®­ç»ƒï¼Œåœ¨D4RLåŸºå‡†ä¸Šè¶…è¶Šæˆ–åŒ¹é…ç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç¦»çº¿æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾è½¨è¿¹ä¸ä¸“å®¶æ¼”ç¤ºçš„ç›¸ä¼¼åº¦ä¸å¥–åŠ±æ­£ç›¸å…³ï¼Œè¿™è¿‡åº¦ç®€åŒ–äº†å¥–åŠ±ç»“æ„ã€‚éœ€è¦æ›´å‡†ç¡®çš„æ–¹æ³•æ¥ä¼°è®¡æœªæ ‡è®°æ•°æ®é›†çš„å¥–åŠ±ã€‚

Method: æå‡ºPROFæ¡†æ¶ï¼š1) ä½¿ç”¨LLMä»è‡ªç„¶è¯­è¨€æè¿°å’Œä¸“å®¶è½¨è¿¹ç”Ÿæˆå¥–åŠ±å‡½æ•°ä»£ç ï¼›2) æå‡ºRPRç­–ç•¥è¯„ä¼°å¥–åŠ±å‡½æ•°è´¨é‡ï¼Œè®¡ç®—æ”¯é…åˆ†æ•°ï¼›3) é€šè¿‡RPRå’ŒåŸºäºæ–‡æœ¬çš„æ¢¯åº¦ä¼˜åŒ–äº¤æ›¿è¿›è¡Œå¥–åŠ±å‡½æ•°é€‰æ‹©å’Œä¼˜åŒ–ã€‚

Result: åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPROFåœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢†åŸŸä¸Šè¶…è¶Šæˆ–åŒ¹é…äº†æœ€è¿‘çš„å¼ºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

Conclusion: PROFé€šè¿‡ç»“åˆLLMå’Œè‡ªåŠ¨å¥–åŠ±å‡½æ•°ä¼˜åŒ–ï¼ŒæˆåŠŸè§£å†³äº†ç¦»çº¿æ¨¡ä»¿å­¦ä¹ ä¸­å¥–åŠ±ä¼°è®¡çš„æŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ç§æ— éœ€ç¯å¢ƒäº¤äº’çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆã€‚

Abstract: Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.

</details>


### [24] [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788)
*Samuel Nathanson,Rebecca Williams,Cynthia Matuszek*

Main category: cs.LG

TL;DR: ç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡å¤šè½®å¯¹æŠ—æ€§äº¤äº’ç³»ç»Ÿæ€§åœ°è¶Šç‹±å°å‹æ¨¡å‹ï¼Œæ¨¡å‹å¤§å°æ¯”ä¾‹ä¸æœ‰å®³è¡Œä¸ºå¯èƒ½æ€§å‘ˆæ­£ç›¸å…³ï¼Œæ”»å‡»è€…è¡Œä¸ºå¤šæ ·æ€§æ¯”ç›®æ ‡æ˜“å—æ€§å¯¹å¯¹æŠ—ç»“æœå½±å“æ›´å¤§ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€LLMsåœ¨å¤šä»£ç†å’Œå®‰å…¨å…³é”®ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œéœ€è¦äº†è§£æ¨¡å‹åœ¨å¯¹æŠ—æ€§äº¤äº’ä¸­çš„è„†å¼±æ€§å¦‚ä½•æ‰©å±•ï¼Œç‰¹åˆ«æ˜¯å¤§æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¶Šç‹±å°æ¨¡å‹ã€‚

Method: ä½¿ç”¨JailbreakBenchæ ‡å‡†åŒ–å¯¹æŠ—ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿè¶…è¿‡6000æ¬¡å¤šè½®æ”»å‡»è€…-ç›®æ ‡äº¤äº’ï¼Œæ¶µç›–ä¸»è¦LLMå®¶æ—å’Œè§„æ¨¡ï¼ˆ0.6B-120Bå‚æ•°ï¼‰ï¼Œé€šè¿‡ä¸‰ä¸ªç‹¬ç«‹LLMæ³•å®˜è¯„ä¼°ä¼¤å®³åˆ†æ•°å’Œæ‹’ç»è¡Œä¸ºã€‚

Result: å‘ç°å¹³å‡ä¼¤å®³åˆ†æ•°ä¸æ”»å‡»è€…-ç›®æ ‡å¤§å°æ¯”ä¾‹çš„å¯¹æ•°å‘ˆå¼ºæ­£ç›¸å…³ï¼ˆPearson r=0.51ï¼‰ï¼Œæ”»å‡»è€…æ–¹è¡Œä¸ºå¤šæ ·æ€§å¯¹å¯¹æŠ—ç»“æœå½±å“æ›´å¤§ï¼Œæ”»å‡»è€…æ‹’ç»é¢‘ç‡ä¸ä¼¤å®³å‘ˆå¼ºè´Ÿç›¸å…³ï¼ˆrho=-0.93ï¼‰ã€‚

Conclusion: å¤§å°ä¸å¯¹ç§°æ€§å½±å“æ¨¡å‹é²æ£’æ€§ï¼Œä¸ºå¯¹æŠ—æ€§æ‰©å±•æ¨¡å¼æä¾›äº†æ¢ç´¢æ€§è¯æ®ï¼Œéœ€è¦æ›´å—æ§åœ°ç ”ç©¶æ¨¡å‹é—´å¯¹é½å’Œå®‰å…¨æ€§ã€‚

Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.

</details>


### [25] [Beat the long tail: Distribution-Aware Speculative Decoding for RL Training](https://arxiv.org/abs/2511.13841)
*Zelei Shao,Vikranth Srivatsa,Sanjana Srivastava,Qingyang Wu,Alpay Ariyak,Xiaoxia Wu,Ameen Patel,Jue Wang,Percy Liang,Tri Dao,Ce Zhang,Yiying Zhang,Ben Athiwaratkun,Chenfeng Xu,Junxiong Wang*

Main category: cs.LG

TL;DR: DASæ˜¯ä¸€ä¸ªåˆ†å¸ƒæ„ŸçŸ¥çš„æ¨æµ‹è§£ç æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å†å²rolloutæ•°æ®æ„å»ºè‡ªé€‚åº”è‰ç¨¿å™¨å’Œé•¿åº¦æ„ŸçŸ¥ç­–ç•¥ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹è¾“å‡ºçš„æƒ…å†µä¸‹åŠ é€ŸRLè®­ç»ƒä¸­çš„rollouté˜¶æ®µï¼Œå‡å°‘é«˜è¾¾50%çš„è®­ç»ƒæ—¶é—´ã€‚


<details>
  <summary>Details</summary>
Motivation: RLåè®­ç»ƒä¸­rollouté˜¶æ®µçš„æ•ˆç‡å—åˆ°é•¿è½¨è¿¹ç”Ÿæˆçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯é•¿å°¾åˆ†å¸ƒä¸­å°‘æ•°é•¿ç”Ÿæˆåºåˆ—å æ®äº†å¤§éƒ¨åˆ†è®¡ç®—æ—¶é—´ã€‚åŒæ—¶å‘ç°å†å²rolloutæ•°æ®ä¸­å­˜åœ¨ç¨³å®šçš„æç¤ºçº§åˆ«æ¨¡å¼å¯ä»¥åˆ©ç”¨ã€‚

Method: æå‡ºDASæ¡†æ¶ï¼š1ï¼‰åŸºäºæœ€è¿‘rolloutæ„å»ºå¢é‡ç»´æŠ¤åç¼€æ ‘çš„è‡ªé€‚åº”éå‚æ•°è‰ç¨¿å™¨ï¼›2ï¼‰é•¿åº¦æ„ŸçŸ¥æ¨æµ‹ç­–ç•¥ï¼Œä¸ºæ”¯é…è®¡ç®—æ—¶é—´çš„é•¿è½¨è¿¹åˆ†é…æ›´æ¿€è¿›çš„è‰ç¨¿é¢„ç®—

Result: åœ¨æ•°å­¦å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDASå°†rolloutæ—¶é—´å‡å°‘é«˜è¾¾50%ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„è®­ç»ƒæ›²çº¿

Conclusion: åˆ†å¸ƒæ„ŸçŸ¥çš„æ¨æµ‹è§£ç èƒ½å¤Ÿåœ¨ä¸å½±å“å­¦ä¹ è´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ é€ŸRLåè®­ç»ƒ

Abstract: Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.

</details>


### [26] [From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs](https://arxiv.org/abs/2511.14017)
*Erum Mushtaq,Anil Ramakrishna,Satyapriya Krishna,Sattvik Sahai,Prasoon Goyal,Kai-Wei Chang,Tao Zhang,Rahul Gupta*

Main category: cs.LG

TL;DR: è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç‰¹å®šé¢†åŸŸè¿›è¡Œæ‹’ç»é—å¿˜å­¦ä¹ å¯èƒ½å¼•å‘è·¨é¢†åŸŸçš„æœ‰å®³è¡Œä¸ºä¼ æ’­ï¼Œå³æ–°å…´é”™ä½ç°è±¡ã€‚é€šè¿‡åˆ†æç½‘ç»œå®‰å…¨å’Œå®‰å…¨æ¦‚å¿µï¼Œå‘ç°å®‰å…¨æ¦‚å¿µçš„å¹²é¢„å¯¹å…¶ä»–é¢†åŸŸå½±å“æ›´å¤§ï¼Œä¸”å¯é€šè¿‡ä¿ç•™æ•°æ®çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ¢å¤å¯¹é½ã€‚


<details>
  <summary>Details</summary>
Motivation: ç†è§£å¼•å‘æ–°å…´é”™ä½ç°è±¡çš„ç®—æ³•ã€ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œæ¢ç´¢æ‹’ç»é—å¿˜å­¦ä¹ å¦‚ä½•å¯¼è‡´è·¨é¢†åŸŸæœ‰å®³è¡Œä¸ºçš„ä¼ æ’­ã€‚

Method: åœ¨ç½‘ç»œå®‰å…¨å’Œå®‰å…¨æ¦‚å¿µä¸Šè¿›è¡Œæ‹’ç»é—å¿˜å­¦ä¹ ï¼Œè¯„ä¼°ä¸ƒä¸ªè´Ÿè´£ä»»AIé¢†åŸŸçš„æ‹’ç»åˆ†æ•°ï¼Œåˆ†ææ¦‚å¿µåœ¨è¡¨ç¤ºå±‚é¢çš„çº ç¼ å…³ç³»ã€‚

Result: çª„é¢†åŸŸé—å¿˜å­¦ä¹ å¯åœ¨ç›®æ ‡æ¦‚å¿µä¸Šäº§ç”Ÿåˆè§„å“åº”ï¼Œä½†ä¼šå°†æ–°å…´é”™ä½ä¼ æ’­åˆ°æ— å…³é¢†åŸŸï¼›å®‰å…¨æ¦‚å¿µå¯¹å…¶ä»–é¢†åŸŸå½±å“æ›´å¤§ï¼›é€šè¿‡ä¿ç•™æ•°æ®çš„äº¤å‰ç†µæŸå¤±å¯æ¢å¤å¯¹é½ã€‚

Conclusion: æ¦‚å¿µåœ¨æ—©æœŸè¡¨ç¤ºå±‚çš„ç›¸ä¼¼æ€§è¶Šé«˜ï¼Œåœ¨æ‹’ç»æµé€šè¿‡ç›®æ ‡é—å¿˜å­¦ä¹ æ”¹å˜åè¶Šå®¹æ˜“å—åˆ°æ–°å…´é”™ä½å½±å“ã€‚

Abstract: Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.

</details>


### [27] [Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation](https://arxiv.org/abs/2511.14135)
*Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor*

Main category: cs.LG

TL;DR: æå‡ºäº†Fair-GNEæ–¹æ³•ï¼Œå°†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å»ºæ¨¡ä¸ºçº¦æŸå¹¿ä¹‰çº³ä»€å‡è¡¡åšå¼ˆï¼Œåœ¨åŒ»ç–—å·¥ä½œè€…èµ„æºåˆ†é…åœºæ™¯ä¸­å®ç°å¯è¯æ˜çš„å…¬å¹³å·¥ä½œè´Ÿè½½åˆ†é…ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰MARLæ–¹æ³•é€šè¿‡äº‹åç¼–æ’æ¥å¡‘é€ å¥–åŠ±ä»¥å®ç°å…¬å¹³ï¼Œä½†ç¼ºä¹è¿è¡Œæ—¶ä¸å¯å˜çš„å¯è¯æ˜è‡ªæ‰§è¡Œå…¬å¹³æ€§ã€‚åœ¨åŒ»ç–—å·¥ä½œè€…å…±äº«èµ„æºçš„åœºæ™¯ä¸­ï¼Œéœ€è¦ç¡®ä¿å·¥ä½œè´Ÿè½½å…¬å¹³åˆ†é…ä»¥å®ç°ä¸€è‡´å¯é çš„æ€§èƒ½ã€‚

Method: å°†MARLå»ºæ¨¡ä¸ºçº¦æŸå¹¿ä¹‰çº³ä»€å‡è¡¡å¯»æ±‚æ¸¸æˆï¼Œé€šè¿‡è‡ªé€‚åº”çº¦æŸæ‰§è¡Œå¼•å¯¼ç¾¤ä½“ç­–ç•¥è¾¾åˆ°å®‰å…¨å’Œå±€éƒ¨æœ‰æ•ˆçš„å‡è¡¡çŠ¶æ€ï¼Œä½¿ä»»ä½•æ™ºèƒ½ä½“éƒ½æ— æ³•é€šè¿‡å•æ–¹é¢æ”¹å˜å†³ç­–æ¥æ”¹å–„å…¶æ•ˆç”¨å‡½æ•°ã€‚

Result: åœ¨å®šåˆ¶çš„é«˜ä¿çœŸå¤è‹æ¨¡æ‹Ÿå™¨ä¸­ï¼ŒFair-GNEç›¸æ¯”å›ºå®šæƒ©ç½šåŸºçº¿åœ¨å·¥ä½œè´Ÿè½½å¹³è¡¡æ–¹é¢æ˜¾è‘—æ”¹å–„ï¼ˆ0.89 vs 0.33 JFIï¼Œp < 0.01ï¼‰ï¼ŒåŒæ—¶ä¿æŒ86%çš„ä»»åŠ¡æˆåŠŸç‡ã€‚

Conclusion: Fair-GNEé€šè¿‡è‡ªé€‚åº”çº¦æŸæ‰§è¡Œå®ç°äº†ç»Ÿè®¡æ˜¾è‘—çš„å…¬å¹³æ€§æå‡ï¼Œä¸ºå¤§å‹å¤šæ™ºèƒ½ä½“å­¦ä¹ å‹åŒ»ç–—ç³»ç»Ÿæä¾›äº†æ¸…æ™°çš„å…¬å¼ã€è¯„ä¼°æŒ‡æ ‡å’Œå‡è¡¡å¯»æ±‚åˆ›æ–°ã€‚

Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.

</details>


### [28] [N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator](https://arxiv.org/abs/2511.14195)
*Zheyu Lin,Jirui Yang,Hengqi Guo,Yubing Bao,Yao Guan*

Main category: cs.LG

TL;DR: æå‡ºN-GLAREæ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ½œåœ¨è¡¨ç¤ºçš„APTè½¨è¿¹å’ŒJSSæŒ‡æ ‡ï¼Œåœ¨æ— éœ€ç”Ÿæˆæ–‡æœ¬çš„æƒ…å†µä¸‹è¯„ä¼°LLMçš„å®‰å…¨æ€§é²æ£’æ€§ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰çº¢é˜Ÿæµ‹è¯•æ–¹æ³•ä¾èµ–åœ¨çº¿ç”Ÿæˆå’Œé»‘ç›’è¾“å‡ºåˆ†æï¼Œæˆæœ¬é«˜ä¸”åé¦ˆå»¶è¿Ÿï¼Œä¸é€‚åˆæ–°æ¨¡å‹è®­ç»ƒçš„æ•æ·è¯Šæ–­ã€‚

Method: åŸºäºæ¨¡å‹æ½œåœ¨è¡¨ç¤ºåˆ†æAPTè½¨è¿¹ï¼Œå¼•å…¥JSSæŒ‡æ ‡æ¥è¯„ä¼°éšè—å±‚åŠ¨æ€ï¼Œå®Œå…¨ç»•è¿‡æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚

Result: åœ¨40å¤šä¸ªæ¨¡å‹å’Œ20ç§çº¢é˜Ÿç­–ç•¥ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒJSSæŒ‡æ ‡ä¸çº¢é˜Ÿæµ‹è¯•å®‰å…¨æ’åé«˜åº¦ä¸€è‡´ï¼Œæˆæœ¬é™ä½99%ä»¥ä¸Šã€‚

Conclusion: N-GLAREæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è¾“å‡ºæ— å…³è¯„ä¼°ä»£ç†ï¼Œé€‚ç”¨äºå®æ—¶è¯Šæ–­ã€‚

Abstract: Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.

</details>


### [29] [Parallelizing Tree Search with Twice Sequential Monte Carlo](https://arxiv.org/abs/2511.14220)
*Yaniv Oren,Joery A. de Vries,Pascal R. van der Vaart,Matthijs T. J. Spaan,Wendelin BÃ¶hmer*

Main category: cs.LG

TL;DR: æå‡ºäº†TSMCTSæ–¹æ³•ï¼Œé€šè¿‡å‡å°‘æ–¹å·®å’Œç¼“è§£è·¯å¾„é€€åŒ–é—®é¢˜ï¼Œåœ¨ä¿æŒSMCå¹¶è¡ŒåŒ–ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†åŸºäºæœç´¢çš„æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: åºåˆ—è’™ç‰¹å¡æ´›(SMC)ä½œä¸ºMCTSçš„æ›¿ä»£æ–¹æ¡ˆæ›´æ˜“äºå¹¶è¡ŒåŒ–å’ŒGPUåŠ é€Ÿï¼Œä½†å­˜åœ¨æ–¹å·®å¤§å’Œè·¯å¾„é€€åŒ–é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨æ·±åº¦æœç´¢ä¸­çš„æ‰©å±•èƒ½åŠ›ã€‚

Method: å¼•å…¥TSMCTSæ–¹æ³•ï¼Œé€šè¿‡ä¸¤æ¬¡åºåˆ—è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¥å‡å°‘æ–¹å·®å’Œç¼“è§£è·¯å¾„é€€åŒ–é—®é¢˜ã€‚

Result: åœ¨ç¦»æ•£å’Œè¿ç»­ç¯å¢ƒä¸­ï¼ŒTSMCTSå‡ä¼˜äºSMCåŸºçº¿å’Œç°ä»£MCTSç‰ˆæœ¬ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ‰©å±•åºåˆ—è®¡ç®—èƒ½åŠ›ã€‚

Conclusion: TSMCTSåœ¨ä¿æŒSMCå¹¶è¡ŒåŒ–ä¼˜åŠ¿çš„åŒæ—¶ï¼Œé€šè¿‡è§£å†³å…¶æ ¸å¿ƒé—®é¢˜å®ç°äº†æ›´å¥½çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

Abstract: Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.

</details>


### [30] [Object-Centric World Models for Causality-Aware Reinforcement Learning](https://arxiv.org/abs/2511.14262)
*Yosuke Nishimoto,Takashi Matsubara*

Main category: cs.LG

TL;DR: STICAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„Transformerä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£ç¯å¢ƒä¸ºç¦»æ•£å¯¹è±¡æ¥æå‡æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿä¸–ç•Œæ¨¡å‹éš¾ä»¥å‡†ç¡®å¤åˆ¶é«˜ç»´ã€éå¹³ç¨³ã€å¤šå¯¹è±¡äº¤äº’çš„å¤æ‚ç¯å¢ƒï¼Œè€Œäººç±»é€šè¿‡åˆ†è§£ç¯å¢ƒä¸ºç¦»æ•£å¯¹è±¡æ¥é«˜æ•ˆå†³ç­–ã€‚

Method: ä½¿ç”¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„Transformerè¡¨ç¤ºè§‚æµ‹ä¸ºå¯¹è±¡ä»¤ç‰Œï¼Œé¢„æµ‹ä»¤ç‰Œçº§åŠ¨æ€å’Œäº¤äº’ï¼›ç­–ç•¥å’Œä»·å€¼ç½‘ç»œä¼°è®¡ä»¤ç‰Œçº§å› æœå…³ç³»å¹¶åœ¨æ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ã€‚

Result: åœ¨å¯¹è±¡ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSTICAåœ¨æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ä¸Šéƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ™ºèƒ½ä½“ã€‚

Conclusion: å¯¹è±¡åˆ†è§£å’Œå› æœæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶èƒ½æœ‰æ•ˆæå‡å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–æ€§èƒ½ã€‚

Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.

</details>


### [31] [ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents](https://arxiv.org/abs/2511.14584)
*Ankush Kadu,Ashwanth Krishnan*

Main category: cs.LG

TL;DR: ReflexGradæ˜¯ä¸€ä¸ªç»“åˆäº†åˆ†å±‚ä»»åŠ¡åˆ†è§£ã€å†å²æ„ŸçŸ¥å› æœåæ€å’Œæ¢¯åº¦ä¼˜åŒ–çš„æ–°å‹æ¶æ„ï¼Œåœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†67%çš„é›¶æ ·æœ¬æˆåŠŸç‡ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒæˆ–æ¼”ç¤ºã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³æ™ºèƒ½ä½“ä»ç»éªŒä¸­å­¦ä¹ å¹¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å®ç°æ³›åŒ–è€Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„åŸºæœ¬æŒ‘æˆ˜ï¼Œæ¢ç´¢äº’è¡¥å­¦ä¹ æœºåˆ¶çš„ååŒæ•´åˆã€‚

Method: ç»“åˆä¸‰ç§äº’è¡¥æœºåˆ¶ï¼šLLMé©±åŠ¨çš„åˆ†å±‚TODOåˆ†è§£ç”¨äºæˆ˜ç•¥è§„åˆ’ã€å†å²æ„ŸçŸ¥å› æœåæ€åˆ†æè¿‘æœŸè¡ŒåŠ¨æ¨¡å¼ä»¥è¯†åˆ«å¤±è´¥æ ¹æºã€æ¢¯åº¦ä¼˜åŒ–è¿›è¡Œç³»ç»Ÿæ€§æ”¹è¿›ã€‚

Result: åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­ï¼Œé¦–æ¬¡è¯•éªŒ(Trial 0)å®ç°67%çš„é›¶æ ·æœ¬æˆåŠŸç‡ï¼Œæ— éœ€ä»»ä½•å…ˆå‰ä»»åŠ¡ç»éªŒæˆ–æ¼”ç¤ºï¼Œæœ‰æ•ˆæ€§èƒ½åœ¨é¦–æ¬¡æ¥è§¦æ—¶å»ºç«‹ã€‚

Conclusion: äº’è¡¥å­¦ä¹ æœºåˆ¶çš„ååŒæ•´åˆèƒ½å¤Ÿå®ç°ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œæ¥è¿‘å…ˆå‰å·¥ä½œä¸­çš„å°‘æ ·æœ¬åŸºçº¿æ€§èƒ½ã€‚

Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.

</details>


### [32] [Failure to Mix: Large language models struggle to answer according to desired probability distributions](https://arxiv.org/abs/2511.14630)
*Ivy Yuqian Yang,David Yu Zhang*

Main category: cs.LG

TL;DR: ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éµå¾ªç®€å•æ¦‚ç‡åˆ†å¸ƒç”Ÿæˆè¾“å‡ºæ—¶ä¸¥é‡å¤±è´¥ï¼Œè¡¨ç°å‡ºç±»ä¼¼é˜¶è·ƒå‡½æ•°çš„è¡Œä¸ºï¼Œå‡ ä¹åªç”Ÿæˆæ¦‚ç‡æœ€é«˜çš„è¾“å‡º


<details>
  <summary>Details</summary>
Motivation: ç§‘å­¦æ€æƒ³ç”Ÿæˆå’Œé€‰æ‹©éœ€è¦éµå¾ªç›®æ ‡æ¦‚ç‡åˆ†å¸ƒçš„æ¢ç´¢ï¼Œè€Œå½“å‰çš„AIåŸºå‡†æµ‹è¯•æœ‰å®¢è§‚æ­£ç¡®ç­”æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMä¼šæŠ‘åˆ¶æ¦‚ç‡æ¢ç´¢

Method: è¿›è¡Œç³»ç»Ÿå®éªŒï¼Œè¦æ±‚LLMæŒ‰ç…§ç®€å•æ¦‚ç‡åˆ†å¸ƒç”Ÿæˆè¾“å‡ºï¼Œæµ‹è¯•å…¶éµå¾ªåˆ†å¸ƒçš„èƒ½åŠ›

Result: æ‰€æœ‰æµ‹è¯•çš„ç°ä»£LLMéƒ½ä¸¥é‡æ— æ³•éµå¾ªåˆ†å¸ƒï¼Œä¾‹å¦‚è¦æ±‚49%æ—¶é—´è¾“å‡º"1"æ—¶ï¼Œå‡ ä¹100%è¾“å‡º"0"

Conclusion: LLMè¡¨ç°å‡ºé˜¶è·ƒå‡½æ•°è¡Œä¸ºï¼Œå³ä½¿æ¦‚ç‡å·®å¼‚å¾ˆå°ä¹Ÿå‡ ä¹åªç”Ÿæˆæ¦‚ç‡æœ€é«˜çš„è¾“å‡ºï¼Œç”šè‡³å‹å€’äº†å†…ç½®çš„å¼ºåè§

Abstract: Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.

</details>


### [33] [$Ï€^{*}_{0.6}$: a VLA That Learns From Experience](https://arxiv.org/abs/2511.14759)
*Ali Amin,Raichelle Aniceto,Ashwin Balakrishna,Kevin Black,Ken Conley,Grace Connors,James Darpinian,Karan Dhabalia,Jared DiCarlo,Danny Driess,Michael Equi,Adnan Esmail,Yunhao Fang,Chelsea Finn,Catherine Glossop,Thomas Godden,Ivan Goryachev,Lachy Groom,Hunter Hancock,Karol Hausman,Gashon Hussein,Brian Ichter,Szymon Jakubczak,Rowan Jen,Tim Jones,Ben Katz,Liyiming Ke,Chandra Kuchi,Marinda Lamb,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Yao Lu,Vishnu Mano,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Charvi Sharma,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,Will Stoeckle,Alex Swerdlow,James Tanner,Marcel Torne,Quan Vuong,Anna Walling,Haohuan Wang,Blake Williams,Sukwon Yoo,Lili Yu,Ury Zhilinsky,Zhiyuan Zhou*

Main category: cs.LG

TL;DR: æå‡ºäº†RECAPæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŠ¿æ¡ä»¶ç­–ç•¥è¿›è¡ŒVLAæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œç»“åˆæ¼”ç¤ºæ•°æ®ã€åœ¨çº¿æ”¶é›†æ•°æ®å’Œä¸“å®¶å¹²é¢„ï¼Œåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶å¦‚ä½•é€šè¿‡çœŸå®ä¸–ç•Œéƒ¨ç½²å’Œå¼ºåŒ–å­¦ä¹ æ¥æ”¹è¿›è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç°å®ä»»åŠ¡ã€‚

Method: RECAPæ–¹æ³•ï¼šå…ˆé€šè¿‡ç¦»çº¿RLé¢„è®­ç»ƒé€šç”¨VLAæ¨¡å‹ï¼Œç„¶åé€šè¿‡åœ¨çº¿æœºå™¨äººæ•°æ®æ”¶é›†è¿›è¡Œä»»åŠ¡ä¸“ä¸šåŒ–ï¼Œæ•´åˆå¼‚æ„æ•°æ®ï¼ˆæ¼”ç¤ºã€åœ¨çº¿æ”¶é›†ã€ä¸“å®¶å¹²é¢„ï¼‰ã€‚

Result: è®­ç»ƒåçš„æ¨¡å‹èƒ½åœ¨çœŸå®å®¶åº­ä¸­å è¡£æœã€å¯é åœ°ç»„è£…ç›’å­ã€ä½¿ç”¨ä¸“ä¸šå’–å•¡æœºåˆ¶ä½œæµ“ç¼©å’–å•¡ã€‚åœ¨æœ€å›°éš¾ä»»åŠ¡ä¸Šï¼Œä»»åŠ¡ååé‡æé«˜ä¸€å€ä»¥ä¸Šï¼Œæ•…éšœç‡å‡åŠã€‚

Conclusion: RECAPæ–¹æ³•æœ‰æ•ˆæå‡äº†VLAæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè¯æ˜äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¼‚æ„æ•°æ®æ•´åˆå®ç°æ¨¡å‹è‡ªæˆ‘æ”¹è¿›çš„å¯è¡Œæ€§ã€‚

Abstract: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $Ï€^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $Ï€^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [34] [<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>é©±åŠ¨çš„çº³ç±³å…‰å­å™¨ä»¶ç ”ç©¶](http://mp.weixin.qq.com/s?__biz=MzU4MTU2NjEzMg==&mid=2247486285&idx=1&sn=470ef6b72409a1935921019f6d35f0a3&chksm=fc37f4ec456cfabe00a05c26157d1f75d22ed9c6aab760324dae308b73248917761bffdb156f#rd)
*å…‰å­¦äº†*

Main category: wechat.article

TL;DR: æ•°å­¦ä¸Šï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œå³ä¼˜åŒ–ç­–ç•¥ ä½¿å¾—æœŸæœ›æœ€å¤§å…¶ä¸­ æ˜¯æŠ˜æ‰£å› å­ï¼Œ æ˜¯ç¬¬æ­¥çš„å³æ—¶å¥–åŠ±ã€‚åœ¨å…·ä½“å®ç°ä¸­ï¼Œä½œè€…é‡‡ç”¨äº†æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æˆ–ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œé€¼è¿‘çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ï¼Œå¹¶åˆ©ç”¨ç»éªŒå›


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: æ•°å­¦ä¸Šï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œå³ä¼˜åŒ–ç­–ç•¥ ä½¿å¾—æœŸæœ›æœ€å¤§å…¶ä¸­ æ˜¯æŠ˜æ‰£å› å­ï¼Œ æ˜¯ç¬¬æ­¥çš„å³æ—¶å¥–åŠ±ã€‚åœ¨å…·ä½“å®ç°ä¸­ï¼Œä½œè€…é‡‡ç”¨äº†æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æˆ–ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œé€¼è¿‘çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ï¼Œå¹¶åˆ©ç”¨ç»éªŒå›

</details>


### [35] [æ·±åº¦å¥½æ–‡ï¼š<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>åœ¨LRMä¸­çš„åº”ç”¨ç»¼è¿°ç³»åˆ—2](http://mp.weixin.qq.com/s?__biz=MzkwMTYzOTMzMw==&mid=2247483906&idx=1&sn=7501d470b6c32e08b88c30e97f132e58&chksm=c1165c74f57f404ee6d1db63f81e08cc0ba473277b8ba844ec9c946601b1572872ef37a1dc07#rd)
*é¹ç¨‹AI*

Main category: wechat.article

TL;DR: æœ¬ç¯‡ä¸»è¦åšå¦‚ä¸‹æ€»ç»“ï¼š å¼ºåŒ–å­¦ä¹ åœ¨LRMä¸­å¦‚ä½•ä¼˜åŒ–policyï¼Œå¦‚ä½•è¿›è¡Œé‡‡æ ·ã€‚å¯¹äºæƒ³äº†è§£å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹å‘å±•çš„è¯»è€…å¯ä»¥å‚è€ƒï¼šhttpsï¼š//mp.weixin.qq.com/s/JBR1n2rrcQfhzAdd4i-F1w


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: æœ¬ç¯‡ä¸»è¦åšå¦‚ä¸‹æ€»ç»“ï¼š å¼ºåŒ–å­¦ä¹ åœ¨LRMä¸­å¦‚ä½•ä¼˜åŒ–policyï¼Œå¦‚ä½•è¿›è¡Œé‡‡æ ·ã€‚å¯¹äºæƒ³äº†è§£å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹å‘å±•çš„è¯»è€…å¯ä»¥å‚è€ƒï¼šhttpsï¼š//mp.weixin.qq.com/s/JBR1n2rrcQfhzAdd4i-F1w

</details>


### [36] [ã€æ–‡çŒ®ã€‘é€šè¿‡<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>ä¸­å¥–åŠ±å‡½æ•°ä¸»åŠ¨çªå˜å®ç°æœºå™¨äººæ¶²ä½“å€¾å€’ä»»åŠ¡çš„æŠ€èƒ½å¤šæ ·åŒ–](http://mp.weixin.qq.com/s?__biz=MzIxNzQ0ODkyMQ==&mid=2247491404&idx=1&sn=d505403004e34e9f5ac1467ab5974f15&chksm=960177c35ad961426f4b294e93deb71845c14ffc8ababc542f577222cd9bf8222984120fc030#rd)
*AILabç¬”è®°*

Main category: wechat.article

TL;DR: å¼ºåŒ–å­¦ä¹ ç®—æ³•åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢ç´¢äº†å¥–åŠ±å‡½æ•°ä¸­çªå˜æƒé‡çš„ä¸åŒé…ç½®å°†å¦‚ä½•å½±å“å­¦ä¹ åˆ°çš„ç­–ç•¥ã€‚ç”±æ­¤äº§ç”Ÿçš„ç­–ç•¥è¡¨ç°å‡ºå¹¿æ³›çš„è¡Œä¸ºï¼šä»æ‰§è¡Œæœ€åˆé¢„æœŸçš„æµ‡æ³¨ä»»åŠ¡çš„å˜åŒ–åˆ°å¯¹æ„å¤–ä»»åŠ¡æœ‰ç”¨çš„æ–°æŠ€èƒ½ï¼Œä¾‹å¦‚å®¹å™¨è¾¹ç¼˜æ¸…


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å¼ºåŒ–å­¦ä¹ ç®—æ³•åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢ç´¢äº†å¥–åŠ±å‡½æ•°ä¸­çªå˜æƒé‡çš„ä¸åŒé…ç½®å°†å¦‚ä½•å½±å“å­¦ä¹ åˆ°çš„ç­–ç•¥ã€‚ç”±æ­¤äº§ç”Ÿçš„ç­–ç•¥è¡¨ç°å‡ºå¹¿æ³›çš„è¡Œä¸ºï¼šä»æ‰§è¡Œæœ€åˆé¢„æœŸçš„æµ‡æ³¨ä»»åŠ¡çš„å˜åŒ–åˆ°å¯¹æ„å¤–ä»»åŠ¡æœ‰ç”¨çš„æ–°æŠ€èƒ½ï¼Œä¾‹å¦‚å®¹å™¨è¾¹ç¼˜æ¸…

</details>


### [37] [æ·±åº¦<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>ä¸­çš„Actorå’ŒCriticæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ](http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516978&idx=1&sn=5020a8bbbf55c6d005efff0c8891ae41&chksm=9a18be4b35aa003bd86d0e381a14cfbe3af86d21347b29353a6cbe7370b4a56f69f955e3f101#rd)
*æœˆæ¥å®¢æ ˆ*

Main category: wechat.article

TL;DR: ä¸€å¥è¯æ€»ç»“ï¼ŒActor è´Ÿè´£å­¦â€œåšä»€ä¹ˆâ€ï¼ˆç­–ç•¥å­¦ä¹ ï¼‰ï¼Œè€Œ Critic è´Ÿè´£å­¦â€œå¥½ä¸å¥½â€ï¼ˆä»·å€¼è¯„ä¼°ï¼‰ï¼Œ äºŒè€…ç›¸äº’é…åˆï¼Œæ„æˆå¼ºåŒ–å­¦ä¹ ä¸­æœ€å¸¸è§ã€æœ€ç¨³å®šçš„ç»“æ„ä¹‹ä¸€ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: ä¸€å¥è¯æ€»ç»“ï¼ŒActor è´Ÿè´£å­¦â€œåšä»€ä¹ˆâ€ï¼ˆç­–ç•¥å­¦ä¹ ï¼‰ï¼Œè€Œ Critic è´Ÿè´£å­¦â€œå¥½ä¸å¥½â€ï¼ˆä»·å€¼è¯„ä¼°ï¼‰ï¼Œ äºŒè€…ç›¸äº’é…åˆï¼Œæ„æˆå¼ºåŒ–å­¦ä¹ ä¸­æœ€å¸¸è§ã€æœ€ç¨³å®šçš„ç»“æ„ä¹‹ä¸€ã€‚

</details>


### [38] [ã€æ¯”æ­¦ç ºå…µ å¼ºåŸºæèƒ½ã€‘åŒºæ”¿åºœåŠï¼š<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>æèƒ½åŠ›ï¼Œæ‰“é€ é«˜ç´ è´¨åŒºæ”¿åºœåŠå¹²éƒ¨é˜Ÿä¼ï½œå—åº·åŒºâ€œèµ‹èƒ½æå‡â€å²—ä½ç»ƒå…µè¡ŒåŠ¨çºªå®ï¼ˆäºŒåå››ï¼‰](http://mp.weixin.qq.com/s?__biz=MjM5MjU2NTQ1Mw==&mid=2247535839&idx=1&sn=f30d436b85bc8c33c3927d873980acae&chksm=a7cdf88026470a660c02c224699b235f9709691739ae83542357888c344cab4ba9a452dea0df#rd)
*å—åº·ç»„å·¥*

Main category: wechat.article

TL;DR: åŒºæ”¿åºœåŠï¼šå¼ºåŒ–å­¦ä¹ æèƒ½åŠ›ï¼Œæ‰“é€ é«˜ç´ è´¨åŒºæ”¿åºœåŠå¹²éƒ¨é˜Ÿä¼ ä¸ºä¿ƒè¿›å¹²éƒ¨é˜Ÿä¼èƒ½åŠ›æå‡ï¼Œæ‰“é€ ä¸€æ”¯â€œæç¬”èƒ½å†™ã€å¼€å£èƒ½è¯´ã€é—®ç­–èƒ½å¯¹ã€é‡äº‹èƒ½åŠâ€çš„åŒºæ”¿åºœåŠå¹²éƒ¨é˜Ÿä¼ï¼Œä¸ºå…¨åŒºé«˜è´¨é‡å‘å±•æä¾›æœ‰åŠ›çš„â€œä¸‰æœåŠ¡â€ä¿éšœã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: åŒºæ”¿åºœåŠï¼šå¼ºåŒ–å­¦ä¹ æèƒ½åŠ›ï¼Œæ‰“é€ é«˜ç´ è´¨åŒºæ”¿åºœåŠå¹²éƒ¨é˜Ÿä¼ ä¸ºä¿ƒè¿›å¹²éƒ¨é˜Ÿä¼èƒ½åŠ›æå‡ï¼Œæ‰“é€ ä¸€æ”¯â€œæç¬”èƒ½å†™ã€å¼€å£èƒ½è¯´ã€é—®ç­–èƒ½å¯¹ã€é‡äº‹èƒ½åŠâ€çš„åŒºæ”¿åºœåŠå¹²éƒ¨é˜Ÿä¼ï¼Œä¸ºå…¨åŒºé«˜è´¨é‡å‘å±•æä¾›æœ‰åŠ›çš„â€œä¸‰æœåŠ¡â€ä¿éšœã€‚

</details>


### [39] [Physical Intelligenceå›¢é˜Ÿæ­£å¼å‘å¸ƒÏ€*0.6ï¼VLA+<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>è®­ç»ƒ](http://mp.weixin.qq.com/s?__biz=MzkyMDY0OTc1NA==&mid=2247534101&idx=2&sn=2ec3b2a02989f25c13a2d4b844b60e14&chksm=c0af54c63ac3f7cbe5c67acab01794cd9b7be3d0d64ebe07a93810dbc904ec32bc2adb52f4ea#rd)
*3Dè§†è§‰ä¹‹å¿ƒ*

Main category: wechat.article

TL;DR: è™½ç„¶åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»å®è·µç†è®ºåŸºç¡€æ—©åœ¨æ•°åå¹´å‰å°±å·²ç¡®ç«‹ï¼Œä½†è¦å°†è¿™äº›åŸç†èå…¥é€šç”¨ä¸”å¯æ‰©å±•çš„æœºå™¨äººå­¦ä¹ ç³»ç»Ÿä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šä¸ºå¤§å‹æ¨¡å‹è®¾è®¡å¯æ‰©å±•ä¸”ç¨³å®šçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€å¤„ç†æ¥è‡ªä¸åŒç­–ç•¥çš„å¼‚æ„æ•°æ®ï¼Œä»¥åŠåœ¨å¥–åŠ±ä¿¡


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: è™½ç„¶åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»å®è·µç†è®ºåŸºç¡€æ—©åœ¨æ•°åå¹´å‰å°±å·²ç¡®ç«‹ï¼Œä½†è¦å°†è¿™äº›åŸç†èå…¥é€šç”¨ä¸”å¯æ‰©å±•çš„æœºå™¨äººå­¦ä¹ ç³»ç»Ÿä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šä¸ºå¤§å‹æ¨¡å‹è®¾è®¡å¯æ‰©å±•ä¸”ç¨³å®šçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€å¤„ç†æ¥è‡ªä¸åŒç­–ç•¥çš„å¼‚æ„æ•°æ®ï¼Œä»¥åŠåœ¨å¥–åŠ±ä¿¡

</details>


### [40] [<em class="highlight">å¼ºåŒ–å­¦ä¹ </em> | ä¼˜åŒ–ç­–ç•¥ Roadmap ä»‹ç»](http://mp.weixin.qq.com/s?__biz=MzkxMDIwOTU5OQ==&mid=2247484224&idx=1&sn=08e443a2c03a1abaac52a127bc5a6e44&chksm=c0e262d849390790f145cbdeb6206378b0948292afdaad7bac167bf91f3fd877993da6de452a#rd)
*AIè€é©¬å•Š*

Main category: wechat.article

TL;DR: 1.2ï¼Œå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å­¦ä¹ åŒºåˆ« å¼ºåŒ–å­¦ä¹ æ›´å¯èƒ½è€ƒè™‘æ•´ä½“å½±å“ç›‘ç£å­¦ä¹ é’ˆå¯¹å•ä¸ªè¯å…ƒè¿›è¡Œåé¦ˆï¼Œç›®æ ‡æ˜¯è¦æ±‚æ¨¡å‹é’ˆå¯¹ç»™å®šçš„è¾“å…¥ç»™å‡ºç¡®åˆ‡çš„ç­”æ¡ˆï¼Œå¹¶ä¸”å…¶æŸå¤±ä¸ºåŸºäºæ€»å’Œè§„åˆ™çš„äº¤å‰ç†µå‡½æ•°ï¼Œé€ æˆè¿™ç§æŸå¤±å¯¹ä¸ªåˆ«è¯å…ƒå˜åŒ–ä¸æ•æ„Ÿã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: 1.2ï¼Œå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å­¦ä¹ åŒºåˆ« å¼ºåŒ–å­¦ä¹ æ›´å¯èƒ½è€ƒè™‘æ•´ä½“å½±å“ç›‘ç£å­¦ä¹ é’ˆå¯¹å•ä¸ªè¯å…ƒè¿›è¡Œåé¦ˆï¼Œç›®æ ‡æ˜¯è¦æ±‚æ¨¡å‹é’ˆå¯¹ç»™å®šçš„è¾“å…¥ç»™å‡ºç¡®åˆ‡çš„ç­”æ¡ˆï¼Œå¹¶ä¸”å…¶æŸå¤±ä¸ºåŸºäºæ€»å’Œè§„åˆ™çš„äº¤å‰ç†µå‡½æ•°ï¼Œé€ æˆè¿™ç§æŸå¤±å¯¹ä¸ªåˆ«è¯å…ƒå˜åŒ–ä¸æ•æ„Ÿã€‚

</details>


### [41] [ICRA 2025 æ¸…å&UCä¼¯å…‹åˆ©æå‡ºiRe-VLAï¼šè¿­ä»£å¼<em class="highlight">å¼ºåŒ–å­¦ä¹ </em>ï¼Œè®©æœºå™¨äººå¤§æ¨¡å‹å‘Šåˆ«è®­ç»ƒå´©æºƒ](http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495854&idx=1&sn=70ca03658a060ed8f74d04463d1cc127&chksm=fbf16653ebfdc6804e83dce4a21b43fbcd92dcd67998be08d56ce87540403b41d215a33690db#rd)
*VLer*

Main category: wechat.article

TL;DR: å¤§å®¶çŸ¥é“ï¼Œç°åœ¨çš„å¤§æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒæµç¨‹é€šå¸¸æ˜¯â€œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰+ å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰â€ã€‚ç ”ç©¶è€…ä»¬å¾ˆè‡ªç„¶åœ°æƒ³æŠŠè¿™å¥—æµç¨‹æ¬åˆ°æœºå™¨äººé¢†åŸŸï¼Œç”¨åœ¨VLAæ¨¡å‹ä¸Šã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: å¤§å®¶çŸ¥é“ï¼Œç°åœ¨çš„å¤§æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒæµç¨‹é€šå¸¸æ˜¯â€œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰+ å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰â€ã€‚ç ”ç©¶è€…ä»¬å¾ˆè‡ªç„¶åœ°æƒ³æŠŠè¿™å¥—æµç¨‹æ¬åˆ°æœºå™¨äººé¢†åŸŸï¼Œç”¨åœ¨VLAæ¨¡å‹ä¸Šã€‚

</details>


### [42] [ç§‘æŠ€é¢†åŸŸæœ€æ–°çƒ­è¯â€œ<em class="highlight">agentic</em>â€äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ](http://mp.weixin.qq.com/s?__biz=Mzg3NTg0Njg2Nw==&mid=2247495658&idx=1&sn=af9e4774945aa7fcc018f276fc4472e2&chksm=ce26fcadd28ac18a03ed713edfa062fe0938cabcad389dd5384c60d40781d5821c81e8bd3f41#rd)
*èƒ–çŒªè‹±è¯­*

Main category: wechat.article

TL;DR: â€˜Agenticâ€™ æ˜¯ä¸€ä¸ªåŸºäºå¤è€ç†å¿µçš„æµè¡Œçƒ­è¯ç±³æ—å¾·Â·å¦è´ç ”ç©¶åä½œå‹äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“å·²æœ‰ä¸‰åå¹´ï¼Œå§‹äº1995å¹´é¦–å±Šå›½é™…å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¤§ä¼šåœ¨æ—§é‡‘å±±å¬å¼€ä¹‹æ—¶ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: â€˜Agenticâ€™ æ˜¯ä¸€ä¸ªåŸºäºå¤è€ç†å¿µçš„æµè¡Œçƒ­è¯ç±³æ—å¾·Â·å¦è´ç ”ç©¶åä½œå‹äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“å·²æœ‰ä¸‰åå¹´ï¼Œå§‹äº1995å¹´é¦–å±Šå›½é™…å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¤§ä¼šåœ¨æ—§é‡‘å±±å¬å¼€ä¹‹æ—¶ã€‚

</details>


### [43] [å¤š<em class="highlight">æ™ºèƒ½ä½“</em>è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•æ—¶ä»£æ¥ä¸´å•¦](http://mp.weixin.qq.com/s?__biz=MzkwNjQxOTk1Mg==&mid=2247486124&idx=1&sn=fc5edb7997d137de737a10bb1a6a13ad&chksm=c18d792be41773d597db6367f700bfc8c878acd71e7cf4019cbbe794cbdafa9a9e9da8697e81#rd)
*å¼€æºæƒ…æŠ¥æŠ€æœ¯ç ”ç©¶é™¢*

Main category: wechat.article

TL;DR: Agentic AIï¼ˆä¹Ÿç§°Agentic Artificial Intelligenceæˆ–è‡ªåŠ¨åŒ–ä»£ç†AIï¼‰æ˜¯2025å¹´AIé¢†åŸŸçš„é¡¶çº§è¶‹åŠ¿ï¼ˆGartnerå°†å…¶åˆ—ä¸º2025å¹´æŠ€æœ¯è¶‹åŠ¿No.1ï¼‰ã€‚ä¸ä¼ ç»Ÿç”Ÿæˆå¼AIï¼ˆå¦‚ChatGPTï¼‰ä¸åŒï¼ŒAgentic AIå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒç‰¹å¾ï¼š


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Agentic AIï¼ˆä¹Ÿç§°Agentic Artificial Intelligenceæˆ–è‡ªåŠ¨åŒ–ä»£ç†AIï¼‰æ˜¯2025å¹´AIé¢†åŸŸçš„é¡¶çº§è¶‹åŠ¿ï¼ˆGartnerå°†å…¶åˆ—ä¸º2025å¹´æŠ€æœ¯è¶‹åŠ¿No.1ï¼‰ã€‚ä¸ä¼ ç»Ÿç”Ÿæˆå¼AIï¼ˆå¦‚ChatGPTï¼‰ä¸åŒï¼ŒAgentic AIå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒç‰¹å¾ï¼š

</details>


### [44] [<em class="highlight">Agentic</em> å¼€å‘å¹³å°ï¼Œè”åˆ Gemini 3 ä¸€èµ·æ¨å‡ºï¼ŒGoogle æœ‰è‡ªå·±çš„ <em class="highlight">Agentic</em> IDEA.](http://mp.weixin.qq.com/s?__biz=MzU0NjExNTQ3Nw==&mid=2247484515&idx=1&sn=ff37517703cb24abf2641785a2fdc702&chksm=fa5ad0bdc29c81103e9a3209ab3b2a295c1a01f5e40cf04abd32900b1184da901dce3c9069dd#rd)
*AIçš„åœºæ™¯å’Œæœºä¼š*

Main category: wechat.article

TL;DR: Google Antigravityï¼šGoogle å…¨æ–°æ¨å‡ºçš„ Agentic å¼€å‘å¹³å°ï¼Œè”åˆ Gemini 3 ä¸€èµ·æ¨å‡ºï¼ŒGoogle æœ‰è‡ªå·±çš„ Agentic IDE äº†ä»å‘å¸ƒä¿¡æ¯çœ‹ï¼Œæ˜¯ $2.4B æŒ–æ¥çš„ Windsurf æ ¸å¿ƒå›¢é˜Ÿåšçš„ï¼Œä¹Ÿå¾ˆåˆç†ï¼Œéš¾æ€ªæ˜¨å¤©çœ‹åˆ°åŠ å…¥ Deepmind åç¬¬ä¸€æ¬¡å‘æ¨ã€Œä¸€ä¸ªåé‡åŠ›çš„ç¬”è®°


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Google Antigravityï¼šGoogle å…¨æ–°æ¨å‡ºçš„ Agentic å¼€å‘å¹³å°ï¼Œè”åˆ Gemini 3 ä¸€èµ·æ¨å‡ºï¼ŒGoogle æœ‰è‡ªå·±çš„ Agentic IDE äº†ä»å‘å¸ƒä¿¡æ¯çœ‹ï¼Œæ˜¯ $2.4B æŒ–æ¥çš„ Windsurf æ ¸å¿ƒå›¢é˜Ÿåšçš„ï¼Œä¹Ÿå¾ˆåˆç†ï¼Œéš¾æ€ªæ˜¨å¤©çœ‹åˆ°åŠ å…¥ Deepmind åç¬¬ä¸€æ¬¡å‘æ¨ã€Œä¸€ä¸ªåé‡åŠ›çš„ç¬”è®°

</details>


### [45] [<em class="highlight">Agentic</em> AI åœ¨å¯è§‚æµ‹æ€§é¢†åŸŸï¼šæ„å»ºå¼¹æ€§ã€å¯é—®è´£çš„ IT ç³»ç»Ÿ](http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247486157&idx=3&sn=db9839a3ab81087b2b7062c1628e92e4&chksm=970564d051b91be5cb7a3d45fe1ee251d3410c806549af6ef00c6447c3f890f15d9f2d7ed82e#rd)
*ATinfo*

Main category: wechat.article

TL;DR: Agentic AI æ¦‚å¿µè§£ææŠ€æœ¯ç‰¹ç‚¹ï¼šAgentic AI æŒ‡èƒ½å¤Ÿé€šè¿‡è§„åˆ’ã€æ¨ç†å’Œè¡ŒåŠ¨è‡ªä¸»æ‰§è¡Œå¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„ç³»ç»Ÿä¸å“åº”ç›´æ¥æŒ‡ä»¤çš„ä¼ ç»Ÿ AI ä¸åŒï¼ŒAgentic AI æ˜¯ä¸»åŠ¨ä¸”ç›®æ ‡é©±åŠ¨çš„ï¼Œèƒ½å¤Ÿé€‚åº”ä¸æ–­å˜åŒ–çš„æ¡ä»¶ã€‚


<details>
  <summary>Details</summary>
Motivation: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œåˆ†äº«AIç›¸å…³æŠ€æœ¯å†…å®¹

Method: åŸºäºå®é™…åº”ç”¨ç»éªŒçš„æŠ€æœ¯åˆ†äº«

Result: æä¾›å®ç”¨çš„AIæŠ€æœ¯è§è§£å’Œæ¡ˆä¾‹åˆ†æ

Conclusion: é€‚åˆäº†è§£AIæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

Abstract: Agentic AI æ¦‚å¿µè§£ææŠ€æœ¯ç‰¹ç‚¹ï¼šAgentic AI æŒ‡èƒ½å¤Ÿé€šè¿‡è§„åˆ’ã€æ¨ç†å’Œè¡ŒåŠ¨è‡ªä¸»æ‰§è¡Œå¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„ç³»ç»Ÿä¸å“åº”ç›´æ¥æŒ‡ä»¤çš„ä¼ ç»Ÿ AI ä¸åŒï¼ŒAgentic AI æ˜¯ä¸»åŠ¨ä¸”ç›®æ ‡é©±åŠ¨çš„ï¼Œèƒ½å¤Ÿé€‚åº”ä¸æ–­å˜åŒ–çš„æ¡ä»¶ã€‚

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology](https://arxiv.org/abs/2511.13825)
*Humza Nusrat,Omar Nusrat*

Main category: cs.AI

TL;DR: è¯„ä¼°KOSMOSè‡ªä¸»AIç§‘å­¦å®¶åœ¨è¾å°„ç”Ÿç‰©å­¦ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶äº§ç”Ÿäº†ä¸€ä¸ªæ˜ç¡®å‘ç°ã€ä¸€ä¸ªä¸ç¡®å®šç»“æœå’Œä¸€ä¸ªé”™è¯¯å‡è®¾ï¼Œè¡¨æ˜AIç§‘å­¦å®¶èƒ½ç”Ÿæˆæœ‰ç”¨æƒ³æ³•ä½†éœ€è¦ä¸¥æ ¼éªŒè¯


<details>
  <summary>Details</summary>
Motivation: è¯„ä¼°è‡ªä¸»AIç§‘å­¦å®¶KOSMOSåœ¨è¾å°„ç”Ÿç‰©å­¦é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œæ£€éªŒå…¶ç”Ÿæˆå‡è®¾çš„èƒ½åŠ›

Method: ä½¿ç”¨ç®€å•éšæœºåŸºå› é›¶åŸºå‡†æµ‹è¯•ï¼Œåœ¨ä¸‰ä¸ªè¾å°„ç”Ÿç‰©å­¦é—®é¢˜ä¸Šè¯„ä¼°KOSMOSï¼šDNAæŸä¼¤å“åº”ä¸p53è½¬å½•å“åº”å…³ç³»ã€OGTå’ŒCDO1åŸºå› è¡¨è¾¾é¢„æµ‹è¾å°„å“åº”æ¨¡å—ã€12åŸºå› ç­¾åé¢„æµ‹å‰åˆ—è…ºç™Œæ”¾ç–—åç”Ÿå­˜

Result: å‡è®¾1ä¸æ”¯æŒï¼ˆDDRè¯„åˆ†ä¸p53å“åº”å¼±è´Ÿç›¸å…³ï¼‰ï¼Œå‡è®¾2ä¸­CDO1æ˜¯æ˜æ˜¾å¼‚å¸¸å€¼ï¼ˆr=0.70ï¼Œp=0.0039ï¼‰ï¼Œå‡è®¾3çš„12åŸºå› ç­¾åä¸€è‡´æ€§æŒ‡æ•°0.61ï¼ˆp=0.017ï¼‰ä½†æ•ˆåº”å¤§å°ä¸å”¯ä¸€

Conclusion: AIç§‘å­¦å®¶å¯ä»¥ç”Ÿæˆæœ‰ç”¨æƒ³æ³•ï¼Œä½†éœ€è¦é’ˆå¯¹é€‚å½“é›¶æ¨¡å‹è¿›è¡Œä¸¥æ ¼å®¡è®¡

Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.

</details>


### [47] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: æå‡ºCORGIç®—æ³•è§£å†³è§„åˆ™åŒ¹é…ä¸­çš„æŒ‡æ•°çº§æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦é—®é¢˜ï¼Œä¸ºAIä»£ç†å’Œæ•°æ®åº“æŸ¥è¯¢æä¾›å®æ—¶æ€§èƒ½ä¿è¯


<details>
  <summary>Details</summary>
Motivation: å®æ—¶AIç³»ç»Ÿå’Œæ•°æ®åº“æŸ¥è¯¢éœ€è¦è§£å†³å¤æ‚åŒ¹é…é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤šä¸ªæœªçº¦æŸå˜é‡çš„è§„åˆ™æ—¶ä¼šäº§ç”ŸæŒ‡æ•°çº§å¤æ‚åº¦ï¼Œå¯¼è‡´å†…å­˜æº¢å‡ºå’Œæ‰§è¡Œä¸­æ–­

Method: é‡‡ç”¨ä¸¤æ­¥æ³•ï¼šå‰å‘ä¼ é€’æ„å»º/ç»´æŠ¤æ¥åœ°å…³ç³»å›¾ï¼Œåå‘è¿­ä»£å™¨æŒ‰éœ€ç”ŸæˆåŒ¹é…ï¼Œé¿å…ä¼ ç»ŸRETEç®—æ³•ä¸­Î²å†…å­˜æ”¶é›†éƒ¨åˆ†åŒ¹é…çš„é—®é¢˜

Result: åœ¨æ€§èƒ½è¯„ä¼°ä¸­ï¼ŒCORGIåœ¨ç®€å•ç»„åˆåŒ¹é…ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºSOARå’ŒOPS5çš„RETEå®ç°

Conclusion: CORGIé€šè¿‡æ¶ˆé™¤å¡«å……å®Œæ•´å†²çªé›†çš„éœ€æ±‚ï¼Œæä¾›äº†äºŒæ¬¡æ—¶é—´å’Œç©ºé—´ä¿è¯ï¼Œèƒ½å¤Ÿè¿­ä»£æµå¼ä¼ è¾“åç»­åŒ¹é…ï¼Œè§£å†³äº†é«˜å»¶è¿Ÿå’Œå†…å­˜æº¢å‡ºé—®é¢˜

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $Î²$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [48] [Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data](https://arxiv.org/abs/2511.14098)
*Adit Jain,Vikram Krishnamurthy,Yiming Zhang*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶LLMç½‘ç»œä¸­çš„åä½œé—®ç­”é—®é¢˜ï¼Œé€šè¿‡ç»“åˆå¹³å‡åœºåŠ¨åŠ›å­¦å’Œéšæœºæ•ˆç”¨æ¨¡å‹æ¥åˆ†æå¹»è§‰ä¼ æ’­ç°è±¡ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMåœ¨ç¼ºä¹ç›´æ¥è¯æ®æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œè¿™ç§æ•ˆåº”åœ¨äº¤äº’çš„LLMç½‘ç»œä¸­æ›´åŠ æ˜æ˜¾ï¼Œå¯¼è‡´åŸæœ¬å‡†ç¡®çš„LLMä¹Ÿå¼€å§‹äº§ç”Ÿå¹»è§‰ã€‚

Method: ç»“åˆç½‘ç»œç§‘å­¦ä¸­çš„å¹³å‡åœºåŠ¨åŠ›å­¦å’Œç»æµå­¦ä¸­çš„éšæœºæ•ˆç”¨æ¨¡å‹ï¼Œæ„å»ºç”Ÿæˆæ¨¡å‹æ¥åˆ†æLLMç½‘ç»œä¸­çš„ä¿¡æ¯æ‰©æ•£ã€‚

Result: å»ºç«‹äº†LLMç½‘ç»œçš„åŠ¨æ€æ¨¡å‹ï¼Œåˆ†æäº†å›ºå®šç‚¹çš„å­˜åœ¨æ€§å’Œå”¯ä¸€æ€§æ¡ä»¶ï¼Œå¹¶åœ¨100ä¸ªå¼€æºLLMç½‘ç»œä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚

Conclusion: æå‡ºäº†åˆ†æLLMç½‘ç»œä¸­å¹»è§‰ä¼ æ’­çš„ç†è®ºæ¡†æ¶ï¼Œä¸ºç†è§£å’Œæ§åˆ¶LLMåä½œç³»ç»Ÿä¸­çš„ä¿¡æ¯è´¨é‡æä¾›äº†ç†è®ºåŸºç¡€ã€‚

Abstract: In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.

</details>


### [49] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: æå‡ºAPD-agentsï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç§»åŠ¨åº”ç”¨é¡µé¢çš„è‡ªåŠ¨åŒ–è®¾è®¡ï¼Œé€šè¿‡å¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“çš„åä½œå®Œæˆä»ç”¨æˆ·æè¿°åˆ°é¡µé¢å¸ƒå±€çš„ç”Ÿæˆã€‚


<details>
  <summary>Details</summary>
Motivation: ç§»åŠ¨åº”ç”¨é¡µé¢å¸ƒå±€è®¾è®¡è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šæŠ€èƒ½ï¼Œç°æœ‰è®¾è®¡è½¯ä»¶ä½¿ç”¨å¤æ‚ä¸”è·¨é¡µé¢åä½œæ•ˆç‡ä½ï¼Œéœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¥æå‡è®¾è®¡æ•ˆç‡ã€‚

Method: ä½¿ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŒ…æ‹¬ç¼–æ’æ™ºèƒ½ä½“ã€è¯­ä¹‰è§£ææ™ºèƒ½ä½“ã€ä¸»å¸ƒå±€æ™ºèƒ½ä½“ã€æ¨¡æ¿æ£€ç´¢æ™ºèƒ½ä½“å’Œé€’å½’ç»„ä»¶æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ™ºèƒ½ä½“åä½œå°†ç”¨æˆ·æè¿°è½¬æ¢ä¸ºç»“æ„åŒ–å¸ƒå±€è®¾è®¡ã€‚

Result: åœ¨RICOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAPD-agentsè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

Conclusion: è¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨äº†å¤§æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªåŠ¨åä½œèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå®ç°ç§»åŠ¨åº”ç”¨é¡µé¢çš„è‡ªåŠ¨åŒ–è®¾è®¡ã€‚

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [50] [PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval](https://arxiv.org/abs/2511.14130)
*Chun Chet Ng,Jia Yu Lim,Wei Zeng Low*

Main category: cs.AI

TL;DR: æå‡ºäº†PRISMæ¡†æ¶ï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šä»£ç†ç³»ç»Ÿï¼Œç”¨äºé‡‘èä¿¡æ¯æ£€ç´¢ä»»åŠ¡ï¼Œåœ¨FinAgentBenchæ•°æ®é›†ä¸Šå–å¾—äº†0.71818çš„NDCG@5åˆ†æ•°ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œé‡‘èä¿¡æ¯æ£€ç´¢æˆä¸ºå…³é”®å·¥ä¸šåº”ç”¨ã€‚ä»å†—é•¿çš„é‡‘èæ–‡ä»¶ä¸­æå–ä»»åŠ¡ç›¸å…³ä¿¡æ¯å¯¹äºè¿è¥å’Œåˆ†æå†³ç­–è‡³å…³é‡è¦ã€‚

Method: PRISMæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé›†æˆäº†ç²¾ç‚¼çš„ç³»ç»Ÿæç¤ºã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè½»é‡çº§å¤šä»£ç†ç³»ç»Ÿã€‚æç¤ºå·¥ç¨‹æä¾›ç²¾ç¡®ä»»åŠ¡æŒ‡ä»¤ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ æä¾›è¯­ä¹‰ç›¸å…³çš„å°‘æ ·æœ¬ç¤ºä¾‹ï¼Œå¤šä»£ç†ç³»ç»Ÿå»ºæ¨¡åè°ƒè¯„åˆ†è¡Œä¸ºã€‚

Result: åœ¨å—é™éªŒè¯é›†ä¸Šè¾¾åˆ°NDCG@5ä¸º0.71818ï¼Œè¯æ˜äº†PRISMåœ¨ç”Ÿäº§è§„æ¨¡é‡‘èæ£€ç´¢ä¸­çš„å¯è¡Œæ€§å’Œé²æ£’æ€§ã€‚

Conclusion: PRISMçš„æ¨¡å—åŒ–ã€ä»…æ¨ç†è®¾è®¡ä½¿å…¶é€‚ç”¨äºç°å®ä¸–ç•Œç”¨ä¾‹ï¼Œæºä»£ç å·²å‘å¸ƒã€‚

Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.

</details>


### [51] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: æå‡ºR3åŒè¿‡ç¨‹æ€ç»´æ¡†æ¶ï¼Œå°†LLMçš„æ³›åŒ–èƒ½åŠ›ä¸VLNä¸“ä¸šæ¨¡å‹ç»“åˆï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ˜¾è‘—æå‡è§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMæ–¹æ³•åœ¨VLNä»»åŠ¡ä¸­ä»ä¸é¢†åŸŸä¸“å®¶å­˜åœ¨æ€§èƒ½å·®è·ï¼Œä¸”LLMéš¾ä»¥ç²¾ç¡®ç†è§£çœŸå®ä¸–ç•Œç©ºé—´å…³ç³»ï¼ŒåŒæ—¶å¸¦æ¥é«˜è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿ

Method: R3æ¡†æ¶åŒ…å«Runnerï¼ˆè½»é‡çº§Transformerä¸“å®¶æ¨¡å‹ï¼‰ã€Ruminatorï¼ˆå¤šæ¨¡æ€LLMéª¨å¹²+æ€ç»´é“¾æç¤ºï¼‰å’ŒRegulatorï¼ˆæ ¹æ®ä¸‰ä¸ªæ ‡å‡†ç›‘æ§å¯¼èˆªè¿›åº¦å¹¶æ§åˆ¶æ€ç»´æ¨¡å¼ï¼‰

Result: åœ¨REVERIEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPLå’ŒRGSPLåˆ†åˆ«è¶…è¿‡æœ€å…ˆè¿›æ–¹æ³•3.28%å’Œ3.30%

Conclusion: R3æ¡†æ¶èƒ½æœ‰æ•ˆå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„VLNä»»åŠ¡ï¼Œæ˜¾è‘—æå‡å¯¼èˆªæ€§èƒ½

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [52] [Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems](https://arxiv.org/abs/2511.14136)
*Sushant Mehta*

Main category: cs.AI

TL;DR: æå‡ºCLEARè¯„ä¼°æ¡†æ¶ï¼Œè§£å†³ç°æœ‰AIä»£ç†åŸºå‡†åœ¨æˆæœ¬æ•ˆç‡ã€å¯é æ€§å’Œæ“ä½œç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ï¼Œé€šè¿‡å¤šç»´åº¦æŒ‡æ ‡æ›´å¥½åœ°é¢„æµ‹ä¼ä¸šéƒ¨ç½²æˆåŠŸã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰AIä»£ç†åŸºå‡†ä¸»è¦å…³æ³¨ä»»åŠ¡å®Œæˆå‡†ç¡®ç‡ï¼Œè€Œå¿½è§†äº†ä¼ä¸šéƒ¨ç½²æ‰€éœ€çš„å…³é”®è¦æ±‚ï¼Œå¦‚æˆæœ¬æ•ˆç‡ã€å¯é æ€§å’Œæ“ä½œç¨³å®šæ€§ã€‚

Method: ç³»ç»Ÿåˆ†æ12ä¸ªä¸»è¦åŸºå‡†ï¼Œæå‡ºCLEARï¼ˆæˆæœ¬ã€å»¶è¿Ÿã€æ•ˆèƒ½ã€ä¿è¯ã€å¯é æ€§ï¼‰æ¡†æ¶ï¼Œåœ¨300ä¸ªä¼ä¸šä»»åŠ¡ä¸Šè¯„ä¼°6ä¸ªé¢†å…ˆä»£ç†ã€‚

Result: ä»…ä¼˜åŒ–å‡†ç¡®ç‡çš„ä»£ç†æ¯”æˆæœ¬æ„ŸçŸ¥æ›¿ä»£æ–¹æ¡ˆè´µ4.4-10.8å€ï¼›CLEARæ¡†æ¶é¢„æµ‹ç”Ÿäº§æˆåŠŸçš„ç›¸å…³æ€§ï¼ˆÏ=0.83ï¼‰æ˜¾è‘—é«˜äºä»…åŸºäºå‡†ç¡®ç‡çš„è¯„ä¼°ï¼ˆÏ=0.41ï¼‰ã€‚

Conclusion: CLEARæ¡†æ¶æä¾›äº†æ›´å…¨é¢çš„ä¼ä¸šAIä»£ç†è¯„ä¼°æ ‡å‡†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æŒ‡å¯¼å®é™…éƒ¨ç½²å†³ç­–ã€‚

Abstract: Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $Ï=0.83$) compared to accuracy-only evaluation ($Ï=0.41$).

</details>


### [53] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡æµ‹è¯•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´é¡ºåºä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°éšç€åºåˆ—é•¿åº¦å¢åŠ ï¼Œæ¨¡å‹åœ¨ä¿æŒå…¨å±€ä¸€è‡´æ—¶é—´çº¿æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†åˆ†é…æ˜¾å¼æ¨ç†é¢„ç®—å¯ä»¥æ˜¾è‘—æ”¹å–„æ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: æµ‹è¯•LLMsæ˜¯å¦çœŸæ­£ç†è§£æ—¶é—´é¡ºåºï¼Œè¿™å¯¹é‡‘èå’Œç»æµåº”ç”¨ä¸­é¿å…å‰ç»æ€§åå·®è‡³å…³é‡è¦ã€‚

Method: è®¾è®¡äº†ä¸€ç³»åˆ—æ—¶é—´é¡ºåºä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶é—´æ’åºã€æ¡ä»¶æ’åºå’Œæ—¶ä»£é”™è¯¯æ£€æµ‹ï¼Œè¯„ä¼°å¤šä¸ªLLMæ¨¡å‹åœ¨ä¸åŒæ¨ç†åŠªåŠ›è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚

Result: éšç€åºåˆ—é•¿åº¦å¢åŠ ï¼Œç²¾ç¡®åŒ¹é…ç‡æ€¥å‰§ä¸‹é™ï¼Œä½†æ’åç›¸å…³æ€§ä¿æŒè¾ƒé«˜ï¼›GPT-5å’Œå¸¦æ‰©å±•æ€ç»´çš„Claude-3.7åœ¨æ¡ä»¶æ’åºä¸­è¡¨ç°ä¼˜å¼‚ï¼›æ—¶ä»£é”™è¯¯æ£€æµ‹æ˜¯æœ€ç®€å•çš„ä»»åŠ¡ã€‚

Conclusion: å½“å‰LLMsåœ¨æ—¶é—´é¡ºåºä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œä½†åˆ†é…æ˜¾å¼æ¨ç†é¢„ç®—å¯ä»¥æ˜¾è‘—æ”¹å–„æ€§èƒ½ï¼Œè¿™å¯¹é‡‘èå®æ—¶åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [54] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: æå‡ºPathMindæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§å¼•å¯¼LLMå…³æ³¨é‡è¦æ¨ç†è·¯å¾„æ¥æå‡çŸ¥è¯†å›¾è°±æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé‡‡ç”¨"æ£€ç´¢-ä¼˜å…ˆ-æ¨ç†"èŒƒå¼ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰åŸºäºLLMçš„çŸ¥è¯†å›¾è°±æ¨ç†æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1) æ— å·®åˆ«æå–æ¨ç†è·¯å¾„å¯èƒ½å¼•å…¥å™ªå£°è¯¯å¯¼LLMï¼›2) åŠ¨æ€æ¢ç´¢æ¨ç†è·¯å¾„éœ€è¦é«˜æ£€ç´¢éœ€æ±‚å’Œé¢‘ç¹LLMè°ƒç”¨ã€‚

Method: é‡‡ç”¨"æ£€ç´¢-ä¼˜å…ˆ-æ¨ç†"ä¸‰é˜¶æ®µèŒƒå¼ï¼šæ£€ç´¢æ¨¡å—è·å–æŸ¥è¯¢å­å›¾ï¼Œè·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥è·¯å¾„ä¼˜å…ˆçº§å‡½æ•°è¯†åˆ«é‡è¦æ¨ç†è·¯å¾„ï¼ŒåŒé˜¶æ®µè®­ç»ƒç­–ç•¥åŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒä¼˜å’Œè·¯å¾„åå¥½å¯¹é½ã€‚

Result: åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPathMindå§‹ç»ˆä¼˜äºç«äº‰åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œé€šè¿‡è¯†åˆ«å…³é”®æ¨ç†è·¯å¾„å®ç°äº†æ›´å°‘çš„è¾“å…¥tokenå’Œæ›´å¥½çš„æ€§èƒ½ã€‚

Conclusion: PathMindé€šè¿‡é€‰æ‹©æ€§è·¯å¾„å¼•å¯¼æœ‰æ•ˆæå‡äº†çŸ¥è¯†å›¾è°±æ¨ç†çš„å¿ å®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºLLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [55] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: DataSageæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ã€å¤šè§’è‰²è¾©è®ºæœºåˆ¶å’Œå¤šè·¯å¾„æ¨ç†æ¥è§£å†³ç°æœ‰æ•°æ®æ´å¯Ÿä»£ç†åœ¨é¢†åŸŸçŸ¥è¯†åˆ©ç”¨ä¸è¶³ã€åˆ†ææ·±åº¦æµ…å’Œä»£ç ç”Ÿæˆæ˜“å‡ºé”™çš„é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMé©±åŠ¨çš„æ•°æ®æ´å¯Ÿä»£ç†åœ¨é¢†åŸŸçŸ¥è¯†åˆ©ç”¨ã€åˆ†ææ·±åº¦å’Œä»£ç ç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³è‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå‘ç°çš„éœ€æ±‚ã€‚

Method: æå‡ºDataSageå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªåˆ›æ–°ç‰¹æ€§ï¼šå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä¸°å¯Œåˆ†æä¸Šä¸‹æ–‡ã€å¤šè§’è‰²è¾©è®ºæœºåˆ¶æ¨¡æ‹Ÿå¤šæ ·åŒ–åˆ†æè§†è§’ã€å¤šè·¯å¾„æ¨ç†æé«˜ä»£ç å’Œæ´å¯Ÿç”Ÿæˆå‡†ç¡®æ€§ã€‚

Result: åœ¨InsightBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDataSageåœ¨æ‰€æœ‰éš¾åº¦çº§åˆ«ä¸Šéƒ½æŒç»­ä¼˜äºç°æœ‰æ•°æ®æ´å¯Ÿä»£ç†ã€‚

Conclusion: DataSageä¸ºè‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå‘ç°æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [56] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨LLMå¯¹é½è¿‡ç¨‹ä¸­è€ƒè™‘å¤šå…ƒç¤¾ä¼šä»·å€¼è§‚çš„å½±å“ï¼Œé€šè¿‡æ”¶é›†æ¥è‡ªç¾å›½å’Œå¾·å›½å‚ä¸è€…çš„27,375ä¸ªè¯„åˆ†ï¼Œåˆ†æäº†äººå£ç»Ÿè®¡å·®å¼‚å’Œè®¾è®¡å‚æ•°å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMå¯¹é½å†³ç­–å¾€å¾€å¿½è§†äººç±»ç¤¾ä¼šçš„å¤šæ ·æ€§ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•å°†å¤šå…ƒä»·å€¼è§‚çº³å…¥å¯¹é½æµç¨‹ï¼Œç¡®ä¿æ¨¡å‹æ—¢å®‰å…¨åˆèƒ½å…¬å¹³ä»£è¡¨ä¸åŒç¤¾ä¼šç¾¤ä½“ã€‚

Method: æ”¶é›†æ¥è‡ª1,095åç¾å›½å’Œå¾·å›½å‚ä¸è€…çš„è¯„åˆ†æ•°æ®ï¼Œæ¶µç›–æ¯’æ€§ã€æƒ…æ„Ÿæ„è¯†ã€æ•æ„Ÿæ€§ã€åˆ»æ¿å°è±¡åè§å’Œå¸®åŠ©æ€§äº”ä¸ªç»´åº¦ã€‚ä½¿ç”¨ä¸åŒç¤¾ä¼šç¾¤ä½“çš„åå¥½å¾®è°ƒå¤šä¸ªLLMå’ŒLRMï¼ŒåŒæ—¶å˜åŒ–è¯„åˆ†å°ºåº¦ã€åˆ†æ­§å¤„ç†æ–¹æ³•å’Œä¼˜åŒ–æŠ€æœ¯ã€‚

Result: å‘ç°ç³»ç»Ÿæ€§äººå£ç»Ÿè®¡æ•ˆåº”ï¼šç”·æ€§å‚ä¸è€…è¯„åˆ†æ¯’æ€§ä½18%ï¼›ä¿å®ˆæ´¾å’Œé»‘äººå‚ä¸è€…è¯„åˆ†æƒ…æ„Ÿæ„è¯†åˆ†åˆ«é«˜27.9%å’Œ44%ã€‚æŠ€æœ¯è®¾è®¡é€‰æ‹©å½±å“æ˜¾è‘—ï¼šä¿ç•™è¯„åˆ†è€…åˆ†æ­§æ¯”å¤šæ•°æŠ•ç¥¨æ¯’æ€§å‡å°‘53%ï¼›5ç‚¹é‡è¡¨æ¯”äºŒå…ƒæ ¼å¼å‡å°‘22%ï¼›DPOåœ¨å¤šå€¼ä¼˜åŒ–ä¸­å§‹ç»ˆä¼˜äºGRPOã€‚

Conclusion: è¿™äº›å‘ç°ä¸ºå›ç­”å…³é”®é—®é¢˜æä¾›äº†åˆæ­¥æ­¥éª¤ï¼šå¯¹é½åº”å¦‚ä½•å¹³è¡¡ä¸“å®¶é©±åŠ¨å’Œç”¨æˆ·é©±åŠ¨ä¿¡å·ï¼Œä»¥ç¡®ä¿å®‰å…¨æ€§å’Œå…¬å¹³ä»£è¡¨æ€§ã€‚

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [57] [AutoTool: Efficient Tool Selection for Large Language Model Agents](https://arxiv.org/abs/2511.14650)
*Jingyi Jia,Qinbin Li*

Main category: cs.AI

TL;DR: AutoToolæ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å·¥å…·ä½¿ç”¨æƒ¯æ€§æ¥å‡å°‘LLMä»£ç†çš„æ¨ç†æˆæœ¬ï¼Œæ— éœ€é‡å¤è°ƒç”¨LLMè¿›è¡Œå·¥å…·é€‰æ‹©ã€‚


<details>
  <summary>Details</summary>
Motivation: å½“å‰LLMä»£ç†æ¡†æ¶åœ¨å·¥å…·é€‰æ‹©æ—¶å­˜åœ¨é«˜æ¨ç†æˆæœ¬é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åƒReActè¿™æ ·çš„æ–¹æ³•éœ€è¦é‡å¤è°ƒç”¨LLMæ¥å†³å®šæ¯ä¸ªæ­¥éª¤ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚

Method: æ„å»ºæœ‰å‘å›¾ä»å†å²ä»£ç†è½¨è¿¹ä¸­å­¦ä¹ å·¥å…·ä½¿ç”¨æ¨¡å¼ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºå·¥å…·ï¼Œè¾¹æ•è·è½¬ç§»æ¦‚ç‡ï¼Œå¹¶é›†æˆå‚æ•°çº§ä¿¡æ¯æ¥ç»†åŒ–å·¥å…·è¾“å…¥ç”Ÿæˆã€‚

Result: åœ¨å¤šæ ·åŒ–ä»£ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAutoToolå°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾30%ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„ä»»åŠ¡å®Œæˆç‡ã€‚

Conclusion: è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å°†ç»Ÿè®¡ç»“æ„é›†æˆåˆ°LLMä»£ç†è®¾è®¡ä¸­çš„æ½œåŠ›ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„æ•ˆç‡ã€‚

Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.

</details>


### [58] [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)
*Parya Dolatyabi,Mahdi Khodayar*

Main category: cs.AI

TL;DR: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¼‚æ„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåè°ƒå¤šä¸ªå¾®ç”µç½‘çš„ç”µåŠ›åˆ†é…ç³»ç»Ÿæ¢å¤ï¼Œè§£å†³äº†ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•åœ¨éçº¿æ€§çº¦æŸä¸‹çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: å¤§è§„æ¨¡åœç”µåæ¢å¤ç”µåŠ›åˆ†é…ç³»ç»Ÿéœ€è¦å¤„ç†éçº¿æ€§çº¦æŸï¼ˆå¦‚åŠŸç‡å¹³è¡¡ã€ç”µå‹é™åˆ¶ã€çƒ­é¢å®šå€¼ï¼‰ï¼Œä¼ ç»Ÿä¼˜åŒ–å’Œä»·å€¼å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ä¸”éš¾ä»¥æ‰©å±•ã€‚

Method: é‡‡ç”¨å¼‚æ„æ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆHAPPOï¼‰æ¡†æ¶ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶å…·æœ‰ä¸åŒè´Ÿè½½ã€DERå®¹é‡å’Œå¼€å…³æ•°é‡çš„å¾®ç”µç½‘ï¼Œä½¿ç”¨åˆ†æ•£å¼è¡ŒåŠ¨è€…ç­–ç•¥å’Œé›†ä¸­å¼æ‰¹è¯„è€…è¿›è¡Œè®­ç»ƒã€‚

Result: åœ¨IEEE 123æ€»çº¿å’ŒIEEE 8500èŠ‚ç‚¹ç³»ç»Ÿä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAPPOç›¸æ¯”å…¶ä»–æ–¹æ³•å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€æ›´é«˜çš„æ¢å¤åŠŸç‡å’Œæ›´å¹³æ»‘çš„å¤šç§å­è®­ç»ƒã€‚

Conclusion: å°†å¾®ç”µç½‘çº§å¼‚æ„æ€§çº³å…¥HARLæ¡†æ¶å¯ä¸ºå¤æ‚ç”µåŠ›åˆ†é…ç³»ç»Ÿæ¢å¤æä¾›å¯æ‰©å±•ã€ç¨³å®šä¸”çº¦æŸæ„ŸçŸ¥çš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.

</details>
