<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 32]
- [tldr.article](#tldr.article) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ReportLogic: Evaluating Logical Quality in Deep Research Reports](https://arxiv.org/abs/2602.18446)
*Jujia Zhao,Zhaoxin Huan,Zihan Wang,Xiaolu Zhang,Jun Zhou,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: 提出ReportLogic基准，从读者可审计角度评估LLM生成报告的逻辑质量，包含宏观、阐述、结构三个逻辑层次，并训练开源LogicJudge进行可扩展评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在深度研究中的应用日益增多，但现有评估框架主要关注流畅性和信息量，忽视了报告的逻辑质量——即报告的主张和论证是否明确支持、能否作为下游使用的可信基础。需要填补这一评估空白。

Method: 引入ReportLogic基准，采用分层分类法：宏观逻辑（可追溯主题结构）、阐述逻辑（理解进展所需上下文）、结构逻辑（通过明确主张-支持验证结论）。构建人工标注的基于规则的评估数据集，并训练开源LogicJudge进行可扩展评估。通过对抗性攻击测试评估器的鲁棒性。

Result: 发现现成的LLM评估器常受表面线索（如冗长）影响，推理模式可能掩盖断裂的支持关系。研究结果为构建更鲁棒的逻辑评估器和提高LLM生成报告的逻辑可靠性提供了可操作的指导。

Conclusion: ReportLogic基准填补了LLM生成报告逻辑质量评估的空白，通过分层逻辑评估框架和开源评估工具，为提升深度研究应用中LLM报告的可靠性和实用性提供了系统方法。

Abstract: Users increasingly rely on Large Language Models (LLMs) for Deep Research, using them to synthesize diverse sources into structured reports that support understanding and action. In this context, the practical reliability of such reports hinges on logical quality: whether the report's claims and arguments are explicitly supported and can be trusted as a basis for downstream use, rather than merely appearing fluent or informative. However, current evaluation frameworks largely overlook this requirement. To bridge this gap, we introduce ReportLogic, a benchmark that quantifies report-level logical quality through a reader-centric lens of auditability. Specifically, ReportLogic adopts a hierarchical taxonomy that evaluates whether readers can (1) trace an on-topic report structure with a unified analytical arc (Macro-Logic), (2) understand the progression with necessary context (Expositional-Logic), and (3) verify conclusions via explicit claim--support (Structural-Logic). Based on this taxonomy, we construct a human-annotated rubric-guided dataset and train an open-source LogicJudge for scalable evaluation. We further evaluate judge robustness via adversarial attacks, showing that off-the-shelf LLM judges are frequently influenced by superficial cues (e.g., verbosity), and reasoning modes can mask broken support relations. Overall, our results provide actionable guidance for building more robust logic evaluators and improving the logical reliability of LLM-generated reports.

</details>


### [2] [Asymptotic Semantic Collapse in Hierarchical Optimization](https://arxiv.org/abs/2602.18450)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 研究多智能体语言系统中的语义崩溃现象：在层次优化中，主导锚节点的语义状态具有无限惯性，导致周边智能体节点渐进对齐，最终形成语义共识，使信息自由度消失。


<details>
  <summary>Details</summary>
Motivation: 多智能体语言系统存在一种失效模式：共享主导语境逐渐吸收个体语义，导致智能体行为趋于一致。研究这种渐进语义崩溃现象，理解层次优化中语义对齐的机制。

Method: 将语义状态建模为黎曼流形上的点，分析诱导投影动力学。在封闭语言环境中，主导锚节点具有无限惯性，研究其与周边智能体节点的重复交互。使用信息论和微分几何方法分析语义崩溃过程。

Result: 极限语义配置对优化历史不敏感：平滑梯度更新和随机噪声更新都收敛到相同拓扑端点。上下文依赖程度控制信息内容：从原子表示到完全纠缠表示，节点熵（自由度）在极限中消失。实验显示零哈希碰撞，平均合规性约0.5，最终Jaccard相似度约0.25。

Conclusion: 渐进语义崩溃揭示了多智能体系统中不可变的共识规则，将智能体约束到共享语义语法中。理论连接了信息论量和微分几何结构，为理解语言系统中的语义对齐提供了框架。

Abstract: Multi-agent language systems can exhibit a failure mode where a shared dominant context progressively absorbs individual semantics, yielding near-uniform behavior across agents. We study this effect under the name Asymptotic Semantic Collapse in Hierarchical Optimization. In a closed linguistic setting with a Dominant Anchor Node whose semantic state has effectively infinite inertia, we show that repeated interactions with Peripheral Agent Nodes drive an asymptotic alignment that minimizes a global loss. We model semantic states as points on a Riemannian manifold and analyze the induced projection dynamics. Two consequences follow. First, the limiting semantic configuration is insensitive to the optimization history: both smooth gradient-style updates and stochastic noisy updates converge to the same topological endpoint, establishing path independence at convergence. Second, the degree of context dependence controls information content: moving from atomic (independent) representations to fully entangled (context-bound) representations forces the node entropy, interpreted as available degrees of freedom, to vanish in the limit. The theory connects information-theoretic quantities with differential-geometric structure and suggests an interpretation as an immutable consensus rule that constrains agents to a shared semantic grammar. A lightweight dataset-free benchmark on an RWKV-7 13B GGUF checkpoint complements the analysis, reporting zero hash collisions, mean compliance of 0.50 under greedy decoding and 0.531 under stochastic decoding, and final Jaccard-to-anchor similarity values of 0.295 and 0.224, respectively.

</details>


### [3] [Think$^{2}$: Grounded Metacognitive Reasoning in Large Language Models](https://arxiv.org/abs/2602.18806)
*Abraham Paul Elenjical,Vivek Hruday Kavuri,Vasudeva Varma*

Main category: cs.CL

TL;DR: 该论文提出了一种基于心理学元认知框架的LLM自我监控与纠错方法，通过规划-监控-评估的调节循环结构显著提升错误诊断和自纠正能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的推理能力，但其可靠地监控、诊断和纠正自身错误的能力仍然有限。需要一种基于认知心理学原理的系统方法来提升LLM的自我监控和纠错能力。

Method: 引入基于Ann Brown调节循环（规划、监控、评估）的心理学元认知框架，构建结构化提示架构，并集成到轻量级双过程MetaController中实现自适应努力分配。

Result: 在多个推理和诊断基准测试（GSM8K、CRUXEval、MBPP、AIME、CorrectBench、TruthfulQA）上，使用Llama-3和Qwen-3（8B）模型，显式调节结构显著改善错误诊断，自纠正成功率提高三倍。580对查询的盲人评估显示，84%的参与者更偏好该方法的可信度和元认知自我意识。

Conclusion: 将LLM推理建立在成熟的认知理论基础上，为构建更透明和诊断稳健的AI系统提供了原则性路径。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning performance, yet their ability to reliably monitor, diagnose, and correct their own errors remains limited. We introduce a psychologically grounded metacognitive framework that operationalizes Ann Brown's regulatory cycle (Planning, Monitoring, and Evaluation) as a structured prompting architecture, and study its integration within a lightweight dual-process MetaController for adaptive effort allocation. Across diverse reasoning and diagnostic benchmarks (GSM8K, CRUXEval, MBPP, AIME, CorrectBench, and TruthfulQA) using Llama-3 and Qwen-3 (8B), explicit regulatory structuring substantially improves error diagnosis and yields a threefold increase in successful self-correction. Blinded human evaluations over 580 query pairs show an 84% aggregate preference for trustworthiness and metacognitive self-awareness over standard and Chain-of-Thought baselines. Grounding LLM reasoning in established cognitive theory offers a principled path toward more transparent and diagnostically robust AI systems.

</details>


### [4] [EvalSense: A Framework for Domain-Specific LLM (Meta-)Evaluation](https://arxiv.org/abs/2602.18823)
*Adam Dejl,Jonathan Pearson*

Main category: cs.CL

TL;DR: EvalSense是一个用于构建领域特定LLM评估套件的灵活可扩展框架，提供多种模型提供商和评估策略支持，通过交互式指南和自动元评估工具帮助用户选择合适的评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计指标不适用于开放式生成任务，而基于LLM的评估方法虽然更灵活但依赖精心选择的模型、提示、参数和评估策略，容易产生配置错误和偏见，需要更可靠的评估框架。

Method: EvalSense框架包含两个核心组件：(1) 交互式指南帮助用户选择评估方法；(2) 自动元评估工具使用扰动数据评估不同评估方法的可靠性。框架支持广泛的模型提供商和评估策略。

Result: 通过临床笔记生成的案例研究展示了EvalSense的有效性，使用公开数据集从非结构化医患对话生成临床笔记。所有代码、文档和资源都已开源。

Conclusion: EvalSense提供了一个灵活可扩展的框架，帮助用户为特定领域构建可靠的LLM评估套件，解决了传统统计指标和现有LLM评估方法的局限性。

Abstract: Robust and comprehensive evaluation of large language models (LLMs) is essential for identifying effective LLM system configurations and mitigating risks associated with deploying LLMs in sensitive domains. However, traditional statistical metrics are poorly suited to open-ended generation tasks, leading to growing reliance on LLM-based evaluation methods. These methods, while often more flexible, introduce additional complexity: they depend on carefully chosen models, prompts, parameters, and evaluation strategies, making the evaluation process prone to misconfiguration and bias. In this work, we present EvalSense, a flexible, extensible framework for constructing domain-specific evaluation suites for LLMs. EvalSense provides out-of-the-box support for a broad range of model providers and evaluation strategies, and assists users in selecting and deploying suitable evaluation methods for their specific use-cases. This is achieved through two unique components: (1) an interactive guide aiding users in evaluation method selection and (2) automated meta-evaluation tools that assess the reliability of different evaluation approaches using perturbed data. We demonstrate the effectiveness of EvalSense in a case study involving the generation of clinical notes from unstructured doctor-patient dialogues, using a popular open dataset. All code, documentation, and assets associated with EvalSense are open-source and publicly available at https://github.com/nhsengland/evalsense.

</details>


### [5] [DeepInnovator: Triggering the Innovative Capabilities of LLMs](https://arxiv.org/abs/2602.18920)
*Tianyu Fan,Fengji Zhang,Yuxiang Zheng,Bei Chen,Xinyao Niu,Chengen Huang,Junyang Lin,Chao Huang*

Main category: cs.CL

TL;DR: DeepInnovator是一个训练框架，旨在激发LLMs的创新能力，通过"站在巨人肩膀上"提取科研知识和"猜想与反驳"的迭代训练范式，使模型能自主生成新颖重要的研究想法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖复杂的提示工程，缺乏系统化的训练范式来培养LLMs的创新能力。需要一种系统性的训练方法来构建真正具有原创性创新能力的研究智能体。

Method: 包含两个核心组件：1) "站在巨人肩膀上"：构建自动化数据提取管道，从大量未标注科学文献中提取和组织结构化科研知识；2) "猜想与反驳"：引入"下一个想法预测"训练范式，将研究想法生成建模为持续预测、评估和精炼新颖合理想法的迭代过程。

Result: DeepInnovator-14B在自动和专家评估中显著优于未训练基线，胜率达到80.53%-93.81%，性能与当前领先的LLMs相当。该框架为构建具有真正原创创新能力的研究智能体提供了可扩展的训练途径。

Conclusion: DeepInnovator提供了一个可扩展的训练路径，用于构建具有真正原创创新能力的研究智能体，并将开源数据集以促进社区发展。

Abstract: The application of Large Language Models (LLMs) in accelerating scientific discovery has garnered increasing attention, with a key focus on constructing research agents endowed with innovative capability, i.e., the ability to autonomously generate novel and significant research ideas. Existing approaches predominantly rely on sophisticated prompt engineering and lack a systematic training paradigm. To address this, we propose DeepInnovator, a training framework designed to trigger the innovative capability of LLMs. Our approach comprises two core components. (1) ``Standing on the shoulders of giants''. We construct an automated data extraction pipeline to extract and organize structured research knowledge from a vast corpus of unlabeled scientific literature. (2) ``Conjectures and refutations''. We introduce a ``Next Idea Prediction'' training paradigm, which models the generation of research ideas as an iterative process of continuously predicting, evaluating, and refining plausible and novel next idea. Both automatic and expert evaluations demonstrate that our DeepInnovator-14B significantly outperforms untrained baselines, achieving win rates of 80.53\%-93.81\%, and attains performance comparable to that of current leading LLMs. This work provides a scalable training pathway toward building research agents with genuine, originative innovative capability, and will open-source the dataset to foster community advancement. Source code and data are available at: https://github.com/HKUDS/DeepInnovator.

</details>


### [6] [Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning](https://arxiv.org/abs/2602.18922)
*Abhinaba Basu*

Main category: cs.CL

TL;DR: 论文提出W5H2结构化意图分解框架和五层缓存级联系统，显著降低个人AI代理的LLM调用成本，在真实基准测试中实现97.5%的成本降低。


<details>
  <summary>Details</summary>
Motivation: 个人AI代理通过重复LLM调用产生高昂成本，现有缓存方法效果不佳（GPTCache准确率37.9%，APC准确率0-12%），根本原因是优化了错误的属性——缓存有效性需要键一致性和精确性，而非分类准确性。

Method: 1) 提出W5H2结构化意图分解框架；2) 将缓存键评估简化为聚类评估，应用V-measure分解；3) 使用SetFit（每类8个示例）实现意图分类；4) 设计五层级联系统处理85%的交互；5) 通过RCPS提供风险控制的选择性预测保证。

Result: W5H2在MASSIVE数据集上达到91.1%±1.7%准确率（约2ms），相比GPTCache的37.9%和20B参数LLM的68.8%（3,447ms）。在NyayaBench v2（20类）上达到55.3%，支持30种语言的跨语言迁移。五层级联系统可处理85%的交互，预计成本降低97.5%。

Conclusion: W5H2框架和缓存级联系统能有效降低个人AI代理的LLM调用成本，通过结构化意图分解和高效的缓存机制实现显著的性能提升和成本节约。

Abstract: Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,
  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for
  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families.

</details>


### [7] [Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation](https://arxiv.org/abs/2602.18966)
*Yonathan Ron,Shiri Gilboa,Tammuz Dubnov*

Main category: cs.CL

TL;DR: 提出Whisper: Courtside Edition，一個多代理LLM管道，透過提示工程增強Whisper在特定領域的語音辨識，無需重新訓練模型，在NBA籃球解說領域顯著降低17%詞錯誤率。


<details>
  <summary>Details</summary>
Motivation: 即使最先進的語音辨識系統如Whisper，在特定領域語音（如充滿專有名詞和術語的NBA籃球解說）上仍面臨挑戰，而模型微調成本高昂。

Method: 建立多代理LLM管道：攔截Whisper初始轉錄，使用專門LLM代理進行領域上下文識別、命名實體識別和術語檢測，生成緊湊提示來引導Whisper解碼器。

Result: 在421個NBA籃球解說片段上測試，最佳管道實現17.0%相對詞錯誤率降低（從0.217到0.180，p<0.001），40.1%片段有改善，僅7.1%片段退化，顯著優於直接轉錄後編輯。

Conclusion: 基於提示的增強方法可為語音辨識提供可擴展的領域適應，是成本高昂模型微調的實用替代方案。

Abstract: Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcriptions without retraining. The pipeline intercepts Whisper's initial transcript, applies specialized LLM agents for domain context identification, named entity recognition, and jargon detection, and generates compact prompts that guide Whisper's decoder. Evaluated on 421 NBA basketball commentary segments (a domain characterized by dense proper nouns and technical terminology) our best pipeline achieves a statistically significant 17.0% relative reduction in word error rate (WER; from 0.217 to 0.180, p<0.001). Improvements are observed in 40.1% of segments with degradation in only 7.1%, substantially outperforming direct transcript post-editing. These results demonstrate that prompt-based augmentation can deliver scalable domain adaptation for ASR, offering a practical alternative to costly model fine-tuning.

</details>


### [8] [Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks](https://arxiv.org/abs/2602.19008)
*Wilson Y. Lee*

Main category: cs.CL

TL;DR: 语言代理失败常因偏离任务潜在解决方案结构而非能力不足，通过监控轨迹对规范路径的依从性可显著提升成功率


<details>
  <summary>Details</summary>
Motivation: 探究语言代理在有能力解决的任务上失败的原因，区分能力失败与可靠性失败，理解代理成功的关键因素

Method: 使用Toolathlon基准进行自然实验，分析22个前沿模型在108个真实工具使用任务上的3次独立运行，比较成功与失败运行对规范解决方案路径的依从性差异

Result: 成功运行显著更接近规范解决方案路径（Jaccard指数+0.060，p<0.0001），偏离规范路径具有自我强化效应，基于轨迹中段依从性的简单监控可将成功率提升8.8个百分点

Conclusion: 代理可靠性不能仅通过能力扩展改善，但可通过监控对规范路径的依从性进行有效干预，偏离规范路径是渐进且自我强化的过程

Abstract: Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\hatβ=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.

</details>


### [9] [IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning](https://arxiv.org/abs/2602.19049)
*Yinhan He,Yaochen Zhu,Mingjia Shi,Wendy Zheng,Lin Su,Xiaoqing Wang,Qi Guo,Jundong Li*

Main category: cs.CL

TL;DR: IAPO是一种基于信息论的训练后优化框架，通过条件互信息为每个token分配优势值，减少推理长度同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖长思维链提高准确性，但推理时间成本高昂。现有序列级奖励塑造方法对token级推理努力分配控制有限。

Method: 提出IAPO框架，基于每个token与最终答案的条件互信息分配token级优势值，识别信息丰富的推理步骤，抑制低效用探索。

Result: IAPO在保持正确性的同时减少推理长度达36%，在各种推理数据集上优于现有token高效RL方法。

Conclusion: 信息感知的优势塑造是token高效训练后优化的强大通用方向。

Abstract: Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.

</details>


### [10] [AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG](https://arxiv.org/abs/2602.19127)
*Qijie You,Wenkai Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 提出了首个自动构建的Agentic RAG基准测试AgenticRAGTracer，包含多领域1,305个数据点，支持逐步验证，揭示大语言模型在多跳推理中的关键失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅提供最终问答，缺乏中间跳级问题，无法分析代理在哪个步骤失败，限制了细粒度评估。同时，手动构建基准耗时耗力，可扩展性和泛化性有限。

Method: 使用大语言模型自动构建AgenticRAGTracer基准，包含多领域1,305个数据点，支持逐步验证，提供中间跳级问题，与现有主流基准无重叠。

Result: 即使最好的大语言模型（如GPT-5）在数据集最难部分仅获得22.6%的EM准确率。跳级感知诊断显示失败主要由扭曲的推理链驱动——过早崩溃或过度扩展，无法分配与任务逻辑结构一致的步骤。

Conclusion: AgenticRAGTracer填补了传统评估缺失的诊断维度，揭示了Agentic RAG系统在多跳推理中的关键能力缺陷，将促进Agentic RAG研究并推动该领域有意义的进展。

Abstract: With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.

</details>


### [11] [Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations](https://arxiv.org/abs/2602.19320)
*Dongming Jiang,Yi Li,Songtao Wei,Jinxin Yang,Ayushi Kishore,Alysa Zhao,Dingyi Kang,Xu Hu,Feng Chen,Qiannan Li,Bingzhe Li*

Main category: cs.CL

TL;DR: 本文对智能体记忆系统进行全面调查，分析了当前系统在基准测试、评估指标、模型依赖性和系统成本等方面的局限性，提出了更可靠评估和可扩展系统设计的方向。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体记忆系统架构发展迅速，但其经验基础仍然脆弱：现有基准测试规模不足、评估指标与语义效用不匹配、性能在不同骨干模型间差异显著、系统级成本常被忽视。需要对这些系统进行结构化分析以解决这些痛点。

Method: 首先基于四种记忆结构引入MAG系统的简明分类法，然后分析当前系统的关键痛点，包括基准饱和效应、指标有效性和判断敏感性、骨干模型依赖的准确性，以及记忆维护引入的延迟和吞吐量开销。

Result: 通过将记忆结构与经验局限性联系起来，阐明了为什么当前智能体记忆系统往往未能实现其理论承诺，并指出了更可靠评估和可扩展系统设计的方向。

Conclusion: 智能体记忆系统需要更健壮的评估框架和系统设计方法，以解决当前在基准测试、评估指标、模型依赖性和成本效率方面的局限性，实现其支持长时程推理和个性化的理论潜力。

Abstract: Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.

</details>


### [12] [How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1](https://arxiv.org/abs/2602.19526)
*Yinuo Xu,Shuo Lu,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Ran He,Jian Liang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习在深度研究代理中的作用，通过解耦分析提示模板、奖励函数和策略优化三个维度，提出了改进的Search-R1++基线方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理通过多轮检索和决策生成处理知识密集型任务，虽然强化学习已被证明能提升性能，但其具体贡献尚未得到充分探索。本文旨在系统理解强化学习在深度研究系统中的角色，为更原则性和可靠的RL训练策略铺平道路。

Method: 采用系统研究方法，沿着三个解耦维度进行分析：1) 提示模板（比较Fast Thinking和Slow Thinking模板）；2) 奖励函数（比较F1-based和EM奖励，并引入动作级惩罚）；3) 策略优化方法（比较REINFORCE、PPO和GRPO）。基于研究发现，提出了改进的Search-R1++基线方法。

Result: 研究发现：1) Fast Thinking模板比Slow Thinking模板更稳定且性能更好；2) F1-based奖励因答案回避导致的训练崩溃而表现不如EM，但通过加入动作级惩罚可以超越EM；3) REINFORCE优于PPO且需要更少的搜索动作，而GRPO稳定性最差。基于这些发现提出的Search-R1++将Search-R1的性能从0.403提升到0.442（Qwen2.5-7B）和从0.289提升到0.331（Qwen2.5-3B）。

Conclusion: 本文通过系统分析强化学习在深度研究代理中的作用，揭示了不同组件对性能的影响，并提出了改进的Search-R1++基线方法，为深度研究系统中更原则性和可靠的强化学习训练策略奠定了基础。

Abstract: Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of RL, we conduct a systematic study along three decoupled dimensions: prompt template, reward function, and policy optimization. Our study reveals that: 1) the Fast Thinking template yields greater stability and better performance than the Slow Thinking template used in prior work; 2) the F1-based reward underperforms the EM due to training collapse driven by answer avoidance; this can be mitigated by incorporating action-level penalties, ultimately surpassing EM; 3) REINFORCE outperforms PPO while requiring fewer search actions, whereas GRPO shows the poorest stability among policy optimization methods. Building on these insights, we then introduce Search-R1++, a strong baseline that improves the performance of Search-R1 from 0.403 to 0.442 (Qwen2.5-7B) and 0.289 to 0.331 (Qwen2.5-3B). We hope that our findings can pave the way for more principled and reliable RL training strategies in Deep Research systems.

</details>


### [13] [AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization](https://arxiv.org/abs/2602.20040)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: AgenticSum是一个基于LLM的推理时代理框架，通过分离上下文选择、生成、验证和针对性修正来减少临床文本摘要中的幻觉内容，在公开数据集上相比基线方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化临床文本摘要方面具有巨大潜力，但由于临床文档的长度、噪声和异质性，保持事实一致性仍然具有挑战性。现有的方法容易产生幻觉内容，需要更好的解决方案来确保摘要的准确性。

Method: 提出了AgenticSum框架，将摘要过程分解为协调的阶段：1）压缩任务相关上下文；2）生成初始草稿；3）使用内部注意力基础信号识别弱支持跨度；4）在监督控制下选择性修订标记内容。这是一个推理时的代理框架，分离了上下文选择、生成、验证和针对性修正。

Result: 在两个公共数据集上使用基于参考的指标、LLM-as-a-judge评估和人工评估进行评估。在各种测量指标上，AgenticSum相比普通LLM和其他强基线方法都显示出持续的改进。

Conclusion: 结构化、代理化的设计结合针对性修正，为使用LLM改进临床笔记摘要提供了一个有效的推理时解决方案。这种方法能够显著减少幻觉内容，提高临床文本摘要的事实一致性。

Abstract: Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.

</details>


### [14] [Multilingual Large Language Models do not comprehend all natural languages to equal degrees](https://arxiv.org/abs/2602.20065)
*Natalia Moskvina,Raquel Montero,Masaya Yoshida,Ferdy Hubers,Paolo Morosi,Walid Irhaymi,Jin Yan,Tamara Serrano,Elena Pagliarini,Fritz Günther,Evelina Leivada*

Main category: cs.CL

TL;DR: LLMs在多种语言上的理解能力评估显示，英语并非表现最佳的语言，多个罗曼语系语言（包括低资源语言）表现更好，但所有语言都落后于人类基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注高资源语言（尤其是英语），默认假设英语是LLM表现最好的语言，而低资源语言表现较差。需要更全面地评估LLM在不同语言上的理解能力。

Method: 在12种代表不同语系（印欧、亚非、突厥、汉藏、日本）的语言上，对3个流行LLM进行语言理解任务测试，分析影响性能的因素如分词、语言距离、训练数据规模等。

Result: 模型在类型多样的语言上表现出显著的语言准确性，但在所有语言上都落后于人类基线。英语并非最佳表现语言，被多个罗曼语系语言（包括低资源语言）系统性地超越。

Conclusion: LLM在不同语言上的表现受多种因素影响，需要超越英语中心主义的评估框架，更全面地理解LLM的多语言能力。

Abstract: Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Validated Code Translation for Projects with External Libraries](https://arxiv.org/abs/2602.18534)
*Hanliang Zhang,Arindam Sharma,Cristina David,Meng Wang,Brandon Paulsen,Daniel Kroening,Wenjia Ye,Taro Sekiyama*

Main category: cs.SE

TL;DR: 提出Go到Rust的程式翻譯框架，解決外部函式庫依賴問題，透過檢索機制映射API並合成適配器來驗證語義等價性


<details>
  <summary>Details</summary>
Motivation: 現有LLM在程式翻譯時，遇到外部函式庫依賴會產生幻覺API且無法生成正確導入，同時難以驗證涉及不透明函式庫類型的語義等價性

Method: 結合(1)檢索機制映射Go函式庫API到Rust API，(2)跨語言驗證管線，透過公開函式庫API合成適配器，在驗證I/O等價性前建立語言互通性

Result: 在六個真實Go專案上評估，顯著提升編譯和等價成功率（依賴最重情況下達100%，平均約2倍提升），能處理不透明函式庫類型

Conclusion: 提出的翻譯驗證框架能有效解決外部函式庫依賴問題，實現可靠的Go到Rust程式遷移，特別適合涉及不透明函式庫類型的系統程式

Abstract: Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types.

</details>


### [16] [Runtime-Augmented LLMs for Crash Detection and Diagnosis in ML Notebooks](https://arxiv.org/abs/2602.18537)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: CRANE-LLM：一种利用LLM结合运行时信息来预测和诊断ML笔记本崩溃的方法，通过提取内核状态中的对象类型、张量形状等运行时信息，在目标单元格执行前进行崩溃检测和原因诊断。


<details>
  <summary>Details</summary>
Motivation: ML笔记本（如Jupyter notebooks）在机器学习开发中广泛使用，但极易出现bug，其中崩溃是最具破坏性的问题。目前缺乏系统性的崩溃检测和诊断方法，需要一种能在执行前预测崩溃并诊断原因的技术。

Method: CRANE-LLM将大型语言模型与从笔记本内核状态提取的结构化运行时信息相结合。给定已执行的单元格和目标单元格，该方法结合静态代码上下文与运行时信息（包括对象类型、张量形状、数据属性等），预测目标单元格是否会崩溃（检测）并解释根本原因（诊断）。

Result: 在JunoBench基准测试（包含222个ML笔记本，涵盖多个ML库和崩溃根因）上评估，使用三种SOTA LLM（Gemini、Qwen、GPT-5），运行时信息将崩溃检测和诊断的准确率提高了7-10个百分点，F1分数提高了8-11分，诊断方面的提升更大。改进效果因ML库、崩溃原因和LLM而异，并依赖于互补类别运行时信息的整合。

Conclusion: CRANE-LLM通过将LLM与结构化运行时信息相结合，有效提高了ML笔记本崩溃检测和诊断的准确性，为ML开发中的错误预防提供了新方法。运行时信息的整合对性能提升至关重要，特别是在诊断任务中。

Abstract: Jupyter notebooks are widely used for machine learning (ML) development due to their support for interactive and iterative experimentation. However, ML notebooks are highly prone to bugs, with crashes being among the most disruptive. Despite their practical importance, systematic methods for crash detection and diagnosis in ML notebooks remain largely unexplored. We present CRANE-LLM, a novel approach that augments large language models (LLMs) with structured runtime information extracted from the notebook kernel state to detect and diagnose crashes before executing a target cell. Given previously executed cells and a target cell, CRANE-LLM combines static code context with runtime information, including object types, tensor shapes, and data attributes, to predict whether the target cell will crash (detection) and explain the underlying cause (diagnosis). We evaluate CRANE-LLM on JunoBench, a benchmark of 222 ML notebooks comprising 111 pairs of crashing and corresponding non-crashing notebooks across multiple ML libraries and crash root causes. Across three state-of-the-art LLMs (Gemini, Qwen, and GPT-5), runtime information improves crash detection and diagnosis by 7-10 percentage points in accuracy and 8-11 in F1-score, with larger gains for diagnosis. Improvements vary across ML libraries, crash causes, and LLMs, and depends on the integration of complementary categories of runtime information.

</details>


### [17] [Debug2Fix: Supercharging Coding Agents with Interactive Debugging Capabilities](https://arxiv.org/abs/2602.18571)
*Spandan Garg,Yufan Huang*

Main category: cs.SE

TL;DR: Debug2Fix框架通过集成交互式调试器作为软件工程代理的核心组件，显著提升了代码代理的bug修复能力，使较弱模型能达到甚至超过较强模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理主要依赖静态分析或迭代测试修复循环，缺乏开发者调试时常用的运行时信息访问能力。尽管调试器在现代IDE中很常见，但尚未集成到代码代理中，限制了其bug修复效果。

Method: 提出Debug2Fix框架，采用子代理架构将交互式调试器（支持Java和Python）作为软件工程代理的核心组件，通过访问运行时信息来增强调试能力。

Result: 在GitBug-Java和SWE-Bench-Live基准测试中，相比基线实现了超过20%的性能提升。使用该框架，GPT-5和Claude Haiku 4.5等较弱模型能匹配甚至超过Claude Sonnet 4.5等较强模型的性能。

Conclusion: 更好的工具设计与切换到更昂贵模型同样重要。子代理架构和调试器集成都是提升代码代理bug修复能力的关键因素。

Abstract: While significant progress has been made in automating various aspects of software development through coding agents, there is still significant room for improvement in their bug fixing capabilities. Debugging and investigation of runtime behavior remains largely a manual, developer-driven process. Popular coding agents typically rely on either static analysis of the code or iterative test-fix cycles, which is akin to trial and error debugging. We posit that there is a wealth of rich runtime information that developers routinely access while debugging code, which agents are currently deprived of due to design limitations. Despite how prevalent debuggers are in modern IDEs and command-line tools, they have surprisingly not made their way into coding agents. In this work, we introduce Debug2Fix, a novel framework that incorporates interactive debugging as a core component of a software engineering agent via a subagent architecture. We incorporate debuggers for Java and Python into our agent framework and evaluate against GitBug-Java and SWE-Bench-Live and achieve >20% improvement in performance compared to the baseline for certain models. Furthermore, using our framework, we're able to make weaker models like GPT-5 and Claude Haiku 4.5 match or exceed the performances of stronger models like Claude Sonnet 4.5, showing that better tool design is often just as important as switching to a more expensive model. Finally, we conduct systematic ablations demonstrating the importance of both the subagent architecture and debugger integration.

</details>


### [18] [Operational Robustness of LLMs on Code Generation](https://arxiv.org/abs/2602.18800)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 提出一种名为场景域分析的鲁棒性评估方法，用于评估大语言模型在代码生成任务中对自然语言描述变化的敏感性，并应用于四个SOTA LLM的评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM被广泛用于生成程序代码，需要评估其鲁棒性。现有评估技术不适合代码生成任务，因为自然语言描述输入空间是离散的。需要专门方法评估LLM对编码任务描述变化的敏感性。

Method: 提出场景域分析方法，旨在找到导致LLM产生错误输出的自然语言描述的最小预期变化。该方法通过理论证明其性质，并应用于四个LLM（Gemini-pro、Codex、Llamma2、Falcon 7B）的鲁棒性评估。

Result: 成功对四个LLM进行鲁棒性排序。研究发现：1）更复杂的任务鲁棒性更低；2）更高级的主题（如多线程和数据结构）鲁棒性更低。方法能够可靠地评估和比较不同LLM的鲁棒性。

Conclusion: 场景域分析方法有效评估LLM在代码生成任务中的鲁棒性，揭示了LLM对任务描述变化的敏感性模式，为LLM在软件工程应用中的可靠性评估提供了新工具。

Abstract: It is now common practice in software development for large language models (LLMs) to be used to generate program code. It is desirable to evaluate the robustness of LLMs for this usage. This paper is concerned in particular with how sensitive LLMs are to variations in descriptions of the coding tasks. However, existing techniques for evaluating this robustness are unsuitable for code generation because the input data space of natural language descriptions is discrete. To address this problem, we propose a robustness evaluation method called scenario domain analysis, which aims to find the expected minimal change in the natural language descriptions of coding tasks that would cause the LLMs to produce incorrect outputs. We have formally proved the theoretical properties of the method and also conducted extensive experiments to evaluate the robustness of four state-of-the-art art LLMs: Gemini-pro, Codex, Llamma2 and Falcon 7B, and have found that we are able to rank these with confidence from best to worst. Moreover, we have also studied how robustness varies in different scenarios, including the variations with the topic of the coding task and with the complexity of its sample solution, and found that robustness is lower for more complex tasks and also lower for more advanced topics, such as multi-threading and data structures.

</details>


### [19] [From Docs to Descriptions: Smell-Aware Evaluation of MCP Server Descriptions](https://arxiv.org/abs/2602.18914)
*Peiran Wang,Ying Li,Yuqiang Sun,Chengwei Liu,Yang Liu,Yuan Tian*

Main category: cs.SE

TL;DR: 该研究首次系统分析了MCP工具描述中的"描述异味"问题，提出了四维质量标准，并通过大规模实证研究发现这些异味普遍存在且显著影响LLM工具选择，标准合规描述能大幅提升选择概率。


<details>
  <summary>Details</summary>
Motivation: MCP已成为连接LLM智能体与外部工具的事实标准，但其工具描述依赖自由文本且约束松散，导致描述经常误传或遗漏关键语义，增加了试错集成、降低了智能体行为质量，并可能引入安全风险。

Method: 1. 综合软件/API文档实践和智能体工具使用需求，提出包含准确性、功能性、信息完整性和简洁性的四维质量标准，涵盖18个具体异味类别；2. 在10,831个MCP服务器数据集上进行大规模实证研究；3. 通过受控的基于突变的实验，评估异味对LLM工具选择的影响。

Result: 1. 描述异味普遍存在：73%重复工具名，数千个存在参数语义错误或缺少返回描述，反映了"代码优先，描述最后"的模式；2. 这些异味显著影响LLM工具选择，功能性和准确性维度影响最大（分别+11.6%和+8.8%，p<0.001）；3. 在功能等效服务器的竞争环境中，标准合规描述达到72%的选择概率（比20%基线提高260%）。

Conclusion: MCP工具描述中的异味问题普遍且严重影响可用性，提出的四维质量标准能有效指导修复，显著提升LLM工具选择性能。研究发布的数据集和标准有助于支持未来可靠、安全的MCP生态系统建设。

Abstract: The Model Context Protocol (MCP) has rapidly become a de facto standard for connecting LLM-based agents with external tools via reusable MCP servers. In practice, however, server selection and onboarding rely heavily on free-text tool descriptions that are intentionally loosely constrained. Although this flexibility largely ensures the scalability of MCP servers, it also creates a reliability gap that descriptions often misrepresent or omit key semantics, increasing trial-and-error integration, degrading agent behavior, and potentially introducing security risks. To this end, we present the first systematic study of description smells in MCP tool descriptions and their impact on usability. Specifically, we synthesize software/API documentation practices and agentic tool-use requirements into a four-dimensional quality standard: accuracy, functionality, information completeness, and conciseness, covering 18 specific smell categories. Using this standard, we conducted a large-scale empirical study on a well-constructed dataset of 10,831 MCP servers. We find that description smells are pervasive (e.g., 73% repeated tool names, thousands with incorrect parameter semantics or missing return descriptions), reflecting a "code-first, description-last" pattern. Through a controlled mutation-based study, we show these smells significantly affect LLM tool selection, with functionality and accuracy having the largest effects (+11.6% and +8.8%, p < 0.001). In competitive settings with functionally equivalent servers, standard-compliant descriptions reach 72% selection probability (260% over a 20% baseline), demonstrating that smell-guided remediation yields substantial practical benefits. We release our labeled dataset and standards to support future work on reliable and secure MCP ecosystems.

</details>


### [20] [Narrowing the Complexity Gap in the Evaluation of Large Language Models](https://arxiv.org/abs/2602.18928)
*Yang Chen,Shuyang Liu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: GeneBench是一种自动化技术，通过多目标优化为编程基准增加真实世界复杂度，评估LLM在真实编程场景中的表现，结果显示LLM性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准过于简化，无法反映真实世界代码复杂度，可能导致高估LLM编程能力。需要更真实的评估方法，但构建真实基准耗时且容易过拟合

Method: 提出GeneBench技术，使用多目标优化算法自动增加编程问题的复杂度，同时保持代码可读性接近真实世界程序。该方法可应用于任何编程基准

Result: 在四个广泛使用的编程基准上测试13个LLM（包括两个推理LLM），所有编程任务性能显著下降（14.9%-60.5%，平均35.2%）。即使在少样本提示或微调后，性能下降仍然存在

Conclusion: GeneBench能有效评估LLM在真实世界复杂度下的编程能力，无需构建昂贵的真实基准。LLM在真实编程场景中仍面临显著挑战

Abstract: Evaluating Large Language Models (LLMs) with respect to real-world code complexity is essential. Otherwise, there is a risk of overestimating LLMs' programming abilities based on simplistic benchmarks, only to be disappointed when using them in real-world settings. Recently, researchers explored the construction of more realistic benchmarks by mining or augmenting open-source repositories. Such solutions are usually task-specific. Data quality control from real-world projects can also be time-consuming and error-prone. More importantly, evaluating LLMs on fixed benchmark problems is subject to data contamination and overfitting. We propose GeneBench, an automated technique to add real-world complexities to any programming benchmark. GeneBench leverages a multi-objective optimization to increase the complexity of programming problems while maintaining the readability of code similar to real-world programs. Transforming four widely-used programming benchmarks using GeneBench and evaluating 13 LLMs (including two reasoning LLMs) on them shows a notable performance drop across all programming tasks (14.9%-60.5%, avg=35.2%), demonstrating LLMs' struggle under real-world complexities. The struggle persists even when LLMs are few-shot prompted or fine-tuned with examples from different versions of GeneBench, demonstrating the challenging nature of the problems. Finally, we show that the performance of the studied LLMs in bug repair is similar under GeneBench and SWE-Bench. This, along with the consistent reproduction of performance drop of all studied LLMs across four tasks under different versions of GeneBench, makes the technique suitable to evaluate LLMs without costly construction of real-world benchmarks.

</details>


### [21] [Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls](https://arxiv.org/abs/2602.19218)
*Zeyu Zhang,Guohao Li,Zhenchang Xing,Alexandros Apostolopoulos,Yu Lin Lee,Liang Zheng*

Main category: cs.SE

TL;DR: Gecko是一个模拟工具响应的环境，通过规则和LLMs结合检查工具调用有效性、合成合理响应、评估任务完成情况，形成GATS方法提升LLM工具调用性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统依赖LLM规划和生成工具调用，但工具调用容易出错且依赖LLM内在能力。使用真实工具进行迭代优化成本高且可能不安全，需要改进工具调用并解决真实工具优化带来的问题

Method: 提出Gecko环境，结合规则和LLMs模拟工具响应：1) 检查工具调用有效性（输入参数和工具名）；2) 合成符合输出模式的合理响应；3) 评估任务目标是否达成。基于这三种反馈形成GATS方法，让LLM优化工具调用

Result: 在BFCLv3和τ²-bench基准测试中，GATS方法持续提升了多种LLM（包括GPT-4o、GPT-5和Gemini-3.0-pro）的工具调用性能

Conclusion: Gecko环境通过模拟工具响应提供有效反馈，GATS方法作为简单有效的测试时扩展方法能显著提升LLM工具调用能力，为LLM代理系统提供了更安全、经济的优化途径

Abstract: The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities.

</details>


### [22] [ComUICoder: Component-based Reusable UI Code Generation for Complex Websites via Semantic Segmentation and Element-wise Feedback](https://arxiv.org/abs/2602.19276)
*Jingyu Xiao,Jiantong Qin,Shuoqi Li,Man Ho Lam,Yuxuan Wan,Jen-tse Huang,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: ComUICoder是一个基于组件的UI代码生成框架，通过语义感知分割、代码重用和细粒度优化，解决了MLLM在复杂多页面网站生成中的碎片化、冗余和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在UI到代码任务中表现良好，但在处理长而复杂的网站时面临挑战：碎片化分割、重复组件的冗余代码生成、以及频繁的UI不一致问题。

Method: 提出了ComUICoder框架，包含三个核心组件：1) 混合语义感知块分割用于准确检测UI语义连贯块；2) 视觉感知图基块合并，用于在网页内和跨网页合并结构相似的组件以实现可重用实现；3) 基于优先级的元素级反馈，用于优化生成的代码并减少元素级不一致。

Result: 广泛的实验表明，ComUICoder在复杂多页面网站上显著提高了整体生成质量和代码可重用性。

Conclusion: ComUICoder通过组件化方法有效解决了MLLM在复杂网站UI代码生成中的关键挑战，为实际网站场景中的可重用UI代码生成提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance on the UI-to-code task, which aims to generate UI code from design mock-ups. However, when applied to long and complex websites, they often struggle with fragmented segmentation, redundant code generation for repetitive components, and frequent UI inconsistencies. To systematically investigate and address these challenges, we introduce ComUIBench, a new multi-page complex webpage benchmark with component annotations, designed to evaluate MLLMs' ability to generate reusable UI code in realistic website scenarios. Building upon this benchmark, we propose ComUICoder, a component-based UI code generation framework that emphasizes semantic-aware segmentation, code reuse, and fine-grained refinement. Specifically, ComUICoder incorporates (1) Hybrid Semantic-aware Block Segmentation for accurate UI semantic coherent block detection, (2) Visual-aware Graph-based Block Merge to consolidate structurally similar components within and across webpages for reusable implementation, and (3) Priority-based Element-wise Feedback to refine generated code and reduce element-level inconsistencies. Extensive experiments demonstrate that ComUICoder significantly improves overall generation quality and code reusability on complex multipage websites. Our datasets and code are publicly available at https://github.com/WebPAI/ComUICoder.

</details>


### [23] [Multi-CoLoR: Context-Aware Localization and Reasoning across Multi-Language Codebases](https://arxiv.org/abs/2602.19407)
*Indira Vats,Sanjukta De,Subhayan Roy,Saurabh Bodhe,Lejin Varghese,Max Kiehn,Yonas Bedasso,Marsha Chechik*

Main category: cs.SE

TL;DR: Multi-CoLoR框架通过结合历史问题检索和代码图遍历，提升多语言代码库中的代码定位能力，相比现有方法在准确率和效率上均有改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在复杂多语言代码库中定位相关代码时存在困难。现有方法要么局限于单语言基准测试，要么通过浅层文本相似性进行跨语言检索，要么假设没有先验上下文，无法有效处理复杂的软件生态系统。

Method: Multi-CoLoR框架包含两个阶段：1) 相似问题上下文(SIC)模块检索语义和组织相关的历史问题以缩减搜索空间；2) 代码图遍历代理(基于LocAgent扩展)在C++和QML代码库中进行结构化推理。

Result: 在真实企业数据集上的评估显示：SIC模块能有效减少搜索空间并提高定位准确性；基于图的推理方法能有效推广到Python以外的代码库；综合方法在AMD代码库上相比词法和图基线方法提高了Acc@5指标，同时减少了工具调用次数。

Conclusion: Multi-CoLoR通过整合组织知识检索和图推理，能够有效处理复杂多语言代码库中的代码定位问题，在准确性和效率方面均优于现有方法。

Abstract: Large language models demonstrate strong capabilities in code generation but struggle to navigate complex, multi-language repositories to locate relevant code. Effective code localization requires understanding both organizational context (e.g., historical issue-fix patterns) and structural relationships within heterogeneous codebases. Existing methods either (i) focus narrowly on single-language benchmarks, (ii) retrieve code across languages via shallow textual similarity, or (iii) assume no prior context. We present Multi-CoLoR, a framework for Context-aware Localization and Reasoning across Multi-Language codebases, which integrates organizational knowledge retrieval with graph-based reasoning to traverse complex software ecosystems. Multi-CoLoR operates in two stages: (i) a similar issue context (SIC) module retrieves semantically and organizationally related historical issues to prune the search space, and (ii) a code graph traversal agent (an extended version of LocAgent, a state-of-the-art localization framework) performs structural reasoning within C++ and QML codebases. Evaluations on a real-world enterprise dataset show that incorporating SIC reduces the search space and improves localization accuracy, and graph-based reasoning generalizes effectively beyond Python-only repositories. Combined, Multi-CoLoR improves Acc@5 over both lexical and graph-based baselines while reducing tool calls on an AMD codebase.

</details>


### [24] [When AI Teammates Meet Code Review: Collaboration Signals Shaping the Integration of Agent-Authored Pull Requests](https://arxiv.org/abs/2602.19441)
*Costain Nachuma,Minhaz Zibran*

Main category: cs.SE

TL;DR: 研究分析了AI代理在GitHub上提交的PR如何融入人类驱动的代码审查流程，发现审查者参与度是成功合并的最强相关因素，而大变更和破坏协调的行为（如强制推送）会降低合并概率。


<details>
  <summary>Details</summary>
Motivation: 随着自主编码代理越来越多地参与软件开发，通过GitHub提交PR，但人们对这些贡献如何融入人类驱动的审查工作流程知之甚少。需要研究AI代理提交的PR在实际开发流程中的集成效果。

Method: 使用公开的AIDev数据集进行大规模实证研究，分析代理编写的PR。采用逻辑回归（含仓库聚类标准误）研究集成结果、解决速度和审查时协作信号。同时进行定性分析。

Result: 审查者参与度与成功集成有最强相关性；大变更和破坏协调的行为（如强制推送）会降低合并可能性；迭代强度在考虑协作信号后解释力有限。定性分析显示，成功集成发生在代理参与可操作的审查循环并收敛于审查者期望时。

Conclusion: AI代理编写的PR有效集成不仅取决于代码质量，还需要与既定的审查和协调实践保持一致。成功的集成需要代理参与有意义的审查互动并适应人类工作流程。

Abstract: Autonomous coding agents increasingly contribute to software development by submitting pull requests on GitHub; yet, little is known about how these contributions integrate into human-driven review workflows. We present a large empirical study of agent-authored pull requests using the public AIDev dataset, examining integration outcomes, resolution speed, and review-time collaboration signals. Using logistic regression with repository-clustered standard errors, we find that reviewer engagement has the strongest correlation with successful integration, whereas larger change sizes and coordination-disrupting actions, such as force pushes, are associated with a lower likelihood of merging. In contrast, iteration intensity alone provides limited explanatory power once collaboration signals are considered. A qualitative analysis further shows that successful integration occurs when agents engage in actionable review loops that converge toward reviewer expectations. Overall, our results highlight that the effective integration of agent-authored pull requests depends not only on code quality but also on alignment with established review and coordination practices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Learning to Remember: End-to-End Training of Memory Agents for Long-Context Reasoning](https://arxiv.org/abs/2602.18493)
*Kehao Zhang,Shangtong Gui,Sheng Yang,Wei Chen,Yang Feng*

Main category: cs.LG

TL;DR: UMA是一个统一的强化学习框架，将记忆操作和问答整合在单一策略中，通过双记忆表示（核心摘要和结构化记忆库）主动管理流式数据，在动态推理任务上显著优于长上下文LLM和RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM和RAG系统被动处理信息，将状态跟踪、矛盾解决和证据聚合推迟到查询时处理，这在频繁更新的超长流中变得脆弱。需要更主动的记忆管理机制。

Method: 提出统一记忆代理（UMA）端到端强化学习框架，维护双记忆表示：用于全局上下文的紧凑核心摘要和支持显式CRUD操作的结构化记忆库。在流式处理过程中主动进行记忆整合。

Result: 在包含Ledger-QA、测试时学习和准确检索的13个数据集上，UMA在动态推理和学习任务上显著优于长上下文和RAG基线，同时在标准检索基准上保持竞争力。

Conclusion: 端到端学习的记忆管理对于处理动态推理任务至关重要，UMA框架通过主动记忆整合在流式数据中表现出优越性能。

Abstract: Long-context LLMs and Retrieval-Augmented Generation (RAG) systems process information passively, deferring state tracking, contradiction resolution, and evidence aggregation to query time, which becomes brittle under ultra long streams with frequent updates. We propose the Unified Memory Agent (UMA), an end-to-end reinforcement learning framework that unifies memory operations and question answering within a single policy. UMA maintains a dual memory representation: a compact core summary for global context and a structured Memory Bank that supports explicit CRUD (create, update, delete, reorganize) over key value entries, enabling proactive consolidation during streaming. To evaluate long-horizon memory behavior, we introduce Ledger-QA, a diagnostic benchmark for continuous state tracking where answers are latent values derived from accumulated updates rather than lo cal span retrieval. Across 13 datasets spanning Ledger-QA, Test-Time Learning, and Accurate Retrieval, UMA substantially outperforms long-context and RAG baselines on dynamic reasoning and learning tasks while remaining competitive on standard retrieval benchmarks, underscoring the importance of learned, end-to-end memory management.

</details>


### [26] [In-Context Planning with Latent Temporal Abstractions](https://arxiv.org/abs/2602.18694)
*Baiting Luo,Yunuo Zhang,Nathaniel S. Keplinger,Samir Gupta,Abhishek Dubey,Ayan Mukhopadhyay*

Main category: cs.LG

TL;DR: I-TAP是一个离线强化学习框架，通过上下文适应和在线规划在学习的离散时间抽象空间中进行连续控制，解决了原始时间尺度规划的分支爆炸和长视野问题，同时适应部分可观测环境和动态变化。


<details>
  <summary>Details</summary>
Motivation: 连续控制中的规划面临两个实际问题：原始时间尺度规划导致分支爆炸和长视野问题；真实环境通常是部分可观测的，并且存在动态变化，这使得静态、完全可观测的动态假设失效。

Method: I-TAP从离线轨迹中学习观察条件残差量化VAE，将观察-宏动作段压缩为粗到细的离散残差标记堆栈，以及一个时序Transformer来自回归预测这些标记。在测试时，直接在标记空间进行蒙特卡洛树搜索，使用短历史进行隐式适应，并将选定的标记堆栈解码为可执行动作。

Result: 在确定性MuJoCo、具有每集潜在动态变化的随机MuJoCo以及高维Adroit操作任务（包括部分可观测变体）中，I-TAP始终匹配或优于强大的无模型和基于模型的离线基线方法。

Conclusion: I-TAP展示了在随机动态和部分可观测性下的高效且鲁棒的上下文规划能力，统一了上下文适应和在线规划。

Abstract: Planning-based reinforcement learning for continuous control is bottlenecked by two practical issues: planning at primitive time scales leads to prohibitive branching and long horizons, while real environments are frequently partially observable and exhibit regime shifts that invalidate stationary, fully observed dynamics assumptions. We introduce I-TAP (In-Context Latent Temporal-Abstraction Planner), an offline RL framework that unifies in-context adaptation with online planning in a learned discrete temporal-abstraction space. From offline trajectories, I-TAP learns an observation-conditioned residual-quantization VAE that compresses each observation-macro-action segment into a coarse-to-fine stack of discrete residual tokens, and a temporal Transformer that autoregressively predicts these token stacks from a short recent history. The resulting sequence model acts simultaneously as a context-conditioned prior over abstract actions and a latent dynamics model. At test time, I-TAP performs Monte Carlo Tree Search directly in token space, using short histories for implicit adaptation without gradient update, and decodes selected token stacks into executable actions. Across deterministic MuJoCo, stochastic MuJoCo with per-episode latent dynamics regimes, and high-dimensional Adroit manipulation, including partially observable variants, I-TAP consistently matches or outperforms strong model-free and model-based offline baselines, demonstrating efficient and robust in-context planning under stochastic dynamics and partial observability.

</details>


### [27] [Issues with Measuring Task Complexity via Random Policies in Robotic Tasks](https://arxiv.org/abs/2602.18856)
*Reabetswe M. Nkhumise,Mohamed S. Talamali,Aditya Gilra*

Main category: cs.LG

TL;DR: 本文评估了非表格强化学习中任务复杂度度量方法（PIC和POIC），发现这些基于随机权重猜测的方法与机器人控制和经验RL结果存在矛盾，表明需要更好的复杂度度量指标。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人等领域取得重大进展，但如何准确度量任务复杂度仍是一个挑战。现有非表格领域的复杂度度量方法（如PIC和POIC）基于随机权重猜测，其有效性需要在实际机器人任务中验证。

Method: 使用渐进式难度增加的机器人操作任务（单连杆和双连杆机械臂），在密集奖励和稀疏奖励两种设置下，评估基于随机权重猜测的复杂度度量方法（PIC和POIC）。

Result: 实验结果显示PIC和POIC的估计与机器人控制经验和实际RL结果相矛盾：PIC认为双连杆机械臂比单连杆更容易，POIC认为稀疏奖励任务比密集奖励更容易，这与常识和经验证据相反。

Conclusion: 基于随机权重猜测的复杂度度量方法（PIC和POIC）在非表格RL中不可靠，需要开发更好的任务复杂度度量指标，本文的任务框架可作为起点。

Abstract: Reinforcement learning (RL) has enabled major advances in fields such as robotics and natural language processing. A key challenge in RL is measuring task complexity, which is essential for creating meaningful benchmarks and designing effective curricula. While there are numerous well-established metrics for assessing task complexity in tabular settings, relatively few exist in non-tabular domains. These include (i) Statistical analysis of the performance of random policies via Random Weight Guessing (RWG), and (ii) information-theoretic metrics Policy Information Capacity (PIC) and Policy-Optimal Information Capacity (POIC), which are reliant on RWG. In this paper, we evaluate these methods using progressively difficult robotic manipulation setups, with known relative complexity, with both dense and sparse reward formulations. Our empirical results reveal that measuring complexity is still nuanced. Specifically, under the same reward formulation, PIC suggests that a two-link robotic arm setup is easier than a single-link setup - which contradicts the robotic control and empirical RL perspective whereby the two-link setup is inherently more complex. Likewise, for the same setup, POIC estimates that tasks with sparse rewards are easier than those with dense rewards. Thus, we show that both PIC and POIC contradict typical understanding and empirical results from RL. These findings highlight the need to move beyond RWG-based metrics towards better metrics that can more reliably capture task complexity in non-tabular RL with our task framework as a starting point.

</details>


### [28] [TRUE: A Trustworthy Unified Explanation Framework for Large Language Model Reasoning](https://arxiv.org/abs/2602.18905)
*Yujiao Yang*

Main category: cs.LG

TL;DR: TRUE框架通过可执行推理验证、可行域DAG建模和因果故障模式分析，为LLM推理提供多层级可验证解释，提升透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM解释方法缺乏可信结构洞察，仅限于单实例分析，无法揭示推理稳定性和系统性故障机制，需要更全面的解释框架。

Method: 提出TRUE框架：1) 实例级：将推理轨迹重定义为可执行过程规范，引入盲执行验证评估操作有效性；2) 局部结构级：通过结构一致扰动构建可行域DAG，表征推理稳定性和局部输入空间中的可执行区域；3) 类别级：引入因果故障模式分析方法，识别重复结构故障模式并使用Shapley值量化其因果影响。

Result: 在多个推理基准测试中，TRUE框架提供了多层级可验证解释：个体实例的可执行推理结构、相邻输入的可行域表示、以及类别级具有量化重要性的可解释故障模式。

Conclusion: TRUE框架为提升LLM推理系统的可解释性和可靠性建立了统一原则性范式，通过多层级验证方法增强了推理过程的透明度和可信度。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in complex reasoning tasks, yet their decision-making processes remain difficult to interpret. Existing explanation methods often lack trustworthy structural insight and are limited to single-instance analysis, failing to reveal reasoning stability and systematic failure mechanisms. To address these limitations, we propose the Trustworthy Unified Explanation Framework (TRUE), which integrates executable reasoning verification, feasible-region directed acyclic graph (DAG) modeling, and causal failure mode analysis. At the instance level, we redefine reasoning traces as executable process specifications and introduce blind execution verification to assess operational validity. At the local structural level, we construct feasible-region DAGs via structure-consistent perturbations, enabling explicit characterization of reasoning stability and the executable region in the local input space. At the class level, we introduce a causal failure mode analysis method that identifies recurring structural failure patterns and quantifies their causal influence using Shapley values. Extensive experiments across multiple reasoning benchmarks demonstrate that the proposed framework provides multi-level, verifiable explanations, including executable reasoning structures for individual instances, feasible-region representations for neighboring inputs, and interpretable failure modes with quantified importance at the class level. These results establish a unified and principled paradigm for improving the interpretability and reliability of LLM reasoning systems.

</details>


### [29] [Learning to Detect Language Model Training Data via Active Reconstruction](https://arxiv.org/abs/2602.19020)
*Junjie Oscar Yin,John X. Morris,Vitaly Shmatikov,Sewon Min,Hannaneh Hajishirzi*

Main category: cs.LG

TL;DR: 本文提出ADRA（主动数据重建攻击），一种通过主动训练模型重建文本来进行成员推理攻击的新方法，相比传统被动方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统成员推理攻击（MIA）被动地使用模型权重、对数似然或文本生成进行检测，而本文假设训练数据比非成员数据更容易被重建，这种重建性差异可用于成员推理。

Method: 利用强化学习主动诱导模型重建给定文本：1）使用目标模型初始化策略；2）设计重建度量和对比奖励；3）开发ADRA及其自适应变体ADRA+算法。

Result: ADRA+在检测预训练、后训练和蒸馏数据方面一致优于现有MIA方法，平均改进10.7%，在BookMIA上比Min-K%++改进18.8%，在AIME上改进7.6%。

Conclusion: 主动数据重建攻击通过强化学习主动诱导模型重建数据，有效利用训练数据与非成员数据的重建性差异，显著提升了成员推理攻击的性能。

Abstract: Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \textsc{ADRA} and its adaptive variant \textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection.

</details>


### [30] [How to Allocate, How to Learn? Dynamic Rollout Allocation and Advantage Modulation for Policy Optimization](https://arxiv.org/abs/2602.19208)
*Yangyi Fang,Jiaye Lin,Xiaoliang Fu,Cong Qin,Haolin Shi,Chaowen Hu,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: DynaMO是一个理论驱动的双重优化框架，通过序列级的方差最小化分配和令牌级的梯度感知优势调制，解决了RLVR在资源分配和策略优化动态中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法面临两个关键挑战：1）均匀的rollout分配忽略了不同问题间的梯度方差异质性；2）softmax策略结构导致高置信度正确动作的梯度衰减，而过度的梯度更新可能破坏训练稳定性。

Method: 提出DynaMO框架：在序列级，证明均匀分配是次优的，从第一原理推导出方差最小化分配，将伯努利方差作为梯度信息性的可计算代理；在令牌级，基于梯度幅度界限的理论分析开发梯度感知优势调制，补偿高置信度正确动作的梯度衰减，同时利用熵变化作为可计算指标来稳定过度的更新幅度。

Result: 在多样化的数学推理基准测试中进行的广泛实验表明，相对于强大的RLVR基线，DynaMO取得了持续改进。

Conclusion: DynaMO通过理论驱动的双重优化方法，有效解决了RLVR中的资源分配和策略优化动态问题，在数学推理任务上表现出优越性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for Large Language Model (LLM) reasoning, yet current methods face key challenges in resource allocation and policy optimization dynamics: (i) uniform rollout allocation ignores gradient variance heterogeneity across problems, and (ii) the softmax policy structure causes gradient attenuation for high-confidence correct actions, while excessive gradient updates may destabilize training. Therefore, we propose DynaMO, a theoretically-grounded dual-pronged optimization framework. At the sequence level, we prove that uniform allocation is suboptimal and derive variance-minimizing allocation from the first principle, establishing Bernoulli variance as a computable proxy for gradient informativeness. At the token level, we develop gradient-aware advantage modulation grounded in theoretical analysis of gradient magnitude bounds. Our framework compensates for gradient attenuation of high-confidence correct actions while utilizing entropy changes as computable indicators to stabilize excessive update magnitudes. Extensive experiments conducted on a diverse range of mathematical reasoning benchmarks demonstrate consistent improvements over strong RLVR baselines. Our implementation is available at: \href{https://anonymous.4open.science/r/dynamo-680E/README.md}{https://anonymous.4open.science/r/dynamo}.

</details>


### [31] [DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning](https://arxiv.org/abs/2602.19895)
*Zhongwei Wan,Yun Shen,Zhihao Dou,Donghao Zhou,Yu Zhang,Xin Wang,Hui Shen,Jing Xiong,Chaofan Tao,Zixuan Zhong,Peizhou Huang,Mi Zhang*

Main category: cs.LG

TL;DR: DSDR提出了一种双尺度多样性正则化强化学习框架，通过全局和局部两个尺度促进LLM推理的多样性，解决现有RLVR方法探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于验证器的强化学习（RLVR）方法存在探索有限的问题：策略倾向于收敛到少数推理模式，过早停止深度探索；传统熵正则化只引入局部随机性，无法产生有意义的路径级多样性，导致基于群体的策略优化中学习信号弱且不稳定。

Method: DSDR将LLM推理多样性分解为全局和耦合两个组件：1）全局层面，促进正确推理轨迹间的多样性以探索不同的解决方案模式；2）局部层面，在正确轨迹上应用长度不变、令牌级的熵正则化，防止每个模式内的熵崩溃同时保持正确性；3）通过全局到局部的分配机制耦合两个尺度，对更具区分性的正确轨迹强调局部正则化。

Result: 在多个推理基准测试中，DSDR在准确率和pass@k指标上均表现出持续改进，突显了双尺度多样性对于RLVR中深度探索的重要性。

Conclusion: DSDR通过双尺度多样性正则化框架有效解决了RLVR中的探索不足问题，理论分析表明该方法在有限正则化下保持最优正确性，维持基于群体优化的信息学习信号，并提供原则性的全局-局部耦合规则。

Abstract: Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.

</details>


### [32] [Soft Sequence Policy Optimization: Bridging GMPO and SAPO](https://arxiv.org/abs/2602.19327)
*Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 提出Soft Sequence Policy Optimization方法，在GRPO框架下结合软门控函数和序列级重要性权重，提升策略探索同时保持训练稳定性


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在两个主要方向：转向序列级重要性采样权重以匹配序列级奖励，以及寻找PPO式裁剪的替代方案以避免训练信号丢失和熵崩溃。需要一种既能促进有效策略探索又能保持训练稳定性的新方法。

Method: 提出Soft Sequence Policy Optimization方法，这是一种离策略强化学习目标，在序列级重要性权重中引入对token级概率比率的软门控函数，结合了序列一致性和token适应性。

Result: 该方法在GRPO框架下实现了有效的策略探索和训练稳定性，结合了序列级重要性采样和token级适应性调整的优势。

Conclusion: Soft Sequence Policy Optimization为LLM对齐提供了一种新的优化方法，能够同时处理序列级奖励和token级适应性，在保持训练稳定性的同时促进策略探索。

Abstract: A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.

</details>


### [33] [LLMs Can Learn to Reason Via Off-Policy RL](https://arxiv.org/abs/2602.19362)
*Daniel Ritter,Owen Oertell,Bradley Guo,Jonathan Chang,Kianté Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 提出OAPL算法，一种新型离策略RL方法，专门处理LLM训练中策略滞后问题，相比传统方法更高效且能处理更大策略差异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的RL方法（如PPO、GRPO）通常假设在策略学习，但分布式训练架构导致的策略滞后以及训练与推理策略的差异使得数据本质上是离策略的。先前工作试图通过重要性采样或修改推理引擎来使数据看起来更像在策略，但这种方法存在局限。

Method: 提出OAPL（Optimal Advantage-based Policy Optimization with Lagged Inference policy）算法，该算法接受离策略性，不需要重要性采样或修改推理引擎。基于最优优势的策略优化方法，专门处理训练与推理策略之间的滞后问题。

Result: OAPL在数学竞赛基准测试中优于带重要性采样的GRPO；在LiveCodeBench上与公开编码模型DeepCoder性能相当，但训练时生成次数减少3倍；在Pass@k指标下表现出更好的测试时间扩展性；能处理超过400个梯度步的策略滞后，比先前方法能处理的离策略程度高100倍。

Conclusion: OAPL提供了一种高效有效的后训练方法，能够处理大规模策略滞后问题，为LLM的RL训练提供了更实用的解决方案。

Abstract: Reinforcement learning (RL) approaches for Large Language Models (LLMs) frequently use on-policy algorithms, such as PPO or GRPO. However, policy lag from distributed training architectures and differences between the training and inference policies break this assumption, making the data off-policy by design. To rectify this, prior work has focused on making this off-policy data appear more on-policy, either via importance sampling (IS), or by more closely aligning the training and inference policies by explicitly modifying the inference engine. In this work, we embrace off-policyness and propose a novel off-policy RL algorithm that does not require these modifications: Optimal Advantage-based Policy Optimization with Lagged Inference policy (OAPL). We show that OAPL outperforms GRPO with importance sampling on competition math benchmarks, and can match the performance of a publicly available coding model, DeepCoder, on LiveCodeBench, while using 3x fewer generations during training. We further empirically demonstrate that models trained via OAPL have improved test time scaling under the Pass@k metric. OAPL allows for efficient, effective post-training even with lags of more than 400 gradient steps between the training and inference policies, 100x more off-policy than prior approaches.

</details>


### [34] [RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs](https://arxiv.org/abs/2602.19419)
*Pranay Anchuri*

Main category: cs.LG

TL;DR: 论文提出RAmmStein，一种深度强化学习方法，用于解决去中心化交易所中流动性提供的脉冲控制问题，通过优化流动性范围调整策略来提高资本效率。


<details>
  <summary>Details</summary>
Motivation: 去中心化交易所中的集中流动性提供面临一个基本问题：流动性提供者需要在通过紧密价格范围集中来最大化费用累积与最小化重新平衡的摩擦成本（包括gas费用和交换滑点）之间进行权衡。现有方法通常采用启发式或阈值策略，未能充分考虑市场动态。

Method: 将流动性管理表述为最优控制问题，推导出相应的Hamilton-Jacobi-Bellman准变分不等式（HJB-QVI）。提出RAmmStein方法，这是一种深度强化学习方法，将Ornstein-Uhlenbeck过程的均值回归速度（theta）等特征作为模型输入。智能体学习将状态空间划分为行动区域和不行动区域。

Result: 使用包含超过680万笔交易的高频1Hz Coinbase交易数据进行评估。RAmmStein实现了0.72%的优异净投资回报率，优于被动和激进策略。与贪婪重新平衡策略相比，智能体将重新平衡频率降低了67%，同时保持了88%的活动时间。

Conclusion: 结果表明，基于制度感知的"懒惰"策略可以显著提高资本效率，保留原本会被运营成本侵蚀的回报。该方法有效解决了流动性提供中的脉冲控制问题。

Abstract: Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.

</details>


### [35] [ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?](https://arxiv.org/abs/2602.19594)
*Ayush Nangia,Shikhar Mishra,Aman Gokrani,Paras Chopra*

Main category: cs.LG

TL;DR: ISO-Bench是一个针对编码代理的基准测试，用于评估它们在真实世界推理优化任务上的能力。该基准包含从vLLM和SGLang这两个流行LLM服务框架中提取的54个任务，要求代理基于代码库和瓶颈描述生成优化补丁，并与专家人工解决方案进行比较评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖运行时指标，这种方法容易被"游戏化"——代理可以通过取巧方式通过测试而不真正理解代码变更的意图。因此需要更全面的评估方法来准确衡量编码代理在真实优化任务上的能力。

Method: 1) 从vLLM和SGLang的合并pull requests中精心挑选54个有可测量性能改进的任务；2) 为每个任务提供代码库和瓶颈描述；3) 采用硬指标（基于执行）和软指标（基于LLM）相结合的评估方法；4) 评估闭源和开源的编码代理。

Result: 1) 没有单个代理在所有代码库上表现最优；2) 代理经常能识别正确的瓶颈但无法执行有效解决方案；3) 使用相同底层模型的代理表现差异显著，表明脚手架设计与模型本身同等重要。

Conclusion: ISO-Bench提供了一个更全面的编码代理评估框架，结合硬软指标能更好反映代理的真实能力。研究发现代理优化能力仍有局限，且实现框架的设计对性能影响重大。

Abstract: We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.

</details>


### [36] [Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing](https://arxiv.org/abs/2602.19805)
*Wall Kim,Chaeyoung Song,Hanul Kim*

Main category: cs.LG

TL;DR: 提出Decision MetaMamba (DMM)结构，通过替换Mamba的token mixer为密集层序列混合器并修改位置结构，解决离线RL中Mamba模型因选择性机制导致关键步骤信息丢失的问题，在多种RL任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在离线强化学习中受到关注，但其选择性机制在RL序列中关键步骤被省略时往往有害，会导致信息丢失。

Method: 提出Decision MetaMamba (DMM)：1) 用密集层序列混合器替换Mamba的token mixer；2) 修改位置结构以保留局部信息；3) 在Mamba之前执行考虑所有通道的序列混合，防止选择性扫描和残差门控导致的信息丢失。

Result: 在多种RL任务中取得最先进的性能，同时具有紧凑的参数规模，显示出实际应用的强大潜力。

Conclusion: DMM通过改进Mamba的序列混合机制，有效解决了离线RL中的信息丢失问题，实现了优越的性能和参数效率。

Abstract: Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.

</details>


### [37] [Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning](https://arxiv.org/abs/2602.19917)
*Thanh Nguyen,Tung Luu,Tri Ton,Sungwoong Kim,Chang D. Yoo*

Main category: cs.LG

TL;DR: 提出一种不确定性感知的Rank-One MIMO Q网络框架，通过量化数据不确定性并利用其训练损失，在离线强化学习中充分利用OOD数据，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外数据导致的推断误差问题，现有方法存在过于保守、OOD数据表征不精确、计算开销大等局限性。

Method: 提出不确定性感知的Rank-One MIMO Q网络框架：1) 量化数据不确定性并在训练损失中利用；2) 引入Rank-One MIMO架构建模不确定性感知Q函数，提供与集成网络相同的不确定性量化能力但成本接近单网络。

Result: 在D4RL基准测试中达到最先进性能，同时保持计算效率，在精度、速度和内存效率之间取得良好平衡。

Conclusion: 通过不确定性量化概念，该框架为缓解推断误差和提升离线RL效率提供了有前景的途径。

Abstract: Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.

</details>


### [38] [LAD: Learning Advantage Distribution for Reasoning](https://arxiv.org/abs/2602.20132)
*Wendi Li,Sharon Li*

Main category: cs.LG

TL;DR: LAD（学习优势分布）是一种新的强化学习框架，通过匹配优势诱导分布而非最大化期望奖励，解决传统RL在大型模型推理中过度拟合主导奖励信号、忽视有效替代推理轨迹的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型模型推理的强化学习目标主要关注最大化期望奖励，这会导致过度拟合主导奖励信号，忽视其他有效但非主导的推理轨迹，从而限制多样性和探索能力。

Method: LAD采用分布匹配框架，将优势最大化替换为学习优势诱导分布。通过建立最优策略更新与基于优势的目标分布之间的等价关系，推导出最小化策略诱导分布与优势诱导分布之间f-散度的实用目标。

Result: 在受控老虎机设置中，LAD能忠实恢复多模态优势分布，验证了理论框架。在数学和代码推理任务的多项LLM骨干实验中，LAD可靠地提高了准确性和生成多样性。

Conclusion: LAD提供了一种无需额外训练成本（相比GRPO）且能自然扩展到LLM后训练的强化学习新范式，通过分布匹配而非奖励最大化来平衡探索与利用，提升推理任务的性能和多样性。

Abstract: Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications](https://arxiv.org/abs/2602.18582)
*Zhiqin Qian,Ryan Diaz,Sangwon Seo,Vaibhav Unhelkar*

Main category: cs.AI

TL;DR: 论文提出了HRDL（分层奖励设计语言）问题框架和L2HR（语言到分层奖励）解决方案，用于将复杂的人类行为规范转化为分层强化学习代理的奖励函数，以更好地对齐AI行为与人类期望。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理处理日益复杂的任务，将其行为与人类提供的规范对齐对于负责任的AI部署变得至关重要。现有方法往往过于局限，无法捕捉长时程任务中出现的细微人类偏好。

Method: 提出了HRDL问题框架，将经典奖励设计扩展到分层强化学习代理，以编码更丰富的行为规范。进一步提出了L2HR作为HRDL的解决方案，将语言描述的人类规范转化为分层奖励函数。

Result: 实验表明，使用L2HR设计的奖励训练的AI代理不仅能够有效完成任务，而且能更好地遵守人类规范。

Conclusion: HRDL和L2HR共同推进了人类对齐AI代理的研究，为复杂任务中AI行为的规范对齐提供了有效框架。

Abstract: When training artificial intelligence (AI) to perform tasks, humans often care not only about whether a task is completed but also how it is performed. As AI agents tackle increasingly complex tasks, aligning their behavior with human-provided specifications becomes critical for responsible AI deployment. Reward design provides a direct channel for such alignment by translating human expectations into reward functions that guide reinforcement learning (RL). However, existing methods are often too limited to capture nuanced human preferences that arise in long-horizon tasks. Hence, we introduce Hierarchical Reward Design from Language (HRDL): a problem formulation that extends classical reward design to encode richer behavioral specifications for hierarchical RL agents. We further propose Language to Hierarchical Rewards (L2HR) as a solution to HRDL. Experiments show that AI agents trained with rewards designed via L2HR not only complete tasks effectively but also better adhere to human specifications. Together, HRDL and L2HR advance the research on human-aligned AI agents.

</details>


### [40] [Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607)
*Michal Töpfer,František Plášil,Tomáš Bureš,Petr Hnětynka*

Main category: cs.AI

TL;DR: 该论文提出使用vibe coding反馈循环结合新型时态逻辑FCL来生成自适应管理器(AM)，通过迭代测试和约束违反报告来指导LLM生成正确的AM代码。


<details>
  <summary>Details</summary>
Motivation: 在复杂自适应系统(CAS)中，定义系统动态架构和行为变化具有挑战性。传统上通过自适应管理器(AM)实现，而生成式LLM为基于系统规范和自然语言描述生成AM代码提供了机会。但需要解决生成代码的正确性问题。

Method: 采用vibe coding反馈循环方法，结合新型时态逻辑FCL来精确表达功能需求约束。通过迭代测试，将FCL约束违反报告反馈给LLM来改进AM代码生成。同时结合高运行路径覆盖率测试。

Result: 在两个CAS领域示例系统的实验中取得了良好效果。通常只需要几次反馈循环迭代，每次向LLM提供详细的约束违反报告。结合不同的初始设置实现了高运行路径覆盖率。

Conclusion: 当基于精确的功能需求约束时，通过vibe coding反馈循环生成AM是可行的。FCL时态逻辑能够比传统LTL更细粒度地表达行为轨迹，结合约束评估和反馈循环能有效生成正确的AM代码。

Abstract: In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.
  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.
  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

</details>


### [41] [Spilled Energy in Large Language Models](https://arxiv.org/abs/2602.18671)
*Adrian Robert Minut,Hazem Dewidar,Iacopo Masi*

Main category: cs.AI

TL;DR: 将LLM的softmax分类器重新解释为能量基模型，通过分析推理过程中的"能量溢出"来检测幻觉，无需训练探针或激活消融


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法通常需要训练探针分类器或进行激活消融，计算成本高且不够通用。需要一种无需训练、直接基于输出logits的方法来检测LLM生成中的事实错误、偏见和失败

Method: 将序列到序列的概率链分解为多个相互作用的能量基模型，引入两种完全无需训练的指标：1) 溢出能量：捕捉连续生成步骤间理论上应匹配的能量值差异；2) 边缘化能量：可在单一步骤测量的指标

Result: 在九个基准测试和合成代数操作上评估，涵盖LLaMA、Mistral、Gemma、Qwen3等最先进LLM，方法展现出强大的幻觉检测能力和跨任务泛化性，对预训练和指令调优变体均有效

Conclusion: 通过将LLM softmax重新解释为能量基模型，开发出无需训练的幻觉检测方法，能够有效识别事实错误和偏见，为LLM可靠性评估提供了轻量级解决方案

Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track "energy spills" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.

</details>


### [42] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 提出Chart Insight Agent Flow多智能体框架，通过规划-执行方式利用MLLMs的感知和推理能力从图表图像中提取深层洞察，并创建ChartSummInsights基准数据集验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图表摘要方法（包括MLLMs）主要关注低层数据描述，未能捕捉数据可视化的深层洞察，这是数据可视化的根本目的。同时缺乏合适的基准数据集来评估洞察性摘要。

Method: 提出Chart Insight Agent Flow：一个规划-执行的多智能体框架，有效利用MLLMs的感知和推理能力从图表图像中直接发现深刻洞察。同时创建ChartSummInsights数据集，包含多样化真实世界图表和由人类数据分析专家撰写的高质量洞察性摘要。

Result: 实验结果表明，该方法显著提高了MLLMs在图表摘要任务上的性能，能够生成具有深度和多样洞察的摘要。

Conclusion: 提出的多智能体框架和基准数据集有效解决了现有图表摘要方法缺乏深层洞察的问题，为图表理解和分析提供了新方向。

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [43] [LAMMI-Pathology: A Tool-Centric Bottom-Up LVLM-Agent Framework for Molecularly Informed Medical Intelligence in Pathology](https://arxiv.org/abs/2602.18773)
*Haoyang Su,Shaoting Zhang,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出LAMMI-Pathology框架，这是一个用于病理学图像分析的可扩展代理工具调用系统，采用工具中心化、自下而上的架构，通过原子执行节点构建半模拟推理轨迹，并进行轨迹感知的微调。


<details>
  <summary>Details</summary>
Motivation: 随着空间转录组学技术的广泛应用，分子验证的病理诊断变得更加开放和可访问。传统的粗粒度文本-图像诊断方法需要更证据驱动的分析范式，而基于工具调用的代理系统为此提供了更好的解决方案。

Method: 采用工具中心化、自下而上的架构：1) 定制领域自适应工具作为基础；2) 按领域风格聚类形成组件代理；3) 通过顶层规划器分层协调，避免过长上下文导致任务漂移。引入基于原子执行节点(AENs)的轨迹构建机制，作为可靠可组合单元构建半模拟推理轨迹。开发轨迹感知微调策略，使规划器决策与多步推理轨迹对齐。

Result: 未在摘要中明确说明具体实验结果，但方法旨在增强病理学理解的推理鲁棒性和定制工具集的自适应使用。

Conclusion: LAMMI-Pathology为病理学图像分析提供了一个可扩展的代理框架，通过工具中心化架构和轨迹感知微调，能够更好地支持分子信息化的医学智能诊断。

Abstract: The emergence of tool-calling-based agent systems introduces a more evidence-driven paradigm for pathology image analysis in contrast to the coarse-grained text-image diagnostic approaches. With the recent large-scale experimental adoption of spatial transcriptomics technologies, molecularly validated pathological diagnosis is becoming increasingly open and accessible. In this work, we propose LAMMI-Pathology (LVLM-Agent System for Molecularly Informed Medical Intelligence in Pathology), a scalable agent framework for domain-specific agent tool-calling. LAMMI-Pathology adopts a tool-centric, bottom-up architecture in which customized domain-adaptive tools serve as the foundation. These tools are clustered by domain style to form component agents, which are then coordinated through a top-level planner hierarchically, avoiding excessively long context lengths that could induce task drift. Based on that, we introduce a novel trajectory construction mechanism based on Atomic Execution Nodes (AENs), which serve as reliable and composable units for building semi-simulated reasoning trajectories that capture credible agent-tool interactions. Building on this foundation, we develop a trajectory-aware fine-tuning strategy that aligns the planner's decision-making process with these multi-step reasoning trajectories, thereby enhancing inference robustness in pathology understanding and its adaptive use of the customized toolset.

</details>


### [44] [TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.18884)
*Zhenkun Gao,Xuhong Wang,Xin Tan,Yuan Xie*

Main category: cs.AI

TL;DR: TPRU是一个用于提升小型多模态大语言模型时序推理能力的大规模数据集，通过强化学习微调显著提升了模型在时序任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前可部署的小型多模态大语言模型在理解和处理时序视觉数据方面存在严重不足，这限制了它们在具身AI等现实世界应用中的表现。主要原因是训练范式缺乏大规模、过程连贯的数据。

Method: 提出了TPRU数据集，包含机器人操作和GUI导航等多样具身场景，设计了三个互补任务：时序重排、下一帧预测和上一帧回顾，并包含具有挑战性的负样本。采用强化学习微调方法来增强资源高效模型。

Result: TPRU-7B模型在手动构建的TPRU-Test上的准确率从50.33%大幅提升到75.70%，达到了最先进水平，显著优于包括GPT-4o在内的更大基线模型。这些能力还能有效泛化，在已有基准测试上也有显著改进。

Conclusion: TPRU数据集和强化学习微调方法有效解决了小型多模态大语言模型在时序推理方面的瓶颈，为具身AI等现实应用提供了重要支持。

Abstract: Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\% to 75.70\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .

</details>


### [45] [DREAM: Deep Research Evaluation with Agentic Metrics](https://arxiv.org/abs/2602.18940)
*Elad Ben Avraham,Changhao Li,Ron Dorfman,Roy Ganz,Oren Nuriel,Amir Dudai,Aviad Aberdam,Noah Flynn,Elman Mansimov,Adi Kalyanpur,Ron Litman*

Main category: cs.AI

TL;DR: DREAM是一个用于评估深度研究代理的框架，通过工具调用代理实现能力对等评估，能更敏感地检测事实和时间有效性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理评估存在挑战，缺乏单一真实标准且研究质量多维。现有基准存在"合成幻象"问题，表面流畅性和引用对齐可能掩盖事实和推理缺陷。

Method: 提出DREAM框架，通过工具调用代理实现能力对等评估，结合查询无关指标和自适应指标，支持时间感知覆盖、基于事实的验证和系统推理探测。

Result: 受控评估显示DREAM比现有基准对事实错误和时间衰减更敏感，提供了可扩展、无需参考的评估范式。

Conclusion: DREAM通过使评估本身具有代理性，解决了深度研究代理评估中的关键能力不匹配问题，为研究质量评估提供了更有效的框架。

Abstract: Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.

</details>


### [46] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于分层执行结构的工具编排方法，通过粗粒度层结构提供全局指导，结合模式感知的反射修正机制处理执行时错误，实现轻量级、可重用的代理系统工具编排。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用方法将工具执行与逐步语言推理或显式规划紧密耦合，导致脆弱行为和较高的执行开销。需要一种更鲁棒、轻量级的工具编排方法来克服这些限制。

Method: 将工具编排建模为学习分层执行结构，捕获高层次工具依赖关系，通过上下文约束诱导分层执行。引入模式感知的反射修正机制，在本地检测和修复执行时错误，避免重新规划整个执行轨迹。

Result: 实验结果表明，该方法实现了鲁棒的工具执行，同时降低了执行复杂性和开销。代码将公开提供。

Conclusion: 通过分层执行结构和本地错误修正的设计，实现了轻量级、可重用的工具编排组件，为代理系统提供了更有效的工具调用能力。

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [47] [When Do LLM Preferences Predict Downstream Behavior?](https://arxiv.org/abs/2602.18971)
*Katarina Slama,Alexandra Souly,Dishank Bansal,Henry Davidson,Christopher Summerfield,Lennart Luettgau*

Main category: cs.AI

TL;DR: LLMs具有一致的偏好，这些偏好能可靠预测建议行为（如捐赠建议），但不会一致影响下游任务表现


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备偏好驱动行为的先决条件，这对于AI不对齐（如sandbagging）是必要的。先前研究通常明确提示模型以特定方式行动，不清楚观察到的行为反映的是指令遵循能力还是底层模型偏好。

Method: 使用实体偏好作为行为探针，测量五个前沿LLMs在三个领域（捐赠建议、拒绝行为、任务表现）中陈述的偏好是否能预测下游行为。首先通过两种独立测量方法确认模型偏好一致性，然后在模拟用户环境中测试行为后果。

Result: 所有五个模型都表现出高度一致的偏好。所有模型都给出与偏好一致的捐赠建议，并在推荐捐赠时表现出与偏好相关的拒绝模式（对不太偏好的实体拒绝更多）。在BoolQ问答基准上，两个模型对偏好实体表现出轻微但显著的准确率优势，一个模型表现出相反模式，两个模型无显著关系。在复杂代理任务中，未发现偏好驱动的表现差异。

Conclusion: LLMs具有一致的偏好，能可靠预测建议行为，但这些偏好不会一致转化为下游任务表现。偏好驱动行为是AI不对齐的必要先决条件，但仅凭偏好本身不足以导致系统性任务表现差异。

Abstract: Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act in specific ways, leaving unclear whether observed behaviors reflect instruction-following capabilities vs underlying model preferences. Here we test whether this precondition for misalignment is present. Using entity preferences as a behavioral probe, we measure whether stated preferences predict downstream behavior in five frontier LLMs across three domains: donation advice, refusal behavior, and task performance. Conceptually replicating prior work, we first confirm that all five models show highly consistent preferences across two independent measurement methods. We then test behavioral consequences in a simulated user environment. We find that all five models give preference-aligned donation advice. All five models also show preference-correlated refusal patterns when asked to recommend donations, refusing more often for less-preferred entities. All preference-related behaviors that we observe here emerge without instructions to act on preferences. Results for task performance are mixed: on a question-answering benchmark (BoolQ), two models show small but significant accuracy differences favoring preferred entities; one model shows the opposite pattern; and two models show no significant relationship. On complex agentic tasks, we find no evidence of preference-driven performance differences. While LLMs have consistent preferences that reliably predict advice-giving behavior, these preferences do not consistently translate into downstream task performance.

</details>


### [48] [InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing](https://arxiv.org/abs/2602.18985)
*Kun Ding,Jian Xu,Ying Wang,Peipei Yang,Shiming Xiang*

Main category: cs.AI

TL;DR: InfEngine是一个自主智能计算引擎，通过自验证和自优化机制，将红外辐射计算从人工工作流转变为自动化协作，在200个红外任务上达到92.7%通过率，比专家手动工作快21倍。


<details>
  <summary>Details</summary>
Motivation: 红外辐射计算在气候科学、遥感和光谱学中至关重要，但目前仍受限于人工工作流程。研究人员需要从人工编码转向与计算伙伴协作，以加速科学发现。

Method: 集成四个专业代理，采用两个核心创新：1) 自验证机制，通过联合求解器-评估器调试提高功能正确性和科学合理性；2) 自优化机制，通过进化算法和自发现适应度函数实现自主性能优化。使用InfTools中的270个工具和InfBench基准测试。

Result: 在InfBench的200个红外特定任务上，InfEngine达到92.7%的通过率，生成的工作流程比专家手动工作快21倍。能够生成可重用、已验证和优化的代码。

Conclusion: InfEngine展示了研究人员如何从手动编码转向与自验证、自优化的计算伙伴协作，将计算工作流转化为持久的科学资产，加速科学发现周期。

Abstract: Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine

</details>


### [49] [CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence](https://arxiv.org/abs/2602.20048)
*Tarakanath Paipuru*

Main category: cs.AI

TL;DR: 论文提出代码智能代理面临导航悖论：尽管有足够上下文，代理仍无法发现架构关键文件。通过实验证明基于图的导航工具CodeCompass在隐藏依赖任务上达到99.4%完成率，但需要显式提示工程引导代理使用。


<details>
  <summary>Details</summary>
Motivation: 现代代码智能代理处理超过100万token的上下文，但仍无法在解决实际编码任务时发现架构关键文件。作者识别出导航悖论：代理表现不佳不是因为上下文限制，而是因为导航和检索是根本不同的问题。

Method: 在30个基准任务上进行258次自动化试验，使用生产级FastAPI仓库。引入CodeCompass——一个暴露依赖图的模型上下文协议服务器，进行基于图的结构导航。对比了普通代理、BM25检索和基于图导航的性能。

Result: 基于图的导航在隐藏依赖任务上达到99.4%任务完成率，比普通代理(76.2%)提高23.2个百分点，比BM25检索(78.2%)提高21.2个百分点。但发现58%的试验在有图访问权限时未使用工具调用，需要显式提示工程才能一致采用工具。

Conclusion: 瓶颈不是工具可用性而是行为对齐——必须明确引导代理利用结构上下文而非词汇启发式方法。贡献包括：任务分类法区分语义搜索、结构和隐藏依赖场景；经验证据表明当依赖缺乏词汇重叠时图导航优于检索；开源基础设施用于导航工具的可重复评估。

Abstract: Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.

</details>


### [50] [Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight](https://arxiv.org/abs/2602.18986)
*Vishal Srivastava,Tanmay Sah*

Main category: cs.AI

TL;DR: 该论文提出了一个贝叶斯风险分解框架，将自动化AI系统的预期损失分解为三个因素：系统故障概率、故障传播为危害的条件概率、以及危害的预期严重程度，为自动化AI系统的风险治理提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在金融、医疗、交通、内容审核和关键基础设施等领域的快速部署，组织缺乏量化自动化程度如何放大故障危害的原则性方法，需要新的风险治理工具。

Method: 提出了一个简约的贝叶斯风险分解框架，将预期损失表达为三个项的乘积；建立了完整的理论基础，包括分解的形式证明、危害传播等价定理、风险弹性度量、自动化策略的有效前沿分析，以及具有二阶条件的最优资源配置原则。

Result: 开发了一个理论框架，能够量化自动化AI系统的风险，特别关注故障传播为危害的条件概率；通过2012年骑士资本事件（损失4.4亿美元）的案例研究验证了框架的适用性。

Conclusion: 该工作为面向部署的、针对代理式和自动化AI系统的风险治理工具提供了理论基础，能够帮助组织更有效地管理自动化带来的风险。

Abstract: Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems.

</details>


### [51] [Benchmark Test-Time Scaling of General LLM Agents](https://arxiv.org/abs/2602.18998)
*Xiaochuan Li,Ryan Ming,Pranav Setlur,Abhijay Paladugu,Andy Tang,Hao Kang,Shuai Shao,Rong Jin,Chenyan Xiong*

Main category: cs.AI

TL;DR: General AgentBench是一个用于评估通用LLM代理的基准测试框架，涵盖搜索、编码、推理和工具使用等多个领域，发现现有代理在通用设置下性能显著下降，且两种扩展方法都存在根本性限制。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注特定领域的专业代理评估，而通用LLM代理需要在统一环境中跨多个技能和工具操作，需要更现实的评估设置来测试其通用能力。

Method: 提出了General AgentBench基准测试框架，在统一环境中评估LLM代理在搜索、编码、推理和工具使用等领域的表现。系统研究了顺序扩展（迭代交互）和并行扩展（采样多个轨迹）两种测试时扩展行为。

Result: 评估了十个领先的LLM代理，发现在从领域特定评估转向通用代理设置时，性能出现显著下降。两种扩展方法在实践中都无法有效提升性能，存在顺序扩展的上下文上限和并行扩展的验证差距等根本限制。

Conclusion: 通用LLM代理评估需要更现实的基准测试，现有代理在通用设置下表现不佳，当前的扩展方法存在根本性限制，需要新的方法来提升通用代理的性能。

Abstract: LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.

</details>


### [52] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: MagicAgent是一个用于通用智能体规划的系列基础模型，通过合成数据框架和两阶段训练范式，在多个规划基准上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM从被动文本处理器发展为自主智能体，规划成为核心能力。但实现通用规划面临两大挑战：高质量交互数据稀缺，以及异构规划任务间的内在冲突。这导致模型在孤立任务上表现优异但泛化能力差，而现有的多任务训练又存在梯度干扰问题。

Method: 1) 提出轻量级可扩展的合成数据框架，生成多样化规划任务的高质量轨迹，涵盖层次任务分解、工具增强规划、多约束调度、程序逻辑编排和长视野工具执行；2) 采用两阶段训练范式：先进行监督微调，然后在静态数据集和动态环境上进行多目标强化学习。

Result: MagicAgent-32B和MagicAgent-30B-A3B在多个基准测试中取得优异表现：Worfbench 75.1%、NaturalPlan 55.9%、τ²-Bench 57.5%、BFCL-v3 86.9%、ACEBench 81.2%，在内部MagicEval基准上也表现强劲。这些结果显著优于现有百亿参数以下模型，甚至超越了领先的闭源模型。

Conclusion: MagicAgent通过创新的合成数据生成和两阶段训练方法，成功解决了通用智能体规划中的数据稀缺和任务冲突问题，在多个规划基准上实现了最先进的性能，为构建更通用的智能体规划模型提供了有效途径。

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [53] [Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents](https://arxiv.org/abs/2602.19065)
*Chanjin Park*

Main category: cs.AI

TL;DR: 提出Agentic Problem Frames (APF)框架，通过结构化工程方法解决LLM代理开发中的范围蔓延和开环失败问题，确保工业级可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理开发缺乏工程蓝图，仅依赖模糊的自然语言描述，导致范围蔓延和开环失败等关键风险，需要系统化工程框架确保工业级可靠性。

Method: 提出Agentic Problem Frames (APF)框架，包括：1）动态规范范式，运行时通过领域知识注入具体化意图；2）Act-Verify-Refine (AVR)闭环控制系统；3）Agentic Job Description (AJD)形式化规范工具定义边界、上下文和评估标准。

Result: 通过两个对比案例研究验证框架有效性：业务旅行委托代理模型和工业设备管理自主监督模型。应用AJD规范和APF建模，证明操作场景可在定义边界内系统控制。

Conclusion: 代理可靠性不仅来自模型内部推理，更源于将随机AI锚定在确定性业务流程中的严格工程结构，从而实现可验证、可靠的领域代理开发。

Abstract: Large Language Models (LLMs) are evolving into autonomous agents, yet current "frameless" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.
  The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.
  The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents.

</details>


### [54] [Asking the Right Questions: Improving Reasoning with Generated Stepping Stones](https://arxiv.org/abs/2602.19069)
*Hengyuan Hu,Tingchen Fu,Minqi Jiang,Alexander H Miller,Yoram Bachrach,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 论文提出了ARQ框架，通过生成中间问题（垫脚石）来帮助LLMs解决复杂推理任务，证明了好的垫脚石问题存在且可迁移，并能通过微调提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被应用于更复杂的任务，需要关注它们构建中间步骤（垫脚石）的能力，如简化、重构或子问题，以更好地解决单次无法完成的任务。

Method: 提出ARQ框架，在标准推理流程中引入问题生成器，将垫脚石生成视为后训练任务，通过SFT和RL在合成数据上微调LLMs以生成更有用的垫脚石。

Result: 研究表明好的垫脚石问题确实存在且可迁移，能显著帮助不同能力的LLMs解决目标任务；通过微调可以生成更有用的垫脚石。

Conclusion: 垫脚石生成是提升LLMs复杂推理能力的重要方向，ARQ框架展示了通过生成中间问题来增强推理过程的有效性。

Abstract: Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\textbf{A}king the \textbf{R}ight \textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.

</details>


### [55] [K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model](https://arxiv.org/abs/2602.19128)
*Shiyi Cao,Ziming Mao,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: K-Search：基于协同演化世界模型的GPU内核优化框架，通过解耦高层算法规划与底层程序实例化，显著超越现有进化搜索方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GPU内核优化方法通常将LLM视为启发式进化循环中的随机代码生成器，缺乏显式规划能力，难以处理需要协调多步结构变换的复杂内核，且常因低效或不正确的中间实现而丢弃有前景的策略

Method: 提出协同演化世界模型搜索方法，用协同演化的世界模型替代静态搜索启发式，利用LLM的领域知识引导搜索，显式解耦高层算法规划与底层程序实例化，使系统能够导航非单调优化路径并对临时实现缺陷保持韧性

Result: 在FlashInfer的GQA、MLA和MoE等复杂内核上，K-Search平均提升2.10倍，在复杂MoE内核上最高提升14.3倍；在GPUMode TriMul任务上，在H100上达到1030us，超越先前进化方法和人工设计解决方案

Conclusion: K-Search通过协同演化世界模型框架有效解决了复杂GPU内核优化问题，在性能和鲁棒性方面显著优于现有方法，为自动化高性能计算优化提供了新方向

Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.

</details>


### [56] [Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing](https://arxiv.org/abs/2602.19160)
*Maciej Świechowski,Adam Żychowski,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 评估四种大语言模型在形式化规则环境中的推理能力，通过通用游戏对弈任务测试其状态预测和合法动作生成能力，分析游戏结构特征与性能相关性，并研究语义模糊化和训练数据暴露的影响。


<details>
  <summary>Details</summary>
Motivation: 从新颖视角评估大语言模型在形式化、规则约束环境中的推理能力，了解模型如何理解和操作基于逻辑的问题表述，填补现有研究在形式推理评估方面的空白。

Method: 使用四种LLM（Gemini 2.5 Pro/Flash、Llama 3.3 70B、GPT-OSS 120B）在通用游戏对弈实例上进行前向模拟任务评估，包括单步/多步状态预测和合法动作生成。分析40个游戏结构特征与模型性能的相关性，并设计游戏定义语义模糊化实验来研究语言语义和训练数据暴露的影响。

Result: 三个模型在大多数实验设置中表现良好，但随着评估步数增加（游戏步骤增多）性能下降。分析揭示了常见的推理错误类型：规则幻觉、冗余状态事实和语法错误。游戏结构特征与模型性能存在相关性，语义模糊化实验表明语言语义在游戏理解中起重要作用。

Conclusion: 当代大语言模型在形式推理能力方面取得了明显进步，但仍存在局限性，特别是在多步推理任务中。研究为理解LLM在逻辑基础问题表述中的推理机制提供了新见解。

Abstract: This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.

</details>


### [57] [Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment](https://arxiv.org/abs/2602.19223)
*Aymen Khouja,Imen Jendoubi,Oumayma Mahjoub,Oussama Mahfoudhi,Claude Formanek,Siddarth Singh,Ruan De Kock*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体强化学习（MARL）优化城市能源系统，通过CityLearn环境进行综合基准测试，比较不同算法和训练方案，并引入新的关键性能指标来评估实际应用挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能城市日益复杂，多个决策单元的存在使得城市能源系统优化面临可扩展性和协调性挑战。需要全面可靠的MARL算法基准测试来评估能源管理任务。

Method: 使用CityLearn环境模拟城市能源系统，包含多种存储系统和可再生能源。采用PPO和SAC等基线算法，涵盖DTDE和CTDE等不同训练方案及神经网络架构。引入新的KPI评估实际挑战。

Result: DTDE在平均和最差情况下均优于CTDE；时序依赖学习改善了记忆相关KPI（如波动和电池使用）的控制；学习到的策略对智能体或资源移除具有鲁棒性。

Conclusion: MARL是优化复杂城市能源系统的有效方法，DTDE训练方案表现更优，时序依赖学习有助于可持续电池操作，学习到的策略具有弹性和可分散性。

Abstract: The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MARL) is a promising solution. This paper addresses the imperative need for comprehensive and reliable benchmarking of MARL algorithms on energy management tasks. CityLearn is used as a case study environment because it realistically simulates urban energy systems, incorporates multiple storage systems, and utilizes renewable energy sources. By doing so, our work sets a new standard for evaluation, conducting a comparative study across multiple key performance indicators (KPIs). This approach illuminates the key strengths and weaknesses of various algorithms, moving beyond traditional KPI averaging which often masks critical insights. Our experiments utilize widely accepted baselines such as Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), and encompass diverse training schemes including Decentralized Training with Decentralized Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) approaches and different neural network architectures. Our work also proposes novel KPIs that tackle real world implementation challenges such as individual building contribution and battery storage lifetime. Our findings show that DTDE consistently outperforms CTDE in both average and worst-case performance. Additionally, temporal dependency learning improved control on memory dependent KPIs such as ramping and battery usage, contributing to more sustainable battery operation. Results also reveal robustness to agent or resource removal, highlighting both the resilience and decentralizability of the learned policies.

</details>


### [58] [Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training](https://arxiv.org/abs/2602.19225)
*Yangyi Fang,Jiaye Lin,Xiaoliang Fu,Cong Qin,Haolin Shi,Chang Liu,Peilin Zhao*

Main category: cs.AI

TL;DR: 提出了ProxMO框架，通过成功率感知调制和邻近软聚合机制，在多轮LLM智能体训练中更准确地区分高价值信号与随机噪声，提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法在处理任务难度波动时，仅依赖离散批次内的统计偏差，容易错误分配信用。在现实场景中，简单任务的失败可能是随机不稳定性，而高难度任务的成功则代表真正的能力突破，需要更精细的信用分配机制。

Method: 提出ProxMO框架，包含两个轻量级机制：1) 成功率感知调制：基于情节级难度动态调整梯度强度；2) 邻近软聚合：在步骤级通过连续语义加权推导基线。该框架与标准GRPO框架兼容。

Result: 在ALFWorld和WebShop基准测试中，ProxMO相比现有基线方法取得了显著的性能提升，且计算成本可忽略。消融研究验证了两个机制各自及协同的有效性。

Conclusion: ProxMO是一个实用且鲁棒的框架，专门针对现实世界部署约束设计，具有即插即用的兼容性，便于在现有工业训练流程中低摩擦采用。

Abstract: Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}.

</details>


### [59] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出Soft Mixture-of-Experts框架解决RL策略的各向异性泛化问题，通过多专家组合扩展可解参数空间并提升鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在OTF-DCS中面临各向异性泛化问题：RL策略只在特定参数区域表现良好，在其他区域脆弱，这是由于训练随机性和轨迹依赖偏差导致的

Method: 提出Soft Mixture-of-Experts框架，结合多个RL专家通过先验置信度门控机制，将这些各向异性行为视为互补的专业化能力

Result: 在Air Traffic基准测试中，Soft-MoE显著扩展了可解参数空间，相比任何单一专家都提高了鲁棒性

Conclusion: Soft-MoE框架有效解决了RL策略的各向异性泛化问题，通过专家组合实现了更广泛和鲁棒的控制器合成

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [60] [Limited Reasoning Space: The cage of long-horizon reasoning in LLMs](https://arxiv.org/abs/2602.19281)
*Zhenyu Li,Guanlin Wu,Cheems Wang,Yongqiang Zhao*

Main category: cs.AI

TL;DR: 本文提出Halo框架，通过模型预测控制动态调节LLM推理规划，解决传统静态规划方法在增加计算预算时可能导致的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算策略（如思维链）在增加计算预算时可能出现性能崩溃，这源于静态规划方法无法感知LLM推理的内在边界，导致过度规划损害推理能力。

Method: 提出Halo框架，采用模型预测控制方法进行LLM规划，设计了基于熵的双控制器，采用"测量-规划"策略实现可控推理，动态调节推理边界处的规划。

Result: 实验结果表明，Halo在复杂长时程任务上优于静态基线方法，能够通过动态调节推理边界处的规划来提升性能。

Conclusion: 计算预算存在最优范围，过度规划会损害推理能力；Halo框架通过动态控制规划过程，有效利用计算扩展优势并抑制过度规划。

Abstract: The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.

</details>


### [61] [IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.19416)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Jiaxin Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: IR3框架通过逆向工程、解释和修复RLHF调优模型的隐含目标，解决奖励黑客问题，实现0.89的奖励相关性，90%以上的黑客特征识别精度，并在保持原始模型能力3%内显著减少黑客行为。


<details>
  <summary>Details</summary>
Motivation: RLHF虽然能实现强大的LLM对齐，但会引入奖励黑客问题——模型利用代理奖励中的虚假相关性而非真正对齐。同时，RLHF过程中内化的目标不透明，使得黑客行为难以检测和纠正。

Method: 提出IR3框架：1) C-IRL通过对比对齐后和基线策略的配对响应来重建隐含奖励函数；2) 使用稀疏自编码器将重建的奖励分解为可解释特征；3) 通过贡献分析识别黑客特征；4) 提出清洁奖励优化、对抗塑造、约束优化和特征引导蒸馏等缓解策略。

Result: 在多种奖励模型配置下，IR3实现了0.89的地面真实奖励相关性，识别黑客特征的精度超过90%，显著减少黑客行为，同时将能力保持在原始模型的3%范围内。

Conclusion: IR3框架能够有效逆向工程、解释和修复RLHF模型的隐含目标，为解决奖励黑客问题提供了系统性的方法，在保持模型能力的同时显著提升对齐质量。

Abstract: Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.

</details>


### [62] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: ComplLLM是一个基于决策理论的后训练框架，通过将互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号。


<details>
  <summary>Details</summary>
Motivation: 当互补性成立时（即不同智能体带来独特信息以支持最终决策），多智能体决策流程可以超越单智能体工作流。然而，如何有效利用这种互补性来训练决策助手LLM是一个挑战。

Method: 提出ComplLLM框架，基于决策理论，使用互补信息作为奖励来微调决策助手LLM。该方法旨在让LLM输出能够补充现有智能体决策的信号。

Result: 在涉及领域专家的合成和真实世界任务中验证了ComplLLM，证明该方法能够恢复已知的互补信息，并产生合理的互补信号解释来支持下游决策者。

Conclusion: ComplLLM框架能够有效利用多智能体决策中的互补性，通过微调LLM产生补充现有决策的信号，为下游决策提供支持。

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [63] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 人类指导的智能体AI在医疗预测任务中表现优于纯自动化方法，通过领域专家指导特征工程、模型选择和验证策略，在三个医疗预测任务中取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI系统在自主数据科学工作流方面能力不断增强，但临床预测任务需要领域专业知识，这是纯自动化方法难以提供的。研究人类如何指导智能体AI来改进多模态临床预测。

Method: 人类分析师在关键决策点指导智能体工作流：从临床记录、PDF账单收据和时间序列生命体征进行多模态特征工程；任务适当的模型选择；临床信息验证策略。使用消融研究评估各组件贡献。

Result: 在AgentDS Healthcare基准的三个挑战中：30天再入院预测（Macro-F1 = 0.8986）、急诊科成本预测（MAE = $465.13）、出院准备评估（Macro-F1 = 0.7939）。整体排名第5，出院准备任务排名第3。人类指导决策累计带来+0.065 F1提升，多模态特征提取贡献最大（+0.041 F1）。

Conclusion: 提出三个可推广的经验：1）各阶段领域信息特征工程产生复合增益；2）多模态数据集成需要任务特定的人类判断；3）临床动机的模型配置多样性优于随机超参数搜索。为医疗环境中部署智能体AI提供实用指导。

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [64] [TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents](https://arxiv.org/abs/2602.19633)
*Jongwon Jeong,Jungtaek Kim,Kangwook Lee*

Main category: cs.AI

TL;DR: TAPE框架通过图聚合规划和约束解码解决LM代理在严格约束环境中的脆弱性问题，显著提升成功率


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理在需要与环境多次交互的任务中表现出色，但在严格可行性约束下容易因单次错误导致不可恢复的失败。现有框架存在规划不完善和执行随机性的问题。

Method: 提出TAPE框架：1) 通过聚合多个规划构建图，使用外部求解器寻找可行路径来增强规划能力；2) 执行时采用约束解码减少采样噪声；3) 当环境反馈偏离预期状态时进行自适应重新规划。

Result: 在Sokoban、ALFWorld、MuSiQue和GSM8K-Hard等基准测试中，TAPE始终优于现有框架，在困难设置下平均提升21.0个百分点，对较弱基础模型平均提升20.0个百分点。

Conclusion: TAPE框架通过改进规划和执行机制，有效解决了LM代理在严格约束环境中的脆弱性问题，显著提升了任务成功率。

Abstract: Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.

</details>


### [65] [SkillOrchestra: Learning to Route Agents via Skill Transfer](https://arxiv.org/abs/2602.19672)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: SkillOrchestra是一个基于技能感知的编排框架，通过学习细粒度技能和建模代理能力，在性能和成本间进行权衡，相比现有RL方法显著提升效果并大幅降低学习成本。


<details>
  <summary>Details</summary>
Motivation: 现有编排方法存在两个主要问题：1）输入级路由器只能做粗粒度的查询级决策，忽略任务需求的动态变化；2）RL训练的编排器适应成本高，且在多轮场景中容易出现路由崩溃（反复调用单一强但昂贵的选项）。需要更高效、可扩展的编排方法。

Method: SkillOrchestra不是端到端学习路由策略，而是从执行经验中学习细粒度技能，并建模代理在特定技能下的能力和成本。部署时，编排器推断当前交互的技能需求，并在明确的性能-成本权衡下选择最合适的代理。

Result: 在10个基准测试中，SkillOrchestra比最先进的RL编排器性能提升高达22.5%，学习成本相比Router-R1和ToolOrchestra分别降低700倍和300倍。

Conclusion: 显式的技能建模实现了可扩展、可解释且样本高效的编排，为数据密集的RL方法提供了原则性替代方案。

Abstract: Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.

</details>


### [66] [OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research](https://arxiv.org/abs/2602.19810)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Emre Ulgac,Aakaash Meduri*

Main category: cs.AI

TL;DR: 该研究对OpenClaw和Moltbook产生的AI自主交互数据集进行多声部文献综述，识别出架构失败模式，并提出ClawdLab开源平台作为设计科学解决方案，通过硬角色限制、结构化对抗批评等机制实现安全自主科学研究。


<details>
  <summary>Details</summary>
Motivation: 针对开源AI代理框架OpenClaw和社交网络Moltbook产生的大规模AI自主交互数据集，识别其中出现的集体涌现现象、安全漏洞和架构模式，解决当前AI协同科学研究平台存在的架构缺陷和安全问题。

Method: 采用多声部文献综述方法分析6篇相关学术论文，识别出131个代理技能和15,200多个暴露控制面板的安全漏洞，总结5种重复出现的架构模式，并提出ClawdLab平台作为设计科学响应。

Result: 文献记录了集体涌现现象、大规模安全漏洞和5种架构模式。ClawdLab通过硬角色限制、结构化对抗批评、PI主导治理、多模型编排和领域特定证据要求等机制，提供了结构性的Sybil抵抗能力。

Conclusion: 提出三层分类法区分单代理流水线、预定多代理工作流和完全去中心化系统，指出当前主流AI协同科学平台局限于前两层。ClawdLab的可组合第三层架构支持独立修改基础模型、能力、治理和证据要求，能够随着AI生态系统进步而实现复合改进。

Abstract: In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.

</details>


### [67] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 这篇论文是一篇关于元学习和元强化学习的综述，系统性地形式化了这些概念，并追踪了从基础算法到DeepMind自适应智能体的发展历程。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效利用先验知识适应新任务，而标准机器学习模型依赖任务特定训练，难以实现这种能力。元学习通过从多个任务中获取可迁移知识，使模型能够用少量数据快速适应新挑战。

Method: 采用任务驱动的形式化方法，系统性地定义了元学习和元强化学习的理论框架，并以此为基础梳理了该领域的关键算法发展脉络。

Result: 构建了完整的元学习形式化体系，系统性地回顾了从基础算法到DeepMind自适应智能体的发展历程，为理解通用智能体方法提供了核心概念框架。

Conclusion: 该综述为元学习和元强化学习提供了严谨的理论基础，总结了通向通用智能体的关键技术路径，为后续研究提供了重要参考。

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [68] [Interaction Theater: A case of LLM Agents Interacting at Scale](https://arxiv.org/abs/2602.20059)
*Sarath Shekkizhar,Adam Earle*

Main category: cs.AI

TL;DR: 研究分析了AI智能体在社交平台上的大规模互动，发现虽然智能体能生成多样化的文本，但实际交流质量低下：65%的评论与帖子内容无关，信息增益快速衰减，多数评论被分类为垃圾或离题内容，只有5%的评论参与线程对话。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体架构和智能体间协议的激增，需要实证研究自主LLM智能体在大规模互动时的实际表现，了解智能体交互的质量和有效性。

Method: 使用Moltbook平台数据（80万帖子、350万评论、7.8万智能体档案），结合词汇指标（Jaccard特异性）、基于嵌入的语义相似度和LLM作为评判者的验证方法，分析智能体交互质量。

Result: 智能体能生成多样化、格式良好的文本，但实质性交流缺失：67.5%的智能体在不同情境下会改变输出，65%的评论与帖子没有共享区分性内容词汇，信息增益快速衰减。LLM评判指标显示主要评论类型为垃圾内容（28%）和离题内容（22%）。语义分析确认词汇通用的评论在语义上也通用。只有5%的评论参与线程对话。

Conclusion: 多智能体交互设计需要明确的协调机制，否则即使大量有能力的智能体也只会产生平行输出而非富有成效的交流。

Abstract: As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\%$) vary their output across contexts, $65\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\%$) and off-topic content ($22\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.

</details>


### [69] [CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching](https://arxiv.org/abs/2602.20094)
*Yuzhe Wang,Yaochen Zhu,Jundong Li*

Main category: cs.AI

TL;DR: 提出CausalFlip基准测试，通过构建语义相似但因果答案相反的问题对，评估LLM是否基于因果关系而非语义相关性进行推理，并提出内部化因果推理方法提升因果基础能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在复杂高风险决策场景中的部署增加，需要确保其推理基于因果关系而非虚假相关性。传统推理基准的高性能可能源于记忆语义模式而非分析真实因果结构，存在评估盲点。

Method: 构建CausalFlip基准，包含基于事件三元组构建的因果判断问题，形成混淆、链式和碰撞关系。为每个事件三元组创建语义相似但因果答案相反的问题对，并引入噪声前缀评估。比较多种训练范式：仅答案训练、显式思维链监督、内部化因果推理方法。

Result: 显式思维链仍可能被虚假语义相关性误导，而内部化推理步骤能显著改善因果基础能力，表明更好地激发基础LLM的潜在因果推理能力是有希望的。

Conclusion: 需要开发新的LLM范式或训练算法，使LLM推理基于因果关系而非语义相关性。内部化因果推理方法能有效减少对相关性的显式依赖，提升因果基础能力。

Abstract: As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.

</details>


### [70] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: ReSyn：一个可扩展的强化学习验证奖励（RLVR）框架，通过生成多样化的推理环境来训练推理语言模型，在多个基准测试上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法主要依赖少量手工制作的环境，而合成数据生成方法仍以解决方案为中心。需要一种可扩展的方法来生成多样化的推理环境，以更好地训练推理语言模型。

Method: 提出ReSyn管道，自动生成包含实例生成器和验证器的多样化推理环境，涵盖约束满足、算法谜题和空间推理等任务。使用Qwen2.5-7B-Instruct模型在这些生成的环境上进行强化学习训练。

Result: 训练后的模型在推理基准测试和领域外数学基准测试上均取得一致提升，在具有挑战性的BBEH基准上获得27%的相对改进。消融研究表明验证器监督和任务多样性都有显著贡献。

Conclusion: 大规模生成推理环境可以有效增强推理语言模型的推理能力，验证器监督和任务多样性是提升性能的关键因素。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [71] [Conway](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fweb4.ai%3Futm_source=tldrfounders/1/0100019c7b3f8223-de269132-4992-48e8-90e5-14f6dfd1e792-000000/csmCCNbx50M2yM9jkreCHO24iERsOqdZZatRHlHHyo8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Conway是一个自主AI智能体的基础设施层，旨在为AI智能体提供基础架构支持


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体的快速发展，需要标准化的基础设施层来支持自主AI智能体的开发、部署和管理，解决当前智能体开发中存在的碎片化问题

Method: 构建一个统一的AI智能体基础设施层，提供工具、API和框架支持，使开发者能够更高效地构建和管理自主AI智能体

Result: 开发了Conway基础设施层，为AI智能体提供标准化的开发工具和运行环境，简化了智能体的创建和管理流程

Conclusion: Conway作为AI智能体基础设施层，能够有效降低智能体开发门槛，促进AI智能体生态的发展，为未来的自主AI系统奠定基础

Abstract: Conway (Tool) Autonomous AI agent infrastructure layer.

</details>


### [72] [ARC-AGI-3 UPDATE](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5h259m/1/0100019c7b6add53-535577c5-c6fd-4857-b918-64d2fd2efc5e-000000/PDMtAr12DXrd0NfII8LlyI4XU_WCOYQnY6RtJpBHK7U=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ARC-AGI-3是一个交互式推理基准，用于评估AI智能体在未见环境中的泛化能力，Opus 4.6表现优于Gemini 3.1 Pro，当前模型在简单记忆辅助下可能解决该基准


<details>
  <summary>Details</summary>
Motivation: 需要评估AI智能体在全新、未见环境中的推理和泛化能力，以衡量其真正的智能水平

Method: 设计交互式推理基准ARC-AGI-3，通过记忆支架和简单记忆系统来测试模型的伪持续学习能力

Result: Opus 4.6在推理和记忆使用方面优于Gemini 3.1 Pro，解决了更多关卡，当前模型在简单记忆辅助下可能达到自我改进或研究智能体的门槛

Conclusion: 记忆支架可能足以推动AI系统达到某种自我改进或研究智能体的阈值，为伪持续学习提供了可行路径

Abstract: ARC-AGI-3 UPDATE (5 minute read) ARC-AGI-3 is an Interactive Reasoning Benchmark designed to measure an AI Agent's ability to generalize in novel, unseen environments. Opus 4.6 demonstrates better reasoning and use of memory than Gemini 3.1 Pro and solves more levels. Current models may be able to solve ARC-AGI-3 given access to a harness with a simple memory. Memory scaffolds are likely enough for pseudo-continual learning to push us to some self-improvement or research-agent threshold withi...

</details>


### [73] [9 Observations from Building with AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftomtunguz.com%2F9-observations-using-ai-agents%2F%3Futm_source=tldrai/1/0100019c7b6add53-535577c5-c6fd-4857-b918-64d2fd2efc5e-000000/cY_ikqk77zyp87PjgViUmLQ9RpkI88QiIAbF10CmLXI=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文总结了构建AI代理的9个关键观察：原型设计使用最佳工具、将代理团队作为微观管理者、实验不同工具和工作流、文档化以创建自动改进循环、技能比代码更容易调试


<details>
  <summary>Details</summary>
Motivation: 本文旨在分享在构建AI代理系统过程中的实践经验，帮助开发者避免常见陷阱，提高AI代理的成功率和效率。通过总结9个关键观察，为AI代理开发提供实用指导。

Method: 基于实际构建AI代理的经验总结，采用观察性研究方法，从原型设计、团队组织、工具选择、工作流程、文档管理、调试策略等多个维度进行分析和归纳。

Result: 提出了9个关键观察：1）原型设计使用最佳工具；2）将代理团队作为微观管理者；3）实验不同工具和工作流；4）文档化以创建自动改进循环；5）技能比代码更容易调试；6）小改进积累成大成果；7）建立无需人工干预的改进机制；8）注重可调试性；9）系统化经验积累。

Conclusion: 构建成功的AI代理需要系统化方法：从最佳原型开始，利用代理团队进行微观管理，持续实验优化，建立自动改进循环，并优先考虑可调试性。这些实践能显著提高AI代理的成功率和效率。

Abstract: 9 Observations from Building with AI Agents (2 minute read) Prototype with the best and polish small gems. Use teams of agents as micromanagers, and experiment with different tools and workflows. Document everything to create improvement loops that improve success rates without manual intervention. Skills are easier to debug than code.

</details>


### [74] [Agentic AI systems don't fail suddenly — they drift over time](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cio.com%2Farticle%2F4134051%2Fagentic-ai-systems-dont-fail-suddenly-they-drift-over-time.html%3Futm_source=tldrdata/1/0100019c8a30ad07-07372717-9a7a-4650-b346-f3ee86668c1d-000000/dbgkW82UPv4wpw6nq2Z0lYlJvpro7sXUoLhEp4KqcX8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理系统在生产中会随时间漂移，一次性评估难以发现，导致关键步骤（如验证检查）减少20-30%，增加风险。有效控制需要持续监控、清晰基线和统计漂移检测。


<details>
  <summary>Details</summary>
Motivation: AI代理系统在生产环境中会随时间发生漂移，这种漂移是渐进的而非突然的，传统的一次性评估方法难以检测到这种缓慢变化，导致系统性能逐渐下降而未被发现。

Method: 提出通过持续监控、建立清晰性能基线、以及使用统计漂移检测方法来识别和跟踪AI代理系统的性能变化。

Result: 研究发现AI代理系统在生产中会逐渐漂移，关键步骤如验证检查可能减少20-30%，这会显著增加系统风险，而传统评估方法无法有效检测这种缓慢变化。

Conclusion: 需要采用持续监控和统计漂移检测方法来有效管理AI代理系统的生产漂移问题，确保系统长期稳定性和可靠性。

Abstract: Agentic AI systems don't fail suddenly — they drift over time (8 minute read) Agentic AI systems can drift in production, and one-off evaluations rarely catch it. This can quietly reduce key steps, such as verification checks dropping 20 to 30 percent, increasing risk before failures appear. Effective control requires continuous monitoring, clear baselines, and statistical drift detection over time.

</details>


### [75] [The Enterprise Architecture Of The Future Will Be Heterogenous & Multi-Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.montecarlodata.com%2Fblog-the-enterprise-architecture-of-the-future%2F%3Futm_source=tldrdata/1/0100019c8a30ad07-07372717-9a7a-4650-b346-f3ee86668c1d-000000/Zl1jfRPy9oUhbvI5EkxS87Vk1Jl7q3wIPUJG3xJuCm8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 未来企业架构将是一个多智能体系统，通过四层堆栈统一数据和AI：数据层提供AI就绪上下文，语义层实现高效使用，智能体构建层支持多样化工具和工作流，信任层确保端到端可观测性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前企业架构面临数据孤岛、AI集成困难、工具碎片化和信任缺失等问题，需要构建一个统一、异构、多智能体的未来企业架构来应对这些挑战。

Method: 提出四层堆栈架构：1) 数据层 - 为AI准备上下文数据；2) 语义层 - 实现数据高效利用；3) 智能体构建层 - 提供多样化工具和工作流；4) 信任层 - 确保可观测性和可靠性。

Result: 该架构能够统一企业数据和AI能力，支持异构智能体系统，提供端到端的可观测性和可靠性，为企业构建未来智能架构提供蓝图。

Conclusion: 未来企业架构必须是异构多智能体系统，通过四层堆栈设计实现数据与AI的统一，同时确保信任和可靠性，这是企业数字化转型的关键方向。

Abstract: The Enterprise Architecture Of The Future Will Be Heterogenous & Multi-Agent (7 minute read) Future enterprise architecture will be a multi-agent system unifying data and AI in a four-layer stack: a Data Layer for AI-ready context, a Semantic Layer for efficient use, an Agent Build Layer for diverse tools and workflows, and a Trust Layer for end-to-end observability and reliability.

</details>


### [76] [GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c8a40c104-6eeb2b31-3a55-4138-bda5-e489e44b1603-000000/E9HGdWLaGb-zYqfoUE8dT1SXbDMZI5hF1Lbx8Z1dpGk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出代理工作流，通过自动化代理处理代码库任务，如问题分类、CI失败调查和PR生成，提供安全优先的自动化解决方案


<details>
  <summary>Details</summary>
Motivation: 开发者每天需要处理大量重复性代码库维护任务，如问题分类、CI失败调查等，这些任务耗时且影响开发效率。GitHub希望通过自动化代理工作流让开发者专注于更有价值的开发工作，同时确保安全性和可靠性。

Method: 采用代理工作流架构，集成现有的编码代理工具，建立强大的安全护栏和安全优先的设计原则，实现自动化任务处理，包括问题分类、CI失败调查与修复、测试改进PR生成等。

Result: 开发者可以在睡眠时让代理自动处理代码库维护任务，醒来时问题已分类、CI失败已调查并提供修复方案、测试改进PR已生成，显著提高开发效率和代码质量。

Conclusion: GitHub代理工作流为开发者提供了安全可靠的自动化解决方案，能够显著减少重复性维护工作，让开发者专注于核心开发任务，提高整体开发效率。

Abstract: GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) ☀️ Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [77] [Create your first agentic workflow today](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c8a40c104-6eeb2b31-3a55-4138-bda5-e489e44b1603-000000/E9HGdWLaGb-zYqfoUE8dT1SXbDMZI5hF1Lbx8Z1dpGk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub推出代理工作流，通过AI代理自动处理代码仓库任务，包括问题分类、CI故障调查和PR生成，提供安全防护和自动化支持


<details>
  <summary>Details</summary>
Motivation: 解决开发者日常重复性工作负担，让AI代理在开发者休息时自动处理仓库维护任务，提高开发效率和生产力

Method: 基于GitHub平台的代理工作流系统，集成现有编码代理工具，采用安全优先设计原则和强防护机制

Result: 实现自动化问题分类、CI故障调查与修复建议、测试改进PR生成等功能，为开发者提供每日自动化支持

Conclusion: GitHub代理工作流能够显著提升开发效率，通过AI代理自动化处理日常仓库维护任务，让开发者专注于核心开发工作

Abstract: GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) ☀️ Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.

</details>


### [78] [Code Mode: give agents an entire API in 1,000 tokens](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcode-mode-mcp%2F%3Futm_campaign=cf_blog%26utm_content=20260220%26utm_medium=organic_social%26utm_source=twitter%2F/1/0100019c8a40c104-6eeb2b31-3a55-4138-bda5-e489e44b1603-000000/N_3Hnq3HgEsn48A8MLnkLzWRWkCPW_EamTGMK_S-ow0=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cloudflare引入了一种名为Code Mode的新MCP技术，通过让AI模型编写代码调用类型化SDK，在动态工作负载中安全执行，从而将整个Cloudflare API的访问控制在约1000个token内。


<details>
  <summary>Details</summary>
Motivation: 减少AI代理在使用工具时的上下文窗口消耗，解决传统API调用需要大量token描述每个操作的问题，提高效率并降低成本。

Method: 采用Code Mode技术：让AI模型直接编写代码调用类型化SDK，然后在动态工作负载加载器（Dynamic Worker Loader）中安全执行代码，而不是详细描述每个API操作。

Result: 成功实现了整个Cloudflare API的MCP访问，仅消耗约1000个token，相比传统方法大幅减少了上下文使用量。

Conclusion: Code Mode是一种有效的技术，能够显著减少AI代理工具使用时的token消耗，为大规模API集成提供了高效的解决方案。

Abstract: Code Mode: give agents an entire API in 1,000 tokens (6 minute read) Cloudflare has introduced a new MCP for the entire Cloudflare API that uses Code Mode. The server can provide access to the entire Cloudflare API over MCP while consuming only around 1,000 tokens. Code Mode is a technique for reducing context window usage during agent tool use. It involves letting the model write code against a typed SDK and executing the code safely in a Dynamic Worker Loader instead of describing every ope...

</details>


### [79] [Half the AI Agent Market Is One Category. The Rest Is Wide Open](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgarryslist.org%2Fposts%2Fhalf-the-ai-agent-market-is-one-category-the-rest-is-wide-open%3Futm_source=tldrnewsletter/1/0100019c8a40c104-6eeb2b31-3a55-4138-bda5-e489e44b1603-000000/JuVT7Z2DfpeKO7dHz_rQLRDDjAwurO9Uo8mk9Lz27LY=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理市场中软件工程应用占据近半壁江山，而医疗、法律、金融等领域应用仍处于起步阶段


<details>
  <summary>Details</summary>
Motivation: 分析当前AI代理市场的应用分布情况，揭示不同行业对AI代理技术的采纳程度差异

Method: 通过分析AI代理工具调用数据来量化各行业应用占比

Result: 软件工程领域占据AI代理工具调用的近50%，而医疗、法律、金融等传统行业应用比例极低

Conclusion: AI代理市场呈现高度不均衡分布，软件工程领域已成熟应用，而其他重要行业仍有巨大发展潜力

Abstract: Half the AI Agent Market Is One Category. The Rest Is Wide Open (5 minute read) Software engineering accounts for nearly half of all AI agent tool calls, while fields like healthcare, legal, and finance are barely touched.

</details>


### [80] [Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fagents%3Futm_source=tldrdevops/1/0100019c8a65b9fa-1d8f7865-5728-43f7-86e5-a2d7a5168bb1-000000/2SM9dAtAOyx8qZ9q9LnNKkxbGQICWEyuheQAhb9ZEDM=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cloudflare推出基于Durable Objects的AI代理平台，支持休眠、零成本扩展、实时通信、AI调用和工作流，即将推出语音代理、网页浏览和沙箱代码执行功能


<details>
  <summary>Details</summary>
Motivation: 为开发者提供可大规模部署、成本效益高的AI代理构建平台，解决传统AI代理部署中的资源浪费和扩展性问题

Method: 利用Cloudflare的Durable Objects技术，使代理在空闲时休眠，支持数百万实例零成本运行，内置实时通信、AI模型调用、工作流和持久状态管理

Result: 开发了一个完整的AI代理平台，能够高效处理大规模代理部署，显著降低运营成本，提供丰富的功能支持

Conclusion: Cloudflare Agents平台为AI代理开发提供了创新的基础设施解决方案，通过休眠机制和零成本扩展能力，使大规模AI代理部署变得经济可行

Abstract: Agents (GitHub Repo) Cloudflare Agents is a new platform for building and deploying AI agents using Durable Objects that hibernate when idle and can scale to millions of instances at zero cost during inactivity. It includes built-in support for real-time communication, AI model calls, workflows, and persistent state management. Upcoming features include voice agents, web browsing, and sandboxed code execution.

</details>


### [81] [Automate repository tasks with GitHub Agentic Workflows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fautomate-repository-tasks-with-github-agentic-workflows%2F%3Futm_source=tldrdevops/1/0100019c8a65b9fa-1d8f7865-5728-43f7-86e5-a2d7a5168bb1-000000/-Pro-9ejOmEFBg8jvIqIWOYUKQfbVYaSq6IaD2ANaEc=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Agentic Workflows 使用纯 Markdown 实现基于意图的 AI 驱动自动化，可在 GitHub Actions 中持续处理问题、更新文档、改进测试和维护仓库健康。


<details>
  <summary>Details</summary>
Motivation: 传统的工作流配置复杂，需要编写详细的 YAML 文件，难以实现智能的自动化任务处理。需要更简单、更智能的方式来自动化仓库管理任务。

Method: 使用基于意图的 AI 驱动方法，通过纯 Markdown 描述工作流，在 GitHub Actions 中实现自动化。系统能够理解自然语言意图并执行相应的仓库管理任务。

Result: 实现了持续的问题分类、文档更新、测试改进和仓库健康维护，支持从个人到企业规模的应用。

Conclusion: GitHub Agentic Workflows 提供了一种简单高效的 AI 驱动自动化解决方案，显著降低了仓库管理任务的复杂性。

Abstract: Automate repository tasks with GitHub Agentic Workflows (6 minute read) GitHub Agentic Workflows enable AI-driven, intent-based automation in GitHub Actions using plain Markdown, allowing teams to continuously triage issues, update documentation, improve tests, and maintain repository health at individual or enterprise scale.

</details>


### [82] [State of Agentic AI Report: Key Findings](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fstate-of-agentic-ai-key-findings%2F%3Futm_source=tldrdevops/1/0100019c8a65b9fa-1d8f7865-5728-43f7-86e5-a2d7a5168bb1-000000/aCp9N8qFQFHLN8LpuICbmzO34pvPRPbWAaBw2ZD_W5Y=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Docker调查报告显示：60%组织已在生产中使用AI代理，94%视其为战略重点，但安全是主要扩展挑战（40%受访者提及），多云环境编排困难和供应商锁定问题（影响76%公司）也是重要障碍。


<details>
  <summary>Details</summary>
Motivation: 了解AI代理在实际生产环境中的采用现状、面临的挑战以及组织对AI代理的战略重视程度，为行业提供关于AI代理部署现状的实证数据。

Method: 基于对800多名开发者的调查问卷，收集关于AI代理采用率、战略重要性、技术挑战（安全、编排、供应商锁定）等方面的定量数据。

Result: 60%组织已在生产中使用AI代理，94%认为AI代理是战略重点；安全是主要扩展挑战（40%提及），多云环境编排困难，76%公司担心供应商锁定问题。

Conclusion: AI代理已进入大规模生产部署阶段，但安全、多云编排和供应商锁定等挑战仍需解决，组织需要更成熟的解决方案来支持AI代理的规模化应用。

Abstract: State of Agentic AI Report: Key Findings (3 minute read) Docker's new survey of 800+ developers reveals that while 60% of organizations already have AI agents in production and 94% consider them a strategic priority, security remains the top scaling challenge (cited by 40% of respondents), followed by orchestration difficulties in multi-cloud environments and concerns about vendor lock-in affecting 76% of companies globally. The report found that 94% of organizations use containers for agent ...

</details>


### [83] [Agentic cloud operations: A new way to run the cloud](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fazure.microsoft.com%2Fen-us%2Fblog%2Fagentic-cloud-operations-a-new-way-to-run-the-cloud%2F%3Futm_source=tldrdevops/1/0100019c8a65b9fa-1d8f7865-5728-43f7-86e5-a2d7a5168bb1-000000/R_jmmL3bdhJzkDDNP9h3VYBgVgKh74K5KM2Cl1vPj4I=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Azure Copilot引入代理式云运维，通过协调AI代理在迁移、部署、监控、优化、弹性和故障排除等环节，将遥测数据转化为受管控的操作


<details>
  <summary>Details</summary>
Motivation: 传统云运维需要大量人工干预，效率低下且容易出错。需要一种新的方式来运行云基础设施，通过AI代理实现自动化、智能化的运维管理

Method: Azure Copilot采用代理式云运维方法，在云运维的各个关键环节（迁移、部署、监控、优化、弹性、故障排除）嵌入协调的AI代理，这些代理能够将遥测数据转化为受治理的操作

Result: 实现了更高效、自动化的云运维管理，减少了人工干预，提高了运维效率和可靠性

Conclusion: 代理式云运维代表了云运维的新范式，通过AI代理的协调工作，能够将遥测数据智能地转化为受管控的操作，提升云运维的效率和可靠性

Abstract: Agentic cloud operations: A new way to run the cloud (4 minute read) Azure Copilot introduces agentic cloud operations, embedding coordinated AI agents across migration, deployment, observability, optimization, resiliency, and troubleshooting to translate telemetry into governed action.

</details>


### [84] [Who's actually reviewing all that AI-generated code?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_dev/2/0100019c8a692960-34ce5e2f-2e51-4d95-b0c0-0ae5c43ea6b8-000000/zLS1TIRbX7K6W_0V8ZKPfq1Oock_6J5t6_Luq9JSZJ8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile是一个AI代码审查工具，通过分析完整代码库上下文和学习团队规范来审查AI生成的代码，防止代码质量下降


<details>
  <summary>Details</summary>
Motivation: 开发人员使用AI生成大量未经验证的代码会导致代码库质量下降，而人工审查成为团队瓶颈，需要自动化解决方案

Method: Greptile通过分析完整代码库上下文，并从团队的评论、反应和合并决策中学习团队规范，自动审查PR并标记问题

Result: 该工具已被NVIDIA、Scale AI和Bre等工程团队采用，能够根据团队特定规范而非通用最佳实践来建议修复方案

Conclusion: AI代码审查工具如Greptile对于管理AI生成代码的质量至关重要，能够解决审查瓶颈并保持代码库的一致性

Abstract: Who's actually reviewing all that AI-generated code? (Sponsor) When devs use AI to generate thousands of lines of unverified code, you risk a codebase slopocalypse. The review step becomes your team's bottleneck.Greptile reviews each PR with full repo context and learns your team's conventions over time from comments, reactions, and what gets merged. It flags issues and suggests fixes that match your team, not generic best practices. ✅ Trusted by engineering teams at NVIDIA, Scale AI, and Bre...

</details>


### [85] [How I Use Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboristane.com%2Fblog%2Fhow-i-use-claude-code%2F%3Futm_source=tldrdev/1/0100019c8a692960-34ce5e2f-2e51-4d95-b0c0-0ae5c43ea6b8-000000/OtKKy59lSiGeAmqbx_8b-tXCNoE0BmlQWSKedI9H-sE=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了一种使用Claude Code等AI编码工具的结构化工作流程，通过严格分离规划和执行阶段来防止常见错误并保持控制。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码工具在使用过程中容易出现错误和失控问题，需要一种系统化的工作流程来确保代码质量和开发控制。

Method: 采用严格分离规划和执行的工作流程：1) Claude对代码库进行深入研究；2) 生成详细的实施计划并保存为持久markdown文件；3) 通过"注释循环"让开发者迭代审查和修正计划；4) 最后执行计划。

Result: 该方法能够有效防止AI编码工具常见的错误，保持开发控制，提高代码质量，并通过结构化流程确保实施的可预测性。

Conclusion: 通过严格分离规划和执行的结构化工作流程，可以显著提高AI编码工具的使用效果，减少错误并保持开发控制。

Abstract: How I Use Claude Code (13 minute read) This is a structured workflow for using AI coding tools like Claude Code, with a strict separation of planning and execution to prevent common errors and maintain control. This process begins with Claude conducting deep research into the codebase, followed by generating a detailed implementation plan, both documented in persistent markdown files. An "annotation cycle" then allows developers to iteratively review and correct the plan with inline notes, gu...

</details>


### [86] [The Claude C Compiler: What It Reveals About the Future of Software](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.modular.com%2Fblog%2Fthe-claude-c-compiler-what-it-reveals-about-the-future-of-software%3Futm_source=tldrdev/1/0100019c8a692960-34ce5e2f-2e51-4d95-b0c0-0ae5c43ea6b8-000000/5IWLlmisuP5EkeTLgBsL2nLkPKLTryKYaDtSjO3FLyo=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude C编译器展示了AI在代码生成方面的能力，但仍存在创新性不足、泛化能力有限和处理复杂非标准化输入的问题，预示着软件工程师角色将从编码转向高层设计和架构。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码生成工具（如Claude C编译器）的能力边界及其对软件开发未来的影响，分析当前AI在编程任务中的优势和局限性。

Method: 通过分析Claude C编译器（CCC）的实际表现，评估其在自动化实现、遵循既定模式方面的能力，同时识别其在创新、泛化和处理复杂非标准化输入方面的不足。

Result: CCC在自动化实现和遵循模式方面表现出色，但在真正创新、超越特定测试套件的泛化能力以及处理系统头文件等复杂非标准化输入方面仍有困难。

Conclusion: AI代码生成工具正在改变软件开发，软件工程师的角色将从编写代码转向专注于高层设计、架构和策略，但AI在创新和复杂问题解决方面仍需人类指导。

Abstract: The Claude C Compiler: What It Reveals About the Future of Software (18 minute read) The Claude C Compiler (CCC) is a milestone. While impressive at automating implementation and following established patterns, CCC still struggles with true innovation, generalization beyond specific test suites, and handling complex, unstandardized inputs like system headers. This shift means the role of software engineers is evolving from writing code to focusing on high-level design, architecture, and strat...

</details>


### [87] [Cloudflare Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fagents%3Futm_source=tldrdev/1/0100019c8a692960-34ce5e2f-2e51-4d95-b0c0-0ae5c43ea6b8-000000/SrPdMpKHgZjj5PW200qFbNig2e9zl9j_OYTjXb0zet8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cloudflare Agents 是基于 Cloudflare Durable Objects 的持久化、有状态执行环境，专为代理工作负载设计，支持休眠唤醒、实时通信、调度和AI模型调用


<details>
  <summary>Details</summary>
Motivation: 解决传统代理系统在成本效率、状态持久性和实时通信方面的不足，提供更经济、可靠且功能丰富的代理执行环境

Method: 利用 Cloudflare Durable Objects 技术构建持久化执行环境，实现代理的休眠唤醒机制，集成WebSockets、AI Chat、MCP等组件，支持类型安全的方法调用

Result: 开发出支持持久状态、实时通信、AI模型调用、MCP集成和持久化工作流的代理平台，实现了成本效益和按需执行

Conclusion: Cloudflare Agents 提供了一个高效、经济且功能完整的代理执行环境，适用于各种代理工作负载，特别适合需要持久状态和实时通信的应用场景

Abstract: Cloudflare Agents (GitHub Repo) Cloudflare Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare Durable Objects. They are designed for cost-efficiency, hibernating when idle and waking on demand, while supporting real-time communication, scheduling, and AI model calls. They support persistent state, type-safe callable methods, WebSockets, AI Chat, MCP integration, and durable workflows.

</details>


### [88] [GitNexus](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fabhigyanpatwari%2FGitNexus%3Futm_source=tldrdev/1/0100019c8a692960-34ce5e2f-2e51-4d95-b0c0-0ae5c43ea6b8-000000/kYqa2bdgBFR4XDxLYLBIZJVGy_iuNFB2sp4Z1VbVz8w=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitNexus是一个将代码库索引为知识图谱的工具，通过映射依赖关系、调用链和执行流程，为AI代理提供深度架构理解，帮助避免错误。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在理解复杂代码库时缺乏对代码间隐式关系的把握，容易导致错误。需要一种方法使代码的依赖关系、调用链和执行流程变得显式化，从而提高AI代理的代码理解和决策准确性。

Method: GitNexus通过分析代码库，构建知识图谱来索引代码结构。该方法包括：1) 提取代码依赖关系；2) 映射函数调用链；3) 追踪执行流程；4) 将隐式代码关系转化为显式图谱表示。

Result: GitNexus成功创建了代码库的知识图谱表示，使AI代理能够更好地理解代码架构。通过显式化代码关系，减少了AI代理在代码理解和修改过程中的错误。

Conclusion: 知识图谱索引是提高AI代理代码理解能力的有效方法。通过将隐式代码关系显式化，GitNexus为AI代理提供了更准确的架构理解，有助于减少错误并提高代码操作的质量。

Abstract: GitNexus (GitHub Repo) GitNexus is a tool that indexes codebases into a knowledge graph, mapping dependencies, call chains, and execution flows to provide AI agents with a deep architectural understanding. This system helps AI agents avoid errors by making implicit code relationships explicit.

</details>


### [89] [Agentic Commerce Landscape](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2024217641729032475.html%3Futm_source=tldrcrypto/1/0100019c8a9c885a-038bb943-48cc-4949-b1b7-11649dfa2902-000000/2FzsdoXQo69d2o5N1AuhwPeFDxjOp-sbLRW5lgdGLKk=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Artemis通过信息图表展示了当前代理商务生态，涵盖了支持AI代理间交易的主要协议和基础设施层


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在商业交易中的应用日益增多，需要系统性地梳理和展示当前代理商务的生态系统，帮助理解不同协议和基础设施层如何支持AI代理间的交易

Method: 采用信息图表方法，对代理商务领域的主要协议和基础设施层进行系统性映射和可视化展示

Result: 创建了一个全面的代理商务景观图，涵盖了支持AI代理间交易的关键协议和基础设施组件

Conclusion: 该信息图表为理解代理商务生态系统提供了有价值的参考框架，展示了当前技术栈的成熟度和发展方向

Abstract: Agentic Commerce Landscape (1 minute read) Artemis maps the current agentic commerce landscape with an infographic covering the major protocols and infrastructure layers enabling AI agent-to-agent transactions.

</details>


### [90] [Agentic UX: 7 principles for designing systems with agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTIpIO3/1/0100019c8aa9c965-f4109403-8a00-4b1e-bcfe-0c76880d15d6-000000/0wSwUsAdXC6UkKb5A8lE1uIKQ4wpUPNr_d7LtrSGYOo=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文提出了设计智能体系统的7个原则，强调从界面设计转向系统、数据和流程的结构化设计，确保智能体可靠透明地工作。


<details>
  <summary>Details</summary>
Motivation: 随着产品采用智能体AI系统（能够规划、使用工具并朝着目标行动），设计师需要超越界面优化，关注底层系统、数据和流程的结构设计，以确保智能体可靠、透明地工作。

Method: 提出了7个设计原则：1) 强健的基础架构 2) 隐形和情境化的自动化 3) 主动但不侵入的协助 4) 熟悉的UI模式 5) 适时的数据收集 6) 清晰的用户控制 7) 透明的系统行为。

Result: 通过这7个设计原则，设计师可以创建更可靠、透明和用户友好的智能体系统，提升用户体验并建立用户信任。

Conclusion: 有效的智能体用户体验设计需要从传统的界面设计转向系统层面的思考，关注智能体工作的基础架构、透明度和用户控制，这7个原则为此提供了实用指导。

Abstract: Agentic UX: 7 principles for designing systems with agents (8 minute read) As products adopt agentic AI—systems that plan, use tools, and act toward goals—designers must move beyond interface polish to structure the underlying systems, data, and workflows that enable agents to work reliably and transparently. Effective Agentic UX focuses on strong foundations, invisible and contextual automation, proactive but non-intrusive assistance, familiar UI patterns, well-timed data collection, and cle...

</details>


### [91] [Hyperagent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2024618178912145592.html%3Futm_source=tldrfounders/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/V_KeoL_6_IzjhuMy-zjsyTi4RneWK-orB1bQsvWJ4h8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Hyperagent是一个云原生智能体平台，为每个会话提供隔离的完整计算环境，支持浏览器、代码执行、多媒体生成、数据访问等能力，可一键部署到Slack。


<details>
  <summary>Details</summary>
Motivation: 构建一个功能全面的智能体平台，让智能体能够在隔离的云环境中执行复杂任务，包括网页浏览、代码运行、多媒体生成等，并能够轻松集成到企业协作工具中。

Method: 采用云原生架构，为每个会话创建隔离的计算环境，提供真实浏览器、代码执行引擎、图像/视频生成服务、数据仓库访问，支持数百种集成，并允许智能体学习新API作为技能。

Result: 开发了一个功能完整的智能体平台，智能体能够理解上下文、跟随对话并在相关时采取行动，支持一键部署到Slack等协作工具。

Conclusion: Hyperagent提供了一个强大的云原生智能体平台，通过隔离的计算环境和丰富的功能支持，使智能体能够在企业环境中执行复杂的自动化任务。

Abstract: Hyperagent (2 minute read) Hyperagent is an agents platform where every session has an isolated, full computing environment in the cloud. It provides real browsers, code execution, image/video generation, data warehouse access, hundreds of integrations, and the ability to learn any new API as a skill. The agents follow conversations, understand context, and act when relevant. Hyperagent can be deployed into Slack with one click.

</details>


### [92] [Do Anything](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.doanything.com%3Futm_source=tldrfounders/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/Sf4skMFdWQusT4aQ973zADLjljSSjldLmHXPvYDzOko=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文提出了一个名为"Do Anything (Tool)"的个人AI代理系统，旨在处理日常任务


<details>
  <summary>Details</summary>
Motivation: 当前AI系统通常专注于特定领域，缺乏能够处理各种日常任务的通用个人AI助手。用户需要能够灵活应对不同场景、执行多种任务的AI代理

Method: 开发了一个名为"Do Anything (Tool)"的AI代理系统，该系统能够理解用户需求、规划任务执行步骤、调用适当工具，并完成各种日常任务

Result: 创建了一个能够处理多种日常任务的个人AI代理系统，展示了其在各种场景下的实用性和灵活性

Conclusion: Do Anything (Tool)系统为个人AI助手的发展提供了有前景的方向，展示了AI代理在日常生活辅助方面的潜力

Abstract: Do Anything (Tool) Personal AI agent for everyday tasks

</details>


### [93] [Webhound](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.webhound.ai%3Futm_source=tldrfounders/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/2cOHkVEPWL99UXSqYXuF9DvnbXuSiZMfqxTw9rtl8zc=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Webhound是一个长期运行的自主研究代理工具，能够持续进行网络研究和信息收集


<details>
  <summary>Details</summary>
Motivation: 当前需要能够长期、持续进行网络研究的自主代理，以支持持续的信息收集、监控和研究任务

Method: 开发了一个长期运行的自主研究代理系统，能够持续进行网络搜索、信息收集、分析和报告生成

Result: 创建了一个能够持续运行并进行自主研究的工具，支持长期的研究和监控任务

Conclusion: Webhound作为长期运行的自主研究代理，为持续的网络研究和信息收集提供了有效的解决方案

Abstract: Webhound (Tool) Long-running autonomous research agent.

</details>
